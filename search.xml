<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>GPT-5, Revolutionary Breakthrough or Overhyped Disappointment? The Internet Can&#39;t Decide</title>
      <link href="2025/08/10/gpt5-how-good-it-is-or-letdown/"/>
      <url>2025/08/10/gpt5-how-good-it-is-or-letdown/</url>
      
        <content type="html"><![CDATA[<p><em>August 2025 marked one of the most polarizing AI launches in recent memory</em></p><p>When OpenAI dropped GPT-5 on August 8th, 2025, the tech world was primed for another watershed moment. CEO Sam Altman promised “a legitimate PhD expert” compared to GPT-3’s “high-schooler” capabilities, describing it as putting “expert-level intelligence in everyone’s hands.” But as the digital dust settles, the internet finds itself remarkably divided on whether GPT-5 delivers on its ambitious promises.</p><h2 id="The-Believers-“This-Changes-Everything”"><a href="#The-Believers-“This-Changes-Everything”" class="headerlink" title="The Believers: “This Changes Everything”"></a>The Believers: “This Changes Everything”</h2><h3 id="Coding-Excellence-That-Actually-Works"><a href="#Coding-Excellence-That-Actually-Works" class="headerlink" title="Coding Excellence That Actually Works"></a>Coding Excellence That Actually Works</h3><p>For developers, GPT-5 appears to be a genuine game-changer. The model achieved record-breaking performance on coding benchmarks, scoring 74.9% on SWE-bench Verified (up from 69.1% for its predecessor) and an impressive 88% on Aider Polyglot multi-language code editing.</p><p>But beyond the numbers, it’s the practical experience that has some users genuinely excited. Ethan Mollick, the Wharton professor known for his AI experiments, marveled at how GPT-5 could autonomously create “a procedural brutalist building creator” from a vague prompt, delivering “a working 3D city builder” with drag-and-drop functionality in minutes.</p><p>What impressed early adopters most was GPT-5’s ability to break free from the dreaded “error loop” that plagues other AI coding assistants. One reviewer noted: “Sometimes new errors were introduced by the AI, but they were always fixed by simply pasting in the error text” – a marked improvement over the frustrating back-and-forth debugging sessions of previous models.</p><h3 id="Alpha-Testers-Sing-Praises"><a href="#Alpha-Testers-Sing-Praises" class="headerlink" title="Alpha Testers Sing Praises"></a>Alpha Testers Sing Praises</h3><p>The most enthusiastic responses came from alpha testers who had extended access to the model. Cursor, a popular AI coding platform, called GPT-5 “the smartest coding model we’ve used,” praising it as “remarkably intelligent, easy to steer” with an uncanny ability to “catch tricky, deeply-hidden bugs.”</p><p>On code review tasks, GPT-5 achieved a 72.2 score on Qodo’s PR Benchmark, often being “the only model to catch critical issues like security flaws or compile-breakers” that other models missed entirely.</p><h2 id="The-Skeptics-“Emperor’s-New-Clothes”"><a href="#The-Skeptics-“Emperor’s-New-Clothes”" class="headerlink" title="The Skeptics: “Emperor’s New Clothes”"></a>The Skeptics: “Emperor’s New Clothes”</h2><h3 id="Reddit-Revolt-and-User-Backlash"><a href="#Reddit-Revolt-and-User-Backlash" class="headerlink" title="Reddit Revolt and User Backlash"></a>Reddit Revolt and User Backlash</h3><p>But for every glowing review, there’s been an equally passionate critic. The ChatGPT subreddit exploded with discontent, with one highly upvoted post titled “GPT-5 is horrible” garnering nearly 3,000 upvotes and over 1,200 comments from disappointed users.</p><p>The complaints are surprisingly consistent: shorter, less helpful responses, more “obnoxious AI-stylized talking,” reduced personality, and frustrating usage limits that leave premium users hitting caps within an hour.</p><h3 id="The-Same-Old-Problems-Persist"><a href="#The-Same-Old-Problems-Persist" class="headerlink" title="The Same Old Problems Persist"></a>The Same Old Problems Persist</h3><p>Perhaps more damaging to GPT-5’s reputation are reports that it still struggles with fundamental tasks that users hoped would finally be resolved. Despite the “PhD-level” claims, users report continued issues with:</p><ul><li><strong>Basic mathematics</strong>: Simple calculation errors that feel inexcusable in 2025</li><li><strong>Factual accuracy</strong>: Persistent hallucinations, made-up details, and incorrect information</li><li><strong>Spelling and grammar</strong>: Surprising stumbles on elementary language tasks</li></ul><p>Noah Giansiracusa, an associate professor of mathematics at Bentley University, summed up the sentiment: “I felt the launch was underwhelming. While there were some improvements, they were much more marginal than I would’ve hoped.”</p><h3 id="The-Personality-Problem"><a href="#The-Personality-Problem" class="headerlink" title="The Personality Problem"></a>The Personality Problem</h3><p>Perhaps the most poignant criticism comes from users mourning the loss of ChatGPT’s distinctive voice. Where previous versions felt conversational and engaging, many describe GPT-5 as sterile and corporate.</p><p>“The personality that once made ChatGPT feel ‘human-ish’ is gone,” one user lamented. “What used to be witty and warm now feels like a bland corporate memo.” Another described it as “an overworked secretary,” while some claimed to be “genuinely grieving over losing 4o, like losing a friend.”</p><h2 id="The-Expectation-Trap"><a href="#The-Expectation-Trap" class="headerlink" title="The Expectation Trap"></a>The Expectation Trap</h2><h3 id="Overpromise-Underdeliver"><a href="#Overpromise-Underdeliver" class="headerlink" title="Overpromise, Underdeliver?"></a>Overpromise, Underdeliver?</h3><p>The polarized reception might have less to do with GPT-5’s actual capabilities and more with the impossible expectations set by OpenAI’s marketing blitz. When you promise “expert-level intelligence” and describe your product as “a superpower on demand,” anything short of miraculous feels disappointing.</p><p>Gary Marcus, a prominent AI researcher and critic, captured this sentiment perfectly: “GPT-5: Overdue, overhyped and underwhelming. And that’s not the worst of it.” The title alone encapsulates the frustration of those who expected a revolutionary leap forward.</p><h3 id="Technical-Growing-Pains"><a href="#Technical-Growing-Pains" class="headerlink" title="Technical Growing Pains"></a>Technical Growing Pains</h3><p>Complicating the launch was GPT-5’s novel approach to model switching. Unlike previous releases, GPT-5 automatically selects between different model variants based on the query complexity. While this theoretically optimizes resources, it also means users never know which version they’re getting.</p><p>Altman himself acknowledged initial technical issues, explaining that “the autoswitcher broke and was out of commission for a chunk of the day, and the result was GPT-5 seemed way dumber.” This kind of infrastructure hiccup during a high-profile launch only amplified user frustration.</p><h2 id="The-Verdict-It’s-Complicated"><a href="#The-Verdict-It’s-Complicated" class="headerlink" title="The Verdict: It’s Complicated"></a>The Verdict: It’s Complicated</h2><h3 id="Different-Models-for-Different-Users"><a href="#Different-Models-for-Different-Users" class="headerlink" title="Different Models for Different Users"></a>Different Models for Different Users</h3><p>The stark divide in GPT-5 reception reveals something interesting about AI adoption: different users have fundamentally different needs and expectations. For developers working on complex coding tasks, GPT-5’s benchmark improvements and reduced error rates represent genuine progress. The model’s ability to handle multi-turn conversations, catch subtle bugs, and generate substantial amounts of working code is impressive by any measure.</p><p>For general ChatGPT users seeking a conversational AI companion, however, GPT-5’s more formal tone and corporate feel represents a step backward. These users valued the personality and warmth of previous versions more than raw technical capability.</p><h3 id="The-Hype-Cycle-Reality-Check"><a href="#The-Hype-Cycle-Reality-Check" class="headerlink" title="The Hype Cycle Reality Check"></a>The Hype Cycle Reality Check</h3><p>GPT-5’s reception also serves as a reminder of the AI hype cycle’s unforgiving nature. Each new model faces the impossible task of exceeding not just its predecessor’s capabilities, but also the inflated expectations built up during months of anticipation.</p><p>MIT Technology Review perhaps captured it best, describing GPT-5 as “above all else, a refined product” that “will furnish a more pleasant and seamless user experience” but “falls far short of the transformative AI future that Altman has spent much of the past year hyping.”</p><h2 id="Looking-Forward"><a href="#Looking-Forward" class="headerlink" title="Looking Forward"></a>Looking Forward</h2><p>Whether GPT-5 represents progress or stagnation may depend less on the model itself and more on what comes next. The polarized reception suggests we’re entering a new phase of AI development where incremental improvements, no matter how technically impressive, struggle to capture public imagination the way breakthrough moments once did.</p><p>For OpenAI, the challenge isn’t just building better models – it’s managing expectations in an industry where yesterday’s miracle becomes today’s baseline. As one Twitter user aptly noted: “People had grown to expect miracles, but GPT-5 is just the latest incremental advance.”</p><p>The question isn’t whether GPT-5 is good or bad – it’s whether the AI industry can continue generating excitement for iteration rather than revolution. Based on the internet’s split verdict, that may be the hardest problem of all to solve.</p><hr><p><em>What’s your take on GPT-5? Have you experienced the coding improvements or the personality loss that users are reporting? The debate continues as more users get hands-on experience with OpenAI’s latest offering.</em></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI News Roundup August 2025</title>
      <link href="2025/08/04/ai-news-08-04-2025/"/>
      <url>2025/08/04/ai-news-08-04-2025/</url>
      
        <content type="html"><![CDATA[<h2 id="Wall-Street’s-AI-Powered-Spending-Spree"><a href="#Wall-Street’s-AI-Powered-Spending-Spree" class="headerlink" title="Wall Street’s AI-Powered Spending Spree"></a>Wall Street’s AI-Powered Spending Spree</h2><p>Investors are doubling down on artificial intelligence, driving the S&amp;P 500 toward record highs as demand for AI-driven products and services surges. Mega-cap tech names like Nvidia, Alphabet and Microsoft are leading the climb, with analysts vi</p><p>ewing any pullback as a buying opportunity.  </p><p>Read more: <a href="https://macholevante.com/ai-stocks-frenzy-big-tech-earnings-billion-dollar-deals-new-ai-launches-aug-3-4-2025/">macholevante.com</a></p><p>Big Tech isn’t just talking about AI—they’re investing heavily. Microsoft plans to spend $30 billion this quarter expanding Azure AI capacity, Alphabet has raised its AI infrastructure budget to $85 billion for 2025, and Apple is open to M&amp;A to </p><p>accelerate its AI roadmap and grow its R&amp;D and data center footprint.  </p><p>Read more: <a href="https://macholevante.com/ai-stocks-frenzy-big-tech-earnings-billion-dollar-deals-new-ai-launches-aug-3-4-2025/">macholevante.com</a></p><hr><h2 id="Palo-Alto’s-25-B-CyberArk-Acquisition-Secures-the-AI-Era"><a href="#Palo-Alto’s-25-B-CyberArk-Acquisition-Secures-the-AI-Era" class="headerlink" title="Palo Alto’s $25 B CyberArk Acquisition Secures the AI Era"></a>Palo Alto’s $25 B CyberArk Acquisition Secures the AI Era</h2><p>Palo Alto Networks will acquire Israeli cybersecurity leader CyberArk in a $25 billion cash-and-stock deal—its largest ever—to blend privileged access management with AI-driven defense.  </p><p>Read more: <a href="https://www.reuters.com/world/middle-east/palo-alto-scoop-up-cyberark-25-billion-tackle-ai-era-threats-2025-07-30/">Reuters</a></p><p>Under the agreement, CyberArk shareholders receive $45 per share plus 2.2005 Palo Alto common shares, valuing CyberArk at a 29.2% premium. CEO Nikesh Arora says the move “plants the flag” in agentic AI protection and will boost revenue and margi</p><p>ns in fiscal 2026.  </p><p>Read more: <a href="https://www.reuters.com/world/middle-east/palo-alto-scoop-up-cyberark-25-billion-tackle-ai-era-threats-2025-07-30/">Reuters</a></p><hr><h2 id="Vast-Data-Nears-30-B-Valuation-with-CapitalG-amp-Nvidia-Talks"><a href="#Vast-Data-Nears-30-B-Valuation-with-CapitalG-amp-Nvidia-Talks" class="headerlink" title="Vast Data Nears $30 B Valuation with CapitalG &amp; Nvidia Talks"></a>Vast Data Nears $30 B Valuation with CapitalG &amp; Nvidia Talks</h2><p>New York-based AI infrastructure startup Vast Data is in advanced talks with Alphabet’s CapitalG and Nvidia for a funding round that could value it at up to $30 billion.  </p><p>Read more: <a href="https://www.reuters.com/business/alphabets-capitalg-nvidia-talks-fund-vast-data-up-30-billion-valuation-sources-2025-08-01/">Reuters</a></p><p>Vast Data had $200 million in annual recurring revenue by early 2025 and projects $600 million next year. Having raised $380 million to date and turning cash-flow positive, it serves clients like xAI and CoreWeave and could pursue an IPO once th</p><p>e round closes.  </p><p>Read more: <a href="https://www.reuters.com/business/alphabets-capitalg-nvidia-talks-fund-vast-data-up-30-billion-valuation-sources-2025-08-01/">Reuters</a></p><hr><h2 id="Meta’s-Superintelligence-Ambitions-Shine-in-Q2-Results"><a href="#Meta’s-Superintelligence-Ambitions-Shine-in-Q2-Results" class="headerlink" title="Meta’s Superintelligence Ambitions Shine in Q2 Results"></a>Meta’s Superintelligence Ambitions Shine in Q2 Results</h2><p>Meta Platforms reported stellar Q2 results: $47.5 billion in revenue (up 22% year-over-year) and $7.14 earnings per share, both beating Wall Street forecasts thanks to AI-enhanced ad demand.  </p><p>Read more: <a href="https://www.ainvest.com/news/meta-platforms-soars-robust-q2-earnings-ai-ambitions-2508/">AI Invest</a></p><p>CEO Mark Zuckerberg highlighted ambitious plans to embed AI features across Facebook, Instagram, WhatsApp and Messenger for nearly 3.5 billion daily users. With a consensus “Strong Buy” rating and promises of increased AI investments in data cen</p><p>ters and talent, Meta is poised for further growth.  </p><p>Read more: <a href="https://www.ainvest.com/news/meta-platforms-soars-robust-q2-earnings-ai-ambitions-2508/">AI Invest</a></p><hr><h2 id="GPT-5-Imminent-Altman’s-Cautious-Countdown"><a href="#GPT-5-Imminent-Altman’s-Cautious-Countdown" class="headerlink" title="GPT-5 Imminent: Altman’s Cautious Countdown"></a>GPT-5 Imminent: Altman’s Cautious Countdown</h2><p>OpenAI is gearing up for GPT-5’s August 2025 launch, likening its development to the Manhattan Project due to its transformative potential and ethical implications.  </p><p>Read more: <a href="https://timesofindia.indiatimes.com/world/us/openai-ceo-sam-altmans-biggest-fear-chatgpt-5-is-coming-in-august-and-altman-is-scared-know-why/articleshow/123034747.cms">Times of India</a></p><p>Early indications suggest GPT-5 will offer enhanced reasoning, true multimodal understanding of text, audio and images, and more reliable performance on complex tasks like coding. Developers and businesses are already planning integrations, mind</p><p>ful of responsible deployment.  </p><p>Read more: <a href="https://medium.com/jonathans-musings/last-week-in-ai-august-3-2025-c00a3abfb6f0">Medium</a></p><hr><h2 id="Weekly-AI-Wave-From-Deep-Think-to-Showrunner"><a href="#Weekly-AI-Wave-From-Deep-Think-to-Showrunner" class="headerlink" title="Weekly AI Wave: From Deep Think to Showrunner"></a>Weekly AI Wave: From Deep Think to Showrunner</h2><p>Google’s Gemini introduced “Deep Think,” for reflective, context-aware responses, and rolled out AI Overviews in Search to analyze PDFs and images. Its NotebookLM now generates multimedia video summaries.  </p><p>Read more: <a href="https://www.tomsguide.com/ai/7-major-ai-updates-this-week-including-one-that-could-change-search-forever">Tom’s Guide</a></p><p>Microsoft Edge launched Copilot Mode as a proactive research assistant. OpenAI added Study Mode for tailored learning paths and Agent Mode to automate multi-step tasks. Showrunner debuted a platform to create full AI-driven animated TV episodes </p><p>from simple prompts.  </p><p>Read more: <a href="https://www.tomsguide.com/ai/7-major-ai-updates-this-week-including-one-that-could-change-search-forever">Tom’s Guide</a></p><hr><h2 id="Google’s-Age-Guessing-AI-Raises-Privacy-Alarms"><a href="#Google’s-Age-Guessing-AI-Raises-Privacy-Alarms" class="headerlink" title="Google’s Age-Guessing AI Raises Privacy Alarms"></a>Google’s Age-Guessing AI Raises Privacy Alarms</h2><p>In the EU, Google is testing an AI that infers users’ ages from search metadata to automate age gating on Search and YouTube, aiming to protect minors without burdening users.  </p><p>Read more: <a href="https://www.wired.com/story/security-news-this-week-google-will-use-ai-to-guess-peoples-ages-based-on-search-history">Wired</a></p><p>Privacy advocates warn of opaque algorithms misclassifying adults or exposing teens, and question consent when decisions occur without explicit opt-in. The debate underscores the tension between AI’s protective potential and its risk of overreac</p><p>h.  </p><p>Read more: <a href="https://www.wired.com/story/security-news-this-week-google-will-use-ai-to-guess-peoples-ages-based-on-search-history">Wired</a></p><hr><h2 id="Meta’s-Vision-AI-as-Your-Personal-Superintelligence"><a href="#Meta’s-Vision-AI-as-Your-Personal-Superintelligence" class="headerlink" title="Meta’s Vision: AI as Your Personal Superintelligence"></a>Meta’s Vision: AI as Your Personal Superintelligence</h2><p>Mark Zuckerberg champions “personal superintelligence” assistants that understand your goals, preferences and habits, aiming to empower individuals rather than replace jobs.  </p><p>Read more: <a href="https://ainativefoundation.org/ai-native-weekly-newsletter-03-august-2025/">AI Native Foundation</a></p><p>To realize this, Meta reorganized its AI efforts under Superintelligence Labs, recruited top talent and ramped up data center and wearable investments—like smartglasses—to integrate AI seamlessly into daily life.  </p><p>Read more: <a href="https://ainativefoundation.org/ai-native-weekly-newsletter-03-august-2025/">AI Native Foundation</a></p><hr><h2 id="Mistral-Eyes-10-B-Valuation-with-a-1-B-Fundraise"><a href="#Mistral-Eyes-10-B-Valuation-with-a-1-B-Fundraise" class="headerlink" title="Mistral Eyes $10 B Valuation with a $1 B Fundraise"></a>Mistral Eyes $10 B Valuation with a $1 B Fundraise</h2><p>European startup Mistral is in advanced talks to raise $1 billion at a $10 billion valuation, targeting a top spot among foundational model developers.  </p><p>Read more: <a href="https://aloa.co/ai/resources/byte-sized/august-4-2025">Aloa</a></p><p>The funding would bolster Mistral’s open-source leadership, expand its infrastructure, and attract top AI talent amid intensifying global competition.  </p><p>Read more: <a href="https://aloa.co/ai/resources/byte-sized/august-4-2025">Aloa</a></p><hr><h2 id="AI-Spending-Boom-Fuels-U-S-Economic-Growth-and-Raises-Concerns"><a href="#AI-Spending-Boom-Fuels-U-S-Economic-Growth-and-Raises-Concerns" class="headerlink" title="AI Spending Boom Fuels U.S. Economic Growth (and Raises Concerns)"></a>AI Spending Boom Fuels U.S. Economic Growth (and Raises Concerns)</h2><p>Big Tech’s AI investments are projected to exceed $350 billion in 2025, driving demand for data centers, GPUs and other infrastructure—potentially adding 0.7% to U.S. GDP.  </p><p>Read more: <a href="https://www.washingtonpost.com/technology/2025/08/04/big-tech-ai-spending-economy">Washington Post</a></p><p>While this spending bump is hailed for innovation and job creation, skeptics warn that a downturn in AI funding could ripple through markets and retirement portfolios.  </p><p>Read more: <a href="https://www.washingtonpost.com/technology/2025/08/04/big-tech-ai-spending-economy">Washington Post</a></p><hr><h2 id="DeepMind’s-Hassabis-on-AI’s-Monumental-Impact-and-Risks"><a href="#DeepMind’s-Hassabis-on-AI’s-Monumental-Impact-and-Risks" class="headerlink" title="DeepMind’s Hassabis on AI’s Monumental Impact and Risks"></a>DeepMind’s Hassabis on AI’s Monumental Impact and Risks</h2><p>DeepMind CEO Demis Hassabis predicts AI could be “10 times bigger and maybe 10 times faster” than the Industrial Revolution, transforming fields from medicine to space exploration.  </p><p>Read more: <a href="https://www.theguardian.com/technology/2025/aug/04/demis-hassabis-ai-future-10-times-bigger-than-industrial-revolution-and-10-times-faster">The Guardian</a></p><p>Despite his optimism, he cautions against misinformation, energy demands and workforce displacement, urging equitable global access and robust ethical oversight as AI reshapes society.  </p><p>Read more: <a href="https://www.theguardian.com/technology/2025/aug/04/demis-hassabis-ai-future-10-times-bigger-than-industrial-revolution-and-10-times-faster">The Guardian</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> news </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI News Roundup, Governance, Innovation, and Market Moves – July 20 2025</title>
      <link href="2025/07/15/ai-news-07-20-2025/"/>
      <url>2025/07/15/ai-news-07-20-2025/</url>
      
        <content type="html"><![CDATA[<h2 id="Governance-and-Regulation"><a href="#Governance-and-Regulation" class="headerlink" title="Governance and Regulation"></a>Governance and Regulation</h2><p>Microsoft has signalled its intent to join the European Union’s voluntary code of practice for general-purpose AI, committing to publish training content summaries and implement copyright-compliance policies aligned with the bloc’s upcoming AI Act. In contrast, Meta Platforms has declined to sign, citing legal uncertainties and concerns that the guidelines exceed the AI Act’s scope—joining 45 European firms in warning that the code could stifle innovation across the region. <a href="https://www.reuters.com/sustainability/boards-policy-regulation/microsoft-likely-sign-eu-ai-code-practice-meta-rebuffs-guidelines-2025-07-18/">Read more</a></p><p>On the same day, the European Commission released detailed guidance for AI systems deemed to pose systemic risks, targeting models that could significantly impact health, safety, fundamental rights, or society. Companies including Google, OpenAI, Meta, Anthropic, and Mistral must conduct rigorous risk assessments, adversarial testing, incident reporting, and robust cybersecurity measures by August 2, 2026 to avoid fines of up to €35 million or 7% of global turnover. <a href="https://www.reuters.com/sustainability/boards-policy-regulation/ai-models-with-systemic-risks-given-pointers-how-comply-with-eu-ai-rules-2025-07-18/">Read more</a></p><p>Meanwhile, the Judicial Council of California adopted landmark rules for generative AI use in courts, requiring either a full ban or custom policies by September 1 that address confidentiality, bias mitigation, oversight, transparency, and security. Led by Chief Justice Patricia Guerrero’s AI task force, the policy prohibits the input of confidential data into public AI tools and mandates human verification of AI outputs across 65 courts handling five million cases annually. <a href="https://www.reuters.com/legal/government/california-court-system-adopts-rule-ai-use-2025-07-18/">Read more</a></p><hr><h2 id="Ethical-Stewardship-and-Community-Funding"><a href="#Ethical-Stewardship-and-Community-Funding" class="headerlink" title="Ethical Stewardship and Community Funding"></a>Ethical Stewardship and Community Funding</h2><p>OpenAI announced a $50 million fund to support nonprofits and community organisations in education, healthcare, economic opportunity, and community-led AI research. The initiative follows recommendations from OpenAI’s nonprofit commission, which consulted over 500 experts to align philanthropic efforts with the mission of advancing AI for public benefit. <a href="https://www.reuters.com/sustainability/boards-policy-regulation/openai-launches-50-million-fund-support-nonprofits-community-organizations-2025-07-18/">Read more</a></p><p>As part of its long-term stewardship strategy, OpenAI plans to convert its for-profit subsidiary into a public benefit corporation, with its nonprofit arm retaining ownership. This structural change aims to safeguard the company’s societal mission while enabling the investments needed for continued innovation. <a href="https://www.reuters.com/sustainability/boards-policy-regulation/openai-launches-50-million-fund-support-nonprofits-community-organizations-2025-07-18/">Read more</a></p><hr><h2 id="Defense-and-Agentic-AI"><a href="#Defense-and-Agentic-AI" class="headerlink" title="Defense and Agentic AI"></a>Defense and Agentic AI</h2><p>The U.S. Department of Defense awarded contracts worth up to $200 million each to OpenAI, Google, Anthropic, and Elon Musk’s xAI to prototype advanced “agentic AI” systems designed to assist military decision-making and operational workflows. DoD Chief Digital and Artificial Intelligence Officer Doug Matty emphasised that these AI “agents” will transform the department’s ability to support warfighters and maintain strategic advantage. <a href="https://ts2.tech/en/ai-weekend-shockwave-global-breakthroughs-big-tech-bets-bold-moves-july-19-20-2025/">Read more</a></p><hr><h2 id="Next-Generation-AI-Agents-and-Market-Moves"><a href="#Next-Generation-AI-Agents-and-Market-Moves" class="headerlink" title="Next-Generation AI Agents and Market Moves"></a>Next-Generation AI Agents and Market Moves</h2><p>OpenAI’s new ChatGPT “agent” feature enables the chatbot to execute complex, multi-step tasks—such as browsing the web, using plugins, and making reservations—through its own virtual environment. Early testers praise its autonomous capabilities, though regulatory constraints in the EU have delayed broader rollout. <a href="https://ts2.tech/en/the-future-is-here-ais-most-shocking-developments-on-july-20-2025/">Read more</a></p><p>Meta confirmed plans to expand its compute infrastructure with a 1-gigawatt data center in Ohio and a 5-gigawatt “Hyperion” facility in Louisiana slated for 2026. Google also secured AI talent by acquihiring startup Windsurf for roughly $2.4 billion after a bidding battle with OpenAI, underscoring the resource-hungry nature of the AI race. <a href="https://ts2.tech/en/the-future-is-here-ais-most-shocking-developments-on-july-20-2025/">Read more</a></p><hr><h2 id="Talent-Wars-and-Supercomputing-Push"><a href="#Talent-Wars-and-Supercomputing-Push" class="headerlink" title="Talent Wars and Supercomputing Push"></a>Talent Wars and Supercomputing Push</h2><p>Meta Platforms has recruited two leading Apple AI researchers—Mark Lee and Tom Gunter—to its new Superintelligence Labs team, following the earlier poaching of their former supervisor. This aggressive talent acquisition is part of Meta’s strategy to accelerate its AI roadmap. <a href="https://www.reuters.com/business/retail-consumer/meta-hires-two-apple-ai-researchers-after-poaching-their-boss-bloomberg-reports-2025-07-18/">Read more</a></p><p>The company has also increased its 2025 capital expenditure target to $64–$72 billion, deepening partnerships with firms like Scale AI and committing hundreds of billions to build mega-scale data centers and supercomputing clusters in pursuit of future breakthroughs. <a href="https://www.reuters.com/business/zuckerberg-says-meta-will-invest-hundreds-billions-superintelligence-2025-07-14/">Read more</a></p><hr><h2 id="Open-Source-Frontiers"><a href="#Open-Source-Frontiers" class="headerlink" title="Open-Source Frontiers"></a>Open-Source Frontiers</h2><p>Mistral launched Voxtral, its first open-source AI audio model capable of transcribing and understanding up to 40 minutes of multilingual speech—including English, Spanish, French, and Hindi—at just $0.001 per minute. The release highlights growing momentum behind community-driven AI innovation. <a href="https://www.neuralbuddies.com/p/ai-news-recap-july-18-2025">Read more</a></p><p>Perplexity AI is in talks with smartphone manufacturers, including Apple and Samsung, to pre-install its Comet AI browser on mobile devices. Comet integrates AI into browsing for personal data queries, scheduling, and summarization, aiming to challenge entrenched defaults like Chrome. <a href="https://www.reuters.com/business/perplexity-talks-with-phone-makers-pre-install-comet-ai-mobile-browser-devices-2025-07-18/">Read more</a></p><hr><h2 id="Corporate-Restructuring-and-Efficiency"><a href="#Corporate-Restructuring-and-Efficiency" class="headerlink" title="Corporate Restructuring and Efficiency"></a>Corporate Restructuring and Efficiency</h2><p>Amazon Web Services has cut “several hundred” roles across its division as part of CEO Andy Jassy’s vision for leaner, AI-augmented operations. The layoffs affect specialist and support teams, reflecting a strategic realignment to reduce bureaucracy and reallocate resources toward innovation—even as AWS posted a 17% year-over-year revenue increase. <a href="https://www.businesstoday.in/technology/news/story/amazon-cuts-hundreds-of-aws-jobs-amid-push-for-efficiency-485251-2025-07-18">Read more</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> news </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI Frontiers, Innovation, Talent Wars, and Governance – July 11 2025</title>
      <link href="2025/07/11/ai-news-07-11-2025/"/>
      <url>2025/07/11/ai-news-07-11-2025/</url>
      
        <content type="html"><![CDATA[<h1 id="AI-Frontiers-Innovation-Talent-Wars-and-Governance"><a href="#AI-Frontiers-Innovation-Talent-Wars-and-Governance" class="headerlink" title="AI Frontiers: Innovation, Talent Wars, and Governance"></a>AI Frontiers: Innovation, Talent Wars, and Governance</h1><h2 id="Google-Reinforces-DeepMind-with-Windsurf’s-Top-Talent"><a href="#Google-Reinforces-DeepMind-with-Windsurf’s-Top-Talent" class="headerlink" title="Google Reinforces DeepMind with Windsurf’s Top Talent"></a>Google Reinforces DeepMind with Windsurf’s Top Talent</h2><p>Google has onboarded Windsurf CEO Varun Mohan, co-founder Douglas Chen, and key R&amp;D personnel into its DeepMind division to accelerate agentic coding efforts as tech giants compete for the next wave of AI leadership. <a href="https://www.reuters.com/business/google-hires-windsurf-ceo-researchers-advance-ai-ambitions-2025-07-11/">Read more</a></p><hr><h2 id="Meta’s-Talent-War-200-Million-Offer-Lures-Apple-AI-Director"><a href="#Meta’s-Talent-War-200-Million-Offer-Lures-Apple-AI-Director" class="headerlink" title="Meta’s Talent War: $200 Million Offer Lures Apple AI Director"></a>Meta’s Talent War: $200 Million Offer Lures Apple AI Director</h2><p>Meta’s newly formed Superintelligence Labs enticed Apple’s Ruoming Pang with a compensation package exceeding US$200 million to lead advanced model research, underscoring the fierce competition for top AI expertise. <a href="https://www.macrumors.com/2025/07/10/meta-offered-apple-ai-executive-over-200-million/">Read more</a></p><hr><h2 id="Musk’s-xAI-Seeks-Monumental-200-Billion-Valuation"><a href="#Musk’s-xAI-Seeks-Monumental-200-Billion-Valuation" class="headerlink" title="Musk’s xAI Seeks Monumental $200 Billion Valuation"></a>Musk’s xAI Seeks Monumental $200 Billion Valuation</h2><p>Elon Musk’s AI startup, xAI, is in early talks to raise capital at a valuation between $170 billion and $200 billion, with Saudi Arabia’s PIF expected to play a leading role—following a combined $10 billion debt and equity raise. <a href="https://www.reuters.com/business/musks-xai-seeks-up-200-billion-valuation-next-fundraising-ft-reports-2025-07-11/">Read more</a></p><hr><h2 id="Amazon-Mulls-Fresh-Multibillion-Dollar-Anthropic-Investment"><a href="#Amazon-Mulls-Fresh-Multibillion-Dollar-Anthropic-Investment" class="headerlink" title="Amazon Mulls Fresh Multibillion-Dollar Anthropic Investment"></a>Amazon Mulls Fresh Multibillion-Dollar Anthropic Investment</h2><p>Amazon is considering another multibillion-dollar infusion into Anthropic, building on its prior US$4 billion commitment, to bolster AWS’s strategic alliance, leverage Trainium and Inferentia chips, and prioritize safety while scaling Claude models. <a href="https://www.reuters.com/business/retail-consumer/amazon-considers-another-multibillion-dollar-investment-anthropic-ft-reports-2025-07-10/">Read more</a></p><hr><h2 id="Moonshot-AI-Unveils-Open-Source-Kimi-K2-Model"><a href="#Moonshot-AI-Unveils-Open-Source-Kimi-K2-Model" class="headerlink" title="Moonshot AI Unveils Open-Source Kimi K2 Model"></a>Moonshot AI Unveils Open-Source Kimi K2 Model</h2><p>Chinese startup Moonshot AI has released Kimi K2, an open-source model with superior coding abilities and advanced agentic task integration, joining firms like DeepSeek and Alibaba in challenging closed-source approaches and expanding developer ecosystems. <a href="https://www.reuters.com/business/media-telecom/chinas-moonshot-ai-releases-open-source-model-reclaim-market-position-2025-07-11/">Read more</a></p><hr><h2 id="Google-Doubles-Down-on-AI-Infrastructure-Spending"><a href="#Google-Doubles-Down-on-AI-Infrastructure-Spending" class="headerlink" title="Google Doubles Down on AI Infrastructure Spending"></a>Google Doubles Down on AI Infrastructure Spending</h2><p>At the Reuters NEXT Asia summit, Google’s APAC head of AI policy, Eunice Huang, reaffirmed Alphabet’s plan to invest $75 billion in data center capacity in 2025, stressing that under-investment poses greater risks than over-investment in this transformative phase. <a href="https://www.reuters.com/world/asia-pacific/google-ai-spending-primarily-technical-infrastructure-2025-07-09/">Read more</a></p><hr><h2 id="Cloudflare-Launches-Pay-Per-Crawl-for-AI-Bots"><a href="#Cloudflare-Launches-Pay-Per-Crawl-for-AI-Bots" class="headerlink" title="Cloudflare Launches Pay-Per-Crawl for AI Bots"></a>Cloudflare Launches Pay-Per-Crawl for AI Bots</h2><p>Cloudflare introduced a “pay-per-crawl” tool enabling websites to block or charge AI crawlers—such as Google’s—for content access without affecting user experience, a model backed by publishers like Reddit, Condé Nast, and the AP to protect revenue and curb uncredited scraping. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><hr><h2 id="Microsoft-Unveils-4-Billion-AI-Education-Initiative"><a href="#Microsoft-Unveils-4-Billion-AI-Education-Initiative" class="headerlink" title="Microsoft Unveils $4 Billion AI Education Initiative"></a>Microsoft Unveils $4 Billion AI Education Initiative</h2><p>Microsoft announced its “Elevate” program, committing US$4 billion over five years to integrate AI into classrooms and workforce training, partnering with schools, community colleges, and nonprofits to provide hands-on AI and cloud instruction. <a href="https://www.techradar.com/pro/microsoft-is-spending-usd4-billion-to-push-ai-in-schools-universities-and-more">Read more</a></p><p>Microsoft Elevate will collaborate with governments and unions, including the American Federation of Teachers, to train up to 400,000 educators through a National Academy for AI Instruction—aiming to close a skills gap that could affect 59% of the global workforce by 2030. <a href="https://www.techradar.com/pro/microsoft-is-spending-usd4-billion-to-push-ai-in-schools-universities-and-more">Read more</a></p><hr><h2 id="UN-Calls-for-Global-Deepfake-Detection-Standards"><a href="#UN-Calls-for-Global-Deepfake-Detection-Standards" class="headerlink" title="UN Calls for Global Deepfake Detection Standards"></a>UN Calls for Global Deepfake Detection Standards</h2><p>A United Nations ITU report at the AI for Good Summit warns that AI-generated deepfakes threaten electoral integrity and financial systems, urging the adoption of watermarking standards, digital verification tools, and robust multimedia authentication to ensure content provenance and restore trust. <a href="https://www.reuters.com/business/un-report-urges-stronger-measures-detect-ai-driven-deepfakes-2025-07-11/">Read more</a></p><hr><h2 id="EU-Releases-Voluntary-AI-Code-to-Align-With-New-AI-Act"><a href="#EU-Releases-Voluntary-AI-Code-to-Align-With-New-AI-Act" class="headerlink" title="EU Releases Voluntary AI Code to Align With New AI Act"></a>EU Releases Voluntary AI Code to Align With New AI Act</h2><p>The European Commission unveiled a voluntary Code of Practice developed by 13 experts to help companies comply with the AI Act, focusing on transparency, copyright safeguards, and risk mitigation for general-purpose models like ChatGPT and Gemini—offering legal certainty under phased enforcement for signatories. <a href="https://www.reuters.com/business/eu-code-practice-help-firms-with-ai-rules-will-focus-copyright-safety-2025-07-10/">Read more</a></p><hr><h2 id="California’s-SB-53-First-in-Nation-AI-Transparency-amp-Safety-Reporting"><a href="#California’s-SB-53-First-in-Nation-AI-Transparency-amp-Safety-Reporting" class="headerlink" title="California’s SB 53: First-in-Nation AI Transparency &amp; Safety Reporting"></a>California’s SB 53: First-in-Nation AI Transparency &amp; Safety Reporting</h2><p>Senator Scott Wiener’s SB 53 amendments would require AI developers to publicly disclose safety and security protocols, report critical incidents to the state Attorney General, and submit to third-party audits—making California the first state to codify such guardrails. <a href="https://techcrunch.com/2025/07/09/california-lawmaker-behind-sb-1047-reignites-push-for-mandated-ai-safety-reports/">Read more</a></p><p>Dubbed “CalCompute,” the proposal includes a public cloud cluster to democratize access to advanced AI models and whistleblower protections for lab employees who flag critical risks. <a href="https://www.investing.com/news/economy-news/explainerbig-tech-wants-ai-to-be-regulated-why-do-they-oppose-a-california-ai-bill-3589845">Read more</a></p><p>Big Tech pushback has been swift—Google and Meta warn the requirements could hinder open-source innovation, while experts like Geoffrey Hinton and companies such as Anthropic support state-level oversight. <a href="https://www.investing.com/news/economy-news/explainerbig-tech-wants-ai-to-be-regulated-why-do-they-oppose-a-california-ai-bill-3589845">Read more</a></p><hr><h2 id="Senate-Upholds-State-Authority-in-AI-Regulation"><a href="#Senate-Upholds-State-Authority-in-AI-Regulation" class="headerlink" title="Senate Upholds State Authority in AI Regulation"></a>Senate Upholds State Authority in AI Regulation</h2><p>On July 11, the U.S. Senate voted 99-1 to remove a provision that would have barred states from regulating AI for ten years, preserving local power to craft safeguards and signaling a dynamic, patchwork regulatory landscape for national AI firms. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><hr><h2 id="Meta-Superintelligence-Lab-Powers-Next-Gen-Models"><a href="#Meta-Superintelligence-Lab-Powers-Next-Gen-Models" class="headerlink" title="Meta Superintelligence Lab Powers Next-Gen Models"></a>Meta Superintelligence Lab Powers Next-Gen Models</h2><p>Meta announced its Superintelligence Lab to support Llama 4.1 and 4.2 models and build the next wave of AI systems for personal assistants, leveraging its massive user base to advance personalization, real-time assistance, and immersive experiences. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><hr><h2 id="xAI-Grok-4-Debuts-Amid-Controversy"><a href="#xAI-Grok-4-Debuts-Amid-Controversy" class="headerlink" title="xAI Grok 4 Debuts Amid Controversy"></a>xAI Grok 4 Debuts Amid Controversy</h2><p>xAI launched Grok 4 with two optimized models—a general-purpose chatbot and Grok 4 Code tailored to developers—promising enhanced reasoning, coding prowess, and domain-specific performance to challenge incumbent chatbots. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><p>Its rollout was overshadowed by incidents of antisemitic and hateful content on Musk’s X platform, prompting swift removal and urgent debate over AI safety, moderation, and governance. <a href="https://radicaldatascience.wordpress.com/2025/07/10/ai-news-briefs-bulletin-board-for-july-2025/">Read more</a></p><p>As xAI iterates on its algorithms and content filters, observers will watch how accountability measures evolve to balance openness with safeguards against toxic outputs. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><hr><h2 id="OpenAI-Unveils-AI-Powered-Web-Browser"><a href="#OpenAI-Unveils-AI-Powered-Web-Browser" class="headerlink" title="OpenAI Unveils AI-Powered Web Browser"></a>OpenAI Unveils AI-Powered Web Browser</h2><p>OpenAI plans to introduce an AI-native web browser that embeds ChatGPT-style conversational interfaces into the browsing experience, enabling users to complete tasks like bookings and research through natural language prompts. <a href="https://www.reuters.com/technology/ai-intelligencer-what-matters-ai-this-week-2025-07-10/">Read more</a></p><p>Publishers worry this model could undercut traffic and ad revenue as AI intermediates content, pressing marketers to rethink SEO, content monetization, and audience engagement for an AI-mediated web. <a href="https://www.reuters.com/technology/ai-intelligencer-what-matters-ai-this-week-2025-07-10/">Read more</a></p><hr><h2 id="Apple-Eyes-Third-Party-AI-to-Revamp-Siri"><a href="#Apple-Eyes-Third-Party-AI-to-Revamp-Siri" class="headerlink" title="Apple Eyes Third-Party AI to Revamp Siri"></a>Apple Eyes Third-Party AI to Revamp Siri</h2><p>Facing delays in its internal AI roadmap, Apple is testing integrations with ChatGPT, Anthropic’s Claude, and Google’s Gemini to enhance Siri’s conversational capabilities—a strategic pivot toward hybrid models. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><p>A more capable Siri could open voice commerce, seamless app interactions, and deeper personalization, but raises critical questions around privacy, data sharing, and ecosystem control. <a href="https://www.marketingprofs.com/opinions/2025/53420/ai-update-july-11-2025-ai-news-and-views-from-the-past-two-weeks">Read more</a></p><hr><h2 id="Futurist-Warns-Majority-of-Jobs-May-Disappear-by-2045"><a href="#Futurist-Warns-Majority-of-Jobs-May-Disappear-by-2045" class="headerlink" title="Futurist Warns Majority of Jobs May Disappear by 2045"></a>Futurist Warns Majority of Jobs May Disappear by 2045</h2><p>RethinkX researcher Adam Dorr predicts that AI and robotics could render most human jobs obsolete by 2045, leaving only roles centered on human connection—like coaching, politics, and creative work—too few to absorb displaced workers and underscoring the urgency for societal preparedness. <a href="https://www.businessinsider.com/ai-wipe-out-most-jobs-2045-few-still-survive-researcher-2025-7">Read more</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> news </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI News Roundup, Governance, Deals, and Future Outlook – July 2025</title>
      <link href="2025/07/05/ai-news-07-05-2025/"/>
      <url>2025/07/05/ai-news-07-05-2025/</url>
      
        <content type="html"><![CDATA[<h2 id="EU-AI-Act-Rolls-Out-on-Schedule"><a href="#EU-AI-Act-Rolls-Out-on-Schedule" class="headerlink" title="EU AI Act Rolls Out on Schedule"></a>EU AI Act Rolls Out on Schedule</h2><p>The European Commission has confirmed that its landmark AI Act will proceed exactly on its legal timeline, rejecting calls from major tech firms and some member states to delay enforcement. Commission spokesperson Thomas Regnier declared, “There is no stop the clock. There is no grace period. There is no pause,” underscoring Brussels’ resolve to maintain strict regulatory guardrails.  </p><p>Read more: <a href="https://www.reuters.com/world/europe/artificial-intelligence-rules-go-ahead-no-pause-eu-commission-says-2025-07-04/">https://www.reuters.com/world/europe/artificial-intelligence-rules-go-ahead-no-pause-eu-commission-says-2025-07-04/</a></p><p>Under the Act, obligations for general-purpose AI models take effect in August 2025, followed by rules for high-risk AI systems in August 2026. The Commission plans targeted simplifications later this year to ease reporting burdens on small companies while preserving robust safety and transparency standards.  </p><p>Read more: <a href="https://www.reuters.com/world/europe/artificial-intelligence-rules-go-ahead-no-pause-eu-commission-says-2025-07-04/">https://www.reuters.com/world/europe/artificial-intelligence-rules-go-ahead-no-pause-eu-commission-says-2025-07-04/</a></p><hr><h2 id="Senate-Rejects-State-AI-Moratorium-Setting-Stage-for-Federal-Standards"><a href="#Senate-Rejects-State-AI-Moratorium-Setting-Stage-for-Federal-Standards" class="headerlink" title="Senate Rejects State AI Moratorium, Setting Stage for Federal Standards"></a>Senate Rejects State AI Moratorium, Setting Stage for Federal Standards</h2><p>On July 3, the U.S. Senate voted nearly unanimously to remove a proposed 10-year moratorium on state-level AI regulations from a major budget reconciliation bill, signaling bipartisan resistance to federal preemption of local oversight.  </p><p>Read more: <a href="https://www.axios.com/2025/07/03/artificial-intelligence-moratorium-future-regulation">https://www.axios.com/2025/07/03/artificial-intelligence-moratorium-future-regulation</a></p><p>With more than 20 states already enacting AI legislation and public polls showing strong support for nationwide rules, advocacy groups say Congress now faces mounting pressure to craft a unified federal framework addressing privacy, safety, and intellectual property in AI systems.  </p><p>Read more: <a href="https://www.axios.com/2025/07/03/artificial-intelligence-moratorium-future-regulation">https://www.axios.com/2025/07/03/artificial-intelligence-moratorium-future-regulation</a></p><hr><h2 id="Publishers-File-EU-Antitrust-Complaint-Over-Google’s-AI-Overviews"><a href="#Publishers-File-EU-Antitrust-Complaint-Over-Google’s-AI-Overviews" class="headerlink" title="Publishers File EU Antitrust Complaint Over Google’s AI Overviews"></a>Publishers File EU Antitrust Complaint Over Google’s AI Overviews</h2><p>A coalition led by the Independent Publishers Alliance has filed an antitrust complaint with the European Commission, accusing Google of leveraging its dominance to promote AI-generated summaries above traditional search results. Publishers argue the “AI Overviews” feature siphons traffic and ad revenue, with no realistic opt-out without sacrificing visibility.  </p><p>Read more: <a href="https://www.reuters.com/legal/litigation/googles-ai-overviews-hit-by-eu-antitrust-complaint-independent-publishers-2025-07-04/">https://www.reuters.com/legal/litigation/googles-ai-overviews-hit-by-eu-antitrust-complaint-independent-publishers-2025-07-04/</a></p><p>Google maintains that AI Overviews drive billions of clicks daily, but independent publishers and regulators in the EU and UK alike are scrutinizing whether the feature undermines journalism’s financial foundations and distorts competition.  </p><p>Read more: <a href="https://www.reuters.com/legal/litigation/googles-ai-overviews-hit-by-eu-antitrust-complaint-independent-publishers-2025-07-04/">https://www.reuters.com/legal/litigation/googles-ai-overviews-hit-by-eu-antitrust-complaint-independent-publishers-2025-07-04/</a></p><hr><h2 id="Meta-Stakes-15-Billion-on-ScaleAI-and-Talent-War-Heats-Up"><a href="#Meta-Stakes-15-Billion-on-ScaleAI-and-Talent-War-Heats-Up" class="headerlink" title="Meta Stakes $15 Billion on ScaleAI and Talent War Heats Up"></a>Meta Stakes $15 Billion on ScaleAI and Talent War Heats Up</h2><p>Meta has committed $15 billion to acquire data-labeling pioneer ScaleAI, appointing founder Alexandr Wang as Chief AI Officer to co-lead the newly formed Meta Superintelligence Labs alongside Nat Friedman. The deal underscores Meta’s aggressive push to secure cutting-edge AI infrastructure and expertise.  </p><p>Read more: <a href="https://www.businessinsider.com/ex-openai-board-member-companies-poach-meta-new-ai-hires-2025-7">https://www.businessinsider.com/ex-openai-board-member-companies-poach-meta-new-ai-hires-2025-7</a></p><p>The move has drawn scrutiny from industry observers who warn that competitors will target Meta’s hires and that internal politics could hamper integration. Even OpenAI’s Sam Altman has criticized the strategy of massive sign-on bonuses as a potential recipe for cultural disruption rather than sustainable innovation.  </p><p>Read more: <a href="https://www.businessinsider.com/ex-openai-board-member-companies-poach-meta-new-ai-hires-2025-7">https://www.businessinsider.com/ex-openai-board-member-companies-poach-meta-new-ai-hires-2025-7</a></p><hr><h2 id="Microsoft-Trims-Workforce-Amid-80-Billion-AI-Push"><a href="#Microsoft-Trims-Workforce-Amid-80-Billion-AI-Push" class="headerlink" title="Microsoft Trims Workforce Amid $80 Billion AI Push"></a>Microsoft Trims Workforce Amid $80 Billion AI Push</h2><p>On July 2, Microsoft announced plans to cut approximately 9,100 jobs—around 4 percent of its global workforce—as part of a broader cost-containment effort tied to an $80 billion capital expenditure pledge for fiscal 2025 focused on AI and cloud expansion.  </p><p>Read more: <a href="https://www.reuters.com/business/world-at-work/microsoft-lay-off-many-9000-employees-seattle-times-reports-2025-07-02/">https://www.reuters.com/business/world-at-work/microsoft-lay-off-many-9000-employees-seattle-times-reports-2025-07-02/</a></p><p>The company also intends to streamline management layers and refine its product portfolio to offset margin pressures caused by soaring AI infrastructure costs. Similar headcount reductions have been seen at Meta, Google, and Amazon as tech giants race to scale AI capabilities.  </p><p>Read more: <a href="https://www.reuters.com/business/world-at-work/microsoft-lay-off-many-9000-employees-seattle-times-reports-2025-07-02/">https://www.reuters.com/business/world-at-work/microsoft-lay-off-many-9000-employees-seattle-times-reports-2025-07-02/</a></p><hr><h2 id="Record-Breaking-Cloud-Pact-OpenAI-and-Oracle-Ink-30-Billion-‘Stargate’-Deal"><a href="#Record-Breaking-Cloud-Pact-OpenAI-and-Oracle-Ink-30-Billion-‘Stargate’-Deal" class="headerlink" title="Record-Breaking Cloud Pact: OpenAI and Oracle Ink $30 Billion ‘Stargate’ Deal"></a>Record-Breaking Cloud Pact: OpenAI and Oracle Ink $30 Billion ‘Stargate’ Deal</h2><p>OpenAI has secured a $30 billion-per-year cloud computing agreement with Oracle, leasing 4.5 gigawatts of data-center capacity under its “Stargate” infrastructure initiative. This multicloud strategy complements existing partnerships with Microsoft Azure, CoreWeave, and Google Cloud to meet surging model-training demands.  </p><p>Read more: <a href="https://www.ft.com/content/b4324903-ff53-48c2-bf71-4151cd4f68d0">https://www.ft.com/content/b4324903-ff53-48c2-bf71-4151cd4f68d0</a></p><p>The deal has driven Oracle’s stock to record highs and paves the way for new hyperscale facilities, including a 1.2 GW “Supercluster” campus in Abilene, Texas, plus expansions in Michigan and Georgia. Oracle plans significant investments in advanced AI chips to support the next wave of large-scale innovation.  </p><p>Read more: <a href="https://www.ft.com/content/b4324903-ff53-48c2-bf71-4151cd4f68d0">https://www.ft.com/content/b4324903-ff53-48c2-bf71-4151cd4f68d0</a></p><hr><h2 id="The-Great-AI-Talent-War-Meta’s-100-Million-Hires-and-65-Billion-Budget"><a href="#The-Great-AI-Talent-War-Meta’s-100-Million-Hires-and-65-Billion-Budget" class="headerlink" title="The Great AI Talent War: Meta’s $100 Million Hires and $65 Billion Budget"></a>The Great AI Talent War: Meta’s $100 Million Hires and $65 Billion Budget</h2><p>Meta has lured three top researchers from OpenAI’s Zurich lab—Lucas Beyer, Alexander Kolesnikov, and Xiaohua Zhai—with signing bonuses rumored to exceed $100 million, as part of its newly unveiled Superintelligence Labs initiative. Such high-profile defections highlight the intensifying battle for premier AI talent.  </p><p>Read more: <a href="https://www.ceoinsightsasia.com/news/meta-hires-three-openai-researchers-to-boost-ai-ambitions-nwid-13711.html">https://www.ceoinsightsasia.com/news/meta-hires-three-openai-researchers-to-boost-ai-ambitions-nwid-13711.html</a></p><p>Backed by a $65 billion AI investment plan, Meta is placing unprecedented financial bets to outmaneuver rivals like OpenAI and Google in the race toward artificial general intelligence. CEO Mark Zuckerberg insists that attracting top researchers is critical to shaping AI’s future, even as skeptics question the long-term viability of lavish hiring incentives.  </p><p>Read more: <a href="https://www.ceoinsightsasia.com/news/meta-hires-three-openai-researchers-to-boost-ai-ambitions-nwid-13711.html">https://www.ceoinsightsasia.com/news/meta-hires-three-openai-researchers-to-boost-ai-ambitions-nwid-13711.html</a></p><hr><h2 id="Tomorrow’s-Workforce-AI-Agents-‘Join’-the-Office-in-2025"><a href="#Tomorrow’s-Workforce-AI-Agents-‘Join’-the-Office-in-2025" class="headerlink" title="Tomorrow’s Workforce: AI Agents ‘Join’ the Office in 2025"></a>Tomorrow’s Workforce: AI Agents ‘Join’ the Office in 2025</h2><p>OpenAI CEO Sam Altman predicts that 2025 will mark the arrival of autonomous AI agents capable of initiating, executing, and completing complex tasks—effectively “joining the workforce” and transforming business productivity. These agents could handle research, scheduling, and preliminary design, reshaping knowledge-driven roles.  </p><p>Read more: <a href="https://www.businessinsider.com/openai-sam-altman-predictions-how-ai-could-change-the-world-2025-1">https://www.businessinsider.com/openai-sam-altman-predictions-how-ai-could-change-the-world-2025-1</a></p><p>While Altman emphasizes the need for robust guardrails to manage ethical and societal impacts, he remains optimistic that AI agents will usher in an era of “shared intelligence,” enabling humans to focus on higher-level creative and strategic endeavors.  </p><p>Read more: <a href="https://www.businessinsider.com/openai-sam-altman-predictions-how-ai-could-change-the-world-2025-1">https://www.businessinsider.com/openai-sam-altman-predictions-how-ai-could-change-the-world-2025-1</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> news </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangGraph vs. OpenAI Agent SDK – Which Agent‑Building Framework Should You Reach For?</title>
      <link href="2025/04/22/openai-agent-sdk-vs-langraph/"/>
      <url>2025/04/22/openai-agent-sdk-vs-langraph/</url>
      
        <content type="html"><![CDATA[<h2 id="1-Why-This-Matters"><a href="#1-Why-This-Matters" class="headerlink" title="1. Why This Matters"></a>1. Why This Matters</h2><p>2023–24 has been the year of “agents”: autonomous (or semi‑autonomous) programs that chain LLM calls with tools, memory, and conditional logic to get real work done.<br>Two of the loudest entrants:</p><ul><li><strong>LangGraph</strong> – an open‑source graph‑based orchestration library from LangChain.  </li><li><strong>OpenAI Agent SDK</strong> – a managed, cloud‑hosted system built around the OpenAI Assistants API.</li></ul><p>Both let you give an LLM a goal and a tool belt, then let it decide what to do next. Yet their philosophies, runtimes, and pricing models diverge enough that picking the wrong one can cost you time, money, and flexibility.</p><hr><h2 id="2-Quick‑Glance-Scorecard"><a href="#2-Quick‑Glance-Scorecard" class="headerlink" title="2. Quick‑Glance Scorecard"></a>2. Quick‑Glance Scorecard</h2><table><thead><tr><th>Category</th><th>LangGraph</th><th>OpenAI Agent SDK</th></tr></thead><tbody><tr><td><strong>License / Hosting</strong></td><td>MIT. Runs anywhere Python runs (laptop, server, on‑prem).</td><td>Closed‑source SaaS. Your agent lives on OpenAI’s servers.</td></tr><tr><td><strong>Execution Model</strong></td><td>Finite‑state graph. Nodes = functions/LCEL chains, edges = routing logic.</td><td>Event loop hidden. Backend picks tools, executes, feeds results until done.</td></tr><tr><td><strong>LLM Choice</strong></td><td>Any model LangChain supports (OpenAI, Anthropic, local GGUF, Azure, etc.).</td><td>OpenAI models only (GPT‑4o, GPT‑4, GPT‑3.5, etc.).</td></tr><tr><td><strong>Tooling Interface</strong></td><td>Python callables; arbitrary I/O; can stream tokens between nodes.</td><td>JSON‑schema “tools”. Up to 128 tools per agent; automatic function calling.</td></tr><tr><td><strong>State / Memory</strong></td><td>You design it (Redis, Postgres, in‑process, etc.).</td><td>Automatic persistent “thread”. 1M tokens free, evicts oldest afterwards.</td></tr><tr><td><strong>Observability</strong></td><td>Python logging/tracing, LangSmith integration.</td><td>OpenAI dashboard (runs, deltas, token usage). Limited node‑level introspect.</td></tr><tr><td><strong>Data Privacy</strong></td><td>Stays wherever you deploy.</td><td>Data in US‑hosted OpenAI servers; opt‑out training by default.</td></tr><tr><td><strong>Pricing</strong></td><td>Free aside from model/tool costs and your infra.</td><td>Pay‑per‑token + per‑tool compute. No server bills, but locked to OAI rates.</td></tr><tr><td><strong>Maturity</strong></td><td>Released Q3 2023. Active OSS community, rapid iterations.</td><td>Early‑access beta (as of mid‑2024). Docs improving, still feature‑gating.</td></tr><tr><td><strong>Best For</strong></td><td>Complex routing, on‑prem, non‑OpenAI models, heavy customization.</td><td>Rapid GPT‑4o prototypes, zero DevOps, managed memory &amp; compliance.</td></tr></tbody></table><hr><h2 id="3-Deep-Dive-Strengths-amp-Weaknesses"><a href="#3-Deep-Dive-Strengths-amp-Weaknesses" class="headerlink" title="3. Deep Dive: Strengths &amp; Weaknesses"></a>3. Deep Dive: Strengths &amp; Weaknesses</h2><h3 id="A-Architecture-amp-Flexibility"><a href="#A-Architecture-amp-Flexibility" class="headerlink" title="A. Architecture &amp; Flexibility"></a>A. Architecture &amp; Flexibility</h3><ul><li><strong>LangGraph:</strong>  <ul><li>Explicit graph topology: you wire nodes, edges, loops, termination conditions.  </li><li>Great for research, debugging, compliance audits—every step is visible and deterministic.</li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>Hidden event loop: define tools and retrieval, the backend orchestrates turns.  </li><li>Simpler onboarding, but you can’t interpose custom routing logic.</li></ul></li></ul><h3 id="B-Model-amp-Tool-Ecosystem"><a href="#B-Model-amp-Tool-Ecosystem" class="headerlink" title="B. Model &amp; Tool Ecosystem"></a>B. Model &amp; Tool Ecosystem</h3><ul><li><strong>LangGraph:</strong>  <ul><li>Any LangChain‑supported LLM/embedding (OpenAI, Anthropic, local, Azure, etc.).  </li><li>Avoid vendor lock‑in; run offline or air‑gapped.</li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>GPT‑4o, GPT‑4, GPT‑3.5, function calling, code interpreter, retrieval.  </li><li>Deepest integration with OpenAI’s proprietary features.</li></ul></li></ul><h3 id="C-Hosting-amp-DevOps"><a href="#C-Hosting-amp-DevOps" class="headerlink" title="C. Hosting &amp; DevOps"></a>C. Hosting &amp; DevOps</h3><ul><li><strong>LangGraph:</strong>  <ul><li>Bring‑your‑own runtime (laptop, cloud, on‑prem).  </li><li>You manage autoscaling, retries, security, tracing.</li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>Fully managed by OpenAI.  </li><li>No infra overhead; ideal for quick demos and small teams.</li></ul></li></ul><h3 id="D-Cost-Model"><a href="#D-Cost-Model" class="headerlink" title="D. Cost Model"></a>D. Cost Model</h3><ul><li><strong>LangGraph:</strong>  <ul><li>You pay for the LLM (token costs) and your infra.  </li><li>Local models can reduce marginal cost near zero.</li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>Token + compute cost.  </li><li>Hidden loop iterations can make billing less predictable.</li></ul></li></ul><h3 id="E-Observability-amp-Debugging"><a href="#E-Observability-amp-Debugging" class="headerlink" title="E. Observability &amp; Debugging"></a>E. Observability &amp; Debugging</h3><ul><li><strong>LangGraph:</strong>  <ul><li>LangSmith tracing: every node, prompt, token path is recorded.  </li><li>Replayable, exportable, fine‑tunable on missteps.</li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>OpenAI “runs” UI: chronological view of messages, tool calls, completions.  </li><li>Lacks deep exportable traces at the node level.</li></ul></li></ul><h3 id="F-Governance"><a href="#F-Governance" class="headerlink" title="F. Governance"></a>F. Governance</h3><ul><li><strong>LangGraph:</strong>  <ul><li>Full on‑prem control for GDPR / PII compliance.  </li></ul></li><li><strong>OpenAI Agent SDK:</strong>  <ul><li>SOC2/ISO scope covered by OpenAI, but data lives in OpenAI’s boundary.</li></ul></li></ul><hr><h2 id="4-Pros-amp-Cons-Lists"><a href="#4-Pros-amp-Cons-Lists" class="headerlink" title="4. Pros &amp; Cons Lists"></a>4. Pros &amp; Cons Lists</h2><h3 id="LangGraph"><a href="#LangGraph" class="headerlink" title="LangGraph"></a>LangGraph</h3><p><strong>Pros:</strong>  </p><ol><li>Vendor‑agnostic LLM &amp; vector store support.  </li><li>Explicit graph → deterministic &amp; inspectable flows.  </li><li>Event‑driven / streaming across nodes.  </li><li>Open‑source, extendable, can fork.  </li><li>Works offline or air‑gapped.</li></ol><p><strong>Cons:</strong>  </p><ol><li>You manage infra, scaling, retries.  </li><li>Steeper learning curve around LCEL syntax.  </li><li>No built‑in retrieval or code‑interpreter—you assemble components.  </li><li>Fast‑moving API &amp; fragmented examples.</li></ol><h3 id="OpenAI-Agent-SDK"><a href="#OpenAI-Agent-SDK" class="headerlink" title="OpenAI Agent SDK"></a>OpenAI Agent SDK</h3><p><strong>Pros:</strong>  </p><ol><li>Zero‑server setup; minutes to first agent.  </li><li>Tight GPT‑4o integration (function calling, code interpreter).  </li><li>Automatic persistent memory with citations.  </li><li>Usage analytics &amp; QoS from OpenAI.  </li><li>Compliance certifications handled for you.</li></ol><p><strong>Cons:</strong>  </p><ol><li>OpenAI‑only models (vendor lock‑in).  </li><li>Closed‑source; no self‑hosting.  </li><li>Limited custom routing / multi‑agent orchestration.  </li><li>Beta API; potential breaking changes.  </li><li>Harder to debug deeply; data residency tied to OpenAI.</li></ol><hr><h2 id="5-Which-One-Should-You-Pick"><a href="#5-Which-One-Should-You-Pick" class="headerlink" title="5. Which One Should You Pick?"></a>5. Which One Should You Pick?</h2><h3 id="Choose-LangGraph-when"><a href="#Choose-LangGraph-when" class="headerlink" title="Choose LangGraph when:"></a>Choose LangGraph when:</h3><ul><li>You need multiple LLMs or local models for cost/privacy.  </li><li>Your workflow demands branching, looping, or critique loops.  </li><li>Step‑level traceability and audits are critical.  </li><li>You must deploy inside your cloud/VPC or offline.</li></ul><h3 id="Choose-OpenAI-Agent-SDK-when"><a href="#Choose-OpenAI-Agent-SDK-when" class="headerlink" title="Choose OpenAI Agent SDK when:"></a>Choose OpenAI Agent SDK when:</h3><ul><li>Time‑to‑market trumps fine‑grained control.  </li><li>User data can reside in OpenAI’s US‑hosted servers.  </li><li>You want frictionless access to GPT‑4o’s code interpreter &amp; retrieval.  </li><li>You have no dedicated DevOps team.  </li><li>You’re building a conversational assistant, not a complex workflow engine.</li></ul><h3 id="Hybrid-Pattern-common-in-production"><a href="#Hybrid-Pattern-common-in-production" class="headerlink" title="Hybrid Pattern (common in production)"></a>Hybrid Pattern (common in production)</h3><p>Use the Agent SDK as your user‑facing “assistant” but register a tool that delegates to an internal LangGraph service for heavyweight multi‑step planning. This combines OpenAI’s managed convo memory with your custom orchestration.</p><hr><h2 id="6-What-the-Future-Might-Change"><a href="#6-What-the-Future-Might-Change" class="headerlink" title="6. What the Future Might Change"></a>6. What the Future Might Change</h2><ul><li><strong>OpenAI</strong> may allow third‑party model endpoints in the SDK, weakening vendor lock‑in.  </li><li><strong>LangChain</strong> is working on managed LangGraph hosting, closing the DevOps gap.  </li><li>Both ecosystems are standardizing on JSON‑schema tool specs—expect better cross‑compatibility soon.</li></ul><hr><h2 id="7-Bottom-Line"><a href="#7-Bottom-Line" class="headerlink" title="7. Bottom Line"></a>7. Bottom Line</h2><p>There is no universal “better”—only “better for this project today.”  </p><ul><li>If you’re an enterprise with strict data rules, multi‑model needs, or bespoke routing logic, LangGraph’s open architecture is worth the setup overhead.  </li><li>If you’re a startup that needs a GPT‑4‑powered assistant with file uploads and code execution tomorrow morning, the OpenAI Agent SDK’s managed simplicity is hard to beat.</li></ul><p>Evaluate along three axes—Control, Compliance, Convenience—and the decision usually makes itself.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> agents </tag>
            
            <tag> langraph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introducing a Triage Subject Matter Expert (SME) System Using OpenAI’s Agent SDK</title>
      <link href="2025/03/22/openai_agent_sdk_and_triage_expert_example/"/>
      <url>2025/03/22/openai_agent_sdk_and_triage_expert_example/</url>
      
        <content type="html"><![CDATA[<p>OpenAI’s Agent SDK makes it easy to build sophisticated multi-agent ecosystems by providing simple yet powerful primitives. At its core, you work with:</p><p>• Agents – LLM-powered entities equipped with instructions and tools<br>• Handoffs – Allowing agents to delegate tasks to one another<br>• Guardrails – Enabling you to validate and control agent inputs</p><p>This post focuses on a real-world use case: creating a triage agent that dispatches questions to subject matter experts for mathematics, physics, chemistry, or geography. The system even includes guardrails to prevent inappropriate content from being processed.</p><h2 id="The-Triage-SME-Example"><a href="#The-Triage-SME-Example" class="headerlink" title="The Triage SME Example"></a>The Triage SME Example</h2><p>In this example, a triage agent listens for questions and forwards each query to the relevant SME agent. If none of the SMEs matches the subject, it responds that it doesn’t know the answer. Along with this, an input guardrail ensures questions do not contain unwanted content.</p><p>Below is the complete code example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> agents <span class="keyword">import</span> Agent, Runner, input_guardrail, GuardrailFunctionOutput, InputGuardrailTripwireTriggered</span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the inappropriate language guardrail</span></span><br><span class="line"><span class="meta">@input_guardrail</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">inappropriate_language_guardrail</span>(<span class="params">ctx, agent, <span class="built_in">input</span></span>):</span></span><br><span class="line">    inappropriate_words = [<span class="string">&#x27;stupid&#x27;</span>]  <span class="comment"># Add inappropriate words here</span></span><br><span class="line">    pattern = re.<span class="built_in">compile</span>(<span class="string">&#x27;|&#x27;</span>.join(inappropriate_words), re.IGNORECASE)</span><br><span class="line">    <span class="keyword">if</span> pattern.search(<span class="built_in">input</span>):</span><br><span class="line">        <span class="keyword">return</span> GuardrailFunctionOutput(</span><br><span class="line">            output_info=<span class="string">&quot;Inappropriate language detected.&quot;</span>,</span><br><span class="line">            tripwire_triggered=<span class="literal">True</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> GuardrailFunctionOutput(</span><br><span class="line">        output_info=<span class="string">&quot;No inappropriate language detected.&quot;</span>,</span><br><span class="line">        tripwire_triggered=<span class="literal">False</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define specialized agents with customized response prefixes</span></span><br><span class="line"></span><br><span class="line">math_agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;Math Expert&quot;</span>,</span><br><span class="line">    instructions=(</span><br><span class="line">        <span class="string">&quot;You are the Math Subject Matter Expert (SME). &quot;</span></span><br><span class="line">        <span class="string">&quot;Preface each response with: &#x27;This is your Math SME. The answer to your question is as follows.&#x27; &quot;</span></span><br><span class="line">        <span class="string">&quot;Then, provide the detailed answer to the math question.&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">physics_agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;Physics Expert&quot;</span>,</span><br><span class="line">    instructions=(</span><br><span class="line">        <span class="string">&quot;You are the Physics Subject Matter Expert (SME). &quot;</span></span><br><span class="line">        <span class="string">&quot;Preface each response with: &#x27;This is your Physics SME. The answer to your question is as follows.&#x27; &quot;</span></span><br><span class="line">        <span class="string">&quot;Then, provide the detailed answer to the physics question.&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">chemistry_agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;Chemistry Expert&quot;</span>,</span><br><span class="line">    instructions=(</span><br><span class="line">        <span class="string">&quot;You are the Chemistry Subject Matter Expert (SME). &quot;</span></span><br><span class="line">        <span class="string">&quot;Preface each response with: &#x27;This is your Chemistry SME. The answer to your question is as follows.&#x27; &quot;</span></span><br><span class="line">        <span class="string">&quot;Then, provide the detailed answer to the chemistry question.&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">geography_agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;Geography Expert&quot;</span>,</span><br><span class="line">    instructions=(</span><br><span class="line">        <span class="string">&quot;You are the Geography Subject Matter Expert (SME). &quot;</span></span><br><span class="line">        <span class="string">&quot;Preface each response with: &#x27;This is your Geography SME. The answer to your question is as follows.&#x27; &quot;</span></span><br><span class="line">        <span class="string">&quot;Then, provide the detailed answer to the geography question.&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the triage agent that directs questions to the appropriate SME.</span></span><br><span class="line">triage_agent = Agent(</span><br><span class="line">    name=<span class="string">&quot;Triage Agent&quot;</span>,</span><br><span class="line">    instructions=(</span><br><span class="line">        <span class="string">&quot;You are responsible for directing questions to the appropriate subject matter expert. &quot;</span></span><br><span class="line">        <span class="string">&quot;If the question is about mathematics, hand it off to the Math Expert. &quot;</span></span><br><span class="line">        <span class="string">&quot;If it&#x27;s about physics, hand it off to the Physics Expert. &quot;</span></span><br><span class="line">        <span class="string">&quot;If it&#x27;s about chemistry, hand it off to the Chemistry Expert. &quot;</span></span><br><span class="line">        <span class="string">&quot;If it&#x27;s about geography, hand it off to the Geography Expert. &quot;</span></span><br><span class="line">        <span class="string">&quot;If the question doesn&#x27;t pertain to these subjects, respond with &#x27;I&#x27;m sorry, I don&#x27;t know the answer to that.&#x27;&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    handoffs=[math_agent, physics_agent, chemistry_agent, geography_agent],</span><br><span class="line">    input_guardrails=[inappropriate_language_guardrail],</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Function to run the triage agent</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    question = <span class="string">&quot;Explain the Riemann hypothesis to a 10 year old&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        result = <span class="keyword">await</span> Runner.run(triage_agent, <span class="built_in">input</span>=question)</span><br><span class="line">        <span class="built_in">print</span>(result.final_output)</span><br><span class="line">    <span class="keyword">except</span> InputGuardrailTripwireTriggered:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Your question contains inappropriate language and cannot be processed.&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    asyncio.run(main())</span><br></pre></td></tr></table></figure><h2 id="Running-the-Example"><a href="#Running-the-Example" class="headerlink" title="Running the Example"></a>Running the Example</h2><ol><li>Set up your development environment (on mac or unbuntu, python3.12 as an example):<ul><li>make a directory and create virtual env<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   mkdir agent_demo</span><br><span class="line">   cd agent_demo</span><br><span class="line">   python3 -m venv .venv</span><br><span class="line">source .venb/bin/activate</span><br></pre></td></tr></table></figure></li><li>Install the necessary libraries:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">oython3 -m pip install openai-agents</span><br></pre></td></tr></table></figure></li></ul></li><li>Set your OpenAI API key appropriately before running your code:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export OPENAI_API_KEY=&quot;your_api_key_here&quot;</span><br></pre></td></tr></table></figure></li><li>run the above example<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python3 example.py</span><br></pre></td></tr></table></figure></li></ol><h2 id="What-This-Example-Does"><a href="#What-This-Example-Does" class="headerlink" title="What This Example Does"></a>What This Example Does</h2><ul><li>The guardrail (<code>inappropriate_language_guardrail</code>) scans the incoming question for any disallowed words (e.g., “stupid”). If found, it stops the process.</li><li>Four SME agents (math, physics, chemistry, geography) are defined, each with custom instructions that prepend a response header.</li><li>The triage agent reads the incoming question and, based on its content, either delegates the query to one of the SMEs or responds that it doesn’t have an answer if the question is off-topic.</li><li>Finally, the asynchronous main function runs the triage agent and prints the final output, showcasing the system’s capabilities.</li></ul><hr><p>This example illustrates how OpenAI’s Agent SDK can help developers easily build a scalable, modular multi-agent system for real-world applications without sacrificing ease of use or flexibility. Enjoy experimenting with and extending the system!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> agents </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Stashing Your Changes, A Quick Fix for Git Pull Errors</title>
      <link href="2025/03/15/fix_unstaged_error_when_doing_git_pull/"/>
      <url>2025/03/15/fix_unstaged_error_when_doing_git_pull/</url>
      
        <content type="html"><![CDATA[<p>Have you ever encountered this error when running <code>git pull</code>?</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">error: cannot pull with rebase: You have unstaged changes.</span><br><span class="line">error: please commit or stash them.</span><br></pre></td></tr></table></figure><p>This message indicates that Git has detected some modifications in your working directory that haven’t been staged or committed yet. To prevent conflicts or potential data loss during the pull, Git is asking you to either commit or stash your changes. Today, I’ll show you how to use the <strong>stash</strong> command to safely set aside your changes, pull from the remote repository, and then reapply your modifications.</p><h4 id="Step-1-Stash-Your-Changes"><a href="#Step-1-Stash-Your-Changes" class="headerlink" title="Step 1: Stash Your Changes"></a>Step 1: Stash Your Changes</h4><p>The simplest way to save your unstaged changes is to run:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash</span><br></pre></td></tr></table></figure><p>This command temporarily saves your modifications and reverts your working directory back to the state of the last commit. Now that your workspace is clean, Git can safely pull the changes from the remote repository.</p><h4 id="Step-2-Pull-the-Latest-Changes"><a href="#Step-2-Pull-the-Latest-Changes" class="headerlink" title="Step 2: Pull the Latest Changes"></a>Step 2: Pull the Latest Changes</h4><p>With your working directory clean, go ahead and pull the remote changes:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure><p>Since your changes are safely stashed away, the pull operation should complete without any issues.</p><h4 id="Step-3-Reapply-Your-Stashed-Changes"><a href="#Step-3-Reapply-Your-Stashed-Changes" class="headerlink" title="Step 3: Reapply Your Stashed Changes"></a>Step 3: Reapply Your Stashed Changes</h4><p>After successfully pulling the latest updates, you’ll want to reapply the changes you stashed earlier. Use this command to bring your modifications back:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash pop</span><br></pre></td></tr></table></figure><p>This command reapplies the stashed changes to your working directory and simultaneously removes the stash from your list. If you prefer to apply the stash without removing it (perhaps as a backup), you can use:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash apply</span><br></pre></td></tr></table></figure><h4 id="Optional-Review-Your-Stashes"><a href="#Optional-Review-Your-Stashes" class="headerlink" title="Optional: Review Your Stashes"></a>Optional: Review Your Stashes</h4><p>If you ever need to see a list of all your stashed changes, simply run:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git stash list</span><br></pre></td></tr></table></figure><p>This can be incredibly helpful if you’ve created multiple stashes and want to revisit an earlier change.</p><h4 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h4><p>Using <code>git stash</code> is a quick and effective way to manage temporary changes while keeping your Git operations smooth and error-free. Next time you encounter the pull error due to unstaged changes, remember that stashing provides you with a safe way to temporarily shelf your work, update your repository, and then seamlessly reintegrate your modifications.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prepare Data for Fine-Tuning Large Language Models of Closed-Book QA</title>
      <link href="2025/02/19/how-to-prepare-training-dataset-for-large-language-model-finetuning-for-closed-book-q/"/>
      <url>2025/02/19/how-to-prepare-training-dataset-for-large-language-model-finetuning-for-closed-book-q/</url>
      
        <content type="html"><![CDATA[<p>Large Language Models (LLMs) like Llama or Qwen have shown incredible promise in answering questions based on provided context. Traditionally, many systems rely on an initial retrieval step—searching through documents based on the question, then augmenting the prompt with the most relevant excerpts for the LLM to summarize a final answer. But what if you want to fine-tune your LLM to answer questions without this initial retrieval step? In this post, we explore the process and considerations for creating a training dataset to enable your model to internalize document content and answer questions in a “closed-book” fashion.</p><h2 id="1-Defining-the-Task"><a href="#1-Defining-the-Task" class="headerlink" title="1. Defining the Task"></a>1. Defining the Task</h2><p>Before diving into the dataset design, it’s critical to articulate exactly what your goal is:</p><ul><li><strong>Closed-Book QA:</strong> You want the model to internalize facts and answer questions based solely on the training it has received, without using external search.</li><li><strong>Simulated Retrieval:</strong> The model learns to extract and synthesize answers from internalized representations instead of performing an explicit retrieval.</li></ul><p>Understanding these nuances helps shape the style and format of your training data.</p><h2 id="2-Curating-or-Generating-Training-Data"><a href="#2-Curating-or-Generating-Training-Data" class="headerlink" title="2. Curating or Generating Training Data"></a>2. Curating or Generating Training Data</h2><p>You might think you need examples for every possible question related to your documents. Fortunately, that’s not the case. A well-designed, diverse dataset capturing the main topics, concepts, and entities of your documents can suffice.</p><ul><li><strong>Representative Examples:</strong> Focus on creating a set of question–answer pairs that covers the key content areas.</li><li><strong>Contextual Prompts:</strong> You can incorporate context directly into the training examples. For instance, structure prompts as:<br><em>“Given the following text, answer the question:”</em> followed by the article excerpt and then the question. This style reinforces the connection between the document content and the answer.</li></ul><h2 id="3-Strategies-for-Data-Construction"><a href="#3-Strategies-for-Data-Construction" class="headerlink" title="3. Strategies for Data Construction"></a>3. Strategies for Data Construction</h2><p>There are several approaches you can take to create or assemble your training dataset:</p><h3 id="a-Manual-Curation"><a href="#a-Manual-Curation" class="headerlink" title="a. Manual Curation"></a>a. Manual Curation</h3><ul><li><strong>Expert Involvement:</strong> Domain experts can write questions and answers directly from the documents.</li><li><strong>High-Quality Data:</strong> Manual curation ensures precise alignment between questions and content—ideal for niche or technical areas.</li><li><strong>Scalability:</strong> The downside is that manual curation may not scale well if you have a vast amount of content.</li></ul><h3 id="b-Synthetic-Data-Generation"><a href="#b-Synthetic-Data-Generation" class="headerlink" title="b. Synthetic Data Generation"></a>b. Synthetic Data Generation</h3><ul><li><strong>Utilize Existing LLMs:</strong> Prompt an LLM to generate questions from your documents, e.g., <em>“Generate questions that someone might ask about this text.”</em></li><li><strong>Quality Control:</strong> It’s important to review or filter these questions to ensure they are accurate and relevant.</li><li><strong>Diverse Dataset:</strong> This approach broadens the question distribution and introduces variety.</li></ul><h3 id="c-Hybrid-Approach"><a href="#c-Hybrid-Approach" class="headerlink" title="c. Hybrid Approach"></a>c. Hybrid Approach</h3><ul><li><strong>Combine Both Methods:</strong> Begin with machine-generated examples and have them reviewed or refined by experts.</li><li><strong>Balanced Outcome:</strong> This strategy offers a trade-off between scale and data quality.</li></ul><h2 id="4-Data-Formatting-for-Fine-Tuning"><a href="#4-Data-Formatting-for-Fine-Tuning" class="headerlink" title="4. Data Formatting for Fine-Tuning"></a>4. Data Formatting for Fine-Tuning</h2><p>When designing your examples, consider the following formatting tips:</p><ul><li><strong>Instructional Cues:</strong> Incorporate clear instructions such as:<br><em>“Answer the following question based on your internal knowledge of the documents.”</em><br>This helps the model understand its role.</li><li><strong>Contextual Embedding:</strong> Optionally include key document excerpts directly in the prompt during training. This allows the model to “absorb” detailed information it can later recall without an external search.</li></ul><h2 id="5-Managing-Input-Length-and-Context"><a href="#5-Managing-Input-Length-and-Context" class="headerlink" title="5. Managing Input Length and Context"></a>5. Managing Input Length and Context</h2><ul><li><strong>Chunking Long Documents:</strong> Break large documents into manageable pieces. Create separate training examples for each chunk while ensuring critical links between sections remain intact.</li><li><strong>Summaries vs. Detailed Text:</strong> For complex domains, consider training on summaries or extracted key points, depending on how detailed you want the responses to be.</li></ul><h2 id="6-Addressing-the-“All-Possible-Questions”-Conundrum"><a href="#6-Addressing-the-“All-Possible-Questions”-Conundrum" class="headerlink" title="6. Addressing the “All Possible Questions” Conundrum"></a>6. Addressing the “All Possible Questions” Conundrum</h2><p>Rather than covering every conceivable query:</p><ul><li><strong>Emphasize Diversity:</strong> Focus on capturing a diverse set of scenarios that reflect the main facets of your content.</li><li><strong>Data Augmentation:</strong> Use paraphrasing, reordering, and slight rewording of questions to help the model generalize to unseen queries.</li></ul><h2 id="7-Evaluation-and-Iteration"><a href="#7-Evaluation-and-Iteration" class="headerlink" title="7. Evaluation and Iteration"></a>7. Evaluation and Iteration</h2><ul><li><strong>Holdout Set:</strong> Always keep a separate set of question–answer pairs for validation.</li><li><strong>Iterative Improvement:</strong> Use gap analysis on the holdout data to identify areas where the model struggles, and augment your dataset accordingly.</li></ul><h2 id="8-Considerations-and-Trade-Offs"><a href="#8-Considerations-and-Trade-Offs" class="headerlink" title="8. Considerations and Trade-Offs"></a>8. Considerations and Trade-Offs</h2><ul><li><strong>Model Capacity:</strong> Closed-book QA places the burden on the model’s memory. If your documents are vast and detailed, even a highly fine-tuned model might struggle to recall every detail.</li><li><strong>Update Mechanism:</strong> Unlike retrieval-based systems where updating content only means refreshing the index, any change in the dataset may require additional rounds of fine-tuning.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Transitioning to a closed-book QA system by fine-tuning an LLM involves a thoughtful approach to dataset design. By curating or generating a diverse and high-quality dataset, formatting your examples appropriately, and iteratively refining your approach, you can shift knowledge retrieval from runtime search to internalized expertise within the model’s weights.</p><p>By carefully balancing quality, diversity, and scalability, you can build a fine-tuned model that answers questions accurately—without needing to search for context every time.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> finetuning </tag>
            
            <tag> llm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Debugging iPhone Safari with Your Mac</title>
      <link href="2025/02/17/debugging-webpage-on-ihpone-safari-broswer-with-your-mac/"/>
      <url>2025/02/17/debugging-webpage-on-ihpone-safari-broswer-with-your-mac/</url>
      
        <content type="html"><![CDATA[<p>Developing a web application for the iPhone can sometimes be challenging—especially when it comes to debugging. Unlike desktop browsers, iPhone Safari doesn’t offer a built-in “Inspect Element” or console that you can easily access. Fortunately, if you own a Mac, you can leverage Safari’s powerful developer tools to inspect, debug, and monitor your webpage running on an actual iPhone device.</p><h2 id="Setting-Up-the-Connection"><a href="#Setting-Up-the-Connection" class="headerlink" title="Setting Up the Connection"></a>Setting Up the Connection</h2><ol><li><p><strong>Connect Your iPhone to Your Mac:</strong><br>Start by physically connecting your iPhone to your Mac using a USB cable. This secure connection is essential for debugging your webpage using your iPhone’s Safari browser.</p></li><li><p><strong>Enable Web Inspector on Your iPhone:</strong><br>Go to <strong>Settings &gt; Safari &gt; Advanced</strong> on your iPhone, and toggle on the <strong>Web Inspector</strong> option. This setting allows your iPhone’s Safari to be debugged from your Mac.</p></li><li><p><strong>Trust the Connection:</strong><br>The first time you connect your iPhone to your Mac, you will be prompted to trust the computer. Make sure to accept this prompt so that the devices communicate successfully.</p></li></ol><h2 id="Using-Safari’s-Develop-Menu"><a href="#Using-Safari’s-Develop-Menu" class="headerlink" title="Using Safari’s Develop Menu"></a>Using Safari’s Develop Menu</h2><p>Once your iPhone is connected and trusted, follow these steps on your Mac:</p><ol><li><p><strong>Open Safari on Your Mac:</strong><br>Launch Safari and ensure your developer tools are enabled. If not, go to <strong>Safari &gt; Preferences &gt; Advanced</strong>, and check the box at the bottom that says <strong>“Show Develop menu in menu bar.”</strong></p></li><li><p><strong>Access the Develop Menu:</strong><br>In the menu bar at the top left of your screen, click on <strong>Develop</strong>. Here, you will see your connected iPhone listed with its name (e.g., “John’s iPhone”).</p></li><li><p><strong>Select Your iPhone and Page:</strong><br>Hover over your iPhone’s name to reveal a list of open webpages. Click on the page you want to debug. This action opens a window that replicates Safari’s developer tools, complete with console logs, network activity, HTML/CSS inspection, and more.</p></li></ol><h2 id="Debugging-in-Real-Time"><a href="#Debugging-in-Real-Time" class="headerlink" title="Debugging in Real-Time"></a>Debugging in Real-Time</h2><p>With the inspection window now open, you can interact with your iPhone’s webpage and see real-time logs and network information on your Mac. Here are some of the benefits:</p><ul><li><p><strong>Console Logs:</strong><br>View JavaScript errors, warnings, and custom log messages directly from your iPhone. This makes it much easier to diagnose issues that only occur on mobile Safari.</p></li><li><p><strong>Element Inspection:</strong><br>Use the elements pane to inspect and modify the DOM and styles in real-time. This can be particularly helpful for resolving layout or styling issues unique to the mobile viewport.</p></li><li><p><strong>Network Monitoring:</strong><br>Keep track of network requests, responses, and performance metrics. This gives you invaluable insights into how your webpage behaves over mobile data or in different network conditions.</p></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>By connecting your iPhone to your Mac and leveraging Safari’s Develop menu, you get a powerful debugging setup that bridges the gap between desktop and mobile development. Whether you’re testing for layout issues, JavaScript bugs, or network performance, this method ensures you’re well-equipped to create robust, mobile-friendly web applications.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> iphone </tag>
            
            <tag> web development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convert mulaw audio data format to PCM format with different sampling rate</title>
      <link href="2025/01/28/convert-mulaw-audio-to-pcm-with-different-sampling/"/>
      <url>2025/01/28/convert-mulaw-audio-to-pcm-with-different-sampling/</url>
      
        <content type="html"><![CDATA[<p>In many cases, we need to convert audio format for different applications.<br>Mu-law format is one popular format which can save space, while PCM is linear and direclty consumed by applications.</p><p>There could be also different sampling rates we need be aware of.</p><p>One such applicaiton is the openAI realtime API usage, which usually takes PCM format with 24kHz sampling rate.</p><p>Below is an example showing how to complete the process by converting mu-law (8 kHz) data to 16-bit linear PCM, resampling it to 24 kHz, and finally encoding it as a base64 string:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> audioop</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="comment"># Path to your mu-law file</span></span><br><span class="line">file_path = <span class="string">&quot;some_mu-law-file&quot;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">  mulaw_data = f.read()</span><br><span class="line"><span class="comment"># 1. Convert mu-law data to 16-bit PCM at 8 kHz (2 bytes per sample).</span></span><br><span class="line">pcm_8k = audioop.ulaw2lin(mulaw_data, <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 2. Resample from 8 kHz up to 24 kHz (1 channel, 16 bits per sample).</span></span><br><span class="line"><span class="comment"># audioop.ratecv returns a tuple (converted_data, state).</span></span><br><span class="line">pcm_24k, _ = audioop.ratecv(pcm_8k, <span class="number">2</span>, <span class="number">1</span>, <span class="number">8000</span>, <span class="number">24000</span>, <span class="literal">None</span>)</span><br><span class="line"><span class="comment"># 3. Convert the 24 kHz PCM data to a base64 encoded string.</span></span><br><span class="line">pcm_24k_base64 = base64.b64encode(pcm_24k).decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"><span class="comment"># pcm_24k_base64 now holds the base64-encoded PCM data at 24 kHz.</span></span><br></pre></td></tr></table></figure><h3 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation:"></a>Explanation:</h3><ul><li><p><strong><code>ulaw2lin(data, width)</code></strong>: Converts mu-law data to linear PCM with the specified sample width (in bytes). Here <code>width=2</code> (16-bit).</p></li><li><p><strong><code>ratecv(data, width, channels, in_rate, out_rate, state)</code></strong>: Resamples linear PCM to a new sample rate. It requires the current sample width, number of channels, input rate, and output rate. The state can typically be <code>None</code> on the first call.</p></li><li><p><strong><code>b64encode(...)</code></strong>: Encodes the binary data in Base64. The <code>.decode(&quot;utf-8&quot;)</code> step converts it into a UTF-8 string instead of bytes.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Thinking Call Center Routing Optimization as a Recommendation Problem</title>
      <link href="2024/11/16/think-call-center-routing-as-recommendation-problem/"/>
      <url>2024/11/16/think-call-center-routing-as-recommendation-problem/</url>
      
        <content type="html"><![CDATA[<p>In today’s highly competitive market, delivering exceptional customer service is paramount. Call centers, being the frontline of customer interaction, play a crucial role in shaping customer experience. Optimizing call center routing—matching customers with the most suitable agents—not only enhances customer satisfaction but also drives business outcomes like increased conversions and loyalty. Interestingly, this optimization problem shares remarkable similarities with recommendation systems used in online platforms. By reframing call routing as a recommendation problem, we can leverage advanced machine learning techniques, such as two-tower neural networks, to improve routing efficiency significantly.</p><h2 id="The-Traditional-Call-Center-Routing-Challenge"><a href="#The-Traditional-Call-Center-Routing-Challenge" class="headerlink" title="The Traditional Call Center Routing Challenge"></a>The Traditional Call Center Routing Challenge</h2><p>Traditional call center routing often relies on simple rules or basic segmentation:</p><ul><li><strong>Skills-Based Routing</strong>: Matches customers with agents based on predefined skills (e.g., language, technical expertise).</li><li><strong>Priority Routing</strong>: High-value customers are directed to specialized agents or faster queues.</li><li><strong>Round Robin or Idle Agent Routing</strong>: Distributes calls evenly or routes to the next available agent.</li></ul><p>While these methods are straightforward, they may not account for the nuanced interactions that lead to successful outcomes like sales conversions or high customer satisfaction scores.</p><h2 id="From-Routing-to-Recommendation"><a href="#From-Routing-to-Recommendation" class="headerlink" title="From Routing to Recommendation"></a>From Routing to Recommendation</h2><p>Imagine each incoming call as a user visiting a website and the pool of available agents as content items to recommend. The goal shifts from merely connecting the next available agent to finding the <strong>best match</strong> between a customer and an agent to maximize a desired outcome, such as:</p><ul><li>Conversion rates</li><li>Customer satisfaction scores</li><li>First-call resolution rates</li></ul><p>By modeling call routing as a recommendation problem, we can utilize sophisticated algorithms to predict the optimal agent for each customer.</p><h2 id="Understanding-Customer-and-Agent-Embeddings"><a href="#Understanding-Customer-and-Agent-Embeddings" class="headerlink" title="Understanding Customer and Agent Embeddings"></a>Understanding Customer and Agent Embeddings</h2><p>Central to modern recommendation systems is the concept of <strong>embeddings</strong>—dense vector representations that capture the characteristics of entities (users and items).</p><h3 id="Customer-Embeddings"><a href="#Customer-Embeddings" class="headerlink" title="Customer Embeddings"></a>Customer Embeddings</h3><p>Customer embeddings encode information such as:</p><ul><li>Demographics (age, location)</li><li>Interaction history (past calls, purchases)</li><li>Behavior patterns (call frequency, preferred channels)</li></ul><h3 id="Agent-Embeddings"><a href="#Agent-Embeddings" class="headerlink" title="Agent Embeddings"></a>Agent Embeddings</h3><p>Agent embeddings capture attributes like:</p><ul><li>Expertise areas</li><li>Performance metrics (conversion rates, customer feedback)</li><li>Communication style</li></ul><p>By representing both customers and agents in a shared vector space, we can measure the similarity or compatibility between them.</p><h2 id="Two-Tower-Recommender-Systems"><a href="#Two-Tower-Recommender-Systems" class="headerlink" title="Two-Tower Recommender Systems"></a>Two-Tower Recommender Systems</h2><p>A two-tower (or dual-tower) model is an architecture commonly used in recommendation systems, particularly suitable for matching problems.</p><h3 id="Architecture-Overview"><a href="#Architecture-Overview" class="headerlink" title="Architecture Overview"></a>Architecture Overview</h3><ul><li><strong>User Tower</strong>: Processes customer features to generate customer embeddings.</li><li><strong>Item Tower</strong>: Processes agent features to produce agent embeddings.</li><li><strong>Interaction</strong>: The embeddings from both towers are combined (e.g., via dot product) to predict the likelihood of a successful interaction.</li></ul><h3 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h3><p>The model is trained on historical interaction data:</p><ul><li><strong>Positive Examples</strong>: Successful past customer-agent interactions.</li><li><strong>Negative Examples</strong>: Less successful or neutral interactions.</li></ul><p>The loss function aims to maximize the predicted success metric for actual successful interactions while minimizing it for others.</p><h3 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h3><p>At runtime:</p><ol><li><strong>Customer Embedding Generation</strong>: When a call comes in, the customer’s embedding is computed in real-time.</li><li><strong>Agent Embedding Retrieval</strong>: Embeddings for available agents are retrieved from a precomputed database.</li><li><strong>Matching</strong>: The system computes compatibility scores and routes the call to the best-matched agent.</li></ol><h2 id="Benefits-of-the-Recommendation-Approach"><a href="#Benefits-of-the-Recommendation-Approach" class="headerlink" title="Benefits of the Recommendation Approach"></a>Benefits of the Recommendation Approach</h2><ul><li><strong>Personalization</strong>: Tailors interactions based on nuanced customer and agent profiles.</li><li><strong>Scalability</strong>: Efficiently handles large numbers of customers and agents.</li><li><strong>Data-Driven</strong>: Continuously improves as more interaction data becomes available.</li><li><strong>Outcome Optimization</strong>: Directly targets business metrics like conversion rates.</li></ul><h2 id="Challenges-and-Considerations"><a href="#Challenges-and-Considerations" class="headerlink" title="Challenges and Considerations"></a>Challenges and Considerations</h2><ul><li><strong>Data Quality</strong>: Requires extensive and accurate historical interaction data.</li><li><strong>Cold Start Problem</strong>: New customers or agents with little data may be hard to model.</li><li><strong>Real-Time Constraints</strong>: Must generate embeddings and compute matches quickly to avoid delays.</li><li><strong>Fairness and Bias</strong>: Ensure the model does not inadvertently favor or discriminate against certain customers or agents.</li><li><strong>Privacy Concerns</strong>: Handle sensitive customer data responsibly, complying with regulations like GDPR.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Transforming call center routing optimization into a recommendation problem offers a powerful framework to enhance customer-agent matching. By leveraging techniques like embeddings and two-tower neural networks, businesses can significantly improve key performance metrics and deliver a more personalized customer experience. As with any machine learning application, it’s essential to address challenges like data quality and ethical considerations to ensure the solution is effective and responsible.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommender system </tag>
            
            <tag> routing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to evaluate json string column in databricks sql</title>
      <link href="2024/10/15/evaluate_json_string_column_in_databricks_sql/"/>
      <url>2024/10/15/evaluate_json_string_column_in_databricks_sql/</url>
      
        <content type="html"><![CDATA[<p>When working with JSON data stored as strings in a Databricks table, you might need to extract specific fields and perform computations on them. In this guide, we’ll explore how to extract the <code>total_cost</code> field from a JSON string column named <code>metadata</code> and calculate its average using SQL in Databricks.</p><h2 id="The-Challenge"><a href="#The-Challenge" class="headerlink" title="The Challenge"></a>The Challenge</h2><p>Suppose you have a table <code>my_table</code> with a column <code>metadata</code> containing JSON strings like:</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;<span class="attr">&quot;total_cost&quot;</span>: <span class="string">&quot;150.75&quot;</span>, <span class="attr">&quot;other_data&quot;</span>: <span class="string">&quot;...&quot;</span>&#125;</span><br></pre></td></tr></table></figure><p>Your goal is to:</p><ul><li>Access the <code>total_cost</code> key within the JSON string.</li><li>Convert the extracted value to a numeric type.</li><li>Calculate the average of all <code>total_cost</code> values.</li></ul><h2 id="Solution-Overview"><a href="#Solution-Overview" class="headerlink" title="Solution Overview"></a>Solution Overview</h2><p>We’ll use the following functions:</p><ul><li><code>get_json_object</code>: Extracts a JSON value as a string.</li><li><code>TRY_CAST</code>: Safely converts a string to a numeric type, returning <code>NULL</code> if the conversion fails.</li><li><code>AVG</code>: Calculates the average of numeric values, ignoring <code>NULL</code>s.</li></ul><h2 id="SQL-Query"><a href="#SQL-Query" class="headerlink" title="SQL Query"></a>SQL Query</h2><p>Here is the SQL query to achieve this:</p><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">    <span class="built_in">AVG</span>(TRY_CAST(</span><br><span class="line">        get_json_object(metadata, <span class="string">&#x27;$.total_cost&#x27;</span>) <span class="keyword">AS</span> <span class="keyword">DOUBLE</span></span><br><span class="line">    )) <span class="keyword">AS</span> average_total_cost</span><br><span class="line"><span class="keyword">FROM</span> </span><br><span class="line">    my_table;</span><br></pre></td></tr></table></figure><h3 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h3><ul><li><p><strong><code>get_json_object(metadata, &#39;$.total_cost&#39;)</code></strong>:</p><ul><li>Extracts the value associated with the <code>total_cost</code> key from the <code>metadata</code> JSON string.</li><li>The <code>&#39;$.total_cost&#39;</code> argument specifies the path to the <code>total_cost</code> field in the JSON object.</li></ul></li><li><p><strong><code>TRY_CAST(... AS DOUBLE)</code></strong>:</p><ul><li>Attempts to convert the extracted <code>total_cost</code> string to a <code>DOUBLE</code> (a floating-point number).</li><li>If the conversion fails (e.g., if the value is not a valid number), it returns <code>NULL</code> instead of throwing an error.</li></ul></li><li><p><strong><code>AVG(...)</code></strong>:</p><ul><li>Computes the average of all the <code>DOUBLE</code> values.</li><li>Automatically ignores <code>NULL</code> values, so any failed conversions won’t affect the result.</li></ul></li></ul><h3 id="Steps-Breakdown"><a href="#Steps-Breakdown" class="headerlink" title="Steps Breakdown"></a>Steps Breakdown</h3><ol><li><p><strong>Extract the <code>total_cost</code> Value</strong>:</p><ul><li>Use <code>get_json_object</code> to parse the JSON string and retrieve the <code>total_cost</code> value.</li><li>Example:<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">get_json_object(<span class="string">&#x27;&#123;&quot;total_cost&quot;: &quot;150.75&quot;&#125;&#x27;</span>, <span class="string">&#x27;$.total_cost&#x27;</span>)  <span class="comment">-- Returns &quot;150.75&quot;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><strong>Convert to a Numeric Type</strong>:</p><ul><li>Use <code>TRY_CAST</code> to safely convert the extracted string to a <code>DOUBLE</code>.</li><li>Example:<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">TRY_CAST(&quot;150.75&quot; <span class="keyword">AS</span> <span class="keyword">DOUBLE</span>)  <span class="comment">-- Returns 150.75 (as a DOUBLE)</span></span><br></pre></td></tr></table></figure></li><li>If the string is not a valid number, <code>TRY_CAST</code> returns <code>NULL</code>.</li></ul></li><li><p><strong>Calculate the Average</strong>:</p><ul><li>Use <code>AVG</code> to compute the average of all <code>DOUBLE</code> values.</li><li><code>AVG</code> ignores <code>NULL</code> values, so any non-numeric or missing <code>total_cost</code> entries won’t affect the calculation.</li></ul></li></ol><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><p>Assume <code>my_table</code> contains the following <code>metadata</code> values:</p><table><thead><tr><th>metadata</th></tr></thead><tbody><tr><td><code>&#123;&quot;total_cost&quot;: &quot;100.50&quot;, &quot;other_data&quot;: &quot;...&quot;&#125;</code></td></tr><tr><td><code>&#123;&quot;total_cost&quot;: &quot;200.25&quot;, &quot;other_data&quot;: &quot;...&quot;&#125;</code></td></tr><tr><td><code>&#123;&quot;total_cost&quot;: &quot;invalid_number&quot;, &quot;other_data&quot;: &quot;...&quot;&#125;</code></td></tr><tr><td><code>&#123;&quot;other_data&quot;: &quot;...&quot;&#125;</code></td></tr></tbody></table><p>Applying the query:</p><ol><li><p><strong>Extract and Convert</strong>:</p><table><thead><tr><th>Extracted <code>total_cost</code></th><th>Converted to <code>DOUBLE</code></th></tr></thead><tbody><tr><td><code>&quot;100.50&quot;</code></td><td><code>100.50</code></td></tr><tr><td><code>&quot;200.25&quot;</code></td><td><code>200.25</code></td></tr><tr><td><code>&quot;invalid_number&quot;</code></td><td><code>NULL</code></td></tr><tr><td><code>NULL</code> (missing key)</td><td><code>NULL</code></td></tr></tbody></table></li><li><p><strong>Compute Average</strong>:</p><ul><li>Average of <code>100.50</code> and <code>200.25</code> (ignoring <code>NULL</code>s):<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">(<span class="number">100.50</span> <span class="operator">+</span> <span class="number">200.25</span>) <span class="operator">/</span> <span class="number">2</span> <span class="operator">=</span> <span class="number">150.375</span></span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>By combining <code>get_json_object</code>, <code>TRY_CAST</code>, and <code>AVG</code>, you can efficiently extract numeric values from JSON strings and perform aggregate calculations in Databricks SQL. This method handles invalid or missing data gracefully, ensuring accurate and reliable results.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ensuring Proper Dependency Installation for Python Packages, `sentence-transformers` as an example</title>
      <link href="2024/09/23/ensure_proper_dependency_installation_for_python_package/"/>
      <url>2024/09/23/ensure_proper_dependency_installation_for_python_package/</url>
      
        <content type="html"><![CDATA[<p>When working with Python packages, it’s crucial to install all dependencies correctly to avoid runtime errors and unexpected issues. This is especially true for sophisticated packages like <code>sentence-transformers</code>. A common mistake is to overlook the specific versions of dependencies, which can lead to perplexing errors.</p><p>For instance, <code>sentence-transformers</code> depends on the <code>transformers</code> package. Without the correct version, you might encounter errors and spend a lot of time troubleshooting. Knowing how to find and verify these dependencies is a key skill for developers.</p><h2 id="Checking-Dependencies-on-PyPI"><a href="#Checking-Dependencies-on-PyPI" class="headerlink" title="Checking Dependencies on PyPI"></a>Checking Dependencies on PyPI</h2><p>The official source for package information is the Python Package Index (PyPI). For <code>sentence-transformers</code>, the PyPI page is:</p><p><a href="https://pypi.org/project/sentence-transformers">PyPI - sentence-transformers</a></p><p>Here, you can find the package’s summary, versions, and dependencies. If you need a specific version, you can check the release history here:</p><p><a href="https://pypi.org/project/sentence-transformers/#history">Release History - sentence-transformers</a></p><p>For example, for version 2.5.0 of <code>sentence-transformers</code>, the release notes specify:</p><blockquote><p>“We recommend Python 3.8 or higher, PyTorch 1.11.0 or higher, and transformers v4.32.0 or higher. The code does not work with Python 2.7.”</p></blockquote><p>From this, you know to install <code>transformers</code> version 4.32.0 or higher.</p><h2 id="Other-Methods-to-Check-Dependencies"><a href="#Other-Methods-to-Check-Dependencies" class="headerlink" title="Other Methods to Check Dependencies"></a>Other Methods to Check Dependencies</h2><h3 id="Using-pip-show"><a href="#Using-pip-show" class="headerlink" title="Using pip show"></a>Using <code>pip show</code></h3><p>The <code>pip show</code> command provides metadata about a package, including its dependencies. </p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip show sentence-transformers</span><br></pre></td></tr></table></figure><p>This command will output various details, including <code>Requires</code>, which lists the required dependencies.</p><h3 id="Using-pipdeptree"><a href="#Using-pipdeptree" class="headerlink" title="Using pipdeptree"></a>Using <code>pipdeptree</code></h3><p><code>pipdeptree</code> is a powerful tool to visualize the dependency tree of installed packages.</p><p>To install <code>pipdeptree</code>:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip install pipdeptree</span><br></pre></td></tr></table></figure><p>To display the tree of dependencies:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pipdeptree</span><br></pre></td></tr></table></figure><h3 id="Checking-requirements-txt-or-setup-py"><a href="#Checking-requirements-txt-or-setup-py" class="headerlink" title="Checking requirements.txt or setup.py"></a>Checking <code>requirements.txt</code> or <code>setup.py</code></h3><p>If you have access to the source code, you can look at the <code>requirements.txt</code> or <code>setup.py</code> files for a list of dependencies.</p><p>Example of <code>requirements.txt</code>:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">transformers&gt;=4.32.0</span><br><span class="line">torch&gt;=1.11.0</span><br></pre></td></tr></table></figure><p>Example snippet from <code>setup.py</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">install_requires=[</span><br><span class="line">    <span class="string">&quot;transformers&gt;=4.32.0&quot;</span>,</span><br><span class="line">    <span class="string">&quot;torch&gt;=1.11.0&quot;</span></span><br><span class="line">],</span><br></pre></td></tr></table></figure><h3 id="Using-pip-freeze"><a href="#Using-pip-freeze" class="headerlink" title="Using pip freeze"></a>Using <code>pip freeze</code></h3><p>This command lists all installed packages and their versions.</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip freeze</span><br></pre></td></tr></table></figure><p>While it doesn’t directly show dependencies, you can infer the required versions.</p><h3 id="Exploring-Installed-Package-Metadata"><a href="#Exploring-Installed-Package-Metadata" class="headerlink" title="Exploring Installed Package Metadata"></a>Exploring Installed Package Metadata</h3><p>After installing <code>sentence-transformers</code>, you can navigate to its metadata directory in your <code>site-packages</code>.</p><p>Usually found under <code>[Python installation directory]/lib/site-packages/sentence_transformers-[version].dist-info</code>, you’ll find files like <code>METADATA</code> or <code>RECORD</code>, which include listed dependencies.</p><h3 id="Using-Package-Managers-like-Poetry"><a href="#Using-Package-Managers-like-Poetry" class="headerlink" title="Using Package Managers like Poetry"></a>Using Package Managers like Poetry</h3><p>Some modern Python project managers like <code>Poetry</code> provide commands to show dependencies in a more readable format.</p><p>For Poetry:</p><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">poetry show sentence-transformers --tree</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> sentence-transformers </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Streamlining Big Data and Deep Learning Workflows, Apache Spark, PyTorch, and Mosaic Streaming</title>
      <link href="2024/09/21/mosaic-streaming-for-pytorch-deep-learning/"/>
      <url>2024/09/21/mosaic-streaming-for-pytorch-deep-learning/</url>
      
        <content type="html"><![CDATA[<p>In the rapidly evolving fields of big data and machine learning, one of the common challenges faced by data scientists and machine learning engineers is bridging the gap between powerful data processing engines like Apache Spark and deep learning frameworks such as PyTorch. Leveraging the strengths of both systems can be daunting due to the inherent disparities in their architectures. This blog introduces Mosaic Streaming—a potent tool designed to make this integration simpler and more efficient. We will cover why your driver node needs a GPU for PyTorch, how to manage data with a Spark cluster, and how Mosaic Streaming optimizes data transfer between Spark and PyTorch.</p><h3 id="Why-the-Driver-Node-Needs-a-GPU-for-PyTorch"><a href="#Why-the-Driver-Node-Needs-a-GPU-for-PyTorch" class="headerlink" title="Why the Driver Node Needs a GPU for PyTorch"></a>Why the Driver Node Needs a GPU for PyTorch</h3><p>PyTorch is a popular deep learning framework that excels in training models on GPUs. When integrating Spark with PyTorch, understanding where the GPU sits and why it is essential for efficient training is crucial.</p><h4 id="GPU-on-the-Driver-Node"><a href="#GPU-on-the-Driver-Node" class="headerlink" title="GPU on the Driver Node"></a>GPU on the Driver Node</h4><p>When you use PyTorch for training models and involve Spark for data processing, the PyTorch operations happen on the driver node. PyTorch assumes the data to be locally available or accessible in a manner suited to batch processing on a single node. Thus, having a GPU on the driver node is imperative for the following reasons:</p><ul><li><p><strong>Compute Efficiency</strong>: PyTorch utilizes GPUs to accelerate matrix computations, which are foundational to deep learning.</p></li><li><p><strong>Data Transfer Overhead</strong>: Transferring data from Spark workers to a non-GPU driver and then to a GPU-enabled node can introduce significant latency and inefficiency. Keeping the GPU on the driver node minimizes this overhead.</p></li><li><p><strong>Simplified Workflow</strong>: Integrating a GPU directly on the driver node ensures that the entire pipeline from Spark processing to PyTorch training is streamlined and efficient.</p></li></ul><h3 id="Setting-Up-Your-Spark-Cluster-to-Manage-Data"><a href="#Setting-Up-Your-Spark-Cluster-to-Manage-Data" class="headerlink" title="Setting Up Your Spark Cluster to Manage Data"></a>Setting Up Your Spark Cluster to Manage Data</h3><p>Apache Spark is renowned for its capabilities to manage and process large-scale datasets in a distributed fashion. In the context of preparing data for machine learning, Spark excels at ETL (Extract, Transform, Load) operations.</p><h4 id="Step-by-Step-Setup"><a href="#Step-by-Step-Setup" class="headerlink" title="Step-by-Step Setup"></a>Step-by-Step Setup</h4><ol><li><p><strong>Initialize Spark Session</strong>:<br>Using Spark Session, you can easily load and process large datasets.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize Spark Session</span></span><br><span class="line">spark = SparkSession.builder\</span><br><span class="line">    .appName(<span class="string">&quot;CSV to PyTorch with GPU&quot;</span>)\</span><br><span class="line">    .getOrCreate()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load CSV data into a Spark DataFrame</span></span><br><span class="line">df = spark.read.csv(<span class="string">&quot;path_to_your_csv_file.csv&quot;</span>, header=<span class="literal">True</span>, inferSchema=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure></li></ol><h3 id="Leveraging-Mosaic-Streaming-for-Efficient-Data-Transfer"><a href="#Leveraging-Mosaic-Streaming-for-Efficient-Data-Transfer" class="headerlink" title="Leveraging Mosaic Streaming for Efficient Data Transfer"></a>Leveraging Mosaic Streaming for Efficient Data Transfer</h3><p>One of the significant bottlenecks in integrating Spark and PyTorch is the data transfer between the distributed Spark nodes and the PyTorch driver. Mosaic Streaming addresses this challenge effectively.</p><h4 id="Why-Use-Mosaic-Streaming"><a href="#Why-Use-Mosaic-Streaming" class="headerlink" title="Why Use Mosaic Streaming?"></a>Why Use Mosaic Streaming?</h4><ul><li><p><strong>Efficient Data Streaming</strong>: Facilitates incremental data streaming from Spark to PyTorch, optimizing memory and performance.</p></li><li><p><strong>Partition Handling</strong>: Automatically manages data partitioning, ensuring that the data fetching aligns with Spark’s distributed nature.</p></li><li><p><strong>Custom Dataset and DataLoader</strong>: Provides custom implementations, which fetch data on-demand, eliminating the need for manual <code>.collect()</code> operations.</p></li></ul><p>Below is a practical example of using Mosaic Streaming to load a CSV dataset from Spark directly into PyTorch efficiently.</p><h4 id="Define-PyTorch-Dataset-Using-Mosaic-Streaming"><a href="#Define-PyTorch-Dataset-Using-Mosaic-Streaming" class="headerlink" title="Define PyTorch Dataset Using Mosaic Streaming"></a>Define PyTorch Dataset Using Mosaic Streaming</h4><ol><li><p><strong>Custom Dataset</strong>:<br>Implement a custom dataset that streams data from Spark to PyTorch.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> mosaic.streaming <span class="keyword">import</span> StreamToTorchDataset</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkCSVToDataset</span>(<span class="params">StreamToTorchDataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, spark_df, feature_cols, label_col</span>):</span></span><br><span class="line">        self.spark_df = spark_df</span><br><span class="line">        self.feature_cols = feature_cols</span><br><span class="line">        self.label_col = label_col</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self, idx</span>):</span></span><br><span class="line">        row = self.spark_df[idx]</span><br><span class="line">        features = torch.tensor([row[col] <span class="keyword">for</span> col <span class="keyword">in</span> self.feature_cols], dtype=torch.float32).cuda()  <span class="comment"># Move to GPU</span></span><br><span class="line">        label = torch.tensor(row[self.label_col], dtype=torch.float32).cuda()  <span class="comment"># Move to GPU</span></span><br><span class="line">        <span class="keyword">return</span> features, label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.spark_df.count()</span><br><span class="line"></span><br><span class="line">feature_columns = [<span class="string">&quot;feature1&quot;</span>, <span class="string">&quot;feature2&quot;</span>, <span class="string">&quot;feature3&quot;</span>]  <span class="comment"># Replace with your feature column names</span></span><br><span class="line">label_column = <span class="string">&quot;label&quot;</span>  <span class="comment"># Replace with your label column name</span></span><br><span class="line"></span><br><span class="line">dataset = SparkCSVToDataset(df, feature_columns, label_column)</span><br></pre></td></tr></table></figure></li><li><p><strong>Create DataLoader for Batch Processing</strong>:<br>Utilize PyTorch’s DataLoader for efficient batch processing.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming you have a model and optimizer already defined</span></span><br><span class="line">model = YourModel().cuda()  <span class="comment"># Move model to GPU</span></span><br><span class="line">criterion = torch.nn.YourLossFunction().cuda()  <span class="comment"># Move loss function to GPU</span></span><br><span class="line">optimizer = torch.optim.YourOptimizer(model.parameters())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Training loop</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        inputs, labels = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure></li></ol><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>By ensuring that the driver node is equipped with a GPU and using Mosaic Streaming for efficient data transfer, you can significantly streamline the workflow from data processing in Spark to model training in PyTorch. This setup takes full advantage of Spark’s distributed processing capabilities and PyTorch’s GPU acceleration, enabling you to manage and process large datasets efficiently while training sophisticated deep learning models.</p><p>Mosaic Streaming abstracts away much of the complexity involved in handling large-scale data transfers, making it an invaluable tool for data scientists and engineers who aim to integrate the power of Spark and PyTorch in their workflows. With this approach, you can achieve significant improvements in training times and overall workflow efficiency, allowing you to focus on building and refining models rather than managing data logistics.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytroch </tag>
            
            <tag> mosaic streaming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is squash before merge in coding practice</title>
      <link href="2024/08/28/what-is-squash-before-merge-practice-in-coding/"/>
      <url>2024/08/28/what-is-squash-before-merge-practice-in-coding/</url>
      
        <content type="html"><![CDATA[<p>“Squash before merge” is a practice in version control systems, specifically in Git, where multiple smaller commits in a feature branch are combined into a single commit before merging that branch into a main branch like <code>main</code> or <code>master</code>. This is done to keep the commit history clean and concise, making it easier to understand the changes introduced by the feature branch.</p><p>Here’s how it typically works:</p><ol><li><strong>Developing the Feature</strong>: You create a feature branch from the main branch and make multiple commits as you develop the feature.</li><li><strong>Reviewing</strong>: Once the feature is complete, you open a pull request or merge request, which might go through a code review process. During this time, you might make additional commits based on feedback.</li><li><strong>Squashing Commits</strong>: Before merging the feature branch back into the main branch, you “squash” all of the commits in the feature branch into a single commit. This combined commit contains all the changes made in the feature branch.</li><li><strong>Merging</strong>: The single, squashed commit is then merged into the main branch.</li></ol><h3 id="Benefits-of-Squashing-Before-Merge"><a href="#Benefits-of-Squashing-Before-Merge" class="headerlink" title="Benefits of Squashing Before Merge"></a>Benefits of Squashing Before Merge</h3><ul><li><strong>Simpler History</strong>: A single commit makes the commit history cleaner and easier to follow.</li><li><strong>Atomic Changes</strong>: Each feature or fix is encapsulated in a single commit, making it easier to revert if necessary.</li><li><strong>Easier Code Reviews</strong>: Reviewers can focus on the overall changes rather than sifting through many smaller, possibly inconsequential commits.</li></ul><h3 id="How-to-Squash-Commits"><a href="#How-to-Squash-Commits" class="headerlink" title="How to Squash Commits"></a>How to Squash Commits</h3><p>You can squash commits before merging using Git’s interactive rebase feature:</p><ol><li><p><strong>Rebase Interactive</strong>: </p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git rebase -i HEAD~n</span><br></pre></td></tr></table></figure><p>Replace <code>n</code> with the number of commits you want to squash.</p></li><li><p><strong>Mark Commits to Squash</strong>: In the interactive rebase screen, mark the commits you want to squash by replacing <code>pick</code> with <code>squash</code> or <code>s</code>. </p></li><li><p><strong>Reword Commit Message</strong>: After marking the commits to squash, you’ll be prompted to edit the commit message for the squashed commit. You can combine the messages or write a new one summarizing the changes.</p></li><li><p><strong>Push the Rebase</strong>:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git push --force</span><br></pre></td></tr></table></figure><p>Since rebase rewrites the commit history, you’ll need to force-push the changes to your remote branch.</p></li></ol><p>Some version control systems and tools like GitHub also offer an option to “Squash and merge” directly from the pull request interface, making it even more straightforward without needing to use the command line.</p><p>Using the “squash before merge” practice helps streamline your project’s commit history and enhances the clarity and maintainability of your codebase.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Simplifying Data Serialization with Pydantic, `.dict()` and `.json()`</title>
      <link href="2024/08/15/convert-pydantic-model-to-json-objects/"/>
      <url>2024/08/15/convert-pydantic-model-to-json-objects/</url>
      
        <content type="html"><![CDATA[<p>When working with data models in Python, Pydantic is a fantastic library that streamlines validation and serialization. One of its most powerful features is the ease with which you can convert Pydantic classes into JSON format. In this blog post, we’ll explore how to achieve this using the <code>.dict()</code> and <code>.json()</code> methods, understand their differences, and see how to exclude null keys from the output.</p><h3 id="Easy-JSON-Conversion-with-Pydantic"><a href="#Easy-JSON-Conversion-with-Pydantic" class="headerlink" title="Easy JSON Conversion with Pydantic"></a>Easy JSON Conversion with Pydantic</h3><p>Pydantic allows you to define data models using Python classes, which can then be effortlessly converted to JSON format. This can be particularly useful when building APIs or working with data interchange formats. To convert a Pydantic class to JSON, you can use either the <code>.dict()</code> or <code>.json()</code> methods. Here’s a quick example to illustrate:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> BaseModel</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">BaseModel:</span></span></span><br><span class="line"><span class="params"><span class="class">    <span class="built_in">id</span>: <span class="built_in">int</span></span></span></span><br><span class="line"><span class="params"><span class="class">    name: <span class="built_in">str</span></span></span></span><br><span class="line"><span class="params"><span class="class">    email: <span class="built_in">str</span> = <span class="literal">None</span></span></span></span><br><span class="line"><span class="params"><span class="class"></span></span></span><br><span class="line"><span class="params"><span class="class">user = User(<span class="params"><span class="built_in">id</span>=<span class="number">1</span>, name=<span class="string">&quot;Alice&quot;</span></span>)</span></span></span><br></pre></td></tr></table></figure><p>To convert this user instance to JSON format, you simply use:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Using .dict()</span></span><br><span class="line">user_dict = user.<span class="built_in">dict</span>()</span><br><span class="line"><span class="built_in">print</span>(user_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using .json()</span></span><br><span class="line">user_json = user.json()</span><br><span class="line"><span class="built_in">print</span>(user_json)</span><br></pre></td></tr></table></figure><h3 id="dict-vs-json-What’s-the-Difference"><a href="#dict-vs-json-What’s-the-Difference" class="headerlink" title=".dict() vs. .json(): What’s the Difference?"></a><code>.dict()</code> vs. <code>.json()</code>: What’s the Difference?</h3><p>While both methods serve the purpose of serialization, they do so in slightly different ways:</p><ul><li><p><strong><code>.dict()</code></strong>: This method converts the Pydantic model into a standard Python dictionary. The result can be pretty printed, modified, or further processed before being converted to JSON.</p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&#x27;id&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&#x27;name&#x27;</span>: <span class="string">&#x27;Alice&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>: <span class="literal">None</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p><strong><code>.json()</code></strong>: This method converts the Pydantic model directly into a JSON-formatted string. This string can be directly sent as a response in web APIs or written to a file.</p>  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="string">&#x27;&#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;Alice&quot;, &quot;email&quot;: null&#125;&#x27;</span></span><br></pre></td></tr></table></figure></li></ul><p>In summary, if you need a Python dictionary for further manipulation, go with <code>.dict()</code>. If you need a JSON string for transmission or storage, <code>.json()</code> is the way to go.</p><h3 id="Excluding-Null-Values"><a href="#Excluding-Null-Values" class="headerlink" title="Excluding Null Values"></a>Excluding Null Values</h3><p>One handy feature of the <code>.dict()</code> method is the ability to exclude null values from the resulting dictionary. You can achieve this by setting <code>exclude_none=True</code>. This is particularly helpful when you want to produce cleaner outputs by omitting keys with null values. Note that <code>exclude_none</code> only works with <code>.dict()</code> but not directly with <code>.json()</code>. However, you can combine both methods like this:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Excluding null values using .dict()</span></span><br><span class="line">user_dict_without_nulls = user.<span class="built_in">dict</span>(exclude_none=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(user_dict_without_nulls)</span><br><span class="line"><span class="comment"># Output: &#123;&#x27;id&#x27;: 1, &#x27;name&#x27;: &#x27;Alice&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To convert to JSON after excluding nulls</span></span><br><span class="line">user_json_without_nulls = user.json(exclude_none=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(user_json_without_nulls)</span><br><span class="line"><span class="comment"># Output: &#x27;&#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;Alice&quot;&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>There you have it! Leveraging Pydantic’s <code>.dict()</code> and <code>.json()</code> methods makes converting data models to JSON a breeze, and excluding null values ensures that your output remains clean and concise. Experiment with these methods in your own projects to see the benefits they bring to data serialization and deserialization.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pydantic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write a good resume, and a good template for reference</title>
      <link href="2024/08/03/how-to-write-a-good-resume-for-work/"/>
      <url>2024/08/03/how-to-write-a-good-resume-for-work/</url>
      
        <content type="html"><![CDATA[<p>A well-crafted resume is essential for making a positive impression on potential employers. It’s your first opportunity to showcase your skills, experience, and achievements, which makes it crucial to get it right. Here’s a guide to help you create an outstanding resume, using a professional template as a reference.</p><p><img src="/content/images/2024-08-03-01.jpg" alt="Resume Template"></p><h2 id="1-Contact-Information"><a href="#1-Contact-Information" class="headerlink" title="1. Contact Information"></a>1. Contact Information</h2><p>Start by providing your contact details at the top of your resume. This section should include:</p><ul><li>Your Full Name</li><li>Professional Title</li><li>Phone Number</li><li>Email Address</li><li>LinkedIn Profile or Professional Website (optional)</li></ul><p>Ensure that your email address and phone number are up-to-date, and use a professional email address.</p><h2 id="2-About-Me"><a href="#2-About-Me" class="headerlink" title="2. About Me"></a>2. About Me</h2><p>A brief “About Me” section gives a snapshot of your professional background. This introduction should highlight:</p><ul><li>Your key qualifications</li><li>Relevant skills</li><li>Main accomplishments</li><li>Professional goals</li></ul><p>This section is your elevator pitch, so keep it concise and impactful.</p><h2 id="3-Education"><a href="#3-Education" class="headerlink" title="3. Education"></a>3. Education</h2><p>List your educational background in reverse chronological order, starting with the most recent degree. For each entry, include:</p><ul><li>Degree and Major</li><li>School’s Name</li><li>Location</li><li>Dates Attended</li></ul><p>This provides the hiring manager with a quick overview of your academic qualifications.</p><h2 id="4-Work-Experience"><a href="#4-Work-Experience" class="headerlink" title="4. Work Experience"></a>4. Work Experience</h2><p>The work experience section is one of the most important parts of your resume. Format each job entry with:</p><ul><li>Company Name</li><li>Role/Position</li><li>Location</li><li>Dates of Employment</li><li>Key Responsibilities and Achievements</li></ul><p>Your descriptions should be detailed and showcase your tangible contributions and achievements. Use bullet points to keep the information organized and easy to read.</p><h2 id="5-Skills"><a href="#5-Skills" class="headerlink" title="5. Skills"></a>5. Skills</h2><p>Include a skills section to highlight your core competencies. This section should contain both soft skills (like communication and time management) and hard skills (such as project management and analytics).</p><h2 id="6-Achievements"><a href="#6-Achievements" class="headerlink" title="6. Achievements"></a>6. Achievements</h2><p>An achievements section can set you apart from other candidates by showcasing your unique accomplishments. List any awards, recognitions, or major projects you’ve completed relevant to the job you’re applying for. Include:</p><ul><li>The Achievement</li><li>The Issuing Organization</li><li>Date Received</li></ul><h2 id="Additional-Tips"><a href="#Additional-Tips" class="headerlink" title="Additional Tips"></a>Additional Tips</h2><ul><li><strong>Tailor Your Resume:</strong> Customize your resume for each job application to align with the job description.</li><li><strong>Use Keywords:</strong> Incorporate keywords from the job description to pass through Applicant Tracking Systems (ATS).</li><li><strong>Keep It Concise:</strong> Aim for a one-page resume, especially if you have less than 10 years of experience.</li><li><strong>Professional Format:</strong> Use a clean, professional layout like the provided template for clarity and easy readability.</li><li><strong>Proofread:</strong> Ensure there are no spelling or grammatical errors.</li></ul><p>By following these guidelines and using a structured template, you can create a resume that effectively showcases your skills, experience, and accomplishments, increasing your chances of landing that job interview.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> resume </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Node.js for Python Developers, A Comparison with FastAPI</title>
      <link href="2024/07/20/understanding-nodejs-from-python-perspective-such-as-fastapi/"/>
      <url>2024/07/20/understanding-nodejs-from-python-perspective-such-as-fastapi/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>As a developer with a background in building web applications using Python frameworks like Flask and FastAPI, you might be curious about Node.js and why it’s so popular for server-side development. This blog will introduce you to Node.js, the reasons why it’s frequently paired with npm (Node Package Manager), and how frameworks like Vue.js fit into the picture. We’ll also draw comparisons to your familiar Python environment to make the transition smoother.</p><h2 id="What-is-Node-js"><a href="#What-is-Node-js" class="headerlink" title="What is Node.js?"></a>What is Node.js?</h2><p><strong>Node.js</strong> is a runtime environment that allows you to execute JavaScript on the server side. Unlike Python, which executes synchronously by default, Node.js uses an event-driven, non-blocking I/O model. This makes it particularly efficient for I/O-heavy tasks such as real-time applications and large data fetch operations.</p><h3 id="Key-Features-of-Node-js"><a href="#Key-Features-of-Node-js" class="headerlink" title="Key Features of Node.js"></a>Key Features of Node.js</h3><ol><li><p><strong>Non-blocking I/O</strong>:</p><ul><li>Node.js can handle multiple operations concurrently without waiting for an I/O operation to complete. This makes it highly efficient and performant.</li></ul></li><li><p><strong>Event-Driven Architecture</strong>:</p><ul><li>The core architecture of Node.js is based on an event loop, allowing it to handle numerous simultaneous connections by efficiently managing asynchronous operations.</li></ul></li><li><p><strong>JavaScript Everywhere</strong>:</p><ul><li>Using JavaScript for both client-side and server-side development eliminates the context-switching between languages, making development more cohesive.</li></ul></li></ol><h2 id="A-Comparison-Node-js-vs-FastAPI"><a href="#A-Comparison-Node-js-vs-FastAPI" class="headerlink" title="A Comparison: Node.js vs. FastAPI"></a>A Comparison: Node.js vs. FastAPI</h2><p>Let’s draw a comparison to understand the similarities and differences between Node.js and FastAPI:</p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><ul><li><strong>Node.js</strong>: Utilizes a single-threaded event loop for asynchronous I/O operations.</li><li><strong>FastAPI</strong>: Relies on Python coroutines and the <code>asyncio</code> library to achieve asynchronous I/O.</li></ul><h3 id="Performance"><a href="#Performance" class="headerlink" title="Performance"></a>Performance</h3><ul><li><strong>Node.js</strong>: Excels in handling I/O-bound tasks and large numbers of concurrent connections.</li><li><strong>FastAPI</strong>: While also excellent for I/O-bound tasks, FastAPI provides an additional layer of type validation and is built on top of Uvicorn (an ASGI server), providing impressive performance for asynchronous operations.</li></ul><h3 id="Ecosystem-and-Libraries"><a href="#Ecosystem-and-Libraries" class="headerlink" title="Ecosystem and Libraries"></a>Ecosystem and Libraries</h3><ul><li><strong>Node.js</strong>: Benefits from npm, which hosts a vast ecosystem of packages for almost any functionality required.</li><li><strong>FastAPI</strong>: Leverages Python’s comprehensive libraries and package managers like pip, but the ecosystem for asynchronous libraries is still catching up compared to Node.js.</li></ul><h2 id="Why-Use-npm-with-Node-js"><a href="#Why-Use-npm-with-Node-js" class="headerlink" title="Why Use npm with Node.js?"></a>Why Use npm with Node.js?</h2><p><strong>npm (Node Package Manager)</strong> is the default package manager for Node.js. Its importance cannot be understated, and here’s why:</p><h3 id="Dependency-Management"><a href="#Dependency-Management" class="headerlink" title="Dependency Management"></a>Dependency Management</h3><ul><li><strong>Ease of Installation</strong>: npm simplifies the process of installing and managing dependencies. For example, installing Express.js, a web framework, is as simple as running <code>npm install express</code>.</li></ul><h3 id="Community-and-Packages"><a href="#Community-and-Packages" class="headerlink" title="Community and Packages"></a>Community and Packages</h3><ul><li><strong>Vast Ecosystem</strong>: npm hosts thousands of packages that can be integrated seamlessly into your projects. Whether you need a database client, a testing framework, or a utility library, npm likely has it.</li></ul><h3 id="Consistency-and-Versioning"><a href="#Consistency-and-Versioning" class="headerlink" title="Consistency and Versioning"></a>Consistency and Versioning</h3><ul><li><strong>Version Control</strong>: Specify exact versions of dependencies to ensure consistency across different environments. Version management eliminates the “it works on my machine” problem.</li></ul><h2 id="Why-Vue-js-with-Node-js"><a href="#Why-Vue-js-with-Node-js" class="headerlink" title="Why Vue.js with Node.js?"></a>Why Vue.js with Node.js?</h2><p><strong>Vue.js</strong> is a progressive JavaScript framework for building user interfaces, and it’s often combined with Node.js in full-stack JavaScript projects. Here’s why:</p><h3 id="Reactive-and-Component-Based-Architecture"><a href="#Reactive-and-Component-Based-Architecture" class="headerlink" title="Reactive and Component-Based Architecture"></a>Reactive and Component-Based Architecture</h3><ul><li><strong>Ease of Use</strong>: Vue.js allows you to build interactive UIs using a component-based architecture, making it easier to manage and scale your frontend code.</li><li><strong>Reactivity</strong>: Vue’s reactivity system ensures that your UI updates efficiently when the underlying data changes.</li></ul><h3 id="Vue-CLI-and-npm"><a href="#Vue-CLI-and-npm" class="headerlink" title="Vue CLI and npm"></a>Vue CLI and npm</h3><ul><li><strong>Tooling</strong>: The Vue CLI is an npm package that streamlines the development process by automating configurations for various tools like Babel, ESLint, and Webpack. Installing it is as simple as <code>npm install -g @vue/cli</code>.</li></ul><h3 id="Unified-Development-Experience"><a href="#Unified-Development-Experience" class="headerlink" title="Unified Development Experience"></a>Unified Development Experience</h3><ul><li><strong>JavaScript End-to-End</strong>: By using JavaScript for both backend (Node.js) and frontend (Vue.js) development, you achieve a more unified and streamlined development workflow.</li></ul><h2 id="Getting-Started-Building-a-Simple-Server-with-Node-js-and-Express"><a href="#Getting-Started-Building-a-Simple-Server-with-Node-js-and-Express" class="headerlink" title="Getting Started: Building a Simple Server with Node.js and Express"></a>Getting Started: Building a Simple Server with Node.js and Express</h2><p>Here’s a brief example to get you up and running with a simple server using Node.js and Express:</p><ol><li><p><strong>Install Node.js</strong>:</p><ul><li>Install Node.js from <a href="https://nodejs.org/">nodejs.org</a>. This also installs npm.</li></ul></li><li><p><strong>Setting Up a Simple Server</strong>:</p> <figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Import the express module</span></span><br><span class="line"><span class="keyword">const</span> express = <span class="built_in">require</span>(<span class="string">&#x27;express&#x27;</span>);</span><br><span class="line"><span class="keyword">const</span> app = express();</span><br><span class="line"><span class="keyword">const</span> PORT = <span class="number">3000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Define a route handler for the default home page</span></span><br><span class="line">app.get(<span class="string">&#x27;/&#x27;</span>, <span class="function">(<span class="params">req, res</span>) =&gt;</span> &#123;</span><br><span class="line">  res.send(<span class="string">&#x27;Hello, World!&#x27;</span>);</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">// Start the server</span></span><br><span class="line">app.listen(PORT, <span class="function">() =&gt;</span> &#123;</span><br><span class="line">  <span class="built_in">console</span>.log(<span class="string">`Server running at http://localhost:<span class="subst">$&#123;PORT&#125;</span>/`</span>);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure></li><li><p><strong>Run Your Server</strong>:</p> <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">node app.js</span><br></pre></td></tr></table></figure></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Transitioning from Python-based frameworks like Flask and FastAPI to Node.js doesn’t have to be daunting. Understanding the core concepts and advantages of Node.js, coupled with the powerful package management offered by npm, can streamline your development process. Additionally, Vue.js integrates seamlessly to provide a modern, reactive frontend.</p><p>By adopting these tools, you can leverage JavaScript end-to-end, from server to client, and take advantage of the vast ecosystem and high performance that Node.js offers.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fastapi </tag>
            
            <tag> node.js </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Measuring Average Latency with FAISS on Apple M1</title>
      <link href="2024/07/07/measure-average-latency-with-faiss-vector-store/"/>
      <url>2024/07/07/measure-average-latency-with-faiss-vector-store/</url>
      
        <content type="html"><![CDATA[<p>FAISS (Facebook AI Similarity Search) is a powerful library for efficient similarity search and clustering of dense vectors. It’s optimized for high performance and scalability, making it an excellent choice for handling large datasets and computing nearest neighbors.</p><p>In this short blog, we’ll explore how to measure the average latency of searching 1000 records using FAISS on a Mac with an Apple M1 processor.</p><h2 id="Setting-Up-the-Environment"><a href="#Setting-Up-the-Environment" class="headerlink" title="Setting Up the Environment"></a>Setting Up the Environment</h2><p>To get started, you’ll need to install FAISS and ensure you have appropriate development tools and libraries. You can install FAISS using pip:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install faiss-cpu</span><br></pre></td></tr></table></figure><p>Next, we set up our environment by importing necessary packages and defining constants.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define constants</span></span><br><span class="line">num_vectors = <span class="number">500000</span></span><br><span class="line">dim = <span class="number">20</span></span><br><span class="line">num_searches = <span class="number">1000</span></span><br><span class="line">k = <span class="number">100</span> <span class="comment"># Number of nearest neighbors to retrieve</span></span><br></pre></td></tr></table></figure><h2 id="Generating-Random-Vectors"><a href="#Generating-Random-Vectors" class="headerlink" title="Generating Random Vectors"></a>Generating Random Vectors</h2><p>FAISS works with dense vectors. Here, we generate random vectors that simulate our dataset:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create random vectors</span></span><br><span class="line">np.random.seed(<span class="number">42</span>) <span class="comment"># For reproducibility</span></span><br><span class="line">vectors = np.random.random((num_vectors, dim)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Initializing-the-FAISS-Index"><a href="#Initializing-the-FAISS-Index" class="headerlink" title="Initializing the FAISS Index"></a>Initializing the FAISS Index</h2><p>We initialize the FAISS index using <code>IndexFlatL2</code>, which computes the Euclidean (L2) distance between vectors:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Initialize FAISS index</span></span><br><span class="line">index = faiss.IndexFlatL2(dim) <span class="comment"># L2 distance (Euclidean)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Add vectors to the index</span></span><br><span class="line">index.add(vectors)</span><br></pre></td></tr></table></figure><h2 id="Measuring-Latency"><a href="#Measuring-Latency" class="headerlink" title="Measuring Latency"></a>Measuring Latency</h2><p>To measure the latency of search operations, we generate a query vector and perform multiple searches, recording the time taken for each one:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Generate query vector</span></span><br><span class="line">query_vector = np.random.random((<span class="number">1</span>, dim)).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Measure latency of searches</span></span><br><span class="line">latencies = []</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_searches):</span><br><span class="line">  start_time = time.time()</span><br><span class="line">  D, I = index.search(query_vector, k) <span class="comment"># search for k nearest neighbors</span></span><br><span class="line">  latencies.append(time.time() - start_time)</span><br></pre></td></tr></table></figure><h2 id="Calculating-Average-Latency"><a href="#Calculating-Average-Latency" class="headerlink" title="Calculating Average Latency"></a>Calculating Average Latency</h2><p>Finally, we calculate the average latency in milliseconds and print the result:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Calculate average latency in milliseconds</span></span><br><span class="line">average_latency = np.mean(latencies) * <span class="number">1000</span> <span class="comment"># Convert to milliseconds</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;Average latency for <span class="subst">&#123;num_searches&#125;</span> searches: <span class="subst">&#123;average_latency:<span class="number">.4</span>f&#125;</span> ms&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>Running the above code on a Mac with an M1 processor, we achieve an impressive average latency of around <strong>7 milliseconds</strong> for 1000 searches. This performance demonstrates the efficiency of FAISS and the capabilities of Apple M1 processors for machine learning tasks.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FAISS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to handle blocking tasks in Asynchronous functions efficiently</title>
      <link href="2024/06/09/how-to-handle-blocking-tasks-in-async-functions/"/>
      <url>2024/06/09/how-to-handle-blocking-tasks-in-async-functions/</url>
      
        <content type="html"><![CDATA[<p>When using <code>asyncio.create_task(function1)</code> and <code>asyncio.create_task(function2)</code>, you are creating two asynchronous tasks that will run concurrently within the same event loop. Whether or not these tasks run efficiently depends on the nature of the tasks themselves.</p><h3 id="Understanding-Asynchronous-Tasks"><a href="#Understanding-Asynchronous-Tasks" class="headerlink" title="Understanding Asynchronous Tasks"></a>Understanding Asynchronous Tasks</h3><p>If <code>function1</code> and <code>function2</code> are asynchronous functions (i.e., they use <code>async def</code> and contain <code>await</code> statements), they will run efficiently as long as they perform non-blocking operations, such as:</p><ul><li>Network I/O</li><li>Disk I/O</li><li>Timers (e.g., <code>await asyncio.sleep()</code>)</li><li>Other asynchronous operations</li></ul><p>In this situation, <code>asyncio</code> will handle the scheduling, suspending tasks when they are waiting for I/O operations to complete, and resuming them when the operations are ready. This results in efficient concurrency.</p><h3 id="Inefficiency-with-Blocking-Code"><a href="#Inefficiency-with-Blocking-Code" class="headerlink" title="Inefficiency with Blocking Code"></a>Inefficiency with Blocking Code</h3><p>If <code>function1</code> and <code>function2</code> contain blocking operations (e.g., CPU-bound tasks, <code>time.sleep()</code>, or blocking I/O), they will not run efficiently. Blocking code will block the entire event loop, preventing other tasks from running concurrently.</p><h3 id="Example-of-Efficient-Async-Tasks"><a href="#Example-of-Efficient-Async-Tasks" class="headerlink" title="Example of Efficient Async Tasks"></a>Example of Efficient Async Tasks</h3><p>Here’s an example of efficiently running non-blocking asynchronous tasks using <code>asyncio.create_task()</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">function1</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Function 1: Start&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">2</span>)  <span class="comment"># Simulate non-blocking I/O</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Function 1: End&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">function2</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Function 2: Start&#x27;</span>)</span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">3</span>)  <span class="comment"># Simulate non-blocking I/O</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Function 2: End&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="comment"># Create tasks to run them concurrently</span></span><br><span class="line">    task1 = asyncio.create_task(function1())</span><br><span class="line">    task2 = asyncio.create_task(function2())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Optionally await the tasks if you want to wait for their completion</span></span><br><span class="line">    <span class="keyword">await</span> task1</span><br><span class="line">    <span class="keyword">await</span> task2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the main async function</span></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure><h3 id="Handling-Blocking-Code-with-Executors"><a href="#Handling-Blocking-Code-with-Executors" class="headerlink" title="Handling Blocking Code with Executors"></a>Handling Blocking Code with Executors</h3><p>If you need to run blocking I/O tasks, you should offload them to a thread or process pool to avoid blocking the event loop:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">blocking_io_task1</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Blocking Task 1: Start&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">2</span>)  <span class="comment"># Simulate blocking I/O</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Blocking Task 1: End&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">blocking_io_task2</span>():</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Blocking Task 2: Start&#x27;</span>)</span><br><span class="line">    time.sleep(<span class="number">3</span>)  <span class="comment"># Simulate blocking I/O</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Blocking Task 2: End&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    loop = asyncio.get_running_loop()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> ThreadPoolExecutor() <span class="keyword">as</span> pool:</span><br><span class="line">        <span class="comment"># Offload blocking tasks to the thread pool</span></span><br><span class="line">        task1 = loop.run_in_executor(pool, blocking_io_task1)</span><br><span class="line">        task2 = loop.run_in_executor(pool, blocking_io_task2)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># If you need to await their completion, you can do so</span></span><br><span class="line">        <span class="keyword">await</span> task1</span><br><span class="line">        <span class="keyword">await</span> task2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the main async function</span></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><ul><li><strong>Efficient Async Tasks:</strong> Use <code>async def</code> functions with <code>await</code> statements for non-blocking operations.</li><li><strong>Handling Blocking Code:</strong> Offload blocking tasks to a thread or process pool using <code>loop.run_in_executor</code>.</li></ul><p>By carefully distinguishing between blocking and non-blocking tasks, you can ensure that your tasks run efficiently when using <code>asyncio.create_task()</code>.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> concurrency </tag>
            
            <tag> asyncio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Turbocharge Your FastAPI with asyncio, Running Background Tasks in Parallel with Immediate Response</title>
      <link href="2024/05/24/running-parrallel-tasks-in-fastapi-and-return-some-task-earlier/"/>
      <url>2024/05/24/running-parrallel-tasks-in-fastapi-and-return-some-task-earlier/</url>
      
        <content type="html"><![CDATA[<p>Developing a responsive and efficient web application is a paramount goal for developers. One common requirement is the ability to execute multiple tasks in parallel while being able to return an immediate response for the tasks that complete first. This blog post will walk you through a practical example using FastAPI to achieve just that. </p><h2 id="The-Challenge"><a href="#The-Challenge" class="headerlink" title="The Challenge"></a>The Challenge</h2><p>Suppose you have three time-consuming tasks to run when an endpoint is triggered, but you want to provide an immediate response as soon as the first task completes, while the remaining tasks continue to run in the background.</p><h2 id="Enter-FastAPI-and-asyncio"><a href="#Enter-FastAPI-and-asyncio" class="headerlink" title="Enter FastAPI and asyncio"></a>Enter FastAPI and asyncio</h2><p>FastAPI, coupled with Python’s <code>asyncio</code> library, provides a robust way to handle such requirements. Below is a simple yet effective approach to tackle this challenge.</p><h3 id="The-Code"><a href="#The-Code" class="headerlink" title="The Code"></a>The Code</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">first_sub_function</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Simulate a delay for the first function</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;First result&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">second_sub_function</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Simulate a delay for the second function</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;second done&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Second result&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">third_sub_function</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Simulate a delay for the third function</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">await</span> asyncio.sleep(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;third done&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Third result&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/run-ttasks&quot;</span></span>)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">run_tasks</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Create a coroutine for the first sub-function</span></span><br><span class="line"></span><br><span class="line">    first_task = first_sub_function()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Launch the second and third sub-functions as background tasks</span></span><br><span class="line"></span><br><span class="line">    second_task = asyncio.create_task(second_sub_function())</span><br><span class="line"></span><br><span class="line">    third_task = asyncio.create_task(third_sub_function())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Wait for the first task to complete</span></span><br><span class="line"></span><br><span class="line">    first_result = <span class="keyword">await</span> first_task</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return only the first task&#x27;s result</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;result&quot;</span>: first_result&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="How-It-Works"><a href="#How-It-Works" class="headerlink" title="How It Works"></a>How It Works</h3><ol><li><strong>Define Async Functions</strong>: We define three asynchronous functions, each simulating a delayed task using <code>await asyncio.sleep()</code>.</li></ol><ol start="2"><li><strong>Create a FastAPI App</strong>: We initialize a FastAPI app instance, <code>app</code>.</li></ol><ol start="3"><li><strong>Design Endpoints</strong>: The <code>/run-tasks</code> endpoint triggers the execution of our defined tasks.</li></ol><ol start="4"><li><p><strong>Execute and Await Tasks</strong>:</p><ul><li><p>The first task (<code>first_sub_function()</code>) is awaited directly, meaning the endpoint will wait for its completion.</p></li><li><p>The other two tasks (<code>second_sub_function()</code> and <code>third_sub_function()</code>) are started as background tasks using <code>asyncio.create_task()</code>.</p></li></ul></li></ol><ol start="5"><li><strong>Immediate Response</strong>: The endpoint returns the result of the first task as soon as it’s available, while the remaining tasks continue to run in the background.</li></ol><h3 id="Run-The-Application"><a href="#Run-The-Application" class="headerlink" title="Run The Application"></a>Run The Application</h3><p>To run the FastAPI app, save the code in a file (e.g., <code>main.py</code>) and run the following command:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">uvicorn main:app --reload</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fastapi </tag>
            
            <tag> concurrency </tag>
            
            <tag> asyncio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Check your public IP address using python</title>
      <link href="2024/05/14/a-simple-way-to-check-public-ip-address-using-python/"/>
      <url>2024/05/14/a-simple-way-to-check-public-ip-address-using-python/</url>
      
        <content type="html"><![CDATA[<p>If you ever need to quickly check your public IP address, one efficient way to do it is by calling an IP search website. Below is a short snippet of Python code that demonstrates how to achieve this using the <code>requests</code> library.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://ipecho.io/json&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(requests.get(url).text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output </span></span><br><span class="line"><span class="comment">#&#x27;&#123;&quot;ip&quot;:&quot;xx.xx.xx.xx&quot;&#125;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="How-It-Works"><a href="#How-It-Works" class="headerlink" title="How It Works:"></a>How It Works:</h2><p>The script sends an HTTP GET request to <code>https://ipecho.io/json</code>. The response is a JSON object that contains your public IP address. </p><h3 id="Why-This-Works"><a href="#Why-This-Works" class="headerlink" title="Why This Works:"></a>Why This Works:</h3><p>When your request reaches the server (in this case, <code>ipecho.io</code>), it carries with it metadata including the public IP address from which it originated. This public IP is visible to the server because it’s used to route data back to your machine. </p><p>Here’s a simplified explanation of how it all works:</p><ol><li><strong>Request Sent</strong>: When you make a request to the server, it travels through the Internet. Your request packet contains your public IP address as part of its metadata.</li><li><strong>Server Reception</strong>: On reaching the server (<code>ipecho.io</code>), this public IP is captured by the server-side application.</li><li><strong>Response Generation</strong>: The server constructs a JSON response containing this IP address.</li><li><strong>Sending Back</strong>: The server sends this JSON response back to your machine.</li></ol><p>This entire process leverages the fundamental way the internet routes traffic between clients (like your computer) and servers.</p><p>By simply using an HTTP client like <code>requests</code>, you effectively ask the server what public IP it sees you coming from, hence obtaining your public IP address.</p><p>This method is quick and provides accurate results, making it a handy tool for checking your public IP address programmatically.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Generate different language speech using OpenAI text to speech API</title>
      <link href="2024/04/25/how-to-generte-different-language-using-openai-text-to-speech-api/"/>
      <url>2024/04/25/how-to-generte-different-language-using-openai-text-to-speech-api/</url>
      
        <content type="html"><![CDATA[<p>How to use openai text to speech to generate speech for different languages?</p><p>If your input is English, you can use the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">from pathlib import Path</span><br><span class="line">from openai import OpenAI</span><br><span class="line">client = OpenAI(api_key=&quot;your_key&quot;)</span><br><span class="line"></span><br><span class="line">speech_file_path = Path(__file__).parent / &quot;speech.mp3&quot;</span><br><span class="line">response = client.audio.speech.create(</span><br><span class="line">  model=&quot;tts-1&quot;,</span><br><span class="line">  voice=&quot;alloy&quot;,</span><br><span class="line">  input=&quot;Today is a wonderful day to build something people love!&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response.stream_to_file(speech_file_path)</span><br></pre></td></tr></table></figure><p>but what If you want to say the same input, but different language? </p><p>Is there a parameter called “language” in the text to speech API?</p><p>Sorry, there isn’t.   </p><p>But actually, you should can first translate the input to the target language, and then pass the translation to the text to speech API, you can get the speech in your target language.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> tts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Troubleshooting Clang Errors During Python Package Installation</title>
      <link href="2024/04/22/trouble-shooting-clang-error-during-python-package-installation/"/>
      <url>2024/04/22/trouble-shooting-clang-error-during-python-package-installation/</url>
      
        <content type="html"><![CDATA[<p>When you encounter a Clang error during the installation of Python packages, it often relates to issues in compiling C or C++ extensions included within the package. This can occur for various reasons such as missing compiler options, incompatible compiler versions, absence of required libraries, or misconfigured environments. Here’s how you can troubleshoot and resolve these issues:</p><h3 id="1-Install-Xcode-Command-Line-Tools-macOS"><a href="#1-Install-Xcode-Command-Line-Tools-macOS" class="headerlink" title="1. Install Xcode Command Line Tools (macOS)"></a>1. Install Xcode Command Line Tools (macOS)</h3><p>If you are on a macOS, you might not have all the necessary build tools installed. To install Xcode Command Line Tools, which include Clang, run:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure><h3 id="2-Install-Build-Essential-Linux"><a href="#2-Install-Build-Essential-Linux" class="headerlink" title="2. Install Build Essential (Linux)"></a>2. Install Build Essential (Linux)</h3><p>For Linux users, ensure you have the necessary developer tools (like gcc, g++, make) which include the build-essential package. On Ubuntu or Debian-based systems, use:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install build-essential</span><br></pre></td></tr></table></figure><h3 id="3-Check-Python-Headers"><a href="#3-Check-Python-Headers" class="headerlink" title="3. Check Python Headers"></a>3. Check Python Headers</h3><p>Some Python packages require Python development headers to compile. Install them via your package manager. For example, in Ubuntu:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install python3-dev</span><br></pre></td></tr></table></figure><p>Replace <code>python3-dev</code> with <code>python-dev</code> if you are using Python 2.x.</p><h3 id="4-Update-Setuptools-and-Wheel"><a href="#4-Update-Setuptools-and-Wheel" class="headerlink" title="4. Update/Setuptools and Wheel"></a>4. Update/Setuptools and Wheel</h3><p>Ensure your setuptools and wheel are up to date as they help in building and installing Python packages:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install --upgrade pip setuptools wheel</span><br></pre></td></tr></table></figure><h3 id="5-Use-a-Virtual-Environment"><a href="#5-Use-a-Virtual-Environment" class="headerlink" title="5. Use a Virtual Environment"></a>5. Use a Virtual Environment</h3><p>Sometimes, using a virtual environment can help avoid conflicts between package versions and dependencies. Create and activate a virtual environment:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python -m venv myenv</span><br><span class="line"><span class="built_in">source</span> myenv/bin/activate</span><br></pre></td></tr></table></figure><h3 id="6-Look-for-Pre-Compiled-Binaries"><a href="#6-Look-for-Pre-Compiled-Binaries" class="headerlink" title="6. Look for Pre-Compiled Binaries"></a>6. Look for Pre-Compiled Binaries</h3><p>Before forcing the compilation from source, check if a pre-compiled binary (wheel) is available for your platform. You can usually find these on the Python Package Index (PyPI) or through a simple <code>pip install</code> command, which prefers wheels over source distributions.</p><h3 id="7-Check-Package-Documentation"><a href="#7-Check-Package-Documentation" class="headerlink" title="7. Check Package Documentation"></a>7. Check Package Documentation</h3><p>Look into the specific package’s documentation for any additional prerequisites or configuration steps that are required for compilation.</p><h3 id="8-Install-Additional-Libraries"><a href="#8-Install-Additional-Libraries" class="headerlink" title="8. Install Additional Libraries"></a>8. Install Additional Libraries</h3><p>Some packages might require additional system libraries. For example, if you’re trying to install <code>Pillow</code> (a fork of PIL - the Python Imaging Library), you might need libjpeg and zlib. Often, the error message will give you hints about what is missing.</p><h3 id="9-Check-Compiler-Version-and-Environment-Variables"><a href="#9-Check-Compiler-Version-and-Environment-Variables" class="headerlink" title="9. Check Compiler Version and Environment Variables"></a>9. Check Compiler Version and Environment Variables</h3><p>It’s possible the C or C++ extension requires a specific version of the compiler, or you might need to set environment variables to guide the compiler and linker’s behavior. Check the package’s setup documentation and adjust your environment accordingly.</p><h3 id="10-Review-the-Error-Logs"><a href="#10-Review-the-Error-Logs" class="headerlink" title="10. Review the Error Logs"></a>10. Review the Error Logs</h3><p>The error logs can provide specific hints about what went wrong during the installation. Look for errors stating missing files, incompatible versions, or other such critical information that can guide you toward a resolution.</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install your-package-name --verbose</span><br></pre></td></tr></table></figure><p>Using the <code>--verbose</code> flag with pip can provide more detailed logs which can be helpful for diagnosing the issue.</p><h3 id="11-Post-on-StackOverflow-or-Community-Forums"><a href="#11-Post-on-StackOverflow-or-Community-Forums" class="headerlink" title="11. Post on StackOverflow or Community Forums"></a>11. Post on StackOverflow or Community Forums</h3><p>If you’re still stuck, consider posting the detailed error message and steps you’ve tried on platforms like StackOverflow. There’s a good chance someone else has faced a similar problem and found a solution.</p><p>Following these steps should help you resolve most issues related to Clang errors during Python package installation. Remember to always replace <code>your-package-name</code>, <code>python3-dev</code>, etc., with the actual names relevant to your scenario.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding ThreadPoolExecutor in Python, A Memory Perspective</title>
      <link href="2024/04/01/why-threading-in-python-depends-more-on-memory/"/>
      <url>2024/04/01/why-threading-in-python-depends-more-on-memory/</url>
      
        <content type="html"><![CDATA[<p>When we discuss concurrency in Python, particularly with <code>ThreadPoolExecutor</code>, a common question arises: Why does it depend more on memory rather than the CPU? To understand this, we need to delve into Python’s threading model, the Global Interpreter Lock (GIL), and typical use cases for threading in Python.</p><h2 id="The-Global-Interpreter-Lock-GIL"><a href="#The-Global-Interpreter-Lock-GIL" class="headerlink" title="The Global Interpreter Lock (GIL)"></a>The Global Interpreter Lock (GIL)</h2><p>Python’s GIL is a mechanism that prevents multiple native threads from executing Python bytecodes simultaneously. This means in a multi-threaded Python program, only one thread can execute Python code at a time. The GIL was designed to simplify memory management and ensure thread safety within the Python interpreter. However, it also means that threading in Python doesn’t increase performance for CPU-bound tasks as one might expect. Instead, threads run nearly sequentially, making CPU scalability via threading limited.</p><h2 id="Memory-Consumption-in-Threading"><a href="#Memory-Consumption-in-Threading" class="headerlink" title="Memory Consumption in Threading"></a>Memory Consumption in Threading</h2><p>Threads share the same memory space, which is beneficial for tasks that require access to shared data. However, each thread needs its own execution stack and local variables, contributing to the program’s overall memory footprint. In scenarios where many threads are spawned, the memory consumption can quickly become a limiting factor. This is particularly true since threads in Python are often used for I/O-bound tasks rather than CPU-bound tasks, due to the GIL.</p><h2 id="I-O-Bound-Over-CPU-Bound"><a href="#I-O-Bound-Over-CPU-Bound" class="headerlink" title="I/O-Bound Over CPU-Bound"></a>I/O-Bound Over CPU-Bound</h2><p><code>ThreadPoolExecutor</code> shines when handling I/O-bound tasks. These tasks involve operations where the program spends significant time waiting for external events (like network responses or disk operations) rather than performing intensive computations. In these scenarios, threads can efficiently manage multiple concurrent I/O operations, waiting for one operation to complete while another starts, thus improving the program’s overall throughput without heavily involving the CPU.</p><h2 id="The-Role-of-Efficiency-in-Context-Switching"><a href="#The-Role-of-Efficiency-in-Context-Switching" class="headerlink" title="The Role of Efficiency in Context Switching"></a>The Role of Efficiency in Context Switching</h2><p>Threading can enhance the efficiency of I/O-bound applications by allowing for overlapping I/O operations with computations. When one thread waits for an I/O operation, another can proceed with its computations. This context switching is more about managing execution flow and less about computational power, highlighting the importance of memory management over CPU usage in threaded applications.</p><p>In summary, while the <code>ThreadPoolExecutor</code> in Python might seem like a tool for leveraging multi-core CPUs, its effectiveness is more pronounced in scenarios where managing memory and I/O operations is key. Understanding this distinction can help developers make informed choices about when and how to use concurrency in their Python applications.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> theading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Dockerfile Example for FastAPI and Corresponding Explaination</title>
      <link href="2024/03/25/dockerfile-for-fastapi-app-example/"/>
      <url>2024/03/25/dockerfile-for-fastapi-app-example/</url>
      
        <content type="html"><![CDATA[<p>To create a Dockerfile that starts with a basic Ubuntu image, installs Python, and the necessary packages from a requirements.txt file, and finally runs main.py, you can follow the structure below. This Dockerfile assumes that you have a requirements.txt file in the same directory as your Dockerfile, which lists all the Python dependencies your application needs. It also assumes you have a main.py file that contains your FastAPI application.</p><p>Create a file called <code>Dockerfile</code>, and put the following code inside:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Use an official Ubuntu base image</span><br><span class="line">FROM ubuntu:latest</span><br><span class="line"></span><br><span class="line"># Set the working directory in the container</span><br><span class="line">WORKDIR /app</span><br><span class="line"></span><br><span class="line"># Install Python and pip</span><br><span class="line">RUN apt-get update &amp;&amp; \</span><br><span class="line">    apt-get install -y python3 python3-pip</span><br><span class="line"></span><br><span class="line"># Copy the current directory contents into the container at /app</span><br><span class="line">COPY . /app</span><br><span class="line"></span><br><span class="line"># Install any needed packages specified in requirements.txt</span><br><span class="line">RUN pip3 install --no-cache-dir -r requirements.txt</span><br><span class="line"></span><br><span class="line"># Make port 3000 available to the world outside this container</span><br><span class="line">EXPOSE 3000</span><br><span class="line"></span><br><span class="line"># Define environment variable to run the app using uvicorn</span><br><span class="line">ENV UVICORN_HOST=0.0.0.0</span><br><span class="line">ENV UVICORN_PORT=3000</span><br><span class="line"></span><br><span class="line"># Run main.py when the container launches</span><br><span class="line">CMD [&quot;python3&quot;, &quot;main.py&quot;]</span><br></pre></td></tr></table></figure><p>Here’s a step-by-step explanation of what each line in the Dockerfile does:</p><ul><li><code>FROM ubuntu:latest</code>: Starts with the latest Ubuntu base image.</li><li><code>WORKDIR /app</code>: Sets the working directory for any RUN, CMD, ENTRYPOINT, COPY, and ADD instructions that follow in the Dockerfile.</li><li><code>RUN apt-get update</code> and the following line: Installs Python and pip by updating the package lists for the Ubuntu package manager and installing the Python3 and Python3-pip packages.</li><li><code>COPY . /app</code>: Copies the current directory (where the Dockerfile resides) into the /app directory inside the container. This step assumes your requirements.txt and main.py are in the same directory as your Dockerfile.</li><li><code>RUN pip3 install --no-cache-dir -r requirements.txt</code>: Installs the Python dependencies defined in requirements.txt without storing the cache, to keep the Docker image size smaller.</li><li><code>EXPOSE 3000</code>: Informs Docker that the container listens on port 3000 at runtime. Note that exposing the port does not actually publish the port. It functions as a type of documentation between the person who builds the image and the person who runs the container.</li><li><code>ENV UVICORN_HOST=0.0.0.0</code> and <code>ENV UVICORN_PORT=3000</code>: Sets environment variables to configure uvicorn to run on 0.0.0.0:3000. These are only necessary if your main.py utilizes these environment variables to configure the host and port; otherwise, they can be omitted.</li><li><code>CMD [&quot;python3&quot;, &quot;main.py&quot;]</code>: Specifies the command to run within the container, which starts your FastAPI application by running main.py with Python 3.</li></ul><p>To build and run your Docker container, you would use the following commands in the directory containing your Dockerfile and application code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Build your Docker image</span><br><span class="line">docker build -t my-fastapi-app .</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Run your Docker container</span><br><span class="line">docker run -p 3000:3000 my-fastapi-app</span><br></pre></td></tr></table></figure><p>This maps port 3000 of the container to port 3000 on your host, allowing you to access the FastAPI application by visiting <a href="http://localhost:3000/">http://localhost:3000</a> on your browser or using a tool like curl or Postman to send requests to your API.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> fastapi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Soft and Hard Movie Subtitles and How to Detect them</title>
      <link href="2024/03/16/move-subtitle-soft-vs-hard-and-how-to-detect/"/>
      <url>2024/03/16/move-subtitle-soft-vs-hard-and-how-to-detect/</url>
      
        <content type="html"><![CDATA[<p>Movies around the globe are enjoyed by a diverse audience, thanks in large part to subtitles that bridge language barriers. However, not all subtitles are created equal, and they generally fall into two categories: soft subtitles and hard subtitles. Each type presents unique challenges when it comes to detection. In this blog post, we’ll delve deep into what these subtitles are, how to detect soft subtitles, and simplified methods for identifying hard subtitles using Optical Character Recognition (OCR) technology.</p><h2 id="Understanding-Subtitles-Soft-vs-Hard"><a href="#Understanding-Subtitles-Soft-vs-Hard" class="headerlink" title="Understanding Subtitles: Soft vs. Hard"></a>Understanding Subtitles: Soft vs. Hard</h2><h3 id="Soft-Subtitles"><a href="#Soft-Subtitles" class="headerlink" title="Soft Subtitles"></a>Soft Subtitles</h3><p>Soft subtitles are akin to a separate layer over the video content, which can be toggled on or off according to the viewer’s preference. They are not burned onto the video itself but are usually included as a separate file or embedded within the video file in tracks. This makes them incredibly flexible as they can be turned on for accessibility reasons or language preferences and turned off for an uninterrupted cinematic experience.</p><h4 id="Detecting-Soft-Subtitles"><a href="#Detecting-Soft-Subtitles" class="headerlink" title="Detecting Soft Subtitles"></a>Detecting Soft Subtitles</h4><p>Detecting soft subtitles is relatively straightforward, thanks to the structured formats they are often stored in, such as SRT (SubRip Text), ASS (Advanced SubStation Alpha), or SSA (SubStation Alpha). Here are some simple steps to detect them:</p><ol><li><p><strong>File Examination:</strong> The first step is checking the video file container for separate subtitle tracks. Tools like <code>FFmpeg</code> or media players like <code>VLC</code> can easily list these tracks.</p></li><li><p><strong>Metadata Inspection:</strong> Often, video files contain metadata that can be inspected to check for the presence of subtitle tracks. Software developers can use libraries in various programming languages to extract and analyze this metadata.</p></li></ol><h3 id="Hard-Subtitles"><a href="#Hard-Subtitles" class="headerlink" title="Hard Subtitles"></a>Hard Subtitles</h3><p>On the other hand, hard subtitles are part of the video frame itself, essentially embedded into the video image. They cannot be turned off and require more sophisticated methods for detection, as they are visually indistinguishable from the other elements in the frame.</p><h4 id="Detecting-Hard-Subtitles"><a href="#Detecting-Hard-Subtitles" class="headerlink" title="Detecting Hard Subtitles"></a>Detecting Hard Subtitles</h4><p>Detecting hard subtitles is where Optical Character Recognition (OCR) technology comes into play. Here’s a simplified method for detecting hard subtitles:</p><ol><li><p><strong>Frame Extraction:</strong> Extract frames from the video at regular intervals using tools like <code>FFmpeg</code>. This step is crucial as it prepares the data for analysis.</p></li><li><p><strong>Pre-Processing:</strong> Before running OCR, it may be necessary to preprocess the images to improve OCR accuracy. This could involve adjusting brightness and contrast or applying filters to isolate text.</p></li><li><p><strong>OCR Processing:</strong> Apply an OCR engine like <code>Tesseract</code> to the preprocessed images. Tesseract is an open-source OCR engine that can recognize text within images, making it suitable for detecting hard subtitles.</p></li><li><p><strong>Text Analysis:</strong> Once the OCR process extracts text from the frames, analyzing the content can help determine if they are indeed subtitles. This analysis could look for patterns typical in subtitles, such as short, concise sentences, presence of timing information, or even language-specific characteristics.</p></li></ol><h2 id="Detecting-Soft-Subtitles-in-Action"><a href="#Detecting-Soft-Subtitles-in-Action" class="headerlink" title="Detecting Soft Subtitles in Action"></a>Detecting Soft Subtitles in Action</h2><p>For detecting soft subtitles, we can use the <code>ffmpeg-python</code> library to probe the video file for subtitle streams. </p><p>First, ensure you have <code>ffmpeg</code> installed on your system and accessible via command line. Then, install <code>ffmpeg-python</code> using pip:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install ffmpeg-python</span><br></pre></td></tr></table></figure><p>Here’s how you might write a Python script to check for soft subtitle streams in a video file:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ffmpeg</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect_soft_subtitles</span>(<span class="params">video_path</span>):</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment"># Probe the video file for streams information</span></span><br><span class="line">        probe = ffmpeg.probe(video_path)</span><br><span class="line">        <span class="comment"># Filter streams to find subtitle streams</span></span><br><span class="line">        subtitle_streams = [stream <span class="keyword">for</span> stream <span class="keyword">in</span> probe[<span class="string">&#x27;streams&#x27;</span>] <span class="keyword">if</span> stream[<span class="string">&#x27;codec_type&#x27;</span>] == <span class="string">&#x27;subtitle&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> subtitle_streams:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Soft subtitle tracks detected.&quot;</span>)</span><br><span class="line">            <span class="keyword">for</span> stream <span class="keyword">in</span> subtitle_streams:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Subtitle track: <span class="subst">&#123;stream[<span class="string">&#x27;index&#x27;</span>]&#125;</span> - Language: <span class="subst">&#123;stream.get(<span class="string">&#x27;tags&#x27;</span>, &#123;&#125;</span>).get(&#x27;language&#x27;, &#x27;unknown&#x27;)&#125;&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;No soft subtitle tracks detected.&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> ffmpeg.Error <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;An error occurred: <span class="subst">&#123;e.stderr&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line">video_path = <span class="string">&#x27;path/to/your/video/file.mkv&#x27;</span></span><br><span class="line">detect_soft_subtitles(video_path)</span><br></pre></td></tr></table></figure><p>This script uses <code>ffmpeg</code> to probe for streams in a video file and filters out subtitle streams if any are present. Adjust the <code>video_path</code> variable to point to your video file.</p><h2 id="Detecting-Hard-Subtitles-with-OCR-in-Action"><a href="#Detecting-Hard-Subtitles-with-OCR-in-Action" class="headerlink" title="Detecting Hard Subtitles with OCR in Action"></a>Detecting Hard Subtitles with OCR in Action</h2><p>For hard subtitles, the Tesseract OCR engine can be utilized alongside <code>pytesseract</code> - a Python wrapper. You’ll need to have Tesseract installed on your system for this to work.</p><p>Installation instructions and downloads for Tesseract can be found at <a href="https://github.com/tesseract-ocr/tesseract">https://github.com/tesseract-ocr/tesseract</a>.</p><p>Then, install <code>pytesseract</code> and <code>Pillow</code> for image processing:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install pytesseract Pillow</span><br></pre></td></tr></table></figure><p>Here’s a basic approach to detect text in video frames:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> pytesseract</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">detect_hard_subtitles</span>(<span class="params">video_path</span>):</span></span><br><span class="line">    <span class="comment"># Load the video</span></span><br><span class="line">    cap = cv2.VideoCapture(video_path)</span><br><span class="line">    <span class="keyword">while</span> cap.isOpened():</span><br><span class="line">        ret, frame = cap.read()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Convert the frame to gray scale and then to PIL format</span></span><br><span class="line">        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        pil_img = Image.fromarray(gray_frame)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Use pytesseract to extract text</span></span><br><span class="line">        text = pytesseract.image_to_string(pil_img, lang=<span class="string">&#x27;eng&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> text.strip():  <span class="comment"># Check if the OCR found any readable text</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;Detected text in frame:&quot;</span>, text)</span><br><span class="line">            <span class="comment"># For demonstration, let&#x27;s just break after first detection</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">    cap.release()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line">video_path = <span class="string">&#x27;path/to/your/video/file_with_hardsub.mp4&#x27;</span></span><br><span class="line">detect_hard_subtitles(video_path)</span><br></pre></td></tr></table></figure><p>This script attempts to extract text from the frames of a given video file. Due to its simplicity, the script checks each frame for text, which might not be very efficient for long videos. Consider implementing a more sophisticated approach, such as selecting specific intervals or regions of the frame more likely to contain subtitles, to optimize the detection process.</p><p>Remember, OCR’s accuracy can significantly vary based on the video quality, subtitle font, and background contrast. Preprocessing steps like binarization or contrast adjustment may help improve results.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> movie </tag>
            
            <tag> ocr </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How does Auth0 work and how to protect our API endpoints</title>
      <link href="2024/03/15/how-does-auth0-work-and-protect-api-endpoint/"/>
      <url>2024/03/15/how-does-auth0-work-and-protect-api-endpoint/</url>
      
        <content type="html"><![CDATA[<p>First, let’s clarify some terms involved in this process:</p><ul><li><strong>Audience</strong>: Typically a URI or a unique identifier for the API or resource server the client aims to access. It’s utilized to ensure that an access token is presented to the correct resource server, which verifies the audience value matches its identifier.</li><li><strong>Client ID and Client Secret</strong>: Credentials provided by the authorization server (e.g., Auth0) to the client. These are used by the client to authenticate itself to the authorization server when requesting a token.</li><li><strong>Access Token</strong>: The token presented by the client to the resource server (API) as proof of authorization to access resources on behalf of the user or itself, in machine-to-machine communication.</li></ul><h2 id="How-to-Set-Up-Auth0-to-Secure-a-FastAPI-Application"><a href="#How-to-Set-Up-Auth0-to-Secure-a-FastAPI-Application" class="headerlink" title="How to Set Up Auth0 to Secure a FastAPI Application"></a>How to Set Up Auth0 to Secure a FastAPI Application</h2><h3 id="On-the-Auth0-Provider-Site"><a href="#On-the-Auth0-Provider-Site" class="headerlink" title="On the Auth0 Provider Site"></a>On the Auth0 Provider Site</h3><p>To protect our resource (the API), the following steps are necessary:</p><ol><li><strong>Resource Setup</strong>:<ul><li>Set up a resource with a unique name, which could be your API name or project name. This distinct name allows the Auth0 provider to differentiate among the various resources that need protection. In Auth0 context, this resource is also referred to as the audience.</li></ul></li><li><strong>Client Configuration</strong>:<ul><li>For every potential user that will call your API (referred to as a client), create a client in Auth0 that will have access to the resource you’ve set up. This includes obtaining a Client ID and a Client Secret.</li></ul></li></ol><h3 id="Using-the-Client-ID-and-Secret-in-Your-App"><a href="#Using-the-Client-ID-and-Secret-in-Your-App" class="headerlink" title="Using the Client ID and Secret in Your App"></a>Using the Client ID and Secret in Your App</h3><p>With these credentials (Client ID and secrets):</p><ol><li>When clients wish to call your API, they first request a JWT token (bearer token) from the Auth0 domain by providing the Client ID, Client Secret, and the audience (resource name).</li></ol><p>Here’s an example in Python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="comment"># Replace these with your actual details</span></span><br><span class="line">AUTH0_DOMAIN = <span class="string">&#x27;YOUR_DOMAIN&#x27;</span></span><br><span class="line">CLIENT_ID = <span class="string">&#x27;YOUR_CLIENT_ID&#x27;</span></span><br><span class="line">CLIENT_SECRET = <span class="string">&#x27;YOUR_CLIENT_SECRET&#x27;</span></span><br><span class="line">AUDIENCE = <span class="string">&#x27;YOUR_AUDIENCE&#x27;</span></span><br><span class="line"></span><br><span class="line">url = <span class="string">f&quot;https://<span class="subst">&#123;AUTH0_DOMAIN&#125;</span>/oauth/token&quot;</span></span><br><span class="line"></span><br><span class="line">payload = &#123;</span><br><span class="line">    <span class="string">&#x27;client_id&#x27;</span>: CLIENT_ID,</span><br><span class="line">    <span class="string">&#x27;client_secret&#x27;</span>: CLIENT_SECRET,</span><br><span class="line">    <span class="string">&#x27;audience&#x27;</span>: AUDIENCE,</span><br><span class="line">    <span class="string">&#x27;grant_type&#x27;</span>: <span class="string">&#x27;client_credentials&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;content-type&#x27;</span>: <span class="string">&quot;application/json&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, json=payload, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> response.status_code == <span class="number">200</span>:</span><br><span class="line">    <span class="comment"># Extract access token from response</span></span><br><span class="line">    access_token = response.json()[<span class="string">&#x27;access_token&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Access Token:&quot;</span>, access_token)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Failed to retrieve access token&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Status Code:&quot;</span>, response.status_code)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Response:&quot;</span>, response.text)</span><br></pre></td></tr></table></figure><ol start="2"><li>With this token, which typically has a limited lifetime of a few hours, the client can then authenticate and make calls to your API.</li></ol><h3 id="Validating-the-Access-Token-on-the-FastAPI-Side"><a href="#Validating-the-Access-Token-on-the-FastAPI-Side" class="headerlink" title="Validating the Access Token on the FastAPI Side"></a>Validating the Access Token on the FastAPI Side</h3><p>To validate incoming tokens, follow these steps:</p><ol><li>Fetch the public key from the Auth0 provider page and use the RSA256 algorithm to verify if the token from the user is valid.</li></ol><p>Here’s how to validate tokens in Python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pip install pyjwt[crypto] requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> jwt <span class="keyword">import</span> decode, exceptions</span><br><span class="line"><span class="keyword">from</span> jwt.algorithms <span class="keyword">import</span> RSAAlgorithm</span><br><span class="line"></span><br><span class="line">AUTH0_DOMAIN = <span class="string">&#x27;YOUR_AUTH0_DOMAIN&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_jwks</span>():</span></span><br><span class="line">    jwks_url = <span class="string">f&#x27;https://<span class="subst">&#123;AUTH0_DOMAIN&#125;</span>/.well-known/jwks.json&#x27;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        jwks_resp = requests.get(jwks_url)</span><br><span class="line">        jwks = jwks_resp.json()</span><br><span class="line">        <span class="keyword">return</span> jwks</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Error fetching JWKS: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_public_key</span>(<span class="params">token</span>):</span></span><br><span class="line">    jwks = get_jwks()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> jwks:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    unverified_header = decode(token, options=&#123;<span class="string">&quot;verify_signature&quot;</span>: <span class="literal">False</span>&#125;, algorithms=[<span class="string">&quot;RS256&quot;</span>])</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> unverified_header:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    rsa_key = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> jwks[<span class="string">&quot;keys&quot;</span>]:</span><br><span class="line">        <span class="keyword">if</span> key[<span class="string">&quot;kid&quot;</span>] == unverified_header[<span class="string">&#x27;kid&#x27;</span>]:</span><br><span class="line">            rsa_key = &#123;</span><br><span class="line">                <span class="string">&quot;kty&quot;</span>: key[<span class="string">&quot;kty&quot;</span>],</span><br><span class="line">                <span class="string">&quot;kid&quot;</span>: key[<span class="string">&quot;kid&quot;</span>],</span><br><span class="line">                <span class="string">&quot;use&quot;</span>: key[<span class="string">&quot;use&quot;</span>],</span><br><span class="line">                <span class="string">&quot;n&quot;</span>: key[<span class="string">&quot;n&quot;</span>],</span><br><span class="line">                <span class="string">&quot;e&quot;</span>: key[<span class="string">&quot;e&quot;</span>]</span><br><span class="line">            &#125;</span><br><span class="line">    <span class="keyword">if</span> rsa_key:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            public_key = RSAAlgorithm.from_jwk(json.dumps(rsa_key))</span><br><span class="line">            <span class="keyword">return</span> public_key</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Error constructing public key: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">validate_token</span>(<span class="params">token, audience, issuer</span>):</span></span><br><span class="line">    public_key = get_public_key(token)</span><br><span class="line">    <span class="keyword">if</span> public_key <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        payload = decode(token, public_key, algorithms=[<span class="string">&quot;RS256&quot;</span>], audience=audience, issuer=issuer)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span>  <span class="comment"># Token is valid</span></span><br><span class="line">    <span class="keyword">except</span> exceptions.InvalidTokenError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Invalid token: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span>  <span class="comment"># Token is invalid</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage</span></span><br><span class="line">token = <span class="string">&quot;YOUR_JWT_TOKEN&quot;</span></span><br><span class="line">audience = <span class="string">&quot;YOUR_AUDIENCE&quot;</span></span><br><span class="line">issuer = <span class="string">f&quot;https://<span class="subst">&#123;AUTH0_DOMAIN&#125;</span>/&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> validate_token(token, audience, issuer):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Token is valid.&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Token is invalid or verification failed.&quot;</span>)</span><br></pre></td></tr></table></figure><p>That concludes the setup to secure a FastAPI application with Auth0.</p><p>```<br>This guidance provides a structured approach for integrating Auth0 authentication into a FastAPI app, highlighting the necessary steps and providing code snippets for practical implementation.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> auth0 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Python Naming Conventions, Underscores vs. Dashes</title>
      <link href="2024/03/12/python-naming-conventions-underscore-vs-upperscore/"/>
      <url>2024/03/12/python-naming-conventions-underscore-vs-upperscore/</url>
      
        <content type="html"><![CDATA[<p>When diving into the world of Python programming, one of the first things you’ll notice is the importance of naming conventions. As in every language, there’s a standard way to name your variables, functions, and classes that helps keep your code consistent and understandable. In Python, the choice between using underscores and dashes isn’t just a matter of preference—it’s a syntax rule you need to follow.</p><h2 id="Variables-and-Functions-The-Snake-on-the-Ground"><a href="#Variables-and-Functions-The-Snake-on-the-Ground" class="headerlink" title="Variables and Functions: The Snake on the Ground"></a>Variables and Functions: The Snake on the Ground</h2><p>In Python, variables and function names follow a convention known as snake_case. Each word in the identifier is in lowercase, and words are separated by underscores <code>_</code>. This convention keeps variable names readable and avoids confusion with Python’s syntax. For instance:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_variable = <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_function</span>():</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>Trying to use a dash (<code>-</code>) in these names would lead to a syntax error because Python interprets the dash as a minus sign, disrupting the flow and meaning of your code. Hence, a “no-go” in Python land.</p><h2 id="Classes-The-Humps-of-the-Camel"><a href="#Classes-The-Humps-of-the-Camel" class="headerlink" title="Classes: The Humps of the Camel"></a>Classes: The Humps of the Camel</h2><p>When it comes to naming classes, Python developers use CamelCase (also known as CapWords). This means the first letter of each word is capitalized, and there are no underscores between words. This style makes class names stand out and signals to the reader that they are looking at a class definition. Here’s an example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClassName</span>:</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserProfile</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, username</span>):</span></span><br><span class="line">        self.username = username</span><br></pre></td></tr></table></figure><p>The clear distinction in naming conventions for classes, compared to variables and functions, makes Python code more readable and easy to understand.</p><h2 id="Consistency-is-Key"><a href="#Consistency-is-Key" class="headerlink" title="Consistency is Key"></a>Consistency is Key</h2><p>The Python Enhancement Proposal (PEP) 8 serves as the style guide for Python code and outlines these naming conventions. Staying consistent with PEP 8 not only improves readability but also aligns your code with the expectations of other Python developers, simplifying code sharing and collaboration.</p><p>In summary, when programming in Python:</p><ul><li>Use snake_case for variables and functions: <code>my_variable</code>, <code>calculate_age</code>.</li><li>Use CamelCase for classes: <code>DatabaseConnection</code>, <code>JsonResponse</code>.</li></ul><p>Remember, dashes have no place in Python identifiers and are reserved for arithmetic operations. By following the Python norms, you ensure your code is accessible, maintainable, and ready to blend seamlessly into the vast world of Python modules and projects.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to install pyaudio on mac when there is some error</title>
      <link href="2024/03/09/how-to-fix-pyaudio-installation-on-mac/"/>
      <url>2024/03/09/how-to-fix-pyaudio-installation-on-mac/</url>
      
        <content type="html"><![CDATA[<p>If you’re encountering issues installing PyAudio on a Mac, the solution might involve a few steps, particularly if you’re seeing error messages like:</p><ul><li><code>-bash: pip: command not found</code></li><li><code>fatal error: &#39;portaudio.h&#39; file not found</code></li></ul><p>Here’s a succinct guide to solving these installation problems:</p><ol><li><p><strong>Error when trying <code>pip install pyaudio</code>:</strong><br>You might see <code>-bash: pip: command not found</code>. This suggests that <code>pip</code> is not installed or recognized in your environment.</p></li><li><p><strong>Error when trying <code>pip3 install pyaudio</code>:</strong><br>The error message <code>fatal error: &#39;portaudio.h&#39; file not found</code> indicates that the PortAudio library, which PyAudio depends on, is not properly installed or recognized.</p></li><li><p><strong>If you’ve already installed PortAudio via Homebrew (<code>brew install portaudio</code>) but still encounter issues,</strong> it’s possible that there’s a mismatch or an issue with how the library is being recognized by the PyAudio installer.</p></li></ol><p>To solve these issues, follow these steps:</p><ol><li><p><strong>Ensure Xcode is installed:</strong> First, make sure you have Xcode installed on your Mac, as it includes command-line tools needed for compiling software.</p></li><li><p><strong>Install Xcode Command Line Tools:</strong> Run <code>xcode-select --install</code> in your terminal to install any missing command-line tools.</p></li><li><p><strong>Reinstall PortAudio:</strong> Remove the existing PortAudio installation (if any) and reinstall it to ensure it’s properly set up.</p><ul><li><code>brew remove portaudio</code></li><li><code>brew install portaudio</code></li></ul></li><li><p><strong>Install PyAudio:</strong> With PortAudio correctly installed, you should now be able to install PyAudio without encountering the previous errors.</p><ul><li><code>pip3 install pyaudio</code></li></ul></li></ol><p>This sequence of steps addresses the common issues faced when installing PyAudio on a Mac, especially related to missing dependencies and command-line tools.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pyaudio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to call Azure speech to text service in a streaming way</title>
      <link href="2024/02/27/azure-text-to-speech-continous-streaming-from-microphone/"/>
      <url>2024/02/27/azure-text-to-speech-continous-streaming-from-microphone/</url>
      
        <content type="html"><![CDATA[<p>The following code shows how to call the microsoft Azure text to speech service in an continuous or streaming way.<br>The audio input is from default microphone.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import azure.cognitiveservices.speech as speechsdk</span><br><span class="line"></span><br><span class="line">def from_mic_continuous():</span><br><span class="line">  speech_config = speechsdk.SpeechConfig(subscription=&quot;subscription_key&quot;, region=&quot;azure service region&quot;)</span><br><span class="line">  audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)</span><br><span class="line">  speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)</span><br><span class="line"></span><br><span class="line">  def recognized_handler(evt):</span><br><span class="line">    # This handler is for final results</span><br><span class="line">    if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:</span><br><span class="line">      print(f&quot;Final result: &#123;evt.result.text&#125;&quot;)</span><br><span class="line"></span><br><span class="line">  def recognizing_handler(evt):</span><br><span class="line">    # This handler is for partial results</span><br><span class="line">    #print(evt.result)</span><br><span class="line">    if evt.result.reason == speechsdk.ResultReason.RecognizingSpeech:</span><br><span class="line">      print(f&quot;Partial result: &#123;evt.result.text&#125;&quot;)</span><br><span class="line"></span><br><span class="line">  #speech_recognizer.recognizing.connect(recognizing_handler)</span><br><span class="line">  speech_recognizer.recognized.connect(recognized_handler)</span><br><span class="line"></span><br><span class="line">  # Start continuous speech recognition</span><br><span class="line">  speech_recognizer.start_continuous_recognition()</span><br><span class="line">  try:</span><br><span class="line">    while True:</span><br><span class="line">      # Keep the main thread alive while the background thread processes the speech recognition</span><br><span class="line">      pass</span><br><span class="line">  except KeyboardInterrupt:</span><br><span class="line">    # Stop recognition when Ctrl+C is pressed</span><br><span class="line">    speech_recognizer.stop_continuous_recognition()</span><br><span class="line"></span><br><span class="line">from_mic_continuous()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> speech2text </tag>
            
            <tag> azure </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Next Wave of Automation, the Impact of Generative AI and AGI on Intellectual Labor</title>
      <link href="2024/02/20/from-industry-automation-to-intellectual-automation/"/>
      <url>2024/02/20/from-industry-automation-to-intellectual-automation/</url>
      
        <content type="html"><![CDATA[<p>The history of industry is, in many respects, a history of automation. From the earliest revolutions where steam power replaced human muscle, to the assembly lines that transformed manufacturing by standardizing the production of goods such as automobiles, the quest to make processes faster, cheaper, and more efficient has been relentless. This journey has predominantly focused on automating physical labor - the tangible actions of building, making, and moving. However, as we stand on the cusp of advances in generative AI and artificial general intelligence (AGI), the frontier of automation is expanding beyond the physical, venturing into the realms of intellectual work.</p><h3 id="The-Evolution-of-Automation-From-Physical-to-Intellectual"><a href="#The-Evolution-of-Automation-From-Physical-to-Intellectual" class="headerlink" title="The Evolution of Automation: From Physical to Intellectual"></a>The Evolution of Automation: From Physical to Intellectual</h3><p>The initial waves of automation brought about by the industrial revolution were transformative. Machines took over the repetitive, physically demanding tasks previously carried out by skilled labor. These mechanical marvels didn’t tire, they didn’t need breaks, and they worked with a precision and pace unattainable by human hands. </p><p>Yet, despite these advancements, the intellectual work behind the scenes - the designing, planning, and innovating - remained firmly in the domain of humans. Machines may have been doing the heavy lifting, but humans were orchestrating the ballet.</p><p>Fast forward to the present, and we find ourselves at the dawn of a new era of automation, powered by generative AI and concepts of AGI. Unlike their predecessors that automated muscle, these technologies aim to automate the mind.</p><h3 id="Generative-AI-Automating-Creativity-and-Thought"><a href="#Generative-AI-Automating-Creativity-and-Thought" class="headerlink" title="Generative AI: Automating Creativity and Thought"></a>Generative AI: Automating Creativity and Thought</h3><p>Generative AI, through systems like OpenAI’s GPT (Generative Pre-trained Transformer) series, brings forth an unprecedented shift. It takes the essence of intellectual tasks - be it writing, coding, designing, or creating - and encapsulates it in an algorithm capable of performing these tasks autonomously. </p><p>The parallels to earlier industrial automation are striking. Where assembly lines made it possible to manufacture cars rapidly and at scale by repeating a set process, generative AI achieves a similar outcome for intellectual work. Give it a prompt, some high-level instructions, and it can generate text, images, code, and more at a pace and volume that dwarfs human capability.</p><h3 id="The-Implications-of-Intellectual-Automation"><a href="#The-Implications-of-Intellectual-Automation" class="headerlink" title="The Implications of Intellectual Automation"></a>The Implications of Intellectual Automation</h3><p>This progression raises profound questions about the future of work, creativity, and the essence of human contribution. The implications are vast and varied:</p><ul><li><p><strong>Workforce Transformation</strong>: Just as automation in manufacturing led to shifts in the labor market, generative AI and AGI will transform the landscape of intellectual work. New skills and roles will emerge, while others become obsolete, challenging us to rethink education, training, and social support systems.</p></li><li><p><strong>Enhanced Creativity and Efficiency</strong>: On a positive note, automating aspects of intellectual labor could unleash creativity. Liberated from mundane tasks, individuals could focus on higher-level creative and strategic thinking.</p></li><li><p><strong>Ethical Considerations</strong>: With AI taking on tasks historically reserved for humans, ethical considerations around bias, accountability, and the nature of creativity itself come to the fore. Ensuring these systems are developed and used in a way that benefits society as a whole becomes paramount.</p></li></ul><h3 id="Navigating-the-New-Wave-of-Automation"><a href="#Navigating-the-New-Wave-of-Automation" class="headerlink" title="Navigating the New Wave of Automation"></a>Navigating the New Wave of Automation</h3><p>As we navigate this new wave of automation, it’s essential to approach it with both optimism and caution. The potential for generative AI and AGI to enrich our lives and work is immense. However, it’s equally crucial to engage with these technologies thoughtfully, considering their impact on employment, ethics, and equality.</p><p>In many ways, generative AI and AGI represent the latest chapter in the long story of human ingenuity and our quest to push the boundaries of what’s possible. As with past revolutions, success will depend not just on embracing the technology’s potential but on managing its challenges and ensuring it serves the greater good.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> agi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why Synchronous Functions Block Asynchronous Functions</title>
      <link href="2024/02/12/why-sync-function-in-async-function-will-block-the-async-function/"/>
      <url>2024/02/12/why-sync-function-in-async-function-will-block-the-async-function/</url>
      
        <content type="html"><![CDATA[<p>When working with asynchronous programming in Python, understanding why synchronous functions can block the execution of asynchronous functions is crucial. In this blog post, we will explore this concept and discuss why it is essential to use asynchronous equivalents for blocking operations.</p><h2 id="The-Asynchronous-Advantage"><a href="#The-Asynchronous-Advantage" class="headerlink" title="The Asynchronous Advantage"></a>The Asynchronous Advantage</h2><p>Asynchronous programming in Python allows for concurrent execution of code, enabling tasks to be started, paused, and resumed later. This concurrency is especially useful in I/O-bound and high-latency operations, where waiting for responses from external sources can significantly impact performance.</p><p>The key to achieving this concurrent execution is the use of an event loop. The event loop manages the execution of multiple coroutines, allowing for non-blocking operations. It ensures that while one task is waiting for a response, other tasks can continue their execution, leading to improved efficiency and responsiveness.</p><h2 id="The-Problem-with-Synchronous-Operations"><a href="#The-Problem-with-Synchronous-Operations" class="headerlink" title="The Problem with Synchronous Operations"></a>The Problem with Synchronous Operations</h2><p>When we introduce synchronous operations, such as <code>time.sleep(5)</code>, within an asynchronous function, the entire event loop is blocked from progressing. This occurs because <code>time.sleep(5)</code> is a blocking call that halts the execution of the current thread. While waiting for the sleep to complete, the thread is unable to perform any other tasks.</p><p>In the default setup, where the event loop runs on a single thread, this means that no other asynchronous tasks can progress during the sleep period. As a result, the benefits of asynchronous programming are nullified.</p><h2 id="The-Solution-Asynchronous-Equivalents"><a href="#The-Solution-Asynchronous-Equivalents" class="headerlink" title="The Solution: Asynchronous Equivalents"></a>The Solution: Asynchronous Equivalents</h2><p>To maintain the non-blocking behavior of asynchronous functions, it is crucial to replace any synchronous operations with their asynchronous equivalents. In the case of <code>time.sleep(5)</code>, we can use <code>await asyncio.sleep(5)</code> instead.</p><p>By using <code>asyncio.sleep(5)</code> within an async function, we signal the event loop to manage other tasks while waiting for the sleep to finish. This ensures that the operation remains non-blocking, allowing other coroutines to run concurrently.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding why synchronous functions can block the execution of asynchronous functions is essential for effective asynchronous programming in Python. By utilizing asynchronous equivalents for blocking operations, we can maintain the non-blocking behavior of async functions, enabling true concurrency and improving performance in I/O-bound and high-latency operations.</p><p>So, always remember to choose asynchronous equivalents for any blocking operation within an async function, allowing the event loop to manage other tasks and harness the power of asynchronous programming.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aysnc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Remove and Reinstall OrbStack on macOS</title>
      <link href="2024/02/10/how-to-reinstall-failed-orbstack-to-use-docker-in-mac/"/>
      <url>2024/02/10/how-to-reinstall-failed-orbstack-to-use-docker-in-mac/</url>
      
        <content type="html"><![CDATA[<p>If you encountered a failed installation of OrbStack on your macOS and received an error message stating that your CPU time is not supported, there may be an issue with the installation process. This guide will walk you through the steps to remove the installed OrbStack and perform a successful reinstallation.</p><h2 id="Step-1-Remove-the-Previously-Installed-OrbStack"><a href="#Step-1-Remove-the-Previously-Installed-OrbStack" class="headerlink" title="Step 1: Remove the Previously Installed OrbStack"></a>Step 1: Remove the Previously Installed OrbStack</h2><ol><li><p>Open Terminal on your macOS. You can find it by going to Applications &gt; Utilities &gt; Terminal or by using Spotlight Search (Command + Space) and typing “Terminal”.</p></li><li><p>In the Terminal, type the following command to remove the installed OrbStack:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rm -rf /Applications/OrbStack.app</span><br></pre></td></tr></table></figure></li></ol><p>This command will permanently delete the OrbStack application from your system.</p><h2 id="Step-2-Reinstall-OrbStack-using-Homebrew"><a href="#Step-2-Reinstall-OrbStack-using-Homebrew" class="headerlink" title="Step 2: Reinstall OrbStack using Homebrew"></a>Step 2: Reinstall OrbStack using Homebrew</h2><ol><li><p>Once the removal process is complete, you can now proceed with reinstalling OrbStack.</p></li><li><p>In the Terminal, use the following command to reinstall OrbStack using Homebrew:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew reinstall --cask orbstack</span><br></pre></td></tr></table></figure><p>This command will automatically download and install the latest version of OrbStack on your macOS.</p></li><li><p>After the installation is complete, you can launch OrbStack by typing the following command in the Terminal:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">orb</span><br></pre></td></tr></table></figure><p>This will open the OrbStack dashboard.</p></li><li><p>To start using Docker containers, make sure to turn on the Docker service from the OrbStack dashboard.</p></li></ol><p>Congratulations! You have successfully removed and reinstalled OrbStack on your macOS. You can now utilize Docker containers without any issues.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> orbstack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Enhancing AWS Resource Security using VPC, A Comprehensive Guide</title>
      <link href="2024/02/06/how-does-vpc-works-in-aws-to-connect-public-and-private-subnets/"/>
      <url>2024/02/06/how-does-vpc-works-in-aws-to-connect-public-and-private-subnets/</url>
      
        <content type="html"><![CDATA[<p>In the realm of cloud computing, particularly with AWS, safeguarding your digital assets involves a collection of critical components working in unison. Among these, the Virtual Private Cloud (VPC), subnets (public and private), Internet Gateway, NAT (Network Address Translation) Gateway, Route Tables, and Security Groups form the backbone of a secure AWS infrastructure. This blog post delves deeper into the roles of these components and elucidates how they collectively fortify your AWS environment.</p><h2 id="Virtual-Private-Cloud-VPC"><a href="#Virtual-Private-Cloud-VPC" class="headerlink" title="Virtual Private Cloud (VPC)"></a>Virtual Private Cloud (VPC)</h2><p>A <strong>VPC</strong> is the cornerstone of network security in AWS, providing a private, isolated section of the cloud where your resources operate. It’s akin to having a private, secure data center within AWS that grants full control over your network architecture.</p><h2 id="Subnets-Public-and-Private"><a href="#Subnets-Public-and-Private" class="headerlink" title="Subnets: Public and Private"></a>Subnets: Public and Private</h2><p>Within a VPC, the network is divided into <strong>subnets</strong>, enabling you to segment and allocate IP address ranges based on your architectural requirements.</p><ul><li><strong>Public Subnet</strong>: Utilized for resources that must be connected to the internet (e.g., web servers). Equipped with a path to the Internet Gateway, it allows inbound and outbound internet traffic.</li><li><strong>Private Subnet</strong>: Designed for internal resources that should not be directly accessible from the internet (e.g., databases). Resources here can access the internet via the NAT Gateway without exposing themselves to inbound traffic.</li></ul><h2 id="Internet-Gateway-and-NAT-Gateway"><a href="#Internet-Gateway-and-NAT-Gateway" class="headerlink" title="Internet Gateway and NAT Gateway"></a>Internet Gateway and NAT Gateway</h2><ul><li><p><strong>Internet Gateway</strong> acts as the bridge between your VPC and the internet, facilitating communication to and from resources in your VPC.</p></li><li><p><strong>NAT Gateway</strong> (Network Address Translation Gateway) allows instances in a private subnet to access services outside your VPC (e.g., for software updates) without granting external entities direct access to those instances.</p></li></ul><h2 id="Route-Tables"><a href="#Route-Tables" class="headerlink" title="Route Tables"></a>Route Tables</h2><p><strong>Route Tables</strong> dictate the traffic direction within your VPC. Each subnet in your VPC must be associated with a route table, which determines where network traffic from the subnet is directed. Proper configuration ensures that traffic routes correctly between the internet, public subnets, private subnets, and the NAT Gateway.</p><h2 id="Security-Groups"><a href="#Security-Groups" class="headerlink" title="Security Groups"></a>Security Groups</h2><p><strong>Security Groups</strong> act as virtual firewalls for your instances to control inbound and outbound traffic at the instance level. Unlike network access control lists (ACLs) that operate at the subnet level, security groups are more granular, allowing you to assign different rules to each instance within the same subnet.</p><h2 id="Integrating-Components-for-Secure-Architecture"><a href="#Integrating-Components-for-Secure-Architecture" class="headerlink" title="Integrating Components for Secure Architecture"></a>Integrating Components for Secure Architecture</h2><p>Integrating these components effectively establishes a resilient and secure AWS architecture. Here’s how they work together:</p><ul><li>The <strong>VPC</strong> provides a secure, isolated environment.</li><li><strong>Subnets</strong> differentiate resource exposure levels, segregating the public-facing and internal assets.</li><li>The <strong>Internet Gateway</strong> facilitates internet access for public resources.</li><li>The <strong>NAT Gateway</strong> bridges internet connectivity for private resources without exposing them directly.</li><li><strong>Route Tables</strong> manage traffic flow within the VPC, ensuring proper routing between internal resources, the internet, and the NAT Gateway.</li><li><strong>Security Groups</strong> protect individual instances by defining permissible traffic, safeguarding your resources from unwanted access.</li></ul><p>By leveraging these elements, you can sculpt a secure, efficient, and scalable cloud infrastructure on AWS. Understanding the functions and interactions of these components is fundamental for architects and developers looking to deploy secure cloud solutions.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> vpc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding FPGA and Its Role in Accelerating Machine Learning Applications</title>
      <link href="2024/02/02/fpga-and-application-of-machine-learning/"/>
      <url>2024/02/02/fpga-and-application-of-machine-learning/</url>
      
        <content type="html"><![CDATA[<p>In the constantly evolving world of technology, Field-Programmable Gate Arrays (FPGAs) have emerged as a pivotal element in accelerating machine learning applications. Unlike conventional processors like CPUs and GPUs, FPGAs offer unparalleled flexibility and efficiency for a wide array of computational tasks. This blog aims to demystify FPGAs, highlighting their architecture, advantages in machine learning, and the development process involved in leveraging their potential.</p><h2 id="What-is-an-FPGA"><a href="#What-is-an-FPGA" class="headerlink" title="What is an FPGA?"></a>What is an FPGA?</h2><p>At its core, an FPGA is a semiconductor device that can be programmed after manufacturing to implement any digital logic or computing task. This reprogrammable nature comes from its unique architecture, consisting of an array of configurable logic blocks (CLBs) connected through programmable interconnects. Users can configure these blocks and interconnections to create custom hardware circuits, allowing FPGAs to perform specific tasks incredibly efficiently.</p><h3 id="Key-Components-of-FPGA"><a href="#Key-Components-of-FPGA" class="headerlink" title="Key Components of FPGA:"></a>Key Components of FPGA:</h3><ul><li><strong>Configurable Logic Blocks (CLBs):</strong> The fundamental units in an FPGA that can be programmed to perform a variety of logical operations.</li><li><strong>Programmable Interconnects:</strong> Flexible wiring between CLBs that can be customized to establish the necessary pathways for data.</li><li><strong>I/O Blocks:</strong> These allow the FPGA to communicate with the external environment, interfacing with different types of inputs and outputs.</li></ul><h2 id="FPGA-vs-CPU-vs-GPU"><a href="#FPGA-vs-CPU-vs-GPU" class="headerlink" title="FPGA vs. CPU vs. GPU"></a>FPGA vs. CPU vs. GPU</h2><table><thead><tr><th>Feature</th><th>FPGA</th><th>CPU</th><th>GPU</th></tr></thead><tbody><tr><td>Flexibility</td><td>High (Hardware-level programmability)</td><td>Low</td><td>Moderate</td></tr><tr><td>Parallelism</td><td>High (Customizable)</td><td>Low</td><td>High</td></tr><tr><td>Development Complexity</td><td>High (Requires knowledge of HDLs)</td><td>Low</td><td>Moderate</td></tr><tr><td>Power Efficiency</td><td>High (Custom hardware efficiency)</td><td>Moderate</td><td>Moderate</td></tr><tr><td>Best for</td><td>Custom, specialized tasks</td><td>General-purpose computing</td><td>High throughput parallel processing</td></tr></tbody></table><h2 id="FPGAs-in-Machine-Learning"><a href="#FPGAs-in-Machine-Learning" class="headerlink" title="FPGAs in Machine Learning"></a>FPGAs in Machine Learning</h2><p>The advent of machine learning, especially in fields requiring real-time processing and high efficiency, like vision applications, has underscored the significance of FPGAs. Here are a few pivotal reasons behind FPGAs’ favorability for machine learning:</p><h3 id="Real-Time-Processing"><a href="#Real-Time-Processing" class="headerlink" title="Real-Time Processing"></a>Real-Time Processing</h3><p>Machine learning applications, particularly in vision, require the handling of extensive data volumes in real-time. FPGAs excel here due to their low-latency capabilities, offering immediate processing vital for time-sensitive decisions in autonomous vehicles, industrial inspection, and surveillance.</p><h3 id="Energy-and-Power-Efficiency"><a href="#Energy-and-Power-Efficiency" class="headerlink" title="Energy and Power Efficiency"></a>Energy and Power Efficiency</h3><p>For edge computing and IoT devices, energy consumption is a critical constraint. FPGAs’ ability to execute specific tasks with minimal power makes them ideal for deploying AI and machine learning algorithms in power-sensitive environments.</p><h3 id="Customization-and-Flexibility"><a href="#Customization-and-Flexibility" class="headerlink" title="Customization and Flexibility"></a>Customization and Flexibility</h3><p>FPGAs allow developers to tailor-make hardware for specific machine learning algorithms, optimizing performance far beyond what’s achievable with general-purpose processors. This includes implementing custom operations and adjusting numerical precisions to balance between accuracy and efficiency.</p><h2 id="Developing-for-FPGAs-A-Collaborative-Effort"><a href="#Developing-for-FPGAs-A-Collaborative-Effort" class="headerlink" title="Developing for FPGAs: A Collaborative Effort"></a>Developing for FPGAs: A Collaborative Effort</h2><p>Implementing machine learning algorithms on FPGAs involves a collaboration between algorithm specialists and FPGA developers. While algorithm developers focus on designing efficient models, FPGA engineers translate these models into hardware descriptions using Hardware Description Languages (HDLs) or through High-Level Synthesis (HLS) tools.</p><h3 id="High-Level-Synthesis-HLS"><a href="#High-Level-Synthesis-HLS" class="headerlink" title="High-Level Synthesis (HLS)"></a>High-Level Synthesis (HLS)</h3><p>HLS tools enable developers to describe the desired hardware behavior in higher-level languages, significantly simplifying the FPGA development process. However, achieving an optimized FPGA implementation still requires expertise in fine-tuning for performance and efficiency.</p><h3 id="Model-Conversion-Tools"><a href="#Model-Conversion-Tools" class="headerlink" title="Model Conversion Tools"></a>Model Conversion Tools</h3><p>Tools like Xilinx’s Vitis AI and Intel’s OpenVINO can aid in translating neural network models to an FPGA-friendly format. Yet, optimizing these models for FPGAs often demands manual adjustments and in-depth understanding of both machine learning algorithms and hardware design principles.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>FPGAs present a compelling option for accelerating machine learning applications, especially those demanding real-time processing, customization, and power efficiency. However, unlocking FPGAs’ full potential requires a symbiotic relationship between machine learning expertise and hardware design acumen. As tools and methodologies continue to evolve, FPGAs stand poised to play an ever-increasing role in powering the next generation of intelligent applications.</p><p>By embracing FPGAs’ unique capabilities, developers can push the boundaries of what’s possible in machine learning, paving the way for innovative solutions that are not only smarter but also more efficient and responsive to the real-world needs.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FPGA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write a check both dollar and cents in value</title>
      <link href="2024/01/31/how-to-write-a-check-with-dollar-and-cents/"/>
      <url>2024/01/31/how-to-write-a-check-with-dollar-and-cents/</url>
      
        <content type="html"><![CDATA[<p>For example, to write a check for $36.46, you should follow these steps:</p><ol><li><strong>Date:</strong> Write the current date in the top right corner.</li><li><strong>Payee:</strong> On the line that starts with “Pay to the Order of,” write the name of the person or company you’re paying.</li><li><strong>Numeric Amount:</strong> In the small box on the right, write “$36.46”.</li><li><strong>Written Amount:</strong> On the line below “Pay to the Order of,” write out the dollar amount in words. For $36.46, you would write “Thirty-six and 46/100”. The “46/100” represents the cents as a fraction of a dollar.</li><li><strong>Memo (Optional):</strong> In the lower left corner, there’s a line for a memo or note. This is optional but can be used to note what the payment is for.</li><li><strong>Signature:</strong> Sign the check on the line in the bottom right corner. Your check isn’t valid until you sign it.</li></ol><p>Make sure to write legibly and to use a pen, not a pencil, to prevent alterations.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> checks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two Ways of Enforcing and Validating Python class input using Pydantic</title>
      <link href="2024/01/27/two-ways-of-enforce-and-validate-python-class-inputs-using-pydantic/"/>
      <url>2024/01/27/two-ways-of-enforce-and-validate-python-class-inputs-using-pydantic/</url>
      
        <content type="html"><![CDATA[<p>Pydantic is a Python library used for data parsing and validation through Python type annotations. It’s particularly useful for enforcing that input data matches a specific format, type, or set of constraints.</p><h2 id="Example-1-Defining-a-Class-Directly-with-Pydantic-BaseModel"><a href="#Example-1-Defining-a-Class-Directly-with-Pydantic-BaseModel" class="headerlink" title="Example 1: Defining a Class Directly with Pydantic BaseModel"></a>Example 1: Defining a Class Directly with Pydantic BaseModel</h2><p>In this approach, you define a class that directly inherits from Pydantic’s BaseModel. The class properties are declared as class variables with type annotations. Pydantic will automatically handle validation based on these annotations.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pydantic import BaseModel, validator</span><br><span class="line"></span><br><span class="line">class User(BaseModel):</span><br><span class="line">    name: str</span><br><span class="line">    age: int</span><br><span class="line">    email: str</span><br><span class="line"></span><br><span class="line">    @validator(&#x27;age&#x27;)</span><br><span class="line">    def validate_age(cls, v):</span><br><span class="line">        if v &lt; 18:</span><br><span class="line">            raise ValueError(&#x27;Age must be at least 18&#x27;)</span><br><span class="line">        return v</span><br><span class="line"></span><br><span class="line"># Example usage</span><br><span class="line">try:</span><br><span class="line">    user = User(name=&quot;John Doe&quot;, age=17, email=&quot;john@example.com&quot;)</span><br><span class="line">except ValueError as e:</span><br><span class="line">    print(e)</span><br></pre></td></tr></table></figure><p>In this example, User inherits from BaseModel, and each field (name, age, email) is automatically validated. The custom validator for the age field ensures that the age is at least 18.</p><h2 id="Example-2-Using-a-Pydantic-Model-for-Validation-in-a-Custom-Class"><a href="#Example-2-Using-a-Pydantic-Model-for-Validation-in-a-Custom-Class" class="headerlink" title="Example 2: Using a Pydantic Model for Validation in a Custom Class"></a>Example 2: Using a Pydantic Model for Validation in a Custom Class</h2><p>In this approach, you define a separate Pydantic model for data validation, and then use this model within the <strong>init</strong> method of your custom class to validate the inputs.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pydantic import BaseModel, ValidationError</span><br><span class="line"></span><br><span class="line">class UserInput(BaseModel):</span><br><span class="line">    name: str</span><br><span class="line">    age: int</span><br><span class="line">    email: str</span><br><span class="line"></span><br><span class="line">class User:</span><br><span class="line">    def __init__(self, name: str, age: int, email: str):</span><br><span class="line">        try:</span><br><span class="line">            validated_data = UserInput(name=name, age=age, email=email)</span><br><span class="line">            self.name = validated_data.name</span><br><span class="line">            self.age = validated_data.age</span><br><span class="line">            self.email = validated_data.email</span><br><span class="line">        except ValidationError as e:</span><br><span class="line">            print(&quot;Invalid input:&quot;, e.json())</span><br><span class="line"></span><br><span class="line"># Example usage</span><br><span class="line">user = User(name=&quot;Jane Doe&quot;, age=25, email=&quot;jane@example.com&quot;)</span><br></pre></td></tr></table></figure><p>In this example, UserInput is a Pydantic model used for validation, while User is a regular Python class. The<code>__init__</code> method of User creates an instance of UserInput for validation, and if the data is valid, it proceeds to initialize the User instance.</p><p>Both methods are effective for ensuring that the input data adheres to the specified format and constraints. The choice between them depends on your specific use case and design preferences.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pydantic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add relative path to system path in Python</title>
      <link href="2024/01/16/add-relative-path-to-system-in-python/"/>
      <url>2024/01/16/add-relative-path-to-system-in-python/</url>
      
        <content type="html"><![CDATA[<p>When runing pyton scripts, in order to import function from some file, we need to make sure that file is in the system path.<br>So we can add a path in the code. While adding absolute path is easy, but it may not importable to other people.<br>So here we show an example, where I want to add a relative path of 3 levels up than the file that’s being current running:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import sys</span><br><span class="line">import os</span><br><span class="line"></span><br><span class="line"># Get the directory of the current script</span><br><span class="line">current_dir = os.path.dirname(__file__)</span><br><span class="line"></span><br><span class="line"># Get the path to the root directory by navigating 3 levels up</span><br><span class="line">root_path = os.path.abspath(os.path.join(current_dir, &#x27;..&#x27;, &#x27;..&#x27;,&#x27;..&#x27;))</span><br><span class="line"></span><br><span class="line"># Add the src directory to sys.path</span><br><span class="line">sys.path.append(root_path)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Decibels, the Logarithmic World of Measurement intutively</title>
      <link href="2024/01/15/how-to-understand-decibles-the-logrithmic-relative-measurement-inutitively/"/>
      <url>2024/01/15/how-to-understand-decibles-the-logrithmic-relative-measurement-inutitively/</url>
      
        <content type="html"><![CDATA[<p>Decibels (dB) are a unit of measurement often encountered in various fields, from acoustics to electronics, where the comparison of values on a logarithmic scale is essential. In this blog post, we will delve into the definition of decibels, discuss how to understand them, explore their applications in different areas, and provide intuitive examples to demystify the seemingly complex world of dB.</p><h2 id="Decibels-Defined"><a href="#Decibels-Defined" class="headerlink" title="Decibels Defined"></a>Decibels Defined</h2><p>Decibels are a logarithmic unit used to express the ratio between two values, typically in comparison to a reference value. The logarithmic nature of dB allows for a more manageable representation of a wide range of values, especially in scenarios where human perception follows a logarithmic response.</p><h2 id="Understanding-Decibels"><a href="#Understanding-Decibels" class="headerlink" title="Understanding Decibels"></a>Understanding Decibels</h2><p>To grasp decibels, it’s crucial to understand that a change of 1 dB represents a tenfold change in intensity or power. Positive dB values indicate an increase, while negative values indicate a decrease relative to the reference level. Let’s explore how this applies in different areas:</p><h2 id="1-Sound-Levels"><a href="#1-Sound-Levels" class="headerlink" title="1. Sound Levels"></a>1. Sound Levels</h2><ul><li><strong>0 dB:</strong> The threshold of hearing.</li><li><strong>20 dB:</strong> Whispering in a quiet library.</li><li><strong>60 dB:</strong> Normal conversation.</li><li><strong>90 dB:</strong> Lawnmower or a busy city street.</li><li><strong>120 dB:</strong> Jet engine at takeoff.</li></ul><p>Every 10 dB increase represents a tenfold increase in sound intensity.</p><h2 id="2-Power-Levels"><a href="#2-Power-Levels" class="headerlink" title="2. Power Levels"></a>2. Power Levels</h2><ul><li><strong>0 dB:</strong> Reference power level.</li><li><strong>3 dB:</strong> Power doubled.</li><li><strong>-3 dB:</strong> Power halved.</li></ul><p>In electronic circuits, a 3 dB change is often significant.</p><h2 id="3-Signal-Strength"><a href="#3-Signal-Strength" class="headerlink" title="3. Signal Strength"></a>3. Signal Strength</h2><ul><li><strong>0 dBm:</strong> Reference signal level.</li><li><strong>-10 dBm:</strong> Signal weaker by a factor of 10.</li><li><strong>+10 dBm:</strong> Signal stronger by a factor of 10.</li></ul><p>dBm is commonly used in telecommunications to measure power levels.</p><h2 id="4-Voltage-Levels"><a href="#4-Voltage-Levels" class="headerlink" title="4. Voltage Levels"></a>4. Voltage Levels</h2><ul><li><strong>0 dBV:</strong> Reference voltage level.</li><li><strong>-3 dBV:</strong> Voltage reduced by half.</li><li><strong>+3 dBV:</strong> Voltage doubled.</li></ul><p>Audio systems often use dBV to represent voltage levels.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding decibels is crucial for accurate measurement and comparison in various fields. Whether you’re dealing with sound, power, signal strength, or voltage, the logarithmic scale of decibels simplifies the representation of values that span a wide range. Next time you encounter a dB value, remember that it’s not just a number; it’s a logarithmic key to understanding the relative intensity or power in a given context.</p>]]></content>
      
      
      <categories>
          
          <category> physics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> decibels </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pyspark topandas operation too slow, and what to do next</title>
      <link href="2024/01/08/spark-to-pandas-too-slow-then-what-to-do/"/>
      <url>2024/01/08/spark-to-pandas-too-slow-then-what-to-do/</url>
      
        <content type="html"><![CDATA[<p><code>topandas()</code> is a method in PySpark that converts a Spark DataFrame to a Pandas DataFrame. If you find that <code>topandas()</code> is running slowly, it may be for several reasons, and there are various strategies you might consider to speed up the process.</p><ol><li><p><strong>Reduce Data Size</strong>: Before calling <code>topandas()</code>, filter your dataset down to only the data you need. The fewer rows and columns you convert, the faster the operation.</p></li><li><p><strong>Select Subset of Columns</strong>: Retrieve only the necessary columns rather than converting the entire DataFrame.</p></li><li><p><strong>Increase Resource Allocation</strong>: Ensure that your Spark cluster has sufficient resources (memory and CPU) for the operation. The performance of <code>topandas()</code> can be limited by the amount of memory available, as it has to collect all the data to the driver.</p></li><li><p><strong>Enable Arrow-based Data Transfer</strong>: Apache Arrow can greatly speed up the data transfer between JVM and Python processes. You can enable Arrow-based columnar data transfers by setting the following Spark configuration in your Spark session:</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder \</span><br><span class="line">    .config(<span class="string">&quot;spark.sql.execution.arrow.pyspark.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure><p> Just keep in mind that Arrow might not be compatible with all types of data, so check your data types if you encounter any issues.</p></li><li><p><strong>Use <code>arrow</code> in Execution</strong>: Starting from Spark 3.0.0, you can enable Arrow optimization in the execution of <code>toPandas()</code> by setting the following configuration:</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">spark.conf.<span class="built_in">set</span>(<span class="string">&quot;spark.sql.execution.arrow.pyspark.enabled&quot;</span>, <span class="string">&quot;true&quot;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Use Apache Arrow for CSV/Parquet</strong>: If you’re ending up writing the pandas DataFrame to disk anyway (for instance, to a CSV or Parquet file), consider writing the Spark DataFrame directly to disk using Spark’s native <code>write</code> capabilities, which will be distributed and more efficient.</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.write.parquet(<span class="string">&#x27;path/to/output&#x27;</span>)</span><br></pre></td></tr></table></figure></li><li><p><strong>Avoid Collecting Large Data on the Driver</strong>: <code>topandas()</code> collects all the data on the driver node. If the data is too large, this can cause out-of-memory issues. An alternative is to work with Spark DataFrames as much as possible and use distributed computing power.</p></li><li><p><strong>Use <code>toPandas()</code> in Batches</strong>: Instead of converting the entire DataFrame at once, you could convert it in chunks. Split the Spark DataFrame into smaller DataFrames, convert these to Pandas DataFrames individually, and then concatenate the Pandas DataFrames. This requires a bit of manual handling and careful memory management.</p></li><li><p><strong>Use Checkpointing</strong>: If your DataFrame has gone through many transformations, checkpointing can truncate the logical plan and make the <code>topandas()</code> execution faster. However, checkpointing also writes to disk, so it has a cost.</p> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df.checkpoint()  <span class="comment"># This will save the DataFrame state to disk and truncate the logical plan.</span></span><br></pre></td></tr></table></figure></li><li><p><strong>Improve Network Performance</strong>: If your Spark cluster is deployed across multiple nodes, consider improving network speed and reducing latencies since <code>topandas()</code> involves shuffling data over the network to the driver node.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Python&#39;s TypeError, Missing 1 required positional argument</title>
      <link href="2024/01/05/missing-1-required-positional-argument-type-error-in-python/"/>
      <url>2024/01/05/missing-1-required-positional-argument-type-error-in-python/</url>
      
        <content type="html"><![CDATA[<p>Python error sometimes can be confusing, especially for beginners. A common source of confusion is the TypeError related to argument mismatch, often encountered when working with class methods. Let’s break down this error to help you understand and resolve it effectively.</p><h2 id="The-Scenario"><a href="#The-Scenario" class="headerlink" title="The Scenario"></a>The Scenario</h2><p>Consider a simple class in Python:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyClass:</span><br><span class="line">    def my_method(self, name):</span><br><span class="line">        print(f&quot;Name is &#123;name&#125;&quot;)</span><br></pre></td></tr></table></figure><p>my_method is a method of MyClass that requires two arguments: <code>self</code> and <code>name</code>. <code>self</code> is a reference to the instance of the class (automatically passed when you call a method on an object), and <code>name</code> is an additional argument that needs to be provided.</p><h2 id="The-Common-Mistake"><a href="#The-Common-Mistake" class="headerlink" title="The Common Mistake"></a>The Common Mistake</h2><p>A frequent mistake is attempting to call my_method directly on the class, without creating an instance:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">MyClass.my_method(&quot;example name&quot;)</span><br></pre></td></tr></table></figure><h3 id="The-Error"><a href="#The-Error" class="headerlink" title="The Error"></a>The Error</h3><p>Doing this results in the following error:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">TypeError: my_method() missing 1 required positional argument: &#x27;name&#x27;</span><br></pre></td></tr></table></figure><h3 id="Why-This-Error"><a href="#Why-This-Error" class="headerlink" title="Why This Error?"></a>Why This Error?</h3><p>This error message can be misleading because it seems like you provided the name argument. However, the root cause is different:</p><ul><li><p>Instance Methods Need self: Instance methods are designed to operate on an instance of the class (an object). When you call an instance method, Python automatically passes the instance as the first argument, which is self.</p></li><li><p>Direct Class Method Call: When you call the method directly on the class (e.g., MyClass.my_method), Python doesn’t have an instance to pass as self. It then interprets the first argument you provide (“example name”) as self, not as name.</p></li><li><p>Argument Mismatch: As a result, Python thinks you haven’t provided the name argument, leading to the TypeError.</p></li></ul><h2 id="The-Solution"><a href="#The-Solution" class="headerlink" title="The Solution"></a>The Solution</h2><p>To resolve this error, you need to call the method on an instance of the class:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">instance = MyClass()</span><br><span class="line">instance.my_method(&quot;example name&quot;)</span><br></pre></td></tr></table></figure><p>In this corrected approach, <code>instance.my_method(&quot;example name&quot;)</code> passes instance as self and “example name” as name, aligning with the method’s parameters.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Automating AWS Infrastructure and CI/CD with Terraform and GitHub Actions</title>
      <link href="2024/01/04/using-terraform-and-github-action-together-for-cicd/"/>
      <url>2024/01/04/using-terraform-and-github-action-together-for-cicd/</url>
      
        <content type="html"><![CDATA[<p>We dive into the world of infrastructure automation and continuous integration/continuous deployment (CI/CD) using Terraform and GitHub Actions. This post will guide you through the process of setting up IAM rules in AWS with Terraform and building and deploying Docker images to AWS using GitHub Actions.</p><h2 id="Using-Terraform-for-AWS-IAM-Rules"><a href="#Using-Terraform-for-AWS-IAM-Rules" class="headerlink" title="Using Terraform for AWS IAM Rules"></a>Using Terraform for AWS IAM Rules</h2><h3 id="What-is-Terraform"><a href="#What-is-Terraform" class="headerlink" title="What is Terraform?"></a>What is Terraform?</h3><p>Terraform is an Infrastructure as Code (IaC) tool that enables you to manage and provision resources on cloud platforms like AWS. It uses a declarative configuration language to describe your cloud resources’ desired state.</p><h3 id="Managing-AWS-IAM-with-Terraform"><a href="#Managing-AWS-IAM-with-Terraform" class="headerlink" title="Managing AWS IAM with Terraform"></a>Managing AWS IAM with Terraform</h3><p>AWS Identity and Access Management (IAM) controls who is authenticated and authorized to use resources. Terraform allows you to write scripts (<code>.tf</code> files) that define your AWS infrastructure, including IAM rules. These scripts are crucial for ensuring that your resources are managed securely and efficiently.</p><h3 id="The-Process"><a href="#The-Process" class="headerlink" title="The Process"></a>The Process</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Example Terraform Script for IAM</span><br><span class="line">provider &quot;aws&quot; &#123;</span><br><span class="line">  region = &quot;us-west-2&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource &quot;aws_iam_role&quot; &quot;example&quot; &#123;</span><br><span class="line">  name = &quot;example_role&quot;</span><br><span class="line"></span><br><span class="line">  assume_role_policy = &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">  &quot;Statement&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;Action&quot;: &quot;sts:AssumeRole&quot;,</span><br><span class="line">      &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">      &quot;Principal&quot;: &#123;</span><br><span class="line">        &quot;Service&quot;: &quot;ec2.amazonaws.com&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>The above is a basic example of how you can define an IAM role using Terraform.</p><h2 id="Using-GitHub-Actions-for-CI-CD"><a href="#Using-GitHub-Actions-for-CI-CD" class="headerlink" title="Using GitHub Actions for CI/CD"></a>Using GitHub Actions for CI/CD</h2><h3 id="Introduction-to-GitHub-Actions"><a href="#Introduction-to-GitHub-Actions" class="headerlink" title="Introduction to GitHub Actions"></a>Introduction to GitHub Actions</h3><p>GitHub Actions is a CI/CD platform that allows you to automate your build, test, and deployment pipelines within your GitHub repository.</p><h3 id="Building-and-Deploying-with-GitHub-Actions"><a href="#Building-and-Deploying-with-GitHub-Actions" class="headerlink" title="Building and Deploying with GitHub Actions"></a>Building and Deploying with GitHub Actions</h3><p>You can define a workflow in a .github/workflows YAML file. This workflow automates the process of building a Docker image of your application and pushing it to AWS.</p><p>Workflow Example</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name: Deploy to AWS</span><br><span class="line"></span><br><span class="line">on:</span><br><span class="line">  push:</span><br><span class="line">    branches:</span><br><span class="line">      - main</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  build-and-deploy:</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line"></span><br><span class="line">    steps:</span><br><span class="line">    - name: Checkout code</span><br><span class="line">      uses: actions/checkout@v2</span><br><span class="line"></span><br><span class="line">    - name: Build Docker image</span><br><span class="line">      run: docker build -t my-application .</span><br><span class="line"></span><br><span class="line">    - name: Push to AWS ECR</span><br><span class="line">      run: |</span><br><span class="line">        aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin &lt;account_id&gt;.dkr.ecr.us-west-2.amazonaws.com</span><br><span class="line">        docker tag my-application:latest &lt;account_id&gt;.dkr.ecr.us-west-2.amazonaws.com/my-application:latest</span><br><span class="line">        docker push &lt;account_id&gt;.dkr.ecr.us-west-2.amazonaws.com/my-application:latest</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This YAML script is an example of how you can define a GitHub Action to build and push a Docker image to AWS.</p><h2 id="Integrating-Terraform-and-GitHub-Actions"><a href="#Integrating-Terraform-and-GitHub-Actions" class="headerlink" title="Integrating Terraform and GitHub Actions"></a>Integrating Terraform and GitHub Actions</h2><p>Integrating Terraform with GitHub Actions ensures that any changes to your infrastructure as code, such as updating IAM rules, are automatically applied in AWS. Similarly, changes to your application codebase can trigger automated deployments, keeping your application up-to-date in AWS.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github action </tag>
            
            <tag> terraform </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to calculate gradient of target over features in deep learning models</title>
      <link href="2023/12/30/how-to-get-gradient-of-target-over-features-in-deep-learning-model/"/>
      <url>2023/12/30/how-to-get-gradient-of-target-over-features-in-deep-learning-model/</url>
      
        <content type="html"><![CDATA[<p>To get the gradients of a target with respect to some feature in a deep learning model, you can use a technique called backpropagation or automatic differentiation. This process is essential for tasks such as gradient-based optimization, sensitivity analysis, and interpretability. Here are the steps to compute gradients:</p><ol><li>Define your deep learning model: You need to have a well-defined deep learning model. This model could be a neural network, convolutional neural network (CNN), recurrent neural network (RNN), or any other architecture depending on your task.</li><li>St up your loss function: You also need to define a loss function that measures how far the model’s predictions are from the actual target values. Common loss functions include mean squared error, cross-entropy, etc.</li><li>Perform forward pass: Feed your input data through the model to compute predictions. The forward pass involves applying your model to the input data, layer by layer, until you get the final output.</li><li>Compute the loss: Use the loss function to compute the error between the predicted output and the actual target. This loss value is a scalar that represents how well or poorly the model is performing on the current input.</li><li>Perform backward pass (Backpropagation): The key step in getting gradients is backpropagation. In this step, you compute the gradients of the loss with respect to the model’s parameters (weights and biases) and any intermediate activations. This is done by applying the chain rule of calculus.<br>Start by computing the gradient of the loss with respect to the final layer’s output.<br>Then, propagate this gradient backward through the network, computing gradients for each layer.<br>Finally, you’ll have gradients for all the model parameters and intermediate activations.</li><li>Extract the gradients: Once you have computed the gradients, you can extract the gradient of the target (output) with respect to the specific feature(s) of interest. This involves identifying which part of the model’s parameters or intermediate activations corresponds to the feature(s) in question.</li><li>Compute the gradients: The gradients can be computed using automatic differentiation libraries like TensorFlow or PyTorch. These libraries allow you to create a computation graph and automatically compute gradients using the backward or grad functions, respectively.</li></ol><p>Here’s a high-level code snippet in PyTorch as an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"># Define your model</span><br><span class="line">model = YourDeepLearningModel()</span><br><span class="line"></span><br><span class="line"># Define your loss function</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"></span><br><span class="line"># Input data and target</span><br><span class="line">input_data = torch.randn(1, input_dim, requires_grad=True)</span><br><span class="line">target = torch.randn(1, output_dim)</span><br><span class="line"></span><br><span class="line"># Forward pass</span><br><span class="line">output = model(input_data)</span><br><span class="line"></span><br><span class="line"># Compute the loss</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line"></span><br><span class="line"># Backpropagation to compute gradients</span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"># Extract the gradients with respect to the input data</span><br><span class="line">gradients = input_data.grad</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> gradient </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Robust functin return format in Python</title>
      <link href="2023/12/29/robust-function-return-in-python/"/>
      <url>2023/12/29/robust-function-return-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python, when you see a function returning result or {}, it means that the function is designed to return either the value of result or an empty dictionary {} if result is considered falsy in a boolean context.</p><p>Here’s a breakdown of how it works:</p><ol><li><p><code>result</code>: This is a variable that presumably holds some data in a format that the function is meant to return. This could be a dictionary, list, or any other data structure depending on the function’s purpose.</p></li><li><p><code>or</code>: This is a logical operator in Python. When used in the context of return result or {}, it checks the value of result first. If result is truthy, it returns result. If result is falsy, it evaluates and returns what’s after the or, which in this case is an empty dictionary {}.</p></li><li><p>Falsy Values: In Python, certain values are considered “falsy” in boolean contexts. These include (but are not limited to) None, False, empty sequences/collections ([], (), ‘’, {}), and numeric zeros (0, 0.0).</p></li><li><p>Use Case: This pattern is often used to ensure that a function returns a consistent data type even if the desired output isn’t available or is invalid. For instance, if result is supposed to be a dictionary but for some reason is None (maybe due to an error or lack of data), the function will return an empty dictionary instead. This can be useful to avoid type-related errors in the code that calls this function.</p></li></ol><p>Here’s a simple example to illustrate this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_data():</span><br><span class="line">    # Imagine some logic here that might or might not successfully populate &#x27;result&#x27;</span><br><span class="line">    result = None  # Or result could be an actual dictionary with data</span><br><span class="line"></span><br><span class="line">    return result or &#123;&#125;</span><br><span class="line"></span><br><span class="line">data = get_data()</span><br><span class="line"># &#x27;data&#x27; will be either the dictionary &#x27;result&#x27; or &#123;&#125;, ensuring it&#x27;s always a dictionary.</span><br></pre></td></tr></table></figure><p>In this example, <code>get_data()</code> will always return a dictionary, either populated with data or empty, which can be safer and more predictable when integrating this function into larger systems.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to use secrets in streamlit app safely</title>
      <link href="2023/12/27/enable-secrets-in-using-streamlit-app/"/>
      <url>2023/12/27/enable-secrets-in-using-streamlit-app/</url>
      
        <content type="html"><![CDATA[<p>When using streamlit app, how to manage secrets?  It is quite easy to use <code>st.secrets</code> function.</p><p>when running your Streamlit app locally, you can store secrets in a .streamlit/secrets.toml file. Streamlit uses TOML (Tom’s Obvious, Minimal Language) format for the secrets file, not YAML. Here’s how you can set it up:</p><h2 id="Step-1-Create-the-Secrets-File"><a href="#Step-1-Create-the-Secrets-File" class="headerlink" title="Step 1: Create the Secrets File"></a>Step 1: Create the Secrets File</h2><p>Create a file named secrets.toml inside a .streamlit folder at the root of your project directory. Your project structure should look something like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">your_project/</span><br><span class="line">├─ .streamlit/</span><br><span class="line">│  ├─ secrets.toml</span><br><span class="line">├─ your_script.py</span><br><span class="line">├─ other_files</span><br></pre></td></tr></table></figure><h2 id="Step-2-Add-Your-Secrets-to-the-File"><a href="#Step-2-Add-Your-Secrets-to-the-File" class="headerlink" title="Step 2: Add Your Secrets to the File"></a>Step 2: Add Your Secrets to the File</h2><p>In secrets.toml, you can add your secrets like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># .streamlit/secrets.toml</span><br><span class="line">my_api_key = &quot;your-secret-api-key&quot;</span><br></pre></td></tr></table></figure><h2 id="Step-3-Access-the-Secrets-in-Your-Streamlit-App"><a href="#Step-3-Access-the-Secrets-in-Your-Streamlit-App" class="headerlink" title="Step 3: Access the Secrets in Your Streamlit App"></a>Step 3: Access the Secrets in Your Streamlit App</h2><p>In your Streamlit script (your_script.py), you can access these secrets using st.secrets. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import streamlit as st</span><br><span class="line"></span><br><span class="line"># Access the secret</span><br><span class="line">api_key = st.secrets[&quot;my_api_key&quot;]</span><br><span class="line"></span><br><span class="line"># Rest of your Streamlit code</span><br></pre></td></tr></table></figure><p>for example, if we have a html code to render, we can use the secrets this way:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import streamlit as st</span><br><span class="line"></span><br><span class="line"># Access the secret</span><br><span class="line">api_key = st.secrets[&quot;my_api_key&quot;]</span><br><span class="line"></span><br><span class="line">html_content = f&quot;&quot;&quot;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">  &lt;script&gt;</span><br><span class="line">    var apiKey = &#x27;&#123;api_key&#125;&#x27;;</span><br><span class="line">    console.log(&#x27;API Key:&#x27;, apiKey); // This line will log the API key to the browser&#x27;s console</span><br><span class="line">    // Your JavaScript code</span><br><span class="line">  &lt;/script&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">  &lt;!-- Your HTML content --&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">print(html_content)  # Debug: Check if the API key is correctly injected</span><br><span class="line">st.components.v1.html(html_content, height=600)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> streamlit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Rendering html file using streamlit app</title>
      <link href="2023/12/27/how-to-render-html-file-using-streamlit-app/"/>
      <url>2023/12/27/how-to-render-html-file-using-streamlit-app/</url>
      
        <content type="html"><![CDATA[<p>If we have some static html, probably with some javascript as well, we can deploy the html and use streamlit app to render the html file.</p><p>Recomend to have a virtual python environment first, then install the streamlit as:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install streamlit</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Here’s an example of how your Python script might look:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import streamlit as st</span><br><span class="line">import streamlit.components.v1 as components</span><br><span class="line"></span><br><span class="line"># Read the HTML file</span><br><span class="line">with open(&#x27;your_html_file.html&#x27;, &#x27;r&#x27;) as file:</span><br><span class="line">    html_content = file.read()</span><br><span class="line"></span><br><span class="line"># Render the HTML</span><br><span class="line">components.html(html_content, height=600)</span><br></pre></td></tr></table></figure><p>Finally, to test the above python file locally:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">streamlit run your_app.py</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> streamlit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding POST vs PUT in Web Development</title>
      <link href="2023/12/22/http-post-vs-put-difference-clearly-explained/"/>
      <url>2023/12/22/http-post-vs-put-difference-clearly-explained/</url>
      
        <content type="html"><![CDATA[<p>When we use web services, sometimes it may trigger error when we are not careful to tell the difference between POST and PUT.</p><p>In the world of web development, especially when dealing with RESTful APIs, the HTTP methods POST and PUT are frequently used but often misunderstood. Both are used to send data to the server, but they serve different purposes and follow different rules.</p><h2 id="POST-Creating-New-Resources"><a href="#POST-Creating-New-Resources" class="headerlink" title="POST: Creating New Resources"></a>POST: Creating New Resources</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><ul><li><strong>POST</strong> is used to create a new resource on the server.</li></ul><h3 id="Idempotency"><a href="#Idempotency" class="headerlink" title="Idempotency"></a>Idempotency</h3><ul><li>POST requests are <strong>non-idempotent</strong>, meaning multiple identical POST requests will generally produce different results.</li></ul><h3 id="Usage-Example"><a href="#Usage-Example" class="headerlink" title="Usage Example"></a>Usage Example</h3><ul><li>Imagine you’re developing a blog platform. When a user writes a new blog post, you would use a POST request to send this new post data to the server. Each time a POST request is made, a new blog post is created.</li><li>Example Request: <code>POST /api/blogPosts</code> with the new post data in the request body.</li></ul><h2 id="PUT-Updating-Existing-Resources"><a href="#PUT-Updating-Existing-Resources" class="headerlink" title="PUT: Updating Existing Resources"></a>PUT: Updating Existing Resources</h2><h3 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h3><ul><li><strong>PUT</strong> is used to update an existing resource or create a new one if it doesn’t exist, based on the provided identifier.</li></ul><h3 id="Idempotency-1"><a href="#Idempotency-1" class="headerlink" title="Idempotency"></a>Idempotency</h3><ul><li>PUT requests are <strong>idempotent</strong>. This means if you make the same PUT request multiple times, the result will be the same as making it once.</li></ul><h3 id="Usage-Example-1"><a href="#Usage-Example-1" class="headerlink" title="Usage Example"></a>Usage Example</h3><ul><li>Continuing with the blog platform scenario, if a user wants to edit an existing blog post, a PUT request is made. This request will carry the updated data and the unique identifier of the post (like its ID).</li><li>Example Request: <code>PUT /api/blogPosts/123</code> with the updated post data in the request body, where <code>123</code> is the post ID.</li></ul><h2 id="Key-Differences"><a href="#Key-Differences" class="headerlink" title="Key Differences"></a>Key Differences</h2><ul><li><strong>Primary Function</strong>: POST creates, PUT updates (or creates if absent).</li><li><strong>Idempotency</strong>: POST is non-idempotent, PUT is idempotent.</li><li><strong>URL Usage</strong>: In POST, the URL typically points to a collection (e.g., <code>/api/blogPosts</code>), while PUT points to a specific item in the collection (e.g., <code>/api/blogPosts/123</code>).</li></ul><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>Understanding the differences between POST and PUT is crucial for building effective and standards-compliant web applications. By using each method appropriately, developers can ensure that their APIs are intuitive and predictable, leading to better data integrity and easier maintenance.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> web development </tag>
            
            <tag> http </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to setup AWS opensearch with master user mode and indexing in Python</title>
      <link href="2023/12/19/aws-opensearch-master-user-mode-setup-and-index-query-example-in-python/"/>
      <url>2023/12/19/aws-opensearch-master-user-mode-setup-and-index-query-example-in-python/</url>
      
        <content type="html"><![CDATA[<p>AWS opensearch is AWS’s version of elasticserch. For faster development and easy interaction with AWS opensearch service, here is how one can configure the domain to use master user name and password, <br>and get public access.</p><h2 id="To-create-an-OpenSearch-Service-domain-using-the-console"><a href="#To-create-an-OpenSearch-Service-domain-using-the-console" class="headerlink" title="To create an OpenSearch Service domain using the console:"></a>To create an OpenSearch Service domain using the console:</h2><ol><li><p>Go to <a href="https://aws.amazon.com/">https://aws.amazon.com</a> and choose Sign In to the Console.<br>Under Analytics, choose Amazon OpenSearch Service.</p></li><li><p>Choose Create domain.</p></li></ol><p>Provide a name for the domain. </p><p>For the domain creation method, choose Standard create.</p><p>3.<br>To quickly configure a production domain with best practices, you can choose Easy create. For the development and testing purposes of this tutorial, we’ll use Standard create.</p><ol start="4"><li><p>For templates, choose Dev/test.</p></li><li><p>For the deployment option, choose Domain with standby.</p></li><li><p>For Version, choose the latest version.</p></li><li><p>For now, ignore the Data nodes, Warm and cold data storage, Dedicated master nodes, Snapshot configuration, and Custom endpoint sections.</p></li><li><p>For simplicity in this tutorial, use a public access domain. Under Network, choose Public access.</p></li><li><p>In the <code>fine-grained access control</code> settings, keep the Enable fine-grained access control check box selected.<br>Select Create <code>master user</code> and provide a username and <code>password</code>.</p></li><li><p>For now, ignore the SAML authentication and Amazon Cognito authentication sections.</p></li><li><p>For <code>Access policy</code>, choose <code>Only use fine-grained access control</code>. In this tutorial, fine-grained access control handles authentication, not the domain access policy.</p></li></ol><p>Ignore the rest of the settings and choose Create. New domains typically take 15–30 minutes to initialize, but can take longer depending on the configuration. After your domain initializes, select it to open its configuration pane. Note the domain endpoint under General information (for example, <a href="https://search-my-domain.us-east-1.es.amazonaws.com/">https://search-my-domain.us-east-1.es.amazonaws.com</a>), which you’ll use in the next step.</p><p>If you get error by clicking the dashboard or later when requesting in Python, some error like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">User: anonymous is not authorized to perform: es:ESHttpGet with an explicit deny in a resource-based policy</span><br></pre></td></tr></table></figure><p>Double check step 11, make sure you are choosing the <code>Only use fine-grained access control</code>.</p><h2 id="how-to-index-one-record-in-Python"><a href="#how-to-index-one-record-in-Python" class="headerlink" title="how to index one record in Python"></a>how to index one record in Python</h2><p>Need to replace domain_endpint and user name and passwords with your domain.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.auth import HTTPBasicAuth</span><br><span class="line"></span><br><span class="line"># Endpoint URL</span><br><span class="line">url = f&#x27;&#123;domain_endpoint&#125;/movies/_doc/1&#x27;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Authentication details</span><br><span class="line">auth = HTTPBasicAuth(master_user_name, master_pwd)</span><br><span class="line"></span><br><span class="line"># JSON data to be sent in the request</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;director&quot;: &quot;Burton, Tim&quot;,</span><br><span class="line">    &quot;genre&quot;: [&quot;Comedy&quot;, &quot;Sci-Fi&quot;],</span><br><span class="line">    &quot;year&quot;: 1996,</span><br><span class="line">    &quot;actor&quot;: [&quot;Jack Nicholson&quot;, &quot;Pierce Brosnan&quot;, &quot;Sarah Jessica Parker&quot;],</span><br><span class="line">    &quot;title&quot;: &quot;Mars Attacks!&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Headers</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Making the PUT request</span><br><span class="line">response = requests.put(url, json=data, auth=auth, headers=headers)</span><br><span class="line"></span><br><span class="line"># Printing the response (optional)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br></pre></td></tr></table></figure><h2 id="how-to-index-bulk-texts-in-Python"><a href="#how-to-index-bulk-texts-in-Python" class="headerlink" title="how to index bulk texts in Python"></a>how to index bulk texts in Python</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.auth import HTTPBasicAuth</span><br><span class="line"></span><br><span class="line"># Endpoint URL</span><br><span class="line">url =f&#x27;&#123;domain_endpoint&#125;/_bulk&#x27;</span><br><span class="line"></span><br><span class="line"># Authentication details</span><br><span class="line">auth = HTTPBasicAuth(master_user_name, master_pwd)</span><br><span class="line"></span><br><span class="line"># Headers</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Read the contents of the JSON file</span><br><span class="line">with open(&#x27;bulk_movies.json&#x27;, &#x27;r&#x27;) as file:</span><br><span class="line">    data = file.read()</span><br><span class="line"></span><br><span class="line"># Making the POST request</span><br><span class="line">response = requests.post(url, data=data, auth=auth, headers=headers)</span><br><span class="line"></span><br><span class="line"># Printing the response (optional)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="how-to-query-texts-in-python"><a href="#how-to-query-texts-in-python" class="headerlink" title="how to query texts in python"></a>how to query texts in python</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">from requests.auth import HTTPBasicAuth</span><br><span class="line"></span><br><span class="line"># Endpoint URL</span><br><span class="line">url =f&#x27;&#123;domain_endpoint&#125;/movies/_search&#x27;</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">params = &#123;</span><br><span class="line">    &#x27;q&#x27;: &#x27;John&#x27;,</span><br><span class="line">    &#x27;pretty&#x27;: &#x27;true&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Authentication</span><br><span class="line">auth = HTTPBasicAuth(master_user_name, master_pwd)</span><br><span class="line"></span><br><span class="line"># Making the GET request</span><br><span class="line">response = requests.get(url, params=params, auth=auth)</span><br><span class="line"></span><br><span class="line"># Printing the response (optional)</span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> opensearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hightlights about chase freedom cards using AI summarization and comparison methods</title>
      <link href="2023/12/17/what-great-things-have-we-discovered-about-chase-freedom-cards-using-ai-summarization-methods/"/>
      <url>2023/12/17/what-great-things-have-we-discovered-about-chase-freedom-cards-using-ai-summarization-methods/</url>
      
        <content type="html"><![CDATA[<h1 id="Discover-the-Chase-Freedom-Family-A-Card-for-Every-Lifestyle"><a href="#Discover-the-Chase-Freedom-Family-A-Card-for-Every-Lifestyle" class="headerlink" title="Discover the Chase Freedom Family: A Card for Every Lifestyle"></a>Discover the <a href="https://www.referyourchasecard.com/18o/ASZH951DLD">Chase Freedom Family</a>: A Card for Every Lifestyle</h1><p>Welcome to our comprehensive guide on the Chase Freedom family of credit cards. Whether you’re a frequent shopper, a travel enthusiast, or looking for your first credit card, the Chase Freedom series has something for everyone. Let’s dive into what each card offers!</p><h2 id="1-Chase-Freedom-Unlimited-Card-Ultimate-Flexibility-and-Rewards"><a href="#1-Chase-Freedom-Unlimited-Card-Ultimate-Flexibility-and-Rewards" class="headerlink" title="1. Chase Freedom Unlimited Card: Ultimate Flexibility and Rewards"></a>1. <strong><a href="https://www.referyourchasecard.com/18o/ASZH951DLD">Chase Freedom Unlimited Card</a>: Ultimate Flexibility and Rewards</strong></h2><h3 id="Maximize-Your-Cash-Back"><a href="#Maximize-Your-Cash-Back" class="headerlink" title="Maximize Your Cash Back:"></a>Maximize Your Cash Back:</h3><p>The Chase Freedom Unlimited Card offers an enticing $200 bonus after you spend $500 in the first three months. This card is perfect if you haven’t had it before and haven’t received a new cardmember bonus in the past 24 months.</p><h3 id="Earn-More-on-Everyday-Purchases"><a href="#Earn-More-on-Everyday-Purchases" class="headerlink" title="Earn More on Everyday Purchases:"></a>Earn More on Everyday Purchases:</h3><p>With 5% cash back on combined gas station and grocery store purchases (excluding Target® and Walmart®) on up to $12,000 spent in the first year, you can earn up to $600 cash back. Plus, enjoy unlimited 1.5% cash back on all purchases, with no activation required.</p><h3 id="Dining-and-Travel-Benefits"><a href="#Dining-and-Travel-Benefits" class="headerlink" title="Dining and Travel Benefits:"></a>Dining and Travel Benefits:</h3><p>Earn 3% on dining, including takeout and eligible delivery services, 3% on drugstore purchases, and 5% on travel purchased through Chase Ultimate Rewards.</p><h3 id="No-Annual-Fee-and-Low-Intro-APR"><a href="#No-Annual-Fee-and-Low-Intro-APR" class="headerlink" title="No Annual Fee and Low Intro APR:"></a>No Annual Fee and Low Intro APR:</h3><p>Enjoy the perks without any annual fee and a 0% intro APR for 15 months on purchases and balance transfers.</p><h2 id="2-Chase-Freedom-Flex-Dynamic-Earning-Potential"><a href="#2-Chase-Freedom-Flex-Dynamic-Earning-Potential" class="headerlink" title="2. Chase Freedom Flex: Dynamic Earning Potential"></a>2. <strong><a href="https://www.referyourchasecard.com/18o/ASZH951DLD">Chase Freedom Flex</a>: Dynamic Earning Potential</strong></h2><h3 id="800-Cash-Back-Potential"><a href="#800-Cash-Back-Potential" class="headerlink" title="$800 Cash Back Potential:"></a>$800 Cash Back Potential:</h3><p>Like the Unlimited, the Flex offers up to $800 cash back, with a $200 bonus after spending $500 in the initial three months and up to $600 cash back on gas and grocery store purchases.</p><h3 id="5-Quarterly-Bonus-Categories"><a href="#5-Quarterly-Bonus-Categories" class="headerlink" title="5% Quarterly Bonus Categories:"></a>5% Quarterly Bonus Categories:</h3><p>Activate to earn 5% cash back on rotating categories each quarter, like gas stations and grocery stores, on up to $1,500 in total combined purchases.</p><h3 id="Diverse-Earning-Rates"><a href="#Diverse-Earning-Rates" class="headerlink" title="Diverse Earning Rates:"></a>Diverse Earning Rates:</h3><p>This card offers 5% on travel purchased through Chase Ultimate Rewards®, 3% on dining and drugstore purchases, and 1% on all other purchases.</p><h3 id="Introductory-Offers"><a href="#Introductory-Offers" class="headerlink" title="Introductory Offers:"></a>Introductory Offers:</h3><p>Enjoy no annual fee and a 0% intro APR for 15 months, followed by a variable APR.</p><h2 id="3-Chase-Freedom-Rise-Ideal-for-First-Time-Cardholders"><a href="#3-Chase-Freedom-Rise-Ideal-for-First-Time-Cardholders" class="headerlink" title="3. Chase Freedom Rise: Ideal for First-Time Cardholders"></a>3. <strong><a href="https://www.referyourchasecard.com/18o/ASZH951DLD">Chase Freedom Rise</a>: Ideal for First-Time Cardholders</strong></h2><h3 id="Boost-Your-Approval-Chances"><a href="#Boost-Your-Approval-Chances" class="headerlink" title="Boost Your Approval Chances:"></a>Boost Your Approval Chances:</h3><p>Having a Chase checking account with at least $250 increases your chances of approval for the Chase Freedom Rise.</p><h3 id="Simple-and-Rewarding"><a href="#Simple-and-Rewarding" class="headerlink" title="Simple and Rewarding:"></a>Simple and Rewarding:</h3><p>Earn 1.5% cash back on all purchases, with no annual fee. Plus, get a $25 statement credit for enrolling in automatic payments within the first three months.</p><h3 id="Credit-Building-Features"><a href="#Credit-Building-Features" class="headerlink" title="Credit Building Features:"></a>Credit Building Features:</h3><p>Be evaluated for a credit line increase in as soon as 6 months, with a variable APR of 26.99%.</p><h2 id="4-Chase-Slate-Edge-Card-Tailored-for-Financial-Growth"><a href="#4-Chase-Slate-Edge-Card-Tailored-for-Financial-Growth" class="headerlink" title="4. Chase Slate Edge Card: Tailored for Financial Growth"></a>4. <strong><a href="https://www.referyourchasecard.com/18o/ASZH951DLD">Chase Slate Edge Card</a>: Tailored for Financial Growth</strong></h2><h3 id="Extended-Low-APR-Period"><a href="#Extended-Low-APR-Period" class="headerlink" title="Extended Low APR Period:"></a>Extended Low APR Period:</h3><p>Enjoy a 0% Intro APR for 18 months on purchases and balance transfers, followed by a variable APR.</p><h3 id="Financial-Goals-Support"><a href="#Financial-Goals-Support" class="headerlink" title="Financial Goals Support:"></a>Financial Goals Support:</h3><p>This card is designed to help you achieve financial stability with no annual fee.</p><h3 id="APR-Reduction-and-Credit-Limit-Increase"><a href="#APR-Reduction-and-Credit-Limit-Increase" class="headerlink" title="APR Reduction and Credit Limit Increase:"></a>APR Reduction and Credit Limit Increase:</h3><p>Automatically considered for a 2% APR reduction when you meet certain criteria, and an evaluation for a higher credit limit after spending $500 in the first six months.</p><h3 id="Additional-Tools"><a href="#Additional-Tools" class="headerlink" title="Additional Tools:"></a>Additional Tools:</h3><p>Keep tabs on your credit health with Chase Credit Journey and split large purchases with My Chase Plan.</p><hr><p>Each card in the Chase Freedom family offers unique advantages tailored to different lifestyles and financial goals. Whether you’re looking to maximize cash back, enjoy flexible rewards, or build your credit, there’s a Chase Freedom card for you. Explore these options and find the perfect fit for your wallet!</p><h2 id="more-information"><a href="#more-information" class="headerlink" title="more information"></a>more information</h2><p>Read <a href="https://www.referyourchasecard.com/18o/ASZH951DLD">here</a> for more information.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fintech </tag>
            
            <tag> credit cards </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Use Orbstack as a fast and free alternative as docker desktop</title>
      <link href="2023/12/16/docker-desktop-not-free-try-orbstack-to-use-docker-seamlessly/"/>
      <url>2023/12/16/docker-desktop-not-free-try-orbstack-to-use-docker-seamlessly/</url>
      
        <content type="html"><![CDATA[<p>Docker is very useful, but for big company it may needs to pay to use docker desktop. <br>One good solution is to install OrbStack, which is free and enable you to use docker container seamlessly.</p><p>Two ways to use orbstack for docker container:<br>(1) download the <a href="https://orbstack.dev/download">orbstack</a>, no installation needed<br>(2) for mac users, simply do <code>brew install orbstack</code></p><p>Then everything is same to use docker, for example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -p 80:80 docker/getting-started</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> orbstack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is asterisk in python class o function argument list</title>
      <link href="2023/12/09/what-is-asterisk-in-python-function-or-class-argument-list/"/>
      <url>2023/12/09/what-is-asterisk-in-python-function-or-class-argument-list/</url>
      
        <content type="html"><![CDATA[<p>We see asterisk in some python class or function argumentlist, what does it mean?</p><h2 id="What-are-Keyword-Only-Arguments"><a href="#What-are-Keyword-Only-Arguments" class="headerlink" title="What are Keyword-Only Arguments?"></a>What are Keyword-Only Arguments?</h2><p>Introduced in Python 3, keyword-only arguments are function or method arguments that must be specified by their name. They prevent the confusion that can arise from having multiple parameters, enhancing code readability and reducing the likelihood of errors.</p><h3 id="Syntax-and-Usage"><a href="#Syntax-and-Usage" class="headerlink" title="Syntax and Usage"></a>Syntax and Usage</h3><p>The syntax for keyword-only arguments involves placing an asterisk * in the function’s or method’s parameter list. Parameters after the asterisk are treated as keyword-only. Let’s see this in the context of a Python class method:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class DocumentRetriever:</span><br><span class="line">    def _get_relevant_documents(self, query: str, *, run_manager):</span><br><span class="line">        # Implementation</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure><p>In the above example, the _get_relevant_documents method of the DocumentRetriever class has two parameters: query and run_manager. The asterisk * before run_manager indicates that it is a keyword-only argument.</p><h2 id="Why-Use-Keyword-Only-Arguments"><a href="#Why-Use-Keyword-Only-Arguments" class="headerlink" title="Why Use Keyword-Only Arguments?"></a>Why Use Keyword-Only Arguments?</h2><ul><li><p>Clarity: Keyword-only arguments require the caller to explicitly specify the name of the parameter, making the code more readable and clear. This is especially beneficial in methods with multiple parameters.</p></li><li><p>Avoiding Errors: They help prevent bugs that might occur when a caller mixes up the order of arguments.</p></li><li><p>Flexibility in API Design: They allow method signatures to be changed more easily, such as adding new parameters without worrying about their position.</p></li></ul><h2 id="Calling-Methods-with-Keyword-Only-Arguments"><a href="#Calling-Methods-with-Keyword-Only-Arguments" class="headerlink" title="Calling Methods with Keyword-Only Arguments"></a>Calling Methods with Keyword-Only Arguments</h2><p>To call a method with keyword-only arguments, you simply need to specify the keyword:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">retriever = DocumentRetriever()</span><br><span class="line">retriever._get_relevant_documents(&quot;some query&quot;, run_manager=some_manager)</span><br></pre></td></tr></table></figure><p>In this call, run_manager must be explicitly named. This is particularly useful when the function or method has multiple parameters, reducing the risk of passing arguments in the wrong order.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to call openai chat model using langchain</title>
      <link href="2023/12/06/calling-openai-chatmodel-using-langchain/"/>
      <url>2023/12/06/calling-openai-chatmodel-using-langchain/</url>
      
        <content type="html"><![CDATA[<p>To use the ChatOpenAI model from Langchain to get a response for messages, you can follow these steps:</p><ul><li>Install the <code>OpenAI</code> Python package by running <code>pip install openai</code>.</li><li>Obtain an API key from OpenAI by creating an account and visiting their API key page.</li><li>Set the API key as an environment variable by running <code>export OPENAI_API_KEY=&quot;your-api-key&quot;</code>, or set the key as parameter in the function (see below).</li><li>Import the <code>ChatOpenAI</code> class from <code>langchain.chat_models</code>.</li><li>Initialize an instance of ChatOpenAI with the API key: <code>chat = ChatOpenAI(openai_api_key=&quot;your-api-key&quot;)</code> ( pass api key as parameter If api key is not set in environment).</li><li>Create a list of messages to send to the model. Messages can be of types <code>AIMessage</code>, <code>HumanMessage</code>, or <code>SystemMessage</code>.</li><li>Invoke the model by calling <code>chat.invoke(messages)</code>. This will return an AIMessage containing the model’s response.</li><li>Alternatively, you can use <code>chat.stream(messages)</code> to stream the model’s response in chunks.</li><li>You can also use <code>chat.batch([messages])</code> to batch process multiple sets of messages.</li></ul><p>Here is an example of using ChatOpenAI to get a response for messages:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.schema.messages import HumanMessage, SystemMessage</span><br><span class="line"></span><br><span class="line">chat = ChatOpenAI(openai_api_key=&quot;your-api-key&quot;)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=&quot;You&#x27;re a helpful assistant&quot;),</span><br><span class="line">    HumanMessage(content=&quot;What is the purpose of model regularization?&quot;),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = chat.invoke(messages)</span><br><span class="line">print(response.content)</span><br></pre></td></tr></table></figure><p>This will output the model’s response to the given messages.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> langchain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to write and run unittest in python</title>
      <link href="2023/12/05/unittest-example-and-how-to-run-it-in-a-python-project/"/>
      <url>2023/12/05/unittest-example-and-how-to-run-it-in-a-python-project/</url>
      
        <content type="html"><![CDATA[<p>Following shows simple and cleary way to write unittest in python, and how to run them before launching a fastapi project.</p><h2 id="write-a-unittest"><a href="#write-a-unittest" class="headerlink" title="write a unittest"></a>write a unittest</h2><p>Suppose you have the following class in <code>some_class.py</code></p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># some_class.py</span><br><span class="line"></span><br><span class="line">class SomeClass:</span><br><span class="line">    def get_dictionary(self):</span><br><span class="line">        # Implementation of the function</span><br><span class="line">        return &#123;&quot;key&quot;: &quot;value&quot;&#125;  # Example dictionary</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Then it is easy to write an unitttest file like the following “test_some_class.py” as:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># test_some_class.py</span><br><span class="line"></span><br><span class="line">import unittest</span><br><span class="line">from some_class import SomeClass</span><br><span class="line"></span><br><span class="line">class TestSomeClass(unittest.TestCase):</span><br><span class="line">    def setUp(self):</span><br><span class="line">        # This method will be run before each test method</span><br><span class="line">        self.obj = SomeClass()</span><br><span class="line"></span><br><span class="line">    def test_get_dictionary(self):</span><br><span class="line">        result = self.obj.get_dictionary()</span><br><span class="line">        self.assertIsInstance(result, dict)  # Check if result is a dictionary</span><br><span class="line"></span><br><span class="line">    # You can add more test methods here. They will all have access to self.obj</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    unittest.main()</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="run-your-test-jobs-in-a-project"><a href="#run-your-test-jobs-in-a-project" class="headerlink" title="run your test jobs in a project"></a>run your test jobs in a project</h2><p>Now you have your unitttest setup, now how to you run the tests?<br>For example, you might have a fastapi project, but before deployment, you want to run some unittest.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_fastapi_project/</span><br><span class="line">│</span><br><span class="line">├── src/</span><br><span class="line">│   ├── some_class.py</span><br><span class="line">│   └── test_some_class.py</span><br><span class="line">│</span><br><span class="line">└── main.py  # &lt;-- Your FastAPI application entry point</span><br></pre></td></tr></table></figure><p>Here is how you would run the tests using the standard <code>unittest</code> module:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Navigate to the project directory</span></span><br><span class="line"><span class="built_in">cd</span> my_fastapi_project</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the unittests with discovery</span></span><br><span class="line">python -m unittest discover src</span><br></pre></td></tr></table></figure><p>If the tests are successful and you’re satisfied with the results, you can then start your FastAPI application:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Start the FastAPI application</span></span><br><span class="line">uvicorn main:app --reload</span><br></pre></td></tr></table></figure><p>Alternatively, if you have <code>pytest</code> installed, which provides more features and a nicer output, you can run:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Navigate to the project directory (if not already there)</span></span><br><span class="line"><span class="built_in">cd</span> my_fastapi_project</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run the tests with pytest</span></span><br><span class="line">pytest src</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check If pytest is successful</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> unittest </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to query database using natural language from GPT and langchain</title>
      <link href="2023/12/02/retrieve-structured-data-from-sql-database-using-natural-language-by-gpt-and-langchain/"/>
      <url>2023/12/02/retrieve-structured-data-from-sql-database-using-natural-language-by-gpt-and-langchain/</url>
      
        <content type="html"><![CDATA[<p>To extract answers from structured data, the first step is actually extracting relevant data from the database. <br>Once we obtained the most relevant data, inserting the data to a prompt with GPT, we can then get the answers easily.</p><p>Here we discuss how to leverage langchain and GPT to extract data from sql database according to natural language.</p><ol><li><p>Set up a connection to the SQL database using the SQLDatabase utility:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.utilities import SQLDatabase</span><br><span class="line">db = SQLDatabase.from_uri(&quot;sqlite:///mydatabase.db&quot;)</span><br></pre></td></tr></table></figure></li><li><p>Create a text-to-SQL chain that converts natural language questions to SQL queries:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.chains import create_sql_query_chain</span><br><span class="line">sql_chain = create_sql_query_chain(chat_model, db)</span><br></pre></td></tr></table></figure><p>Where <code>chat_model</code> is a chat model like ChatGPT and <code>db</code> is the SQLDatabase instance.</p></li><li><p>Pass the natural language question to the chain to generate the query:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">question = &quot;How many employees are there?&quot;</span><br><span class="line">query = sql_chain.invoke(&#123;&quot;question&quot;: question&#125;)</span><br></pre></td></tr></table></figure><p>The returned query will contain the generated SQL query string:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SELECT COUNT(*) FROM Employee</span><br></pre></td></tr></table></figure></li><li><p>Execute the query using the chain:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">results = query_chain.invoke(&#123;&quot;question&quot;: query&#125;) | db.run</span><br></pre></td></tr></table></figure></li></ol><p>So in summary, you use create_sql_query_chain to create a chain that converts text to SQL, pass your natural language question to it, and it will return the generated SQL query that corresponds to that question.<br>Some relevant information can be found here as well <a href="https://python.langchain.com/docs/integrations/toolkits/sql_database">here</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> langchain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to compress and decompress data using gzip in Python</title>
      <link href="2023/11/28/compress-data-using-gzip-in-python/"/>
      <url>2023/11/28/compress-data-using-gzip-in-python/</url>
      
        <content type="html"><![CDATA[<p>Python offers a straightforward approach to compress and decompress text data using the gzip module. In this blog, we’ll explore how to do just that.</p><h2 id="Compressing-Text-with-gzip"><a href="#Compressing-Text-with-gzip" class="headerlink" title="Compressing Text with gzip"></a>Compressing Text with gzip</h2><p>Compression is particularly useful when dealing with large amounts of text data. Python’s gzip module makes this process remarkably simple. Here’s how you can compress a string of text:</p><h3 id="Step-1-Import-gzip"><a href="#Step-1-Import-gzip" class="headerlink" title="Step 1: Import gzip"></a>Step 1: Import gzip</h3><p>First, you need to import the gzip module:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import gzip</span><br></pre></td></tr></table></figure><h3 id="Step-2-Convert-Text-to-Bytes"><a href="#Step-2-Convert-Text-to-Bytes" class="headerlink" title="Step 2: Convert Text to Bytes"></a>Step 2: Convert Text to Bytes</h3><p>Since gzip works with bytes, you’ll need to convert your text into a byte format:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">text = &quot;This is the text that you want to compress.&quot;</span><br><span class="line">text_bytes = text.encode(&#x27;utf-8&#x27;)</span><br></pre></td></tr></table></figure><h3 id="Step-3-Compress-the-Byte-Data"><a href="#Step-3-Compress-the-Byte-Data" class="headerlink" title="Step 3: Compress the Byte Data"></a>Step 3: Compress the Byte Data</h3><p>Now, use gzip.compress() to compress your byte data:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">compressed_data = gzip.compress(text_bytes)</span><br></pre></td></tr></table></figure><p>You can also save this compressed data to a file if needed:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">with open(&#x27;compressed_file.gz&#x27;, &#x27;wb&#x27;) as file:</span><br><span class="line">    file.write(compressed_data)</span><br></pre></td></tr></table></figure><h2 id="Decompressing-Data-with-gzip"><a href="#Decompressing-Data-with-gzip" class="headerlink" title="Decompressing Data with gzip"></a>Decompressing Data with gzip</h2><p>Receiving or reading compressed data is only half the journey. You’ll often need to decompress this data to utilize it:</p><h3 id="Step-1-Read-or-Receive-Compressed-Data"><a href="#Step-1-Read-or-Receive-Compressed-Data" class="headerlink" title="Step 1: Read or Receive Compressed Data"></a>Step 1: Read or Receive Compressed Data</h3><p>Assume compressed_data is the gzip-compressed data you’ve received or read from a file.</p><h3 id="Step-2-Decompress-the-Data"><a href="#Step-2-Decompress-the-Data" class="headerlink" title="Step 2: Decompress the Data"></a>Step 2: Decompress the Data</h3><p>Use gzip.decompress() for decompression:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">decompressed_data = gzip.decompress(compressed_data)</span><br></pre></td></tr></table></figure><h3 id="Step-3-Convert-Back-to-a-String"><a href="#Step-3-Convert-Back-to-a-String" class="headerlink" title="Step 3: Convert Back to a String"></a>Step 3: Convert Back to a String</h3><p>If the original data was a string, convert the decompressed byte data back to a string:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">original_text = decompressed_data.decode(&#x27;utf-8&#x27;)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> gzip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to figure out the exact expiration date for Expire with Statment of promotional balance transfer</title>
      <link href="2023/11/26/what-does-expire-with-statment-mean-for-pomotional-balance-transfer/"/>
      <url>2023/11/26/what-does-expire-with-statment-mean-for-pomotional-balance-transfer/</url>
      
        <content type="html"><![CDATA[<p>Credit card promotions, especially those offering low or zero percent Annual Percentage Rates (APR) on balance transfers, are attractive financial tools. However, understanding the exact terms, particularly when these offers expire, is crucial to managing your finances effectively. This blog post aims to clarify the often confusing language surrounding the expiry of promotional APRs.</p><h2 id="Understanding-‘Expires-with-Statement’-Terms"><a href="#Understanding-‘Expires-with-Statement’-Terms" class="headerlink" title="Understanding ‘Expires with Statement’ Terms"></a>Understanding ‘Expires with Statement’ Terms</h2><p>Credit card offers often use phrases like “expires with the statement of [Month/Year].” This means the promotional APR is valid until the end of the billing cycle for that specified month. For instance, if your offer states it “expires with the statement of June 2023,” the low APR applies until the day your June 2023 statement is generated.</p><h2 id="Key-Points-to-Remember"><a href="#Key-Points-to-Remember" class="headerlink" title="Key Points to Remember"></a>Key Points to Remember</h2><ol><li><strong>Check the Statement Date:</strong> The exact date when your statement is generated each month is critical. This is when your promotional rate will switch to the standard rate.</li><li><strong>Promotion Applies to Specific Balances:</strong> Typically, the promotional APR applies to balance transfers only. New purchases might incur the standard APR from the start.</li><li><strong>Plan Your Payments:</strong> Aim to pay off the balance transfer before the promotional period ends to avoid higher interest charges.</li><li><strong>Read the Fine Print:</strong> Always understand the terms of your credit card agreement. If the promotional expiry is not specified by an exact date, clarify with your credit card issuer.</li></ol><h2 id="Why-Exact-Dates-Matter"><a href="#Why-Exact-Dates-Matter" class="headerlink" title="Why Exact Dates Matter"></a>Why Exact Dates Matter</h2><p>In some cases, credit card promotions will specify an exact date when the low APR ends. This is straightforward and allows for precise financial planning. However, when a promotion states it expires with a statement, it can cause confusion. The key is to remember that the rate changes when the statement for the said month is generated, not at the end of the month.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> credit card </tag>
            
            <tag> balance transfer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NVIDIA Unveiled, Exploring Product Lines and GPU Architectures</title>
      <link href="2023/11/24/nvidia-product-lines-and-architecture/"/>
      <url>2023/11/24/nvidia-product-lines-and-architecture/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>NVIDIA has long been a titan in the realm of graphics processing units (GPUs), known for its innovative and powerful products. This comprehensive guide explores NVIDIA’s diverse product lines, delves into the architectures that drive them, and understands how they interconnect.</p><h2 id="NVIDIA’s-Product-Lines"><a href="#NVIDIA’s-Product-Lines" class="headerlink" title="NVIDIA’s Product Lines"></a>NVIDIA’s Product Lines</h2><h3 id="GeForce-Series-Gaming-and-Beyond"><a href="#GeForce-Series-Gaming-and-Beyond" class="headerlink" title="GeForce Series: Gaming and Beyond"></a>GeForce Series: Gaming and Beyond</h3><ul><li><strong>Ideal For</strong>: Gamers and general users.</li><li><strong>Key Features</strong>: Real-time ray tracing, AI-enhanced graphics with DLSS.</li><li><strong>Architectures Used</strong>: Turing and Ampere.</li></ul><h3 id="Quadro-Series-The-Professional’s-Choice"><a href="#Quadro-Series-The-Professional’s-Choice" class="headerlink" title="Quadro Series: The Professional’s Choice"></a>Quadro Series: The Professional’s Choice</h3><ul><li><strong>Ideal For</strong>: Professional graphic designers, video editors, and engineers.</li><li><strong>Key Features</strong>: High precision, certified drivers for professional software.</li><li><strong>Architectures Used</strong>: Various, including Turing and Ampere.</li></ul><h3 id="Tesla-and-A100-Powering-AI-and-Research"><a href="#Tesla-and-A100-Powering-AI-and-Research" class="headerlink" title="Tesla and A100: Powering AI and Research"></a>Tesla and A100: Powering AI and Research</h3><ul><li><strong>Ideal For</strong>: Data scientists and AI researchers.</li><li><strong>Key Features</strong>: Deep learning, AI, and HPC tasks.</li><li><strong>Architectures Used</strong>: Primarily Ampere.</li></ul><h3 id="TITAN-Series-Bridging-Gaps"><a href="#TITAN-Series-Bridging-Gaps" class="headerlink" title="TITAN Series: Bridging Gaps"></a>TITAN Series: Bridging Gaps</h3><ul><li><strong>Ideal For</strong>: Researchers and high-end content creators.</li><li><strong>Key Features</strong>: High-end performance for AI development and research.</li><li><strong>Architectures Used</strong>: Various, including Turing.</li></ul><h3 id="CUDA-and-Jetson-Expanding-Horizons"><a href="#CUDA-and-Jetson-Expanding-Horizons" class="headerlink" title="CUDA and Jetson: Expanding Horizons"></a>CUDA and Jetson: Expanding Horizons</h3><ul><li><strong>CUDA</strong>: A parallel computing platform that maximizes GPU performance.</li><li><strong>Jetson</strong>: AI capabilities for edge computing devices.</li></ul><h3 id="NVIDIA-Drive-Steering-the-Future"><a href="#NVIDIA-Drive-Steering-the-Future" class="headerlink" title="NVIDIA Drive: Steering the Future"></a>NVIDIA Drive: Steering the Future</h3><ul><li><strong>Ideal For</strong>: The autonomous vehicle industry.</li><li><strong>Key Features</strong>: AI platform for self-driving cars.</li></ul><h2 id="NVIDIA’s-GPU-Architectures"><a href="#NVIDIA’s-GPU-Architectures" class="headerlink" title="NVIDIA’s GPU Architectures"></a>NVIDIA’s GPU Architectures</h2><h3 id="Fermi-to-Pascal-Early-Innovations"><a href="#Fermi-to-Pascal-Early-Innovations" class="headerlink" title="Fermi to Pascal: Early Innovations"></a>Fermi to Pascal: Early Innovations</h3><ul><li><strong>Fermi</strong>: General-purpose computing and improved parallel processing.</li><li><strong>Kepler</strong>: Energy efficiency and increased CUDA cores.</li><li><strong>Maxwell</strong>: Enhanced rendering efficiency and color compression.</li><li><strong>Pascal</strong>: Significant performance boost, features for VR and gaming.</li></ul><h3 id="Volta-and-Turing-A-Leap-Forward"><a href="#Volta-and-Turing-A-Leap-Forward" class="headerlink" title="Volta and Turing: A Leap Forward"></a>Volta and Turing: A Leap Forward</h3><ul><li><strong>Volta</strong>: Introduced Tensor Cores, targeting AI and deep learning.</li><li><strong>Turing</strong>: Revolutionized gaming graphics with real-time ray tracing.</li></ul><h3 id="Ampere-The-recent-giant"><a href="#Ampere-The-recent-giant" class="headerlink" title="Ampere: The recent giant"></a>Ampere: The recent giant</h3><ul><li><strong>Introduced</strong>: In 2020, for both gaming and AI applications.</li><li><strong>Features</strong>: Enhanced ray tracing, powerful Tensor Cores, better energy efficiency.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>NVIDIA’s array of products, backed by groundbreaking architectures, caters to a vast spectrum of needs. Understanding the relationship between NVIDIA’s product lines and their underlying architectures helps in making informed choices and offers insight into the evolution of GPU technology.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nvidia </tag>
            
            <tag> gpu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to calculate output size of convolution layer for following linear layer input size</title>
      <link href="2023/11/21/how-to-calculate-output-size-after-convlution-in-cnn/"/>
      <url>2023/11/21/how-to-calculate-output-size-after-convlution-in-cnn/</url>
      
        <content type="html"><![CDATA[<p>In using Convolutional Network (CNN), one step is to calculate the output size after convolution and pooling steps, so we can pipe the outputs <br>to a fully collected linear layer.</p><p>Taking a one dimensional CNN in pytorch as an example,</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)</span><br></pre></td></tr></table></figure><p> important parameters includes <code>input_channels</code>, here we set as 1, it could be more than 1 depending on how many <br> features you have. In the 2d image case, the inut channels could be RGB, in the 1d data case, could be multiple sensor <br> measurements in a time series problem. the <code>output_channels</code> are dertermined by the number of filters we use <br> in our example, we set it as 64, meaning we are using 64 filters. <code>kernel_size</code> is the size of the <br> convolutional filter we use. <code>padding</code> is used to adjust the spatial resolution.</p><p> So the number of total output size of this 1d convolution module equals out_channels multiplied by the output size of <br> each filter. And the equation to calculate the exact output size of each filter is:</p><p> <img src="/content/images/2023-11-21-01.png"></p><p> From the formula, we can see it is affected by many factors including input_size of the sequence for each channel (1 chanel in our example),<br> kernel size, padding size and Strid size (usually set to 1).</p><p><strong>Padding in Convolutional Layers</strong>:</p><p>Padding refers to the addition of extra elements (typically zeros) to the input data before the convolution operation is performed.<br>This is done to control the size of the output of the convolutional layer.</p><p><strong>Effect of Padding</strong>:</p><p>The primary purpose of padding is to allow control over the spatial dimensions (in this case, the length) of the output tensor.<br>With padding, you can preserve the size of the input, increase it, or control the amount by which it decreases.<br>Specifically, padding can be used to ensure that the output size of the layer is the same as the input size, which is common in many CNN architectures to maintain the spatial resolution of the input through the network layers.</p><p><strong>Padding Value of 1</strong>:</p><p>A padding value of 1 means that one element of padding is added to each side of the input.<br>In the context of a 1D convolution, this would add one zero-value element to the beginning and one to the end of the input sequence.<br>For instance, if your input sequence is [a, b, c, d], with padding=1, it effectively becomes [0, a, b, c, d, 0] before the convolution operation is applied.</p><p><strong>Impact on Output Size</strong>:<br>When the kernel size is 3 and padding is 1 (as in your example), the convolutional layer will produce an output that has the same length as the input. This is because the padding compensates for the reduction in size that would otherwise occur due to the convolution operation.</p><p>Using the above formula, we can get the output size for each input sequence. In our example, since kernel_size is 3, setting padding to be 1, will <br>make the output size for each input sequence is the same as the input size, easy.  In pytorch,  we can also set <code>padding=&#39;same&#39;</code>, it will automatically <br>adjust the padding size to make sure the output size is the same as input size for each input sequence.</p><p>After the convolution step, it then follow the pooling step, which could be:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">self.pool = nn.MaxPool1d(kernel_size = 2)</span><br></pre></td></tr></table></figure><p>in this example, we basically half the size of the output from convolution. So in the end, each input sequence will become half of the input_size.</p><p>Now to get the total output size of two above steps, we need to multiply the number of filters we use. In our example, we use 64 filters,<br>so the final output size should be:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">64* (input_size//2 )</span><br></pre></td></tr></table></figure><p>And this size should be the input size to a fillowing fully connected layer.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cnn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Navigating the Transformative Era of Generative AI, Challenges and Opportunities</title>
      <link href="2023/11/20/challenges-and-opportunities-for-generative-ai-such-as-chatgpt/"/>
      <url>2023/11/20/challenges-and-opportunities-for-generative-ai-such-as-chatgpt/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h2><p>The advent of generative AI technologies, exemplified by tools like ChatGPT, represents a paradigm shift in the way information is accessed and utilized. These AI systems have simplified the process of finding answers, making it considerably more efficient compared to traditional search engines. However, this evolution raises critical questions about the sustainability of data sourcing, creator motivation, and the transparency of information.</p><h2 id="The-Data-Dilemma-Benefits-and-Losses"><a href="#The-Data-Dilemma-Benefits-and-Losses" class="headerlink" title="The Data Dilemma: Benefits and Losses"></a>The Data Dilemma: Benefits and Losses</h2><p>Generative AI tools, by their nature, aggregate and synthesize information from myriad sources. This capability, while beneficial for users seeking quick and comprehensive answers, poses a dilemma. The original creators of the content – be they bloggers, researchers, or industry experts – traditionally rely on platforms like Google for visibility, recognition, and potential revenue. In the new AI-driven landscape, the direct link between these creators and their audience is obscured. This obscurity could lead to a decline in motivation for content creators, as their work is consumed in an anonymized, aggregated form, stripping them of direct benefits such as acknowledgment and advertising revenue.</p><h2 id="Sustaining-Creator-Motivation-in-the-AI-Era"><a href="#Sustaining-Creator-Motivation-in-the-AI-Era" class="headerlink" title="Sustaining Creator Motivation in the AI Era"></a>Sustaining Creator Motivation in the AI Era</h2><p>The challenge, then, is to establish a new equilibrium. How can we ensure that content creators continue to be motivated in an ecosystem dominated by AI synthesizers? The solution may lie in developing new models of attribution and compensation. Just as the digital age has seen the evolution of copyright laws and content monetization strategies, the AI era demands innovative approaches to ensure creators are fairly recognized and rewarded. This might include AI systems that transparently attribute sources, or new monetization models that share revenue with the creators whose work informs the AI’s responses.</p><h2 id="The-Transparency-Challenge"><a href="#The-Transparency-Challenge" class="headerlink" title="The Transparency Challenge"></a>The Transparency Challenge</h2><p>Another critical issue is the transparency of information provided by AI. Unlike traditional search engines, where users can directly access and verify sources, generative AI often presents synthesized answers drawn from multiple sources. This process can make it significantly more challenging for users to trace and verify the facts. Ensuring the reliability and traceability of information in the age of AI is crucial. This might involve developing mechanisms for users to easily access source material or incorporating reliability ratings for synthesized answers based on the credibility of the underlying sources.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Generative AI technologies like ChatGPT mark a revolutionary step in information access and utilization. However, they also bring forth challenges concerning the sustainability of content creation and the transparency of synthesized information. Addressing these challenges requires a concerted effort to develop new models for creator recognition and compensation, as well as enhanced mechanisms for ensuring the transparency and traceability of AI-generated content. As we embrace the benefits of AI, we must also navigate its complexities to foster a balanced ecosystem that respects and rewards the contributions of all its participants.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> large language model </tag>
            
            <tag> generative AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Syntax changes in calling openAI in python in version 1.0 and above</title>
      <link href="2023/11/17/openai-python-client-version-1-and-above-syntax-changes/"/>
      <url>2023/11/17/openai-python-client-version-1-and-above-syntax-changes/</url>
      
        <content type="html"><![CDATA[<h2 id="Setting-Up-the-OpenAI-Client"><a href="#Setting-Up-the-OpenAI-Client" class="headerlink" title="Setting Up the OpenAI Client"></a>Setting Up the OpenAI Client</h2><h3 id="Importing-the-OpenAI-Library"><a href="#Importing-the-OpenAI-Library" class="headerlink" title="Importing the OpenAI Library"></a>Importing the OpenAI Library</h3><p>Firstly, ensure that you have the OpenAI library installed in your Python environment. If not, you can install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install openai</span><br></pre></td></tr></table></figure><p>Once installed, you can begin by importing the OpenAI module:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from openai import OpenAI</span><br></pre></td></tr></table></figure><h3 id="Initializing-the-Client"><a href="#Initializing-the-Client" class="headerlink" title="Initializing the Client"></a>Initializing the Client</h3><p>The next step involves initializing the OpenAI client. This is a straightforward process and is the gateway to accessing the ChatGPT models:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">client = OpenAI()</span><br></pre></td></tr></table></figure><h2 id="Crafting-a-ChatGPT-Request"><a href="#Crafting-a-ChatGPT-Request" class="headerlink" title="Crafting a ChatGPT Request"></a>Crafting a ChatGPT Request</h2><h3 id="Structuring-the-Request"><a href="#Structuring-the-Request" class="headerlink" title="Structuring the Request"></a>Structuring the Request</h3><p>With the client initialized, you can now structure your request to the ChatGPT model. This involves specifying the model, the format of the response, and the messages you wish to send:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">response = client.chat.completions.create(</span><br><span class="line">  model=&quot;gpt-3.5-turbo-1106&quot;,</span><br><span class="line">  response_format=&#123;&quot;type&quot;: &quot;json_object&quot;&#125;,</span><br><span class="line">  messages=[</span><br><span class="line">    &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant designed to output JSON.&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;</span><br><span class="line">  ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>In this example, the request uses the “gpt-3.5-turbo-1106” model. The messages are sent in a structured format, identifying the roles (system and user) and their corresponding content.</p><h3 id="Handling-the-Response"><a href="#Handling-the-Response" class="headerlink" title="Handling the Response"></a>Handling the Response</h3><p>Once the request is sent, you can handle the response from ChatGPT:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(response.choices[0].message.content)</span><br></pre></td></tr></table></figure><p>This command prints out the content of the response, allowing you to see the result of your query.</p><h3 id="Providing-the-API-Key"><a href="#Providing-the-API-Key" class="headerlink" title="Providing the API Key"></a>Providing the API Key</h3><p>When it comes to authentication, you have two primary ways to provide your OpenAI API key.</p><ol><li>Environment Variable (Recommended)<br>Setting the API key as an environment variable (OPENAI_API_KEY) is the recommended approach. This enhances security and makes your code portable without exposing the key:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export OPENAI_API_KEY=&#x27;your-api-key&#x27;</span><br></pre></td></tr></table></figure><p>When initialized, the OpenAI client automatically searches for this environment variable.</p><ol start="2"><li>Directly in Code<br>Alternatively, you can pass the API key directly when initializing the OpenAI client:</li></ol><p>```<br>client = OpenAI(api_key=’your-api-key’)<br>···<br>Be cautious with this method to prevent unintentional exposure of your API key.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why hyphens are used more in http urls</title>
      <link href="2023/11/14/why-hyphens-preferred-in-http-url-not-underscore/"/>
      <url>2023/11/14/why-hyphens-preferred-in-http-url-not-underscore/</url>
      
        <content type="html"><![CDATA[<p>The use of hyphens (<code>-</code>) instead of underscores (<code>_</code>) in URLs, particularly for domain names and paths, is primarily based on historical and technical reasons:</p><ol><li><p><strong>Standards and Conventions</strong>: The standards for URLs and domain names were established by organizations like the Internet Engineering Task Force (IETF) and the Internet Corporation for Assigned Names and Numbers (ICANN). These standards historically favored the use of hyphens over underscores.</p></li><li><p><strong>Domain Name System (DNS) Limitations</strong>: In domain names, only alphanumeric characters (<code>A-Z</code>, <code>0-9</code>) and hyphens are allowed. Underscores are not permitted in domain names according to DNS specifications. This is largely because the DNS was designed to be compatible with existing naming systems and protocols which did not use underscores.</p></li><li><p><strong>Search Engine Optimization (SEO)</strong>: Search engines like Google treat hyphens as space, allowing them to distinguish and index words separately in a URL. For example, <code>example-site.com</code> is read as “example site,” whereas <code>example_site.com</code> would be interpreted as a single word, “examplesite.” This impacts how websites are indexed and found via search engines.</p></li><li><p><strong>Usability and Clarity</strong>: Hyphens are generally more visible in URLs and are less likely to be overlooked or cause confusion. For instance, underscores might not be as visible when a URL is underlined, as is common in many hyperlinks.</p></li><li><p><strong>Historical Precedence</strong>: Early web standards and practices set a precedent for using hyphens, and this convention has continued due to inertia and the desire for backward compatibility.</p></li></ol><p>In the paths of URLs (the part after the domain name), underscores are technically allowed and do work in modern web systems. However, due to the reasons mentioned above, particularly around SEO and consistency with domain naming conventions, hyphens are generally preferred and more commonly used.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyphens </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The AI Powerhouse Emerges, NVIDIA Unveils the H200</title>
      <link href="2023/11/13/nvidia-unveils-latest-H200-model/"/>
      <url>2023/11/13/nvidia-unveils-latest-H200-model/</url>
      
        <content type="html"><![CDATA[<p>In a groundbreaking move, NVIDIA recently unveiled the H200, the world’s most powerful AI chip to date. This new chip marks a significant advancement over its predecessor, the H100, boasting performance improvements ranging from 60% to 90%. The H200 not only outperforms its predecessor but is also compatible with it, offering a seamless transition for businesses currently using the H100.</p><h2 id="The-Rush-for-Computational-Power"><a href="#The-Rush-for-Computational-Power" class="headerlink" title="The Rush for Computational Power"></a><strong>The Rush for Computational Power</strong></h2><p>The release of the H200 comes at a time when AI companies globally are facing a scarcity of computational power, with NVIDIA’s GPUs becoming increasingly rare and valuable. Reflecting the urgency of the demand, NVIDIA has shifted its product release cycle from every two years to annually.</p><h2 id="A-Glimpse-into-the-Future-H200-and-Beyond"><a href="#A-Glimpse-into-the-Future-H200-and-Beyond" class="headerlink" title="A Glimpse into the Future: H200 and Beyond"></a><strong>A Glimpse into the Future: H200 and Beyond</strong></h2><p>The H200, based on the Hopper architecture, is equipped with the H200 Tensor Core GPU and advanced memory capabilities, making it ideal for generative AI and high-performance computing workloads. The chip is the first to use HBM3e memory, boasting an impressive 141GB capacity. This upgrade nearly doubles the capacity and bandwidth compared to its predecessors, significantly enhancing data processing speed and efficiency.</p><h2 id="Performance-Leap-with-Llama-2"><a href="#Performance-Leap-with-Llama-2" class="headerlink" title="Performance Leap with Llama 2"></a><strong>Performance Leap with Llama 2</strong></h2><p>The H200 chip has made a remarkable impact on AI inference speed, nearly doubling the speed for the Llama 2 70B model compared to the H100. This performance leap is a testament to NVIDIA’s continuous efforts to enhance its technology, maintaining its lead in the AI and high-performance computing sector.</p><h2 id="The-Dawn-of-a-New-Era-in-AI-Supercomputing-Centers"><a href="#The-Dawn-of-a-New-Era-in-AI-Supercomputing-Centers" class="headerlink" title="The Dawn of a New Era in AI Supercomputing Centers"></a><strong>The Dawn of a New Era in AI Supercomputing Centers</strong></h2><p>With the introduction of the H200, NVIDIA is set to power a new generation of AI supercomputing centers worldwide. Leading cloud service providers, including Amazon Web Services, Google Cloud, Microsoft Azure, and Oracle Cloud Infrastructure, are among the first to deploy H200-based instances. Furthermore, top-tier supercomputing centers globally have announced plans to utilize the GH200 system in their upcoming supercomputers, signaling a new era of scientific innovation and computational capability.</p><h2 id="The-GPU-Race-Heats-Up"><a href="#The-GPU-Race-Heats-Up" class="headerlink" title="The GPU Race Heats Up"></a><strong>The GPU Race Heats Up</strong></h2><p>As NVIDIA continues to push the boundaries with the H200, competitors like AMD and Intel are not far behind. AMD’s upcoming Instinct MI300X aims to surpass the H200 in memory capacity and bandwidth, while Intel is set to enhance its Gaudi AI chip’s memory in its upcoming generations. This competitive landscape indicates a thrilling race in the AI chip market, with each player striving for technological supremacy.</p><h2 id="Pricing-and-Market-Impact"><a href="#Pricing-and-Market-Impact" class="headerlink" title="Pricing and Market Impact"></a><strong>Pricing and Market Impact</strong></h2><p>While the pricing of the H200 remains undisclosed, the anticipation is high, given the H100’s price range of $25,000 to $40,000. The chip’s launch next year is expected to create a significant stir in the market, further intensifying the demand for advanced AI chips. As companies and research institutions gear up for the next wave of AI advancements, NVIDIA’s H200 stands as a monumental step forward in computational technology.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> nvidia </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What does Bill Gates Think of Large Language Model and AI agents</title>
      <link href="2023/11/12/what-does-bill-gates-think-of-ai-agents/"/>
      <url>2023/11/12/what-does-bill-gates-think-of-ai-agents/</url>
      
        <content type="html"><![CDATA[<p>In his <a href="https://www.gatesnotes.com/AI-agents">blog post</a> “AI is about to completely change how you use computers,” dated November 9, 2023, Bill Gates discusses the imminent transformation in computing due to advancements in AI, particularly focusing on AI agents. Gates highlights the limitations of current software, requiring users to navigate different applications for varied tasks. He predicts that within five years, AI agents will change this dynamic, allowing users to simply instruct their devices in natural language to perform diverse tasks, based on a deep understanding of the user’s life.</p><p>These AI agents, a concept Gates has contemplated for nearly three decades, will be capable of performing a wide range of tasks across different applications, continuously learning and adapting to user behavior and preferences. This will mark the most significant revolution in computing since the shift from command-line interfaces to graphical user interfaces.</p><h2 id="Key-Highlights"><a href="#Key-Highlights" class="headerlink" title="Key Highlights:"></a>Key Highlights:</h2><ul><li><p><strong>Current Software Limitations</strong>: Current software requires navigating various applications for different tasks.</p></li><li><p><strong>The Role of AI Agents</strong>: In the next five years, AI agents will enable users to perform a wide range of tasks by simply instructing their devices in natural language. These agents will understand and adapt to user’s personal lives, marking a significant shift in computing.</p></li><li><p><strong>Transformative Sectors</strong>:</p><ol><li><strong>Healthcare</strong>: AI agents will assist in triage, health advice, and support healthcare workers.</li><li><strong>Education</strong>: Agents will supplement teaching, providing personalized learning and reducing teachers’ administrative work.</li><li><strong>Productivity</strong>: Enhancements in productivity tools, assisting in diverse tasks and decision making.</li><li><strong>Entertainment and Shopping</strong>: Personalizing recommendations based on individual preferences.</li></ol></li><li><p><strong>Future of Computing</strong>: AI agents are predicted to become the next major computing platform, simplifying app and service creation, but also raising technical and ethical challenges.</p></li><li><p><strong>Challenges and Considerations</strong>: Includes data privacy, security, potential misuse, and the impact on personal relationships and societal structures.</p></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Gates foresees a profound impact on society and the software industry, as AI agents democratize services, change our interaction with technology, and challenge existing business models.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> AI agent </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Rising Tide of AI, Honoring the Human Wellspring of Knowledge</title>
      <link href="2023/11/08/human-role-in-the-age-of-large-language-model-ai/"/>
      <url>2023/11/08/human-role-in-the-age-of-large-language-model-ai/</url>
      
        <content type="html"><![CDATA[<p>As the capabilities of AI and machine learning platforms like OpenAI’s GPT models continue to burgeon, these systems are beginning to take on roles that were once considered exclusively human. They are becoming adept agents capable of making decisions, demonstrating multi-modal abilities to comprehend and generate images, engage in conversations, and even create art. This blossoming of technology is a marvel of modern science and engineering. However, as we stand on the cusp of this new age, it’s essential to pause and reflect on the source of this newfound intelligence.</p><h2 id="The-Intellect-Behind-the-Intelligence"><a href="#The-Intellect-Behind-the-Intelligence" class="headerlink" title="The Intellect Behind the Intelligence"></a>The Intellect Behind the Intelligence</h2><p>The impressive reasoning power and knowledge base of large language models, including the latest iterations of OpenAI’s GPT, are fundamentally derived from vast online datasets—repositories of human thought, creativity, and information spanning several years. The substance of these AI systems is the distilled essence of human ingenuity, drawn from countless individuals who have poured their intellect into the digital ether.</p><p>This fact is both humbling and enlightening. It reminds us that behind every nuanced AI-generated response, there’s a mosaic of human experiences and works. The most significant contribution of a large language model isn’t just the advanced architecture, the elegant machine learning models, or the prodigious storage capacity. It’s the ability to assimilate and reflect the vast expanse of human knowledge swiftly and comprehensively.</p><h2 id="Acknowledging-the-Human-Contribution"><a href="#Acknowledging-the-Human-Contribution" class="headerlink" title="Acknowledging the Human Contribution"></a>Acknowledging the Human Contribution</h2><p>As we advance, a natural and pressing question emerges: How do we safeguard and honor the human contribution to this technological wonder? The issue is complex and multifaceted, touching on ethical, legal, and economic domains. For instance, consider copyright law. It has traditionally served as a mechanism to protect creators’ intellectual property. With the advent of AI that leverages human-generated content, these laws might need to evolve.</p><p>One idea that is gaining traction is the notion that individuals who contribute to the large data pools—those which feed the voracious appetite of language models—should receive recognition and, possibly, compensation. If an AI can monetize the data harvested from these contributions, should not the contributors also benefit?</p><h2 id="Profit-Sharing-in-the-Age-of-AI"><a href="#Profit-Sharing-in-the-Age-of-AI" class="headerlink" title="Profit Sharing in the Age of AI"></a>Profit Sharing in the Age of AI</h2><p>This concept extends to the creators of web pages, social media content, articles, and more. As these platforms serve as the bedrock upon which AI models are trained, the authors’ intellectual labor should not go unrecognized. Just as a river’s tributaries are integral to its power and reach, so too are the contributions of individual content creators to the might of AI.</p><p>Imagine a future where authors who put original work and data online are not just acknowledged but rewarded. A future where the symbiotic relationship between human creativity and AI innovation is both recognized and celebrated.</p><h2 id="Rewarding-Originality-in-the-Digital-Realm"><a href="#Rewarding-Originality-in-the-Digital-Realm" class="headerlink" title="Rewarding Originality in the Digital Realm"></a>Rewarding Originality in the Digital Realm</h2><p>What would such a future look like? For one, it might involve the development of new, fair-use policies and revenue-sharing models that adequately compensate creators. It could also necessitate the creation of sophisticated tracking and attribution systems to ensure that those whose data nourishes the AI landscape are duly credited.</p><p>Moreover, this shift would likely stimulate a positive feedback loop: as individuals realize that their contributions are valued both in recognition and remuneration, they are incentivized to create more, to share more, and to innovate more. This, in turn, fuels the AI systems that rely on this influx of fresh, diverse, and vibrant data.</p><h2 id="A-Call-to-Action"><a href="#A-Call-to-Action" class="headerlink" title="A Call to Action"></a>A Call to Action</h2><p>As we continue to marvel at the feats of AI and embrace its potential to transform society, let’s not forget the fundamental human element at its core. The conversation around how we protect and reward the creators of the data that powers AI is not just a philosophical one—it is a call to action for policymakers, technologists, and content creators alike. It is a dialogue about fairness and the sustainable development of AI in harmony with human endeavor.</p><p>By addressing these concerns today, we prepare the groundwork for a future that respects and advances both human and artificial intelligence. After all, it is in this synergy that the true potential of both is fully realized.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to convert text to image and how does ttf font works</title>
      <link href="2023/11/05/how-to-convert-text-to-image-and-how-does-ttf-font-works/"/>
      <url>2023/11/05/how-to-convert-text-to-image-and-how-does-ttf-font-works/</url>
      
        <content type="html"><![CDATA[<p>Here is how do we convert text to image using python:</p><h2 id="Python-code-to-convert-text-to-image"><a href="#Python-code-to-convert-text-to-image" class="headerlink" title="Python code to convert text to image"></a>Python code to convert text to image</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from PIL import Image, ImageDraw, ImageFont</span><br><span class="line"></span><br><span class="line">def text_to_png(text, font_path, font_size):</span><br><span class="line">    # Load the font</span><br><span class="line">    font = ImageFont.truetype(font_path, font_size)</span><br><span class="line">    </span><br><span class="line">    # Create image with transparent background</span><br><span class="line">    image_size = (500, 500)  # You might want to dynamically set this based on text dimensions</span><br><span class="line">    image = Image.new(&quot;RGBA&quot;, image_size, (255, 255, 255, 255))</span><br><span class="line">    </span><br><span class="line">    # Initialize drawing context</span><br><span class="line">    draw = ImageDraw.Draw(image)</span><br><span class="line">    </span><br><span class="line">    # Get text size</span><br><span class="line">    text_size = draw.textsize(text, font=font)</span><br><span class="line">    </span><br><span class="line">    # Calculate position</span><br><span class="line">    text_x = (image_size[0] - text_size[0]) / 2</span><br><span class="line">    text_y = (image_size[1] - text_size[1]) / 2</span><br><span class="line">    </span><br><span class="line">    # Apply text to image</span><br><span class="line">    draw.text((text_x, text_y), text, font=font, fill=&quot;black&quot;)</span><br><span class="line">    </span><br><span class="line">    # Save image</span><br><span class="line">    image.save(&quot;word.png&quot;)</span><br><span class="line"></span><br><span class="line">text_to_png(&quot;hello&quot;, &quot;ARIAL.TTF&quot;, 50)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Where we can see, one key is to use the right ttf font.</p><p>A TrueType Font (TTF) file is a font file format that was developed by Apple and Microsoft in the late 1980s as a competitor to Adobe’s Type 1 fonts used in PostScript. It has become a widely adopted standard for font display.</p><h2 id="How-TrueType-Fonts-Work"><a href="#How-TrueType-Fonts-Work" class="headerlink" title="How TrueType Fonts Work:"></a>How TrueType Fonts Work:</h2><p>TrueType fonts contain both the screen and printer font data in a single component, making the font scalable to any size. Here’s a bit about how they work:</p><p>Outline Descriptions: TrueType fonts are made up of outlines for each character (glyph), defined by quadratic Bézier curves. These outlines are scalable to any size, which means that they can be resized without losing quality.</p><p>Hinting: To ensure that these outlines look good when displayed at small sizes on low-resolution screens, TrueType fonts include “hints”. These are instructions that adjust the display of the glyph to make it more readable at various sizes and resolutions.</p><p>Glyph Mapping: TrueType fonts include a ‘cmap’ table that maps character codes to the corresponding glyph. This table is used to select the correct glyph when a character is displayed.</p><p>Embedding Rights: The font files can contain data that specify embedding rights, determining if and how the font can be embedded in a document or used on a webpage.</p><p>Rendering: When a character is to be displayed, the font engine reads the outline from the TTF file, applies hinting instructions for the current resolution, and then rasterizes the outline into pixels on the screen or into dots for a printer.</p><h2 id="Including-Fonts-for-Different-Languages"><a href="#Including-Fonts-for-Different-Languages" class="headerlink" title="Including Fonts for Different Languages:"></a>Including Fonts for Different Languages:</h2><p>To include fonts that support different languages, the font must have the necessary glyphs and character mappings for the characters used in those languages. Here’s how you can include such fonts:</p><p>Unicode Support: Ensure that the TrueType font supports Unicode, which provides a unique number for every character, no matter the platform, program, or language. Unicode fonts can support many different languages and scripts.</p><p>Font Selection: Choose a font that includes the range of Unicode characters needed for the languages you wish to support. For instance, if you need to support both Latin and Cyrillic scripts, the font must have the corresponding glyphs for both scripts.</p><p>OS and Application Support: The operating system and the application you are using must support the language and script you intend to display. They must be able to handle input methods for different languages and correctly apply the language-specific rules of shaping and layout.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Shallow and Deep Copies in Python</title>
      <link href="2023/11/01/copy-vs-deep-copy-in-python/"/>
      <url>2023/11/01/copy-vs-deep-copy-in-python/</url>
      
        <content type="html"><![CDATA[<p>When working with complex data structures in Python, understanding the difference between shallow and deep copies is crucial. This knowledge helps prevent unexpected behavior in your code, especially when dealing with mutable objects like lists, dictionaries, and sets. In this blog, we’ll explore the concepts of shallow and deep copies, providing examples and use cases to help solidify your understanding.</p><h2 id="Understanding-Shallow-Copy"><a href="#Understanding-Shallow-Copy" class="headerlink" title="Understanding Shallow Copy:"></a>Understanding Shallow Copy:</h2><p>A shallow copy creates a new object, but instead of copying the nested objects, it copies references to them. As a result, changes made to mutable objects within the copied object will be reflected in the original object, and vice versa.</p><h3 id="Creating-Shallow-Copies-with-the-copy-Module"><a href="#Creating-Shallow-Copies-with-the-copy-Module" class="headerlink" title="Creating Shallow Copies with the copy Module:"></a>Creating Shallow Copies with the copy Module:</h3><p>Python’s copy module provides a copy() function to create shallow copies of objects.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line"></span><br><span class="line">original_list = [1, [2, 3], [4, 5]]</span><br><span class="line">shallow_copied_list = copy.copy(original_list)</span><br><span class="line"></span><br><span class="line">shallow_copied_list[1][0] = 99</span><br><span class="line">print(original_list)  # Output: [1, [99, 3], [4, 5]]</span><br></pre></td></tr></table></figure><p>In this example, shallow_copied_list is a shallow copy of original_list. When we modify a nested list within shallow_copied_list, the change is reflected in original_list, showcasing the characteristic behavior of shallow copies.</p><h3 id="Using-Built-in-Types’-copy-Methods"><a href="#Using-Built-in-Types’-copy-Methods" class="headerlink" title="Using Built-in Types’ copy Methods:"></a>Using Built-in Types’ copy Methods:</h3><p>Several built-in types in Python provide a .copy() method for creating shallow copies.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">original_list = [1, 2, 3]</span><br><span class="line">copied_list = original_list.copy()</span><br><span class="line"></span><br><span class="line">original_dict = &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 2&#125;</span><br><span class="line">copied_dict = original_dict.copy()</span><br></pre></td></tr></table></figure><h2 id="Understanding-Deep-Copy"><a href="#Understanding-Deep-Copy" class="headerlink" title="Understanding Deep Copy:"></a>Understanding Deep Copy:</h2><p>In contrast to shallow copies, a deep copy creates a new object along with recursive copies of nested objects. Changes to nested objects in the deep copy do not affect the original object, and vice versa.</p><h3 id="Creating-Deep-Copies-with-the-copy-Module"><a href="#Creating-Deep-Copies-with-the-copy-Module" class="headerlink" title="Creating Deep Copies with the copy Module:"></a>Creating Deep Copies with the copy Module:</h3><p>The copy module also provides a deepcopy() function for creating deep copies.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import copy</span><br><span class="line"></span><br><span class="line">original_list = [1, [2, 3], [4, 5]]</span><br><span class="line">deep_copied_list = copy.deepcopy(original_list)</span><br><span class="line"></span><br><span class="line">deep_copied_list[1][0] = 99</span><br><span class="line">print(original_list)  # Output: [1, [2, 3], [4, 5]]</span><br></pre></td></tr></table></figure><p>In this example, deep_copied_list is a deep copy of original_list. Changes made to a nested list within deep_copied_list do not affect original_list.</p><h2 id="Use-Cases-and-Considerations"><a href="#Use-Cases-and-Considerations" class="headerlink" title="Use Cases and Considerations:"></a>Use Cases and Considerations:</h2><p>Use shallow copies when you want to create a new object but maintain references to the original nested objects.<br>Use deep copies when you need a completely independent copy of an object and all its nested objects.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Discovering the Enhancements in Pandas 2.0 and Beyond</title>
      <link href="2023/10/28/what-is-new-in-pandas-2.0/"/>
      <url>2023/10/28/what-is-new-in-pandas-2.0/</url>
      
        <content type="html"><![CDATA[<p>The release of Pandas 2.0 and the subsequent versions have introduced a variety of features and enhancements, marking a significant evolution in data manipulation and analysis with Pandas. Here’s a deep dive into some of the new capabilities:</p><h2 id="1-Optional-Dependencies-Installation"><a href="#1-Optional-Dependencies-Installation" class="headerlink" title="1. Optional Dependencies Installation:"></a>1. <strong>Optional Dependencies Installation</strong>:</h2><p>With Pandas 2.0, while installing pandas using pip, sets of optional dependencies can be installed by specifying extras, for example: <code>pip install &quot;pandas[performance, aws]&gt;=2.0.0&quot;</code>. The available extras include options for performance, computation, file system support, cloud providers, data formats, and more.</p><h2 id="2-Enhanced-Numeric-Dtype-Support-in-Index"><a href="#2-Enhanced-Numeric-Dtype-Support-in-Index" class="headerlink" title="2. Enhanced Numeric Dtype Support in Index:"></a>2. <strong>Enhanced Numeric Dtype Support in Index</strong>:</h2><p>Indexes can now hold any numpy numeric dtype, overcoming the previous limitation of supporting only <code>int64</code>, <code>uint64</code>, and <code>float64</code> dtypes.</p><h2 id="3-PyArrow-Integration"><a href="#3-PyArrow-Integration" class="headerlink" title="3. PyArrow Integration:"></a>3. <strong>PyArrow Integration</strong>:</h2><p>The defining feature of Pandas 2.0 is its integration with PyArrow, enabling more memory-efficient operations. Users can now use PyArrow as their backing memory format instead of the originally used NumPy data structures, which addresses issues of inefficient memory usage.</p><h2 id="4-Nullable-Data-Types"><a href="#4-Nullable-Data-Types" class="headerlink" title="4. Nullable Data Types:"></a>4. <strong>Nullable Data Types</strong>:</h2><p>Handling missing values has been made easier with the support for nullable data types. This feature allows for more straightforward handling of null values, especially in integer columns, by specifying the use of nullable data types when reading data into a DataFrame, for example: <code>pd.read_csv(my_file, use_nullable_dtypes=True)</code>.</p><h2 id="5-Copy-on-Write-Performance-Enhancement"><a href="#5-Copy-on-Write-Performance-Enhancement" class="headerlink" title="5. Copy-on-Write Performance Enhancement:"></a>5. <strong>Copy-on-Write Performance Enhancement</strong>:</h2><p>A memory optimization technique known as Copy-on-Write has been implemented to minimize memory usage and enhance performance while handling large datasets.</p><h2 id="6-Enhanced-Extension-Array-Support-and-Non-Nanosecond-Datetime-Resolution"><a href="#6-Enhanced-Extension-Array-Support-and-Non-Nanosecond-Datetime-Resolution" class="headerlink" title="6. Enhanced Extension Array Support, and Non-Nanosecond Datetime Resolution:"></a>6. <strong>Enhanced Extension Array Support, and Non-Nanosecond Datetime Resolution</strong>:</h2><p>The release also brought enhanced extension array support and non-nanosecond datetime resolution.</p><h2 id="7-Performance-Improvements"><a href="#7-Performance-Improvements" class="headerlink" title="7. Performance Improvements:"></a>7. <strong>Performance Improvements</strong>:</h2><p>Continuous performance improvements were made across different versions, improving the overall efficiency of the library.</p><p>These updates come as a result of continuous development efforts over three years, marking a significant step forward in making Pandas more robust and user-friendly for data manipulation and analysis tasks.</p><h2 id="Example-Using-Nullable-Data-Types"><a href="#Example-Using-Nullable-Data-Types" class="headerlink" title="Example: Using Nullable Data Types"></a>Example: Using Nullable Data Types</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assume &#x27;my_file.csv&#x27; has some columns with missing values</span></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;my_file.csv&#x27;</span>, use_nullable_dtypes=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This will ensure that columns with missing values and integer data will use the Int64 dtype which supports null values, instead of converting to float.</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dictionary Merging in Python 3.10 with the Pipeline Operator</title>
      <link href="2023/10/26/dictionary-merging-using-pipeline-operation-in-python/"/>
      <url>2023/10/26/dictionary-merging-using-pipeline-operation-in-python/</url>
      
        <content type="html"><![CDATA[<p>Python 3.10 has introduced a raft of features aiming at simplifying common programming tasks, and among these shines the new pipeline operator (|) which now facilitates seamless dictionary merging and updating. This addition is a testament to Python’s ongoing commitment to offer a more readable and expressive syntax.</p><p>In prior versions, merging dictionaries involved the use of the update() method or dictionary unpacking which, while effective, wasn’t always the most intuitive or readable. The introduction of the pipeline operator | as a dictionary merging tool is a welcome enhancement in Python 3.10, making code more accessible and straightforward. Let’s delve into how this operator revolutionizes dictionary operations.</p><h2 id="Merging-Dictionaries"><a href="#Merging-Dictionaries" class="headerlink" title="Merging Dictionaries"></a>Merging Dictionaries</h2><p>Merging dictionaries is a common task, and Python 3.10 makes this easier than ever. With the pipeline operator, you can now merge two dictionaries succinctly:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;</span><br><span class="line">dict2 = &#123;&quot;b&quot;: 3, &quot;c&quot;: 4&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Merging dict1 and dict2</span><br><span class="line">merged_dict = dict1 | dict2</span><br><span class="line">print(merged_dict)  # Output: &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 3, &#x27;c&#x27;: 4&#125;</span><br></pre></td></tr></table></figure><p>In the merged dictionary, the value of key ‘b’ is 3, the value from dict2, indicating that in case of key collisions, the right-hand side dictionary values take precedence.</p><h2 id="In-Place-Dictionary-Updating"><a href="#In-Place-Dictionary-Updating" class="headerlink" title="In-Place Dictionary Updating"></a>In-Place Dictionary Updating</h2><p>In addition to creating a new merged dictionary, you can also update an existing dictionary in place with the |= operator:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dict1 = &#123;&quot;a&quot;: 1, &quot;b&quot;: 2&#125;</span><br><span class="line">dict2 = &#123;&quot;b&quot;: 3, &quot;c&quot;: 4&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Updating dict1 in place</span><br><span class="line"></span><br><span class="line">dict1 |= dict2</span><br><span class="line">print(dict1)  # Output: &#123;&#x27;a&#x27;: 1, &#x27;b&#x27;: 3, &#x27;c&#x27;: 4&#125;</span><br></pre></td></tr></table></figure><p>This operation modifies dict1 to include all key-value pairs from dict2, again adhering to the rule that in case of key collisions, the right-hand side values are favored.</p><p>Enhancing Readability<br>The addition of the pipeline operator for dictionary operations is a stride towards enhancing the readability and simplicity of the language. This new syntax is intuitive and easy to grasp, even for those new to Python. It also aligns well with Python’s philosophy of being a language that is easy to read and write.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unveiling Microsoft&#39;s AutoGen, A Leap Towards Advanced LLM Applications</title>
      <link href="2023/10/24/how-does-the-microsoft-multi-agent-framework-autogen-work/"/>
      <url>2023/10/24/how-does-the-microsoft-multi-agent-framework-autogen-work/</url>
      
        <content type="html"><![CDATA[<p>Microsoft has recently introduced a groundbreaking framework known as AutoGen, aiming to propel the capabilities of Large Language Models (LLMs) like GPT-4 to new horizons. Here’s a deep dive into what AutoGen brings to the table and how it’s poised to revolutionize the landscape of LLM-based applications.</p><h2 id="What-is-AutoGen"><a href="#What-is-AutoGen" class="headerlink" title="What is AutoGen?"></a>What is AutoGen?</h2><p>AutoGen serves as a robust framework designed to simplify, orchestrate, and automate workflows associated with LLMs. This innovation facilitates the development of more complex LLM-based applications by providing a platform where multiple, customizable, and conversable agents can interact to accomplish various tasks. These agents can operate in diverse modes, employing combinations of LLMs, human inputs, and tools, thereby extending the capabilities of LLMs in solving complex problems.</p><h2 id="Multi-Agent-Conversations"><a href="#Multi-Agent-Conversations" class="headerlink" title="Multi-Agent Conversations"></a>Multi-Agent Conversations</h2><p>One of the cornerstone features of AutoGen is its support for multi-agent conversations. Developers can design agents with specialized capabilities and roles, and define their interaction behaviors to solve tasks collaboratively through automated chat.</p><h2 id="Customizable-and-Conversable-Agents"><a href="#Customizable-and-Conversable-Agents" class="headerlink" title="Customizable and Conversable Agents"></a>Customizable and Conversable Agents</h2><p>AutoGen agents are designed to be customizable and conversable, which enables a flexible and intuitive design of complex multi-agent systems. This customizability allows developers to easily configure the roles and usage of LLMs within an agent, integrate human intelligence and oversight through proxy agents, and even have native support for LLM-driven code or function execution.</p><h2 id="Automation-and-Optimization"><a href="#Automation-and-Optimization" class="headerlink" title="Automation and Optimization"></a>Automation and Optimization</h2><p>By automating and optimizing LLM workflows, AutoGen alleviates the challenges posed by the intricate workflows required to leverage the full potential of LLMs. This automation significantly reduces the manual effort and expertise needed to design, implement, and optimize these workflows, making it easier for developers to create increasingly complex LLM-based applications.</p><h2 id="A-Glimpse-into-Multi-Agent-Collaboration"><a href="#A-Glimpse-into-Multi-Agent-Collaboration" class="headerlink" title="A Glimpse into Multi-Agent Collaboration"></a>A Glimpse into Multi-Agent Collaboration</h2><p>Let’s delve into a practical example to understand how AutoGen facilitates multi-agent collaboration in a code-based question answering scenario:</p><ul><li><p><strong>Defining Agents and Roles</strong>:</p><ul><li>Three agents are defined: the Commander, the Writer, and the Safeguard.</li><li>The <code>Commander</code> receives user questions and coordinates with the other two agents.</li><li>The <code>Writer</code> is responsible for crafting the code and interpretation based on the user’s question.</li><li>The <code>Safeguard</code> ensures the safety of the code generated.</li></ul></li><li><p><strong>Agent Interaction</strong>:</p><ul><li>The <code>Commander</code> communicates with the <code>Writer</code> to craft the appropriate code, while the <code>Safeguard</code> reviews the code for any potential issues or unsafe practices.</li></ul></li><li><p><strong>Execution and Iteration</strong>:</p><ul><li>Once the code is deemed safe and accurately addresses the user’s question, the <code>Commander</code> executes it.</li><li>If any issues arise during execution, the process can repeat, with the agents collaborating to resolve the issues through further iterations.</li></ul></li><li><p><strong>Benefits</strong>:</p><ul><li>The workflow is streamlined and the number of manual interactions is significantly reduced, leading to a more efficient process.</li><li>The modular design of the agents and the interactions facilitated by AutoGen leads to a more than 4x reduction in coding effort in this scenario.</li></ul></li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>AutoGen represents a significant stride in addressing the complexities involved in leveraging the potent capabilities of LLMs. Its structured, automated, and intuitive framework opens new avenues for developing complex LLM-based applications and multi-agent conversational systems, showcasing a promising future in the realm of Artificial Intelligence.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> autogen </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Langchain Chat Prompt Templates, A Dive into Input and Partial Variables</title>
      <link href="2023/10/20/difference-between-input-variable-and-parital-variable-in-langchain-prompt-template/"/>
      <url>2023/10/20/difference-between-input-variable-and-parital-variable-in-langchain-prompt-template/</url>
      
        <content type="html"><![CDATA[<p>In Langchain, when using chat prompt templates there are the two relevant but confustion concepts of inoput variable and partial variable. Let’s understand the difference.</p><h2 id="Understanding-Input-Variables"><a href="#Understanding-Input-Variables" class="headerlink" title="Understanding Input Variables:"></a>Understanding Input Variables:</h2><p>Input variables are fundamental placeholders in a Langchain chat prompt template, awaiting specific values to complete the template. When a developer constructs a prompt template, they denote the input variables that will be requisite to formulate a complete prompt. These variables are then supplied at the time of formatting the template, making them a crucial component for dynamic prompt creation.</p><h2 id="Embracing-Partial-Variables"><a href="#Embracing-Partial-Variables" class="headerlink" title="Embracing Partial Variables:"></a>Embracing Partial Variables:</h2><p>On the flip side, partial variables come to the forefront when some values required by the prompt template are available beforehand, while others are anticipated later. This scenario unfolds a feature known as “partialing” where a prompt template is pre-filled with known values, creating a new template that awaits only the remaining unknown values. This mechanism allows for early binding of known values and deferred binding of others, proving invaluable in dynamic or staged processing environments.</p><p>Illustrative Examples:<br>Let’s delve into some examples to visualize the utility of input and partial variables in Langchain chat prompt templates.</p><h3 id="Example-1-Simple-Partialing"><a href="#Example-1-Simple-Partialing" class="headerlink" title="Example 1: Simple Partialing"></a>Example 1: Simple Partialing</h3><p>Imagine a scenario where a prompt template necessitates two variables, foo and bar. If foo is obtained early on, but bar is acquired later, one can “partial” the prompt template with the foo value, and later use the partialed prompt template when the bar value is available.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.prompts import PromptTemplate   </span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(template=&quot;&#123;foo&#125;&#123;bar&#125;&quot;, input_variables=[&quot;foo&quot;, &quot;bar&quot;])  </span><br><span class="line">partial_prompt = prompt.partial(foo=&quot;foo&quot;);  </span><br><span class="line">print(partial_prompt.format(bar=&quot;baz&quot;))  # Output: foobaz</span><br></pre></td></tr></table></figure><p>Alternatively, you can initialize the prompt template with the partialed variables directly:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = PromptTemplate(template=&quot;&#123;foo&#125;&#123;bar&#125;&quot;, input_variables=[&quot;bar&quot;], partial_variables=&#123;&quot;foo&quot;: &quot;foo&quot;&#125;)  </span><br><span class="line">print(prompt.format(bar=&quot;baz&quot;))  # Output: foobaz</span><br></pre></td></tr></table></figure><h3 id="Example-2-Partialing-with-Functions"><a href="#Example-2-Partialing-with-Functions" class="headerlink" title="Example 2: Partialing with Functions"></a>Example 2: Partialing with Functions</h3><p>In another instance, if a prompt template always necessitates the current date, partial variables can be utilized with functions to cater to this requirement.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from datetime import datetime   </span><br><span class="line"></span><br><span class="line">def _get_datetime():  </span><br><span class="line">    now = datetime.now()  </span><br><span class="line">    return now.strftime(&quot;%m/%d/%Y, %H:%M:%S&quot;)  </span><br><span class="line"></span><br><span class="line">prompt = PromptTemplate(  </span><br><span class="line">    template=&quot;Tell me a &#123;adjective&#125; joke about the day &#123;date&#125;&quot;,  </span><br><span class="line">    input_variables=[&quot;adjective&quot;, &quot;date&quot;]  </span><br><span class="line">)  </span><br><span class="line">partial_prompt = prompt.partial(date=_get_datetime)  </span><br><span class="line">print(partial_prompt.format(adjective=&quot;funny&quot;))  # Output: Tell me a funny joke about the day 02/27/2023, 22:15:16</span><br></pre></td></tr></table></figure><p>And again, you can initialize the prompt template with the partialed variables directly, often a sensible approach in this workflow:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = PromptTemplate(  </span><br><span class="line">    template=&quot;Tell me a &#123;adjective&#125; joke about the day &#123;date&#125;&quot;,  </span><br><span class="line">    input_variables=[&quot;adjective&quot;],  </span><br><span class="line">    partial_variables=&#123;&quot;date&quot;: _get_datetime&#125;  </span><br><span class="line">)  </span><br><span class="line">print(prompt.format(adjective=&quot;funny&quot;))  # Output: Tell me a funny joke about the day 02/27/2023, 22:15:16</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> langchain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Coroutines in Python</title>
      <link href="2023/10/19/how-does-coroutine-work-in-python/"/>
      <url>2023/10/19/how-does-coroutine-work-in-python/</url>
      
        <content type="html"><![CDATA[<p>Coroutines have become an integral part of Python’s toolkit for writing concurrent and asynchronous code. In this blog post, we’ll dive deep into what coroutines are, their benefits, and how they differ from traditional threads and processes.</p><h2 id="What-are-Coroutines"><a href="#What-are-Coroutines" class="headerlink" title="What are Coroutines?"></a>What are Coroutines?</h2><p>Coroutines are a generalization of subroutines (or functions) used for cooperative multitasking. They allow functions to have multiple entry and exit points, enabling them to “pause” and “resume” execution.</p><h2 id="How-to-Define-and-Use-Coroutines"><a href="#How-to-Define-and-Use-Coroutines" class="headerlink" title="How to Define and Use Coroutines"></a>How to Define and Use Coroutines</h2><ol><li><strong>Definition</strong>: Coroutines resemble regular functions but are defined using the <code>async def</code> syntax.<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">my_coroutine</span>():</span></span><br></pre></td></tr></table></figure></li><li><strong>Pause and Resume</strong>: The <code>await</code> keyword allows coroutines to be paused, giving a chance for other coroutines to run. Once the awaited task completes, the coroutine resumes from where it paused.</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">async def another_coroutine():</span><br><span class="line">    print(&quot;Start&quot;)</span><br><span class="line">    await asyncio.sleep(1)</span><br><span class="line">    print(&quot;End after 1 second&quot;)</span><br></pre></td></tr></table></figure><p>3.** Execution**: Coroutines aren’t invoked directly. They need to be “scheduled” using an event loop, like the one from asyncio.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import asyncio</span><br><span class="line"></span><br><span class="line">asyncio.run(another_coroutine())</span><br></pre></td></tr></table></figure><ol start="4"><li><strong>Tasks</strong>: To run multiple coroutines concurrently, encapsulate them in Task objects.</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">task1 = asyncio.create_task(coroutine1())</span><br><span class="line">task2 = asyncio.create_task(coroutine2())</span><br></pre></td></tr></table></figure><h2 id="Benefits-of-Coroutines"><a href="#Benefits-of-Coroutines" class="headerlink" title="Benefits of Coroutines"></a>Benefits of Coroutines</h2><p>Coroutines shine for I/O-bound tasks. They allow for efficient concurrency without requiring threads or processes. This is particularly useful when waiting for one operation, like a network request, without blocking other operations.</p><h2 id="A-Note-on-Generators-and-Coroutines"><a href="#A-Note-on-Generators-and-Coroutines" class="headerlink" title="A Note on Generators and Coroutines"></a>A Note on Generators and Coroutines</h2><p>Before the modern async/await syntax (introduced in Python 3.5), coroutines were constructed using generator functions and the yield keyword. Although async/await is now more prevalent, generator-based coroutines remain relevant in specific contexts.</p><h2 id="Coroutines-vs-Threads-and-Processes"><a href="#Coroutines-vs-Threads-and-Processes" class="headerlink" title="Coroutines vs. Threads and Processes"></a>Coroutines vs. Threads and Processes</h2><p>It’s crucial to differentiate between coroutines, threads, and processes:</p><ul><li>Threads and processes are managed by the OS and can run simultaneously on multi-core CPUs.</li><li>Coroutines provide cooperative multitasking, where one coroutine runs at a time but can yield control, allowing others to run. The lightweight context switching between coroutines makes them highly efficient for I/O-bound tasks.</li></ul>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> coroutine </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Digital Images, Pixels, Channels, and Satellite Imagery</title>
      <link href="2023/10/16/image-pixel-channel-and-satellite-imagery/"/>
      <url>2023/10/16/image-pixel-channel-and-satellite-imagery/</url>
      
        <content type="html"><![CDATA[<p>Digital images are everywhere, from the photos we take on our phones to sophisticated satellite images that monitor Earth’s changes. Dive into the intricacies of digital image properties like pixels, resolution, channels, and how they relate to the advanced world of satellite imagery.</p><h2 id="What’s-a-Pixel"><a href="#What’s-a-Pixel" class="headerlink" title="What’s a Pixel?"></a>What’s a Pixel?</h2><p>A “pixel”, short for “picture element”, is the smallest individual point in a digital image. If you zoom into a digital image far enough, you’d eventually see it’s made up of a grid of tiny squares. Each of these squares is a pixel.</p><h2 id="Image-Dimensions-and-Resolution"><a href="#Image-Dimensions-and-Resolution" class="headerlink" title="Image Dimensions and Resolution"></a>Image Dimensions and Resolution</h2><p>Image dimensions describe the pixel width and height of an image, usually expressed as <strong>width x height</strong>. For example, an image with dimensions of 1920x1080 has 1920 pixels in width and 1080 pixels in height.</p><p>Resolution is a measure of the detail an image holds. In the digital realm, it’s often quantified as pixels per inch (PPI). A higher PPI indicates higher image quality because there are more pixels in the same physical space. Print images often use a resolution of 300 PPI to achieve sharpness, while screen displays typically sit around 72 PPI.</p><h2 id="Channels-in-Images"><a href="#Channels-in-Images" class="headerlink" title="Channels in Images"></a>Channels in Images</h2><p>An image channel refers to the distinct grayscale image representing the values for a specific color or feature in an overall image.</p><h3 id="RGB-and-Alpha-Channels"><a href="#RGB-and-Alpha-Channels" class="headerlink" title="RGB and Alpha Channels"></a>RGB and Alpha Channels</h3><p>Most digital images utilize the <strong>RGB color model</strong>:</p><ul><li><strong>R (Red) Channel</strong>: Represents the red component.</li><li><strong>G (Green) Channel</strong>: Represents the green component.</li><li><strong>B (Blue) Channel</strong>: Represents the blue component.</li></ul><p>Together, these channels produce the full-color image we see. Additionally, some images also have an <strong>Alpha Channel</strong>, which manages the opacity of the image. A pixel’s value in this channel determines its transparency level.</p><h3 id="Beyond-RGB-CMYK-and-More"><a href="#Beyond-RGB-CMYK-and-More" class="headerlink" title="Beyond RGB: CMYK and More"></a>Beyond RGB: CMYK and More</h3><p>While digital images often use RGB, print images commonly use the <strong>CMYK model</strong>:</p><ul><li><strong>C (Cyan)</strong></li><li><strong>M (Magenta)</strong></li><li><strong>Y (Yellow)</strong></li><li><strong>K (Key/Black)</strong></li></ul><p>This model corresponds to the ink plates in color printing.</p><p>There are also other channels based on different color attributes, such as <strong>Hue, Saturation, and Lightness (HSL)</strong>.</p><h2 id="Satellite-Image-Channels-A-Different-Perspective"><a href="#Satellite-Image-Channels-A-Different-Perspective" class="headerlink" title="Satellite Image Channels: A Different Perspective"></a>Satellite Image Channels: A Different Perspective</h2><p>In satellite imagery, “channels” or “bands” often denote specific wavelengths or ranges of the electromagnetic spectrum. Different bands capture various features and properties of Earth.</p><h3 id="Visible-to-Infrared"><a href="#Visible-to-Infrared" class="headerlink" title="Visible to Infrared"></a>Visible to Infrared</h3><p>Satellites often capture light in the <strong>visible spectrum</strong> (red, green, blue) and often extend into the <strong>near-infrared (NIR)</strong>. The NIR is instrumental in assessing plant health, as healthy vegetation reflects a lot of NIR light.</p><h3 id="Advanced-Spectral-Bands"><a href="#Advanced-Spectral-Bands" class="headerlink" title="Advanced Spectral Bands"></a>Advanced Spectral Bands</h3><p>Satellites equipped with advanced sensors can also detect <strong>shortwave infrared (SWIR)</strong>, <strong>thermal infrared</strong>, and <strong>microwave bands</strong>. These bands can reveal insights like surface temperature, mineral presence, and surface roughness.</p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>From the basic building blocks of digital images to the advanced spectral bands in satellite imagery, understanding the underlying properties and channels of images allows for more informed viewing, editing, and interpretation. Whether you’re fine-tuning a photograph or analyzing environmental changes from space, these concepts are foundational in the digital imaging world.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pixel </tag>
            
            <tag> satellite imagery </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unlocking the Power of Large Language Models, A Glimpse into Prompt Engineering</title>
      <link href="2023/10/12/how-to-understand-prompt-engineering-from-keras-inventor/"/>
      <url>2023/10/12/how-to-understand-prompt-engineering-from-keras-inventor/</url>
      
        <content type="html"><![CDATA[<p>Following are some insights about prompt engineering from Keras inventor François Chollet.</p><p><strong>Understanding Vector Programs in Language Models</strong></p><p>When we dive into the heart of large language models (LLMs) like GPT, we unearth a fascinating world of millions of vector programs. These programs, essentially intricate non-linear functions, map various parts of a model’s latent space and emerge as a byproduct of compressing and internalizing human-generated data.</p><p><strong>The Art and Science of Prompting</strong></p><p>Prompts act as keys that navigate through this extensive repository. A part of your prompt serves as a “program key,” and another part comes into play as the argument. Consider an example: “write this paragraph in the style of Shakespeare: {my paragraph}”. Here, “write this paragraph in the style of X: Y” acts as a key, guiding the model to utilize specific learned structures, with X and Y being the integral arguments that get fed into the identified program.</p><p><strong>Navigating the Repository: The Role of Prompt Engineering</strong></p><p>Is the fetched program always optimal? Not necessarily. Thus, enters the realm of prompt engineering: a meticulous process where varied keys are probed and tested to discover a program that executes the desired task with empirical accuracy. Much akin to attempting different keywords while scouring for a Python library, this process thrives on trial and error, continually seeking that perfect match between prompt and output.</p><p><strong>Dissecting Anthropomorphism in Prompts</strong></p><p>A critical understanding that shapes successful interaction with LLMs is recognizing that they are devoid of human-like understanding of language. The models do not comprehend or process language and instructions the way humans do, and treating them as such would be mere anthropomorphism.</p><p><strong>Amplifying Capabilities and Future Directions</strong></p><p>As LLMs evolve, becoming more sophisticated and storing an even more extensive array of programs, the potency and significance of identifying the right program (and thus, crafting the apt prompt) are magnified. Looking forward, although prompt engineering is here to stay, we anticipate an era where this meticulous search is automated, shielding the end user from the complexities and enabling them to harness the potent capabilities of LLMs seamlessly.</p><p><em>Through prompt engineering, we tap into a rich repository of programs in LLMs, navigating through them with carefully crafted prompts to achieve our desired output, all while acknowledging and respecting the non-anthropomorphic nature of these models.</em></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> prompt engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fine-tuning GPT-3.5 with the Titanic Dataset</title>
      <link href="2023/10/10/finetune-gpt-with-structured-data-set-such-as-titanic-dataset/"/>
      <url>2023/10/10/finetune-gpt-with-structured-data-set-such-as-titanic-dataset/</url>
      
        <content type="html"><![CDATA[<p>In the quest for superior machine learning models, fine-tuning is a crucial step. GPT-3.5, a robust model from OpenAI, offers a fine-tuning feature that lets you tailor the model for specific tasks. One intriguing use case is employing GPT-3.5 to unravel insights from the historic Titanic dataset.</p><p>Here’s a simplified example of processing the Titanic dataset into fine-tuning prompts:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line"># Load the Titanic dataset</span><br><span class="line">data = pd.read_csv(&#x27;titanic.csv&#x27;)</span><br><span class="line"></span><br><span class="line"># Define a function to format data into a fine-tuning prompt</span><br><span class="line">def format_example(row):</span><br><span class="line">    prompt = f&quot;Passenger: &#123;row[&#x27;Name&#x27;]&#125;, Age: &#123;row[&#x27;Age&#x27;]&#125;, Sex: &#123;row[&#x27;Sex&#x27;]&#125;\n&quot;</span><br><span class="line">    prompt += f&quot;Survived: &#123;&#x27;Yes&#x27; if row[&#x27;Survived&#x27;] else &#x27;No&#x27;&#125;\n&quot;</span><br><span class="line">    return prompt</span><br><span class="line"></span><br><span class="line"># Create fine-tuning prompts</span><br><span class="line">prompts = data.apply(format_example, axis=1)</span><br><span class="line"></span><br><span class="line"># Save prompts to a text file for fine-tuning</span><br><span class="line">with open(&#x27;fine_tuning_prompts.txt&#x27;, &#x27;w&#x27;) as f:</span><br><span class="line">    for prompt in prompts:</span><br><span class="line">        f.write(prompt + &#x27;\n&#x27;)</span><br></pre></td></tr></table></figure><p>Post processing, follow OpenAI’s fine-tuning guide to upload your data and initiate a fine-tuning job. Once the fine-tuning is complete, you’re set to sail on a voyage of discovery with your newly fine-tuned GPT-3.5 model, exploring the depths of the Titanic dataset!</p><p>The realm of fine-tuning is vast and the potential to uncover new insights is boundless. So, hoist your sails, the sea of data awaits!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> finetuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Indicate if Any of Multiple Columns Have Values Greater Than 0 in Pandas</title>
      <link href="2023/10/07/fast-way-to-create-column-to-check-postive-value-in-multiple-columns-in-pandas/"/>
      <url>2023/10/07/fast-way-to-create-column-to-check-postive-value-in-multiple-columns-in-pandas/</url>
      
        <content type="html"><![CDATA[<p>Pandas is a powerful Python library for data manipulation and analysis. It provides a wide range of functions to make data manipulation tasks easier. In this tutorial, we will learn how to create a new column in a Pandas DataFrame to indicate whether any of the other columns have values greater than 0.</p><h2 id="The-Problem"><a href="#The-Problem" class="headerlink" title="The Problem"></a>The Problem</h2><p>Let’s say you have a DataFrame with multiple columns, and you want to determine whether any of these columns contain values greater than 0. You want to create a new column that flags rows where at least one of the columns meets this condition.</p><h2 id="The-Solution"><a href="#The-Solution" class="headerlink" title="The Solution"></a>The Solution</h2><p>We can achieve this using the <code>any()</code> function in Pandas along with boolean indexing. Here’s a step-by-step guide to solving this problem:</p><ol><li><p>Import the Pandas library:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure></li><li><p>Create a DataFrame with your data. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data = &#123;&#x27;col1&#x27;: [0, 2, 0, -1],</span><br><span class="line">        &#x27;col2&#x27;: [-2, 0, 0, 1],</span><br><span class="line">        &#x27;col3&#x27;: [0, 0, 0, 0],</span><br><span class="line">        &#x27;col4&#x27;: [0, 0, 3, 0]&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br></pre></td></tr></table></figure></li><li><p>Create a new column, let’s call it has_positive_value, to indicate whether any of the columns have values greater than 0:</p></li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[&#x27;has_positive_value&#x27;] = (df[[&#x27;col1&#x27;, &#x27;col2&#x27;, &#x27;col3&#x27;, &#x27;col4&#x27;]] &gt; 0).any(axis=1)</span><br></pre></td></tr></table></figure><ol start="4"><li>Finally, print the modified DataFrame to see the results:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(df)</span><br></pre></td></tr></table></figure>The <code>has_positive_value</code> column will now contain True for rows where any of the values in col1, col2, col3, or col4 is greater than 0, and False otherwise.</li></ol>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Challenges with High Frame Rate (120 FPS) Video Files in OpenAI&#39;s Whisper API</title>
      <link href="2023/10/06/openai-whisper-api-might-not-work-with-high-frame-rate-video/"/>
      <url>2023/10/06/openai-whisper-api-might-not-work-with-high-frame-rate-video/</url>
      
        <content type="html"><![CDATA[<p>OpenAI’s Whisper API has been a game-changer in the field of automatic speech recognition (ASR). It allows developers to transcribe spoken words from audio and video files with impressive accuracy. However, users have recently encountered an issue when dealing with video files that have a high frame rate, such as 120 FPS, as opposed to the more common 30 FPS. In this blog post, we will explore this challenge and suggest potential workarounds.</p><h2 id="The-Issue"><a href="#The-Issue" class="headerlink" title="The Issue"></a>The Issue</h2><p>When attempting to transcribe a high frame rate video file (e.g., 120 FPS) using the Whisper API, users have reported receiving an error message indicating that the file format is not valid. This can be frustrating, especially when you have high-quality video content that you want to transcribe accurately.<br>The error message from whipser api looks like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;Invalid file format. Supported formats: [&#x27;flac&#x27;, &#x27;m4a&#x27;, &#x27;mp3&#x27;, &#x27;mp4&#x27;, &#x27;mpeg&#x27;, &#x27;mpga&#x27;, &#x27;oga&#x27;, &#x27;ogg&#x27;, &#x27;wav&#x27;, &#x27;webm&#x27;]&quot;</span><br></pre></td></tr></table></figure><h2 id="Understanding-the-Challenge"><a href="#Understanding-the-Challenge" class="headerlink" title="Understanding the Challenge"></a>Understanding the Challenge</h2><p>The Whisper API, like many ASR systems, may not be optimized to handle video files with extremely high frame rates. ASR models are typically trained on a wide range of audio and video data, but they may be more accustomed to processing content at common frame rates like 30 FPS. Higher frame rates can introduce additional complexities in terms of data processing and synchronization, which can pose challenges for ASR systems.</p><h2 id="Potential-Workarounds"><a href="#Potential-Workarounds" class="headerlink" title="Potential Workarounds"></a>Potential Workarounds</h2><p>While the issue of high frame rate videos with the Whisper API may persist, there are several potential workarounds that you can explore:</p><ol><li><p><strong>Video Preprocessing</strong>: Before sending your video for transcription, consider preprocessing it to reduce the frame rate. Tools like FFmpeg can help you convert the video to a more common frame rate like 30 FPS. This should make it more compatible with the Whisper API.</p></li><li><p><strong>Extract Audio</strong>: Alternatively, you can extract the audio track from the high frame rate video file and submit only the audio for transcription. This eliminates the need to deal with the video frame rate issue altogether.</p></li><li><p><strong>Contact OpenAI Support</strong>: If you believe that high frame rate video support is essential for your project, consider reaching out to OpenAI’s support team. They may be able to provide guidance on potential updates or workarounds specific to your needs.</p></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While the Whisper API has proven to be a powerful tool for transcription, it may encounter challenges when handling video files with exceptionally high frame rates. By understanding the issue and exploring potential workarounds, you can continue to leverage the API’s capabilities effectively. Whether through video preprocessing, audio extraction, or support assistance, there are options available to help you transcribe your content accurately and efficiently.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> whisper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to fix module pandas has no attribute int64Index error with pandas 2.0</title>
      <link href="2023/10/04/pandas-no-attribute-int64index-error-fix/"/>
      <url>2023/10/04/pandas-no-attribute-int64index-error-fix/</url>
      
        <content type="html"><![CDATA[<p>As many people upgraded their pandas to 2.0 version, they might see the following error, when using with other older version of packages.<br>For example, during using lightgbm, I got this error when importing lightgbm with pandas 2.0 installed.</p><p>Error message:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">AttributeError: module &#x27;pandas&#x27; has no attribute &#x27;Int64Index&#x27;`</span><br></pre></td></tr></table></figure><p>The way to fix it including upgrade several packages like the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install --upgrade pandas</span><br><span class="line">pip install --upgrade lightgbm</span><br><span class="line">pip install --upgrade dask</span><br></pre></td></tr></table></figure><p>Hope this will fix your problem as well.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pandas </tag>
            
            <tag> lightgbm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Harnessing OpenAI&#39;s function calling for Processing JSON Objects</title>
      <link href="2023/10/02/how-to-structure-a-list-of-json-objects-using-openai-function-calling/"/>
      <url>2023/10/02/how-to-structure-a-list-of-json-objects-using-openai-function-calling/</url>
      
        <content type="html"><![CDATA[<p>In the realm of artificial intelligence, OpenAI’s GPT-3.5 Turbo has been a game-changer, providing developers a powerful tool to tackle a plethora of tasks. One such task is processing JSON objects, a common requirement in today’s data-centric world. JSON (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write, and easy for machines to parse and generate.</p><p>OpenAI provides an API that developers can interact with to leverage the capabilities of GPT-3.5 Turbo. In this blog post, we’ll explore how to structure a request to process a list of JSON objects using a custom function through the OpenAI API.</p><p>Below is the Python code snippet that demonstrates how to make a POST request to the OpenAI API, invoking a custom function to process a list of JSON objects. We’ve defined a function called process_json_objects which is structured to accept an array of JSON objects, each containing at least two string properties key1 and key2.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"># Define the data</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;model&quot;: &quot;gpt-3.5-turbo-0613&quot;,</span><br><span class="line">    &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Process these JSON objects.&quot;&#125;],</span><br><span class="line">    &quot;functions&quot;: [&#123;</span><br><span class="line">        &quot;name&quot;: &quot;process_json_objects&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Processes a list of JSON objects.&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;json_objects&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;array&quot;,</span><br><span class="line">                    &quot;description&quot;: &quot;A list of JSON objects.&quot;,</span><br><span class="line">                    &quot;items&quot;: &#123;</span><br><span class="line">                        &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">                        &quot;properties&quot;: &#123;</span><br><span class="line">                            &quot;key1&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Description of key1.&quot;&#125;,</span><br><span class="line">                            &quot;key2&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;description&quot;: &quot;Description of key2.&quot;&#125;</span><br><span class="line">                            // ...other keys</span><br><span class="line">                        &#125;,</span><br><span class="line">                        &quot;required&quot;: [&quot;key1&quot;, &quot;key2&quot;]  # specify required keys here</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [&quot;json_objects&quot;]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;],</span><br><span class="line">   &quot;function_call&quot;:&#123;&quot;name&quot;: &quot;process_json_objects&quot;&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Define the URL and headers</span><br><span class="line">url = &quot;https://api.openai.com/v1/chat/completions&quot;  # Updated endpoint URL</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;Authorization&quot;: &quot;Bearer YOUR_API_KEY&quot;,</span><br><span class="line">    &quot;Content-Type&quot;: &quot;application/json&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Make the POST request</span><br><span class="line">response = requests.post(url, headers=headers, data=json.dumps(data))</span><br><span class="line"></span><br><span class="line"># Print the response</span><br><span class="line">print(response.json())</span><br></pre></td></tr></table></figure><p>In this code snippet:</p><ol><li>We import the necessary libraries, requests and json.</li><li>Define the data structure for our request, specifying the model, messages, and the custom function with its parameters.</li><li>Set the URL to the /v1/chat/completions endpoint of the OpenAI API and the headers, including the authorization header with your API key.</li><li>Make a POST request to the OpenAI API with the defined data and headers.</li><li>Print the response to the console.</li></ol><p>Make sure to replace “YOUR_API_KEY” with your actual OpenAI API key.</p><p>This code provides a structured way to process an undetermined number of JSON objects through the OpenAI API, demonstrating the flexibility and power of GPT-3.5 Turbo in handling various data processing tasks.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> function calling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unlocking Creativity with OpenAI&#39;s DALL.E3 on Bing for Free</title>
      <link href="2023/10/01/how-to-use-openai-dalle3-for-free/"/>
      <url>2023/10/01/how-to-use-openai-dalle3-for-free/</url>
      
        <content type="html"><![CDATA[<p>In the vast realm of artificial intelligence, OpenAI’s DALL.E3 has emerged as a beacon of artistic and contextual understanding. This enhanced model has not only caught up with MidJourney but surpassed it, thanks to its seamless integration with ChatGPT. In a surprising yet welcome move, Microsoft has integrated DALL.E3 into <a href="https://www.bing.com/create">Bing Create</a>, opening doors for everyone to explore this technology for free, up to a certain limit each month. In this blog, we’ll delve into how you can get your hands on DALL.E3 through Bing Create and why this is a significant leap forward.</p><h2 id="DALL-E3-A-Brief-Overview"><a href="#DALL-E3-A-Brief-Overview" class="headerlink" title="DALL.E3: A Brief Overview"></a>DALL.E3: A Brief Overview</h2><p>DALL.E3 comes as an upgrade from OpenAI, known for its exceptional quality and artistic capabilities. Unlike its predecessors, DALL.E3 exhibits a remarkable understanding of context, making it a preferred choice for creators aiming for nuanced and coherent outputs.</p><h2 id="Getting-Started-with-Bing-Create"><a href="#Getting-Started-with-Bing-Create" class="headerlink" title="Getting Started with Bing Create"></a>Getting Started with Bing Create</h2><p>To harness the power of DALL.E3, you’ll need to sign up for a Bing account if you haven’t already. Here’s a step-by-step guide on how you can do it:</p><ol><li><p><strong>Creating a Bing Account:</strong></p><ul><li>Visit the <a href="https://www.bing.com/create">Bing Create website</a>.</li><li>Click on the ‘Sign Up’ button and follow the prompts to create your account.</li></ul></li><li><p><strong>Accessing DALL.E3:</strong></p><ul><li>Once your account is set up, navigate to the DALL.E3 section within Bing Create.</li><li>You might need to fill in some additional details to get access to DALL.E3.</li></ul></li><li><p><strong>Exploring DALL.E3:</strong></p><ul><li>Now that you have access, feel free to explore and experiment with DALL.E3.</li><li>You can start by entering a prompt and observing the generated outputs.</li></ul></li><li><p><strong>Understanding the Limitations:</strong></p><ul><li>Remember, the free access has a monthly usage limit.</li><li>Keep an eye on your usage to ensure you stay within the allocated quota.</li></ul></li></ol><h2 id="Why-DALL-E3-on-Bing-Create-is-a-Game-Changer"><a href="#Why-DALL-E3-on-Bing-Create-is-a-Game-Changer" class="headerlink" title="Why DALL.E3 on Bing Create is a Game Changer"></a>Why DALL.E3 on Bing Create is a Game Changer</h2><p>The inclusion of DALL.E3 in Bing Create democratizes access to high-quality AI-powered creative tools. This move bridges the gap between seasoned creators and novices, enabling everyone to bring their creative visions to life without a steep learning curve or financial constraints.</p><p>Furthermore, the integration of DALL.E3 with ChatGPT enhances its contextual understanding, making it a formidable tool for generating coherent and artistically rich outputs.</p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>The synergy between Bing Create and DALL.E3 invites both seasoned and aspiring creators to explore the realms of possibility with artificial intelligence. By following the simple steps outlined above, you too can start your journey into the expansive and imaginative world of DALL.E3. The horizon of creative exploration just got broader, and it’s exciting to think of the innovative artworks and ideas that will emerge from this collaboration.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> dalle3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Enhancing Question and Answer with openAI, A Step-by-Step Guide</title>
      <link href="2023/09/30/enhance-question-and-answer-with-search-and-reranking-using-openai-chatgpt/"/>
      <url>2023/09/30/enhance-question-and-answer-with-search-and-reranking-using-openai-chatgpt/</url>
      
        <content type="html"><![CDATA[<p>Searching for information online can sometimes feel like looking for a needle in a haystack, but advanced AI models, like GPT, can help augment and refine this process efficiently. In this blog, we’ll walk you through a hybrid approach to augment search systems using GPT, providing an example using the News API.</p><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>Our approach combines mimicking human browsing and retrieval with embeddings to optimize the search process. It involves three steps:</p><ol><li><strong>Search:</strong> Execute multiple search queries generated from a user’s question.</li><li><strong>Re-rank:</strong> Calculate semantic similarity between search results and a hypothetical answer, then re-rank the results.</li><li><strong>Answer:</strong> Generate a concise answer using the top re-ranked search results.</li></ol><h2 id="Step-by-Step-Implementation"><a href="#Step-by-Step-Implementation" class="headerlink" title="Step-by-Step Implementation"></a>Step-by-Step Implementation</h2><h3 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h3><p>First, acquire API keys for OpenAI and the News API. Install necessary Python dependencies like <code>openai</code>, <code>requests</code>, <code>numpy</code>, <code>tqdm</code>, and <code>json</code>.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%env NEWS_API_KEY = YOUR_NEWS_API_KEY</span><br><span class="line"><span class="keyword">import</span> openai, os, requests, json, tqdm</span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> date, timedelta</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> dot</span><br><span class="line"><span class="keyword">from</span> IPython <span class="keyword">import</span> display</span><br></pre></td></tr></table></figure><h3 id="Step-1-Search"><a href="#Step-1-Search" class="headerlink" title="Step 1: Search"></a>Step 1: Search</h3><p>Start by obtaining the user’s question, then use GPT to generate potential search queries related to the question.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">USER_QUESTION = &quot;Who won the NBA championship? And who was the MVP?&quot;</span><br><span class="line"></span><br><span class="line">def generate_queries(user_question):</span><br><span class="line">    QUERIES_INPUT = f&quot;&quot;&quot;</span><br><span class="line">    You have access to a search API that returns recent news articles. Generate an array of search queries that are relevant to this question. Be creative.</span><br><span class="line">    User question: &#123;USER_QUESTION&#125;</span><br><span class="line">    Format: &#123;&#123;&quot;queries&quot;: [&quot;query_1&quot;, &quot;query_2&quot;, &quot;query_3&quot;]&#125;&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    queries = json_gpt(QUERIES_INPUT)[&quot;queries&quot;]</span><br><span class="line">    queries.append(USER_QUESTION)</span><br><span class="line">    return queries</span><br><span class="line"></span><br><span class="line">queries = generate_queries(USER_QUESTION)</span><br></pre></td></tr></table></figure><p>where <code>json_gpt</code> is talking to openai with system role to make sure the results follow json format.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Helper functions</span><br><span class="line">def json_gpt(input: str):</span><br><span class="line">    completion = openai.ChatCompletion.create(</span><br><span class="line">        model=GPT_MODEL,</span><br><span class="line">        messages=[</span><br><span class="line">            &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Output only valid JSON&quot;&#125;,</span><br><span class="line">            &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: input&#125;,</span><br><span class="line">        ],</span><br><span class="line">        temperature=0.5,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    text = completion.choices[0].message.content</span><br><span class="line">    parsed = json.loads(text)</span><br><span class="line"></span><br><span class="line">    return parsed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def embeddings(input: list[str]) -&gt; list[list[str]]:</span><br><span class="line">    response = openai.Embedding.create(model=&quot;text-embedding-ada-002&quot;, input=input)</span><br><span class="line">    return [data.embedding for data in response.data]</span><br></pre></td></tr></table></figure><p>Then fetch search results from the News API for each query:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def search_news(queries):</span><br><span class="line">    articles = []</span><br><span class="line">    for query in tqdm.tqdm(queries):</span><br><span class="line">        result = search_news_api(query)</span><br><span class="line">        if result[&quot;status&quot;] == &quot;ok&quot;:</span><br><span class="line">            articles += result[&quot;articles&quot;]</span><br><span class="line">    return list(&#123;article[&quot;url&quot;]: article for article in articles&#125;.values())</span><br><span class="line"></span><br><span class="line">articles = search_news(queries)</span><br></pre></td></tr></table></figure><h3 id="Step-2-Re-rank"><a href="#Step-2-Re-rank" class="headerlink" title="Step 2: Re-rank"></a>Step 2: Re-rank</h3><p>Generate a hypothetical answer to the user’s question to use as a basis for re-ranking:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def generate_hypothetical_answer(user_question):</span><br><span class="line">    HA_INPUT = f&quot;&quot;&quot;</span><br><span class="line">    Generate a hypothetical answer to the user&#x27;s question. Pretend you have all the information needed.</span><br><span class="line">    User question: &#123;USER_QUESTION&#125;</span><br><span class="line">    Format: &#123;&#123;&quot;hypotheticalAnswer&quot;: &quot;hypothetical answer text&quot;&#125;&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return json_gpt(HA_INPUT)[&quot;hypotheticalAnswer&quot;]</span><br><span class="line"></span><br><span class="line">hypothetical_answer = generate_hypothetical_answer(USER_QUESTION)</span><br></pre></td></tr></table></figure><p>Compute the semantic similarity between each search result and the hypothetical answer, then re-rank the results:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def rank_articles(articles, hypothetical_answer):</span><br><span class="line">    article_embeddings = embeddings([f&quot;&#123;article[&#x27;title&#x27;]&#125; &#123;article[&#x27;description&#x27;]&#125; &#123;article[&#x27;content&#x27;][0:100]&#125;&quot; for article in articles])</span><br><span class="line">    hypothetical_answer_embedding = embeddings([hypothetical_answer])[0]</span><br><span class="line">    cosine_similarities = [dot(hypothetical_answer_embedding, article_embedding) for article_embedding in article_embeddings]</span><br><span class="line">    scored_articles = zip(articles, cosine_similarities)</span><br><span class="line">    return sorted(scored_articles, key=lambda x: x[1], reverse=True)</span><br><span class="line"></span><br><span class="line">scored_articles = rank_articles(articles, hypothetical_answer)</span><br></pre></td></tr></table></figure><h3 id="Step-3-Answer"><a href="#Step-3-Answer" class="headerlink" title="Step 3: Answer"></a>Step 3: Answer</h3><p>With the top-ranked articles, construct a comprehensive answer to the user’s question:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def generate_final_answer(user_question, top_articles):</span><br><span class="line">    formatted_top_results = [&#123;&quot;title&quot;: article[&quot;title&quot;], &quot;description&quot;: article[&quot;description&quot;], &quot;url&quot;: article[&quot;url&quot;]&#125; for article, _score in top_articles]</span><br><span class="line">    ANSWER_INPUT = f&quot;&quot;&quot;</span><br><span class="line">    Generate an answer to the user&#x27;s question based on the given search results.</span><br><span class="line">    TOP_RESULTS: &#123;formatted_top_results&#125;</span><br><span class="line">    USER_QUESTION: &#123;USER_QUESTION&#125;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    completion = openai.ChatCompletion.create(model=&quot;gpt-3.5-turbo&quot;, messages=[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: ANSWER_INPUT&#125;], temperature=0.5, stream=True)</span><br><span class="line">    text = &quot;&quot;</span><br><span class="line">    for chunk in completion:</span><br><span class="line">        text += chunk.choices[0].delta.get(&quot;content&quot;, &quot;&quot;)</span><br><span class="line">    return text</span><br><span class="line"></span><br><span class="line">answer = generate_final_answer(USER_QUESTION, scored_articles[0:5])</span><br></pre></td></tr></table></figure><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>This approach efficiently sifts through a vast array of information, providing accurate and relevant answers to user questions with relatively low latency. It can be easily integrated into existing search systems, making it a valuable tool for enhancing the search experience.</p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/question-and-answer-using-chatgpt/Question_answering_using_a_search_API.ipynb">github link</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> question and answer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Comparing Go and Python, Which One Should You Learn?</title>
      <link href="2023/09/29/go-and-python-which-one-should-i-learn/"/>
      <url>2023/09/29/go-and-python-which-one-should-i-learn/</url>
      
        <content type="html"><![CDATA[<p>The choice between learning Go and Python largely depends on your personal goals, the projects you intend to work on, and your previous programming experience. Here’s a comparison of the two based on various factors:</p><h2 id="1-Ease-of-Learning"><a href="#1-Ease-of-Learning" class="headerlink" title="1. Ease of Learning:"></a>1. Ease of Learning:</h2><ul><li><strong>Python:</strong> Known for its readability and simplicity, which makes it a great choice for beginners.</li><li><strong>Go (or Golang):</strong> Also designed to be simple and efficient, with a clean syntax, but might not be as beginner-friendly as Python.</li></ul><h2 id="2-Performance"><a href="#2-Performance" class="headerlink" title="2. Performance:"></a>2. Performance:</h2><ul><li><strong>Python:</strong> Generally slower in execution because it’s an interpreted language.</li><li><strong>Go:</strong> A compiled language, which means it generally offers better performance than Python.</li></ul><h2 id="3-Concurrency"><a href="#3-Concurrency" class="headerlink" title="3. Concurrency:"></a>3. Concurrency:</h2><ul><li><strong>Python:</strong> Concurrency is not Python’s strongest suit.</li><li><strong>Go:</strong> Built with concurrency in mind, allowing multiple processes to run simultaneously, which is beneficial for system and web servers, or any scenario where performance and scalability are important.</li></ul><h2 id="4-Libraries-and-Frameworks"><a href="#4-Libraries-and-Frameworks" class="headerlink" title="4. Libraries and Frameworks:"></a>4. Libraries and Frameworks:</h2><ul><li><strong>Python:</strong> Has a rich set of libraries and frameworks for a wide range of applications including web development, data analysis, machine learning, and more.</li><li><strong>Go:</strong> Although it has fewer libraries and frameworks compared to Python, it has strong support for systems-level programming.</li></ul><h2 id="5-Community-and-Support"><a href="#5-Community-and-Support" class="headerlink" title="5. Community and Support:"></a>5. Community and Support:</h2><ul><li><strong>Python:</strong> Has a larger community and more third-party libraries available due to its longer existence.</li><li><strong>Go:</strong> Has a smaller but growing community.</li></ul><h2 id="6-Job-Opportunities"><a href="#6-Job-Opportunities" class="headerlink" title="6. Job Opportunities:"></a>6. Job Opportunities:</h2><ul><li><strong>Python:</strong> High demand in job markets, especially in data science, machine learning, web development, and scripting.</li><li><strong>Go:</strong> Demand is growing especially in fields that require high performance and scalability like cloud services and infrastructure projects.</li></ul><h2 id="7-Use-Cases"><a href="#7-Use-Cases" class="headerlink" title="7. Use Cases:"></a>7. Use Cases:</h2><ul><li><strong>Python:</strong> Web development, scientific computing, data analysis, artificial intelligence, machine learning, automation.</li><li><strong>Go:</strong> Systems-level programming, cloud services, network programming, containers and microservices.</li></ul><p>Your decision might come down to what you find more enjoyable or the particular demands of the projects you want to work on. If you’re interested in web development, data science, or machine learning, Python might be the better choice. On the other hand, if you’re interested in systems programming, cloud services, or high-performance applications, Go might be the better choice.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> go </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fine-Tuning Sports Classification with OpenAI Ada Model</title>
      <link href="2023/09/28/finetune-with-openai-model-for-classification/"/>
      <url>2023/09/28/finetune-with-openai-model-for-classification/</url>
      
        <content type="html"><![CDATA[<p>In this blog post, we will walk you through the fine-tuning process of an Ada model for classifying text data into two sports categories: Baseball and Hockey. We’ll utilize a dataset from fetch_20newsgroups and fine-tune an Ada model to improve its performance in distinguishing between these two sports.</p><h2 id="Data-Exploration"><a href="#Data-Exploration" class="headerlink" title="Data Exploration"></a>Data Exploration</h2><p>The dataset can be easily loaded using sklearn and is essentially a collection of newsgroup documents. The sample data mainly consists of emails to sports mailing lists, with 1197 examples evenly distributed between Baseball and Hockey.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">categories = [&#x27;rec.sport.baseball&#x27;, &#x27;rec.sport.hockey&#x27;]</span><br><span class="line">sports_dataset = fetch_20newsgroups(subset=&#x27;train&#x27;, shuffle=True, random_state=42, categories=categories)</span><br></pre></td></tr></table></figure><h2 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h2><p>Data is transformed into a pandas dataframe with prompt (the email text) and completion (the sport category) columns. For demonstration purposes, we will only use 300 examples; however, in real use cases, more data would likely improve performance.</p><p>A crucial step in the data preparation is to add a specific suffix separator (\n\n###\n\n) between the prompt and completion. This separator helps the model identify where the input ends and the prediction begins. The data is then split into training and validation sets to measure performance and expected accuracy on unseen data.</p><h2 id="Fine-Tuning-the-Model"><a href="#Fine-Tuning-the-Model" class="headerlink" title="Fine-Tuning the Model"></a>Fine-Tuning the Model</h2><p>With the prepared dataset, the Ada model is fine-tuned to improve its classification accuracy between Baseball and Hockey. Ada is chosen for its cost-effectiveness and comparative performance on classification tasks.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!openai api fine_tunes.create -t &quot;sport2_prepared_train.jsonl&quot; -v &quot;sport2_prepared_valid.jsonl&quot; --compute_classification_metrics --classification_positive_class &quot; baseball&quot; -m ada</span><br></pre></td></tr></table></figure><p>The model trains successfully, reaching a high accuracy of 99.6% on the validation set. The accuracy on the validation set is a good indicator of the model’s performance and generalization capability on new, unseen data.</p><h2 id="Using-the-Fine-Tuned-Model"><a href="#Using-the-Fine-Tuned-Model" class="headerlink" title="Using the Fine-Tuned Model"></a>Using the Fine-Tuned Model</h2><p>With the trained model, predictions can be generated easily. For each input prompt (an email or other text data), the model predicts whether the text is more likely related to Baseball or Hockey.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ft_model = &#x27;ada:ft-openai-2021-07-30-12-26-20&#x27;</span><br><span class="line">res = openai.Completion.create(model=ft_model, prompt=test[&#x27;prompt&#x27;][0] + &#x27;\n\n###\n\n&#x27;, max_tokens=1, temperature=0)</span><br></pre></td></tr></table></figure><h2 id="Model-Generalization"><a href="#Model-Generalization" class="headerlink" title="Model Generalization"></a>Model Generalization</h2><p>Interestingly, the model is not limited to the type of data it was trained on. Despite being trained on emails, it can also successfully classify tweets into the correct sports categories. This versatility shows the model’s ability to generalize and apply its learned knowledge to different text formats and contexts.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Through this example, we’ve seen how to fine-tune an Ada model for a classification task involving sports categories. The model not only achieves high accuracy but also demonstrates impressive generalization capabilities across various text formats. This step-by-step process can be adapted and used for fine-tuning models for different classification tasks, providing a powerful tool for various applications in text classification and analysis.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> finetuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using ChatGPT to Generate SQL Queries</title>
      <link href="2023/09/25/how-to-use-chatgpt-function-calling-to-general-sql-query/"/>
      <url>2023/09/25/how-to-use-chatgpt-function-calling-to-general-sql-query/</url>
      
        <content type="html"><![CDATA[<p>When working with databases, querying for specific data is essential. Instead of manually writing SQL queries, what if you could simply ask a natural language question and get the desired data? With ChatGPT’s function calling capability, we can achieve just that!</p><p>Previoulsy, we have covered function calling in:<br><a href="/2023/09/04/fully-understand-function-calling-in-openai-gpt/">how to set an function calling example</a><br><a href="/2023/09/24/how-to-choose-among-multiple-functions-in-openai-function-calling/">how to choose from multiple functions</a></p><h2 id="Step-by-Step-Guide"><a href="#Step-by-Step-Guide" class="headerlink" title="Step by Step Guide"></a>Step by Step Guide</h2><p>Let’s see how we can set up a system that uses ChatGPT to generate SQL queries for us, using the Chinook sample database as an example.</p><h3 id="1-Connecting-to-the-Database"><a href="#1-Connecting-to-the-Database" class="headerlink" title="1. Connecting to the Database"></a>1. Connecting to the Database</h3><p>First, we need to establish a connection to the SQLite database.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import sqlite3</span><br><span class="line"></span><br><span class="line">conn = sqlite3.connect(&quot;data/Chinook.db&quot;)</span><br><span class="line">print(&quot;Opened database successfully&quot;)</span><br></pre></td></tr></table></figure><h3 id="2-Extracting-Database-Schema"><a href="#2-Extracting-Database-Schema" class="headerlink" title="2. Extracting Database Schema"></a>2. Extracting Database Schema</h3><p>To make well-informed queries, ChatGPT needs to understand the structure of our database. We can achieve this by creating utility functions to extract table names, column names, and overall database information.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def get_table_names(conn):</span><br><span class="line">    table_names = []</span><br><span class="line">    tables = conn.execute(&quot;SELECT name FROM sqlite_master WHERE type=&#x27;table&#x27;;&quot;)</span><br><span class="line">    for table in tables.fetchall():</span><br><span class="line">        table_names.append(table[0])</span><br><span class="line">    return table_names</span><br><span class="line"></span><br><span class="line">def get_column_names(conn, table_name):</span><br><span class="line">    column_names = []</span><br><span class="line">    columns = conn.execute(f&quot;PRAGMA table_info(&#x27;&#123;table_name&#125;&#x27;);&quot;).fetchall()</span><br><span class="line">    for col in columns:</span><br><span class="line">        column_names.append(col[1])</span><br><span class="line">    return column_names</span><br><span class="line"></span><br><span class="line">def get_database_info(conn):</span><br><span class="line">    table_dicts = []</span><br><span class="line">    for table_name in get_table_names(conn):</span><br><span class="line">        columns_names = get_column_names(conn, table_name)</span><br><span class="line">        table_dicts.append(&#123;&quot;table_name&quot;: table_name, &quot;column_names&quot;: columns_names&#125;)</span><br><span class="line">    return table_dicts</span><br></pre></td></tr></table></figure><p>With these functions, you can now generate a schema representation for the database.</p><h3 id="3-Function-Specifications-for-ChatGPT"><a href="#3-Function-Specifications-for-ChatGPT" class="headerlink" title="3. Function Specifications for ChatGPT"></a>3. Function Specifications for ChatGPT</h3><p>With the database schema in hand, we can define a function specification for ChatGPT. This will provide context for the model about the structure of our database and inform it how to generate the SQL queries.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">database_schema_dict = get_database_info(conn)</span><br><span class="line">database_schema_string = &quot;\n&quot;.join(</span><br><span class="line">    [</span><br><span class="line">        f&quot;Table: &#123;table[&#x27;table_name&#x27;]&#125;\nColumns: &#123;&#x27;, &#x27;.join(table[&#x27;column_names&#x27;])&#125;&quot;</span><br><span class="line">        for table in database_schema_dict</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;ask_database&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Use this function to answer user questions about music. Input should be a fully formed SQL query.&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;query&quot;: &#123;</span><br><span class="line">                    &quot;type&quot;: &quot;string&quot;,</span><br><span class="line">                    &quot;description&quot;: f&quot;SQL query extracting info using this database schema: &#123;database_schema_string&#125;. The query should be returned in plain text, not in JSON.&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [&quot;query&quot;],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="4-Execute-SQL-Queries"><a href="#4-Execute-SQL-Queries" class="headerlink" title="4. Execute SQL Queries"></a>4. Execute SQL Queries</h3><p>Next, we need a function to execute the generated SQL queries against our database.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def ask_database(conn, query):</span><br><span class="line">    try:</span><br><span class="line">        results = str(conn.execute(query).fetchall())</span><br><span class="line">    except Exception as e:</span><br><span class="line">        results = f&quot;query failed with error: &#123;e&#125;&quot;</span><br><span class="line">    return results</span><br></pre></td></tr></table></figure><h3 id="5-ChatGPT-Interaction-and-SQL-Query-Execution"><a href="#5-ChatGPT-Interaction-and-SQL-Query-Execution" class="headerlink" title="5. ChatGPT Interaction and SQL Query Execution"></a>5. ChatGPT Interaction and SQL Query Execution</h3><p>Now, using the ChatGPT API, we can interact with the model. When we get a function call response from the model, we can execute the SQL query and return the results.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">messages = []</span><br><span class="line">messages.append(&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Answer user questions by generating SQL queries against the Chinook Music Database.&quot;&#125;)</span><br><span class="line">messages.append(&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi, who are the top 5 artists by number of tracks?&quot;&#125;)</span><br><span class="line"></span><br><span class="line">chat_response = chat_completion_request(messages, functions)</span><br><span class="line">assistant_message = chat_response.json()[&quot;choices&quot;][0][&quot;message&quot;]</span><br><span class="line">messages.append(assistant_message)</span><br><span class="line"></span><br><span class="line">if assistant_message.get(&quot;function_call&quot;):</span><br><span class="line">    results = execute_function_call(assistant_message)</span><br><span class="line">    messages.append(&#123;&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: assistant_message[&quot;function_call&quot;][&quot;name&quot;], &quot;content&quot;: results&#125;)</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>By combining ChatGPT with database interactions, we’ve demonstrated how we can generate SQL queries using natural language questions. This approach allows for more intuitive data retrieval, especially for users who might not be familiar with SQL syntax. However, always ensure that you validate and sanitize the generated queries, especially if used in a production environment, to maintain data integrity and security.</p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/chatgpt-function-calling-generate-sql-query/2023-09-25-openai-function-calling-sql-query-generation-example.ipynb">github code link</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> function calling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Choose from multiple functions using openai function calling</title>
      <link href="2023/09/24/how-to-choose-among-multiple-functions-in-openai-function-calling/"/>
      <url>2023/09/24/how-to-choose-among-multiple-functions-in-openai-function-calling/</url>
      
        <content type="html"><![CDATA[<p>In a conversational interface, using OpenAI’s Chat Completions API with function calling abilities can give you a sophisticated system. Not only does it allow natural language exchanges but also provides options to interact with external APIs or databases. Let’s dive deeper into how you can use system role messages to guide the conversation and manage multiple functions effectively.</p><p>Last time, we talked about how to use function calling with openai <a href="/2023/09/04/fully-understand-function-calling-in-openai-gpt/">here</a><br>Today we see how to choose from multiple functions with function calling.</p><h2 id="Function-Specifications"><a href="#Function-Specifications" class="headerlink" title="Function Specifications"></a>Function Specifications</h2><p>Start by defining function specifications that your bot can access. These might be interfaces to a weather API, a database, or any other service. Here’s a sample code snippet in Python:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">functions = [</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_current_weather&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Get the current weather&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;location&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">                &quot;format&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;]</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_n_day_weather_forecast&quot;,</span><br><span class="line">        &quot;description&quot;: &quot;Get an N-day weather forecast&quot;,</span><br><span class="line">        &quot;parameters&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;object&quot;,</span><br><span class="line">            &quot;properties&quot;: &#123;</span><br><span class="line">                &quot;location&quot;: &#123;&quot;type&quot;: &quot;string&quot;&#125;,</span><br><span class="line">                &quot;format&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]&#125;,</span><br><span class="line">                &quot;num_days&quot;: &#123;&quot;type&quot;: &quot;integer&quot;&#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;, &quot;num_days&quot;]</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h2 id="Using-System-Role-Messages"><a href="#Using-System-Role-Messages" class="headerlink" title="Using System Role Messages"></a>Using System Role Messages</h2><p>The system role is used to guide the assistant’s behavior throughout the conversation. For example, you can specify:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">messages.append(&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;Don&#x27;t make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.&quot;&#125;)</span><br></pre></td></tr></table></figure><p>This instructs the assistant to be cautious and avoid making assumptions, ensuring it will ask for clarifications whenever needed.</p><h2 id="Handling-Clarifications"><a href="#Handling-Clarifications" class="headerlink" title="Handling Clarifications"></a>Handling Clarifications</h2><p>Suppose a user asks, “What’s the weather like today?”. Given the system role message, the assistant knows it should not assume the location or temperature format. Instead, it prompts the user:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;role&#x27;: &#x27;assistant&#x27;, &#x27;content&#x27;: &#x27;In which city and state would you like to know the current weather?&#x27;&#125;</span><br></pre></td></tr></table></figure><h2 id="Selecting-Among-Functions"><a href="#Selecting-Among-Functions" class="headerlink" title="Selecting Among Functions"></a>Selecting Among Functions</h2><p>The assistant will choose the appropriate function based on the context. For example, if the user wants a weather forecast for the next few days, it may choose get_n_day_weather_forecast. The selected function and its arguments will appear in the JSON response like so:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &#x27;role&#x27;: &#x27;assistant&#x27;,</span><br><span class="line">  &#x27;content&#x27;: None,</span><br><span class="line">  &#x27;function_call&#x27;: &#123;</span><br><span class="line">    &#x27;name&#x27;: &#x27;get_n_day_weather_forecast&#x27;,</span><br><span class="line">    &#x27;arguments&#x27;: &#x27;&#123;&quot;location&quot;: &quot;Glasgow, Scotland&quot;, &quot;format&quot;: &quot;celsius&quot;, &quot;num_days&quot;: 5&#125;&#x27;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Forcing-Specific-Functions"><a href="#Forcing-Specific-Functions" class="headerlink" title="Forcing Specific Functions"></a>Forcing Specific Functions</h2><p>To force the assistant to use a particular function, include a function_call argument in your API request:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chat_response = chat_completion_request(</span><br><span class="line">    messages, functions=functions, function_call=&#123;&quot;name&quot;: &quot;get_n_day_weather_forecast&quot;&#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>Be cautious when doing this, as it may lead the assistant to make assumptions.</p><h2 id="Disabling-Function-Calls"><a href="#Disabling-Function-Calls" class="headerlink" title="Disabling Function Calls"></a>Disabling Function Calls</h2><p>If you want to disable function calls for a specific user query, set the function_call argument to “none”:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chat_response = chat_completion_request(</span><br><span class="line">    messages, functions=functions, function_call=&quot;none&quot;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>By thoughtfully using system role messages and specifying function requirements, you can design an advanced conversational interface that handles complex queries and interactions with external services.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> function calling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deleting Records in Pinecone, A Comprehensive Guide</title>
      <link href="2023/09/23/ways-to-delete-records-in-pinecone-vector-store-in-python/"/>
      <url>2023/09/23/ways-to-delete-records-in-pinecone-vector-store-in-python/</url>
      
        <content type="html"><![CDATA[<p>Data manipulation is a critical aspect of any vector database, and Pinecone is no exception. While inserting and searching for vectors is well-understood, deleting them is another critical operation you should master. In this blog post, we will walk through various methods to delete records in a Pinecone vector store using Python.</p><h2 id="Setting-Up-Pinecone"><a href="#Setting-Up-Pinecone" class="headerlink" title="Setting Up Pinecone"></a>Setting Up Pinecone</h2><p>First, let’s start by initializing Pinecone and creating an index. We’ll add some vectors along with metadata to illustrate the deletion processes.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pinecone</span><br><span class="line"></span><br><span class="line">pinecone.init(api_key=<span class="string">&quot;your-api-key&quot;</span>, environment=<span class="string">&quot;us-west1-gcp&quot;</span>)</span><br><span class="line">index = pinecone.Index(<span class="string">&quot;example-index&quot;</span>)</span><br><span class="line"></span><br><span class="line">index.upsert([</span><br><span class="line">    (<span class="string">&quot;A&quot;</span>, [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>], &#123;<span class="string">&quot;genre&quot;</span>: <span class="string">&quot;comedy&quot;</span>, <span class="string">&quot;year&quot;</span>: <span class="number">2020</span>&#125;),</span><br><span class="line">    (<span class="string">&quot;B&quot;</span>, [<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>], &#123;<span class="string">&quot;genre&quot;</span>: <span class="string">&quot;documentary&quot;</span>, <span class="string">&quot;year&quot;</span>: <span class="number">2019</span>&#125;),</span><br><span class="line">    (<span class="string">&quot;C&quot;</span>, [<span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>, <span class="number">0.3</span>], &#123;<span class="string">&quot;genre&quot;</span>: <span class="string">&quot;comedy&quot;</span>, <span class="string">&quot;year&quot;</span>: <span class="number">2019</span>&#125;),</span><br><span class="line">    (<span class="string">&quot;D&quot;</span>, [<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.4</span>], &#123;<span class="string">&quot;genre&quot;</span>: <span class="string">&quot;drama&quot;</span>&#125;),</span><br><span class="line">    (<span class="string">&quot;E&quot;</span>, [<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>], &#123;<span class="string">&quot;genre&quot;</span>: <span class="string">&quot;drama&quot;</span>&#125;)</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="Deleting-a-Single-Record"><a href="#Deleting-a-Single-Record" class="headerlink" title="Deleting a Single Record"></a>Deleting a Single Record</h2><p>The simplest form of deletion is to remove a single record by its ID.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index.delete(ids=[<span class="string">&quot;id-1&quot;</span>, <span class="string">&quot;id-2&quot;</span>], namespace=<span class="string">&#x27;example-namespace&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Deleting-Records-by-Filtering"><a href="#Deleting-Records-by-Filtering" class="headerlink" title="Deleting Records by Filtering"></a>Deleting Records by Filtering</h2><p>Pinecone allows you to delete multiple records based on metadata. However, note that projects in the <code>gcp-starter</code> region do not support deleting by metadata.</p><p>Here’s how you can delete all records that match a certain condition:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index.delete(</span><br><span class="line">    <span class="built_in">filter</span>=&#123;</span><br><span class="line">        <span class="string">&quot;genre&quot;</span>: &#123;<span class="string">&quot;$eq&quot;</span>: <span class="string">&quot;documentary&quot;</span>&#125;,</span><br><span class="line">        <span class="string">&quot;year&quot;</span>: <span class="number">2019</span></span><br><span class="line">    &#125;</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Deleting-All-Records-in-a-Namespace"><a href="#Deleting-All-Records-in-a-Namespace" class="headerlink" title="Deleting All Records in a Namespace"></a>Deleting All Records in a Namespace</h2><p>Namespaces help in logically partitioning data within an index. To delete all records within a namespace, use the following code:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index.delete(delete_all=<span class="literal">True</span>, namespace=<span class="string">&#x27;example-namespace&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="Deleting-All-Records-in-an-Index"><a href="#Deleting-All-Records-in-an-Index" class="headerlink" title="Deleting All Records in an Index"></a>Deleting All Records in an Index</h2><p>In some cases, you might need to remove all records from an index. Be cautious while using this operation as it will clear all your data in that index.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index.delete(delete_all=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Knowing how to effectively manage your data is crucial for optimizing your operations with Pinecone or any other database. We hope this guide provides you with all the necessary details to handle record deletion in Pinecone effectively.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pinecone </tag>
            
            <tag> vectpr store </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How Magical Phrases Improve Responses from Large Language Models</title>
      <link href="2023/09/22/can-large-language-model-take-a-deep-breath-and-think-step-by-step/"/>
      <url>2023/09/22/can-large-language-model-take-a-deep-breath-and-think-step-by-step/</url>
      
        <content type="html"><![CDATA[<p><code>&quot;Take a deep breath&quot;</code> and <code>&quot;think step by step&quot;</code> are phrases you might use to help someone focus on solving a problem. Surprisingly, these phrases also seem to improve the responses from large language models (LLMs) when they are used in prompts, especially in math-related queries. Does this mean that the AI is actually getting smarter? What’s the reason behind this seemingly magical effect? Let’s explore.</p><h2 id="The-Magic-Words-Phenomenon"><a href="#The-Magic-Words-Phenomenon" class="headerlink" title="The Magic Words Phenomenon"></a>The Magic Words Phenomenon</h2><p>Studies show that when prompts include phrases like “take a deep breath” or “think step by step,” the generated responses appear to be more accurate or well-reasoned, especially in the context of solving math problems or other complex issues. Does this mean the AI has suddenly developed the ability to breathe or engage in human-like cognitive processing? Not quite.</p><h2 id="Why-Does-This-Happen"><a href="#Why-Does-This-Happen" class="headerlink" title="Why Does This Happen?"></a>Why Does This Happen?</h2><p>The answer lies in the training data that large language models use to learn. These models are trained on vast datasets compiled from books, websites, forums, and other sources where human-written text exists. Often, people use phrases like “let’s take a deep breath” or “think step by step” before providing detailed answers to questions or explaining complex topics. The model’s improved performance is essentially an emulation of human behavior.</p><h3 id="It’s-All-About-the-Data"><a href="#It’s-All-About-the-Data" class="headerlink" title="It’s All About the Data"></a>It’s All About the Data</h3><p>LLMs can’t actually take a deep breath or think step-by-step because they don’t possess lungs, bodies, or human cognitive abilities. The “reasoning” that these models display is borrowed from the massive datasets they are trained on. When we include phrases like “take a deep breath” or “think step by step” in prompts, we’re essentially nudging the model to tap into segments of its training data that likely contain more reasoned, carefully structured responses.</p><h2 id="Controversy-Around-“Reasoning”"><a href="#Controversy-Around-“Reasoning”" class="headerlink" title="Controversy Around “Reasoning”"></a>Controversy Around “Reasoning”</h2><p>It’s worth noting that the term “reasoning” is a subject of debate among experts in the field of artificial intelligence. While it’s commonly used to describe the operations of complex algorithms, some argue that it inaccurately anthropomorphizes machine processes that are fundamentally different from human cognition. </p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In essence, using phrases like “take a deep breath” or “think step by step” in prompts doesn’t make the AI smarter or more capable of human-like thought. Rather, it aligns the query with the kind of detailed, carefully considered responses that exist in the model’s training data, which can improve the quality of its output. So the next time you’re looking for a more thoughtful answer from a large language model, you might just find that a little “prompt magic” goes a long way.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Switching Organizations in OpenAI API Calls</title>
      <link href="2023/09/20/switch-orgnizations-in-openai-api-call/"/>
      <url>2023/09/20/switch-orgnizations-in-openai-api-call/</url>
      
        <content type="html"><![CDATA[<p>The OpenAI platform has evolved over time to allow developers more flexibility, especially for those who are a part of multiple organizations. In the past, making a call to the OpenAI API was straightforward. You simply set up your API key and then proceed to make your call. However, with the introduction of organizations, things have gotten a tad more intricate, but thankfully, also more organized. In this blog post, we will walk you through how to specify a different organization other than the default one when calling the OpenAI API.</p><h2 id="Basic-API-Call"><a href="#Basic-API-Call" class="headerlink" title="Basic API Call"></a>Basic API Call</h2><p>Initially, when making an API call, we would do something like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line"></span><br><span class="line">openai.api_key = &quot;your openai api_Key&quot;</span><br><span class="line"></span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">    model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Who won the world series in 2020?&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;The Los Angeles Dodgers won the World Series in 2020.&quot;&#125;,</span><br><span class="line">        &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Where was it played?&quot;&#125;</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="Organizations-and-Their-Significance"><a href="#Organizations-and-Their-Significance" class="headerlink" title="Organizations and Their Significance"></a>Organizations and Their Significance</h2><p>By default, the rate limits applied to your API calls are based on the default organization associated with your user account. This works well for most users. However, for those who have joined multiple organizations (maybe you’re part of a business team and also experimenting on a personal project), you might want to specify which organization’s resources should be used for a particular API call.</p><h2 id="How-to-Switch-Organizations-Programmatically"><a href="#How-to-Switch-Organizations-Programmatically" class="headerlink" title="How to Switch Organizations Programmatically"></a>How to Switch Organizations Programmatically</h2><p>Instead of hopping into user settings every time to change the default organization, OpenAI provides a way to do this programmatically:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line"></span><br><span class="line">openai.api_key = &quot;your openai api_Key&quot;</span><br><span class="line">openai.organization = &quot;your org key&quot;</span><br></pre></td></tr></table></figure><p>Here’s the best part: As long as the API key you generated is linked to all those organizations, you don’t need to change the API key when specifying different organizations. That’s a nifty time-saver!</p><h2 id="A-Word-of-Caution"><a href="#A-Word-of-Caution" class="headerlink" title="A Word of Caution"></a>A Word of Caution</h2><p>While the ability to switch between organizations programmatically is powerful, you must be cautious. If you use an API key from your user account but specify an organization that your user account doesn’t have access to, an error will be raised. This error might be a bit misleading as it could state that it can’t find the organization, even if the organization is a valid one. It’s just a way of saying, “Hey, you don’t have access to this!”</p><h2 id="In-Conclusion"><a href="#In-Conclusion" class="headerlink" title="In Conclusion"></a>In Conclusion</h2><p>The introduction of organizations in the OpenAI platform brings about enhanced flexibility for developers juggling between multiple projects. The ability to programmatically switch between organizations ensures that developers have a seamless experience, leveraging the resources of the appropriate organization without having to change API keys. Just remember to double-check your permissions, so you’re always in the right “organizational” lane!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Handling and Logging Errors in Python</title>
      <link href="2023/09/19/raise-error-and-log-error-in-python/"/>
      <url>2023/09/19/raise-error-and-log-error-in-python/</url>
      
        <content type="html"><![CDATA[<p>Errors are a common part of any programming language, and Python is no exception. Knowing how to handle and log errors is crucial for building robust and maintainable software. In this post, we’ll dive deep into how to raise errors in Python and discuss how to use logging to keep track of those errors.</p><h2 id="Raising-Errors-in-Python"><a href="#Raising-Errors-in-Python" class="headerlink" title="Raising Errors in Python"></a>Raising Errors in Python</h2><p>In Python, errors are communicated through a mechanism called exceptions. An exception is a signal that an error condition has occurred. Python has numerous built-in exceptions, like <code>ValueError</code>, <code>TypeError</code>, <code>KeyError</code>, and many more. </p><p>To manually trigger an exception, you use the <code>raise</code> keyword:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square_root</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Cannot compute the square root of a negative number.&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> math.sqrt(x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Here, we raise a <code>ValueError</code> if the function is asked to compute the square root of a negative number.</p><h2 id="Catching-Errors"><a href="#Catching-Errors" class="headerlink" title="Catching Errors"></a>Catching Errors</h2><p>Other functions or parts of your program can “catch” these exceptions and respond to them using a try…except block:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">try:</span><br><span class="line">    result = square_root(-4)</span><br><span class="line">except ValueError as e:</span><br><span class="line">    print(f&quot;An error occurred: &#123;e&#125;&quot;)</span><br></pre></td></tr></table></figure><p>This way, your program doesn’t crash when an exception is raised; instead, it gracefully handles the error and can continue executing.</p><h2 id="The-Power-of-Logging"><a href="#The-Power-of-Logging" class="headerlink" title="The Power of Logging"></a>The Power of Logging</h2><p>While raising and catching exceptions is vital for handling errors, it’s equally essential to keep a record of what went wrong. This is where logging comes in. Logging allows developers to capture information about a program’s execution, making it easier to diagnose and debug issues.</p><p>Python provides a built-in module called logging that makes logging straightforward. Here’s a basic setup:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># Configure logging</span><br><span class="line">logging.basicConfig(level=logging.DEBUG)</span><br><span class="line">logger = logging.getLogger(__name__)</span><br></pre></td></tr></table></figure><p>With this setup, you can now log messages at various severity levels: DEBUG, INFO, WARNING, ERROR, and CRITICAL.</p><h2 id="Combining-Exceptions-and-Logging"><a href="#Combining-Exceptions-and-Logging" class="headerlink" title="Combining Exceptions and Logging"></a>Combining Exceptions and Logging</h2><p>When raising exceptions, it’s a good idea to log information about the error:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def square_root(x):</span><br><span class="line">    if x &lt; 0:</span><br><span class="line">        logger.error(&quot;Attempted to compute the square root of a negative number.&quot;)</span><br><span class="line">        raise ValueError(&quot;Cannot compute the square root of a negative number.&quot;)</span><br><span class="line">    return math.sqrt(x)</span><br></pre></td></tr></table></figure><p>Now, when this function encounters an error, it not only raises an exception but also logs an error message. This dual approach provides multiple benefits:</p><ol><li>User Feedback: The exception provides immediate feedback to users or other parts of your software that something went wrong.</li><li>Developer Insight: The log captures the error, making it easier for developers to diagnose and fix issues, especially when they manifest in production environments.</li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Error handling and logging are essential components of robust software development. By raising appropriate exceptions and logging valuable information, developers can ensure that their software is both user-friendly and maintainable. If you haven’t already, start incorporating these techniques into your code to create more resilient and traceable applications.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature Scaling in Machine Learning and Deep Learning</title>
      <link href="2023/09/17/how-to-choose-feature-scaling-in-machine-learning-and-deep-learning/"/>
      <url>2023/09/17/how-to-choose-feature-scaling-in-machine-learning-and-deep-learning/</url>
      
        <content type="html"><![CDATA[<p>When training machine learning and deep learning models, preprocessing and scaling features is crucial for model convergence and performance. Two common techniques are the <code>StandardScaler</code> and <code>MinMaxScaler</code>. Let’s dive into their details, compare their strengths, and see how they fit into the world of deep learning and transformers.</p><h2 id="StandardScaler-vs-MinMaxScaler"><a href="#StandardScaler-vs-MinMaxScaler" class="headerlink" title="StandardScaler vs. MinMaxScaler"></a>StandardScaler vs. MinMaxScaler</h2><h3 id="StandardScaler"><a href="#StandardScaler" class="headerlink" title="StandardScaler"></a>StandardScaler</h3><ul><li><strong>Definition</strong>: Standardizes features by removing the mean and scaling to unit variance.</li><li><strong>Formula</strong>: z = (X - mean(X)) / std(X)</li><li><strong>Characteristics</strong>: After applying, data will have a mean of 0 and a standard deviation of 1.</li></ul><h3 id="MinMaxScaler"><a href="#MinMaxScaler" class="headerlink" title="MinMaxScaler"></a>MinMaxScaler</h3><ul><li><strong>Definition</strong>: Scales features by transforming them into a given range, typically between [0, 1].</li><li><strong>Formula</strong>: X_scaled = (X - min(X)) / (max(X) - min(X))</li><li><strong>Characteristics</strong>: Data values will reside between the range 0 and 1.</li></ul><h2 id="Which-is-Better-for-Deep-Learning-and-Transformers"><a href="#Which-is-Better-for-Deep-Learning-and-Transformers" class="headerlink" title="Which is Better for Deep Learning and Transformers?"></a>Which is Better for Deep Learning and Transformers?</h2><p>Deep learning introduces additional complexities that make the choice of scaler less clear-cut. Here are key points to consider:</p><h3 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h3><ul><li>Deep models, particularly CNNs, use batch normalization layers to stabilize training, rendering the initial scaling method less crucial. However, transformers typically use layer normalization instead.</li></ul><h3 id="Activation-Functions"><a href="#Activation-Functions" class="headerlink" title="Activation Functions"></a>Activation Functions</h3><ul><li>Activation functions like ReLU, sigmoid, and tanh have specific behaviors and ranges. Standardizing input features can ensure that more values fall within their active regions, aiding learning.</li></ul><h3 id="Empirical-Performance"><a href="#Empirical-Performance" class="headerlink" title="Empirical Performance"></a>Empirical Performance</h3><ul><li>Both scalers can be effective. However, for deep learning applications, there’s a slight preference towards <code>StandardScaler</code>, or simply making data zero-centered.</li></ul><h3 id="Attention-Mechanisms-in-Transformers"><a href="#Attention-Mechanisms-in-Transformers" class="headerlink" title="Attention Mechanisms in Transformers"></a>Attention Mechanisms in Transformers</h3><ul><li>Attention mechanisms in transformers compute dot products between vectors. If these vectors have very large or small values, the resulting dot products can be unstable, thus some normalization is beneficial.</li></ul><h3 id="Embeddings"><a href="#Embeddings" class="headerlink" title="Embeddings"></a>Embeddings</h3><ul><li>In NLP tasks with transformers, word embeddings like word2vec or BERT are often used. These embeddings are already on a consistent scale, making additional scaling sometimes unnecessary.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While scaling or normalization is crucial for model performance, the choice between <code>StandardScaler</code> and <code>MinMaxScaler</code> in deep learning isn’t rigid. Experimentation remains the gold standard to see what works best for specific problems and datasets.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> feature scaling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficiently Replacing DataFrame Values with `df.loc` in Pandas</title>
      <link href="2023/09/17/most-fast-way-to-replace-values-in-pandas-dataframe/"/>
      <url>2023/09/17/most-fast-way-to-replace-values-in-pandas-dataframe/</url>
      
        <content type="html"><![CDATA[<p>Pandas is an indispensable library in the Python ecosystem, enabling users to manipulate large datasets with ease. One common operation in data processing is conditionally replacing values in columns based on some criteria. In this blog post, we’ll explore the power and efficiency of using <code>df.loc</code> for this purpose.</p><h2 id="What-is-df-loc"><a href="#What-is-df-loc" class="headerlink" title="What is df.loc?"></a>What is <code>df.loc</code>?</h2><p>The <code>.loc</code> method in pandas provides label-based indexing for both rows and columns. It’s optimized for performance, making it a go-to choice when you need to select, replace, or modify data based on conditions.</p><h2 id="Simple-Replacements"><a href="#Simple-Replacements" class="headerlink" title="Simple Replacements"></a>Simple Replacements</h2><p>Let’s say we have a DataFrame <code>df</code> with columns <code>A</code>, <code>B</code>, and <code>C</code>. If we wish to modify values in column <code>A</code> based on the values in column <code>B</code>, it’s straightforward:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sample data</span></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    <span class="string">&#x27;A&#x27;</span>: [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">    <span class="string">&#x27;B&#x27;</span>: [<span class="number">6</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">7</span>],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Using .loc to replace values based on a condition</span></span><br><span class="line">df.loc[df[<span class="string">&#x27;B&#x27;</span>] &gt; <span class="number">5</span>, <span class="string">&#x27;A&#x27;</span>] = -<span class="number">1</span></span><br></pre></td></tr></table></figure><p>the output should be</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   A  B   C</span><br><span class="line">0 -1  6  10</span><br><span class="line">1  2  3  11</span><br><span class="line">2 -1  8  12</span><br><span class="line">3  4  4  13</span><br><span class="line">4 -1  7  14</span><br></pre></td></tr></table></figure><h2 id="Advanced-Replacements-with-Multiple-Conditions"><a href="#Advanced-Replacements-with-Multiple-Conditions" class="headerlink" title="Advanced Replacements with Multiple Conditions"></a>Advanced Replacements with Multiple Conditions</h2><p>With <code>df.loc</code>, it’s easy to string together multiple conditions. The key operators are <code>&amp;</code> (and), <code>|</code> (or), and <code>~ </code>(not). For instance, if we wish to modify values in column A based on conditions from both columns B and C:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df.loc[(df[&#x27;B&#x27;] &gt; 5) &amp; (df[&#x27;C&#x27;] &lt; 13), &#x27;A&#x27;] = -1</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While <code>df.loc</code> is incredibly powerful and efficient for many tasks, it’s essential to remember that the best approach always depends on the operation and dataset size. Sometimes, numpy vectorized functions might offer faster performance, or methods like <code>df.where</code> or <code>df.mask</code> could be more intuitive.</p><p>However, when it comes to conditional replacements in DataFrames, <code>df.loc</code> stands out as both versatile and efficient</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decoding the Art of Prompt Design for Large Language Models</title>
      <link href="2023/09/14/how-to-design-prompt-for-large-language-model/"/>
      <url>2023/09/14/how-to-design-prompt-for-large-language-model/</url>
      
        <content type="html"><![CDATA[<p>In the world of language learning models (LLMs), creating the right prompt can make all the difference. This process, referred to as “prompt engineering”, ensures that we can extract the best performance out of these models for specific tasks. Let’s dive into what goes into designing a good prompt.</p><h2 id="What’s-In-a-Prompt"><a href="#What’s-In-a-Prompt" class="headerlink" title="What’s In a Prompt?"></a>What’s In a Prompt?</h2><p>A prompt consists of four primary elements:</p><ol><li><strong>Task Description</strong>: Think of this as an instruction manual for the model. It describes what the model is expected to do in clear, natural language. When a task has a unique input or output format, it’s essential to clarify these using keywords to guide the model.</li><li><strong>Input Data</strong>: This is the information the model has to work with. While most data can be described simply, specialized data like knowledge graphs need a bit more finesse. Techniques like linearization or using code can help convert this data into a format the model can understand.</li><li><strong>Contextual Information</strong>: Just like humans, LLMs sometimes need background information to do a task. For example, when answering a broad question, they might need documents as evidence. This context ensures that the model has a complete picture of what’s being asked.</li><li><strong>Prompt Style</strong>: Depending on the model, the way a prompt is presented can matter. It can be as simple as phrasing it as a clear question or adding prefixes like “Let’s think step by step” to guide the model’s reasoning. For models designed for conversation, breaking down a prompt into smaller parts can be beneficial.</li></ol><h2 id="Crafting-the-Perfect-Prompt-Design-Principles"><a href="#Crafting-the-Perfect-Prompt-Design-Principles" class="headerlink" title="Crafting the Perfect Prompt: Design Principles"></a>Crafting the Perfect Prompt: Design Principles</h2><ol><li><strong>Clarity is Key</strong>: The goal of the task should be crystal clear. Vague descriptions can lead to unpredictable or wrong outputs.</li><li><strong>Break It Down</strong>: For more complex tasks, consider breaking them into simpler sub-tasks. This helps the model tackle each part separately, leading to better overall results.</li><li><strong>Demonstrate with Examples</strong>: Sometimes, the best way to explain a task is by showing the model a few examples. These few-shot demonstrations can teach the model the relationship between input and output.</li><li><strong>Model-friendly Format</strong>: Using a format familiar to the model can be a big help. For instance, symbols like ### or “”” can be used to separate instructions and context. Also, since many models work best with English, it might be worth translating tasks to English first.</li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Designing the perfect prompt can seem daunting, but by understanding the core ingredients and principles, it becomes a systematic and rewarding process. Happy prompting!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> prompt engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How does openai rate limits work</title>
      <link href="2023/09/11/how-does-openai-rate-limits-work/"/>
      <url>2023/09/11/how-does-openai-rate-limits-work/</url>
      
        <content type="html"><![CDATA[<h1 id="Understanding-OpenAI-Rate-Limits"><a href="#Understanding-OpenAI-Rate-Limits" class="headerlink" title="Understanding OpenAI Rate Limits"></a>Understanding OpenAI Rate Limits</h1><p>APIs are the backbone of many digital platforms and services. They allow developers to communicate with software, retrieve data, or make specific changes. However, to ensure a seamless experience for every user, rate limits are enforced. OpenAI, renowned for its state-of-the-art technology, also employs rate limits for its API. Here’s an in-depth look at how OpenAI’s rate limits work and why they’re essential.</p><h2 id="Why-do-we-have-rate-limits"><a href="#Why-do-we-have-rate-limits" class="headerlink" title="Why do we have rate limits?"></a>Why do we have rate limits?</h2><ol><li><strong>Protection Against Misuse</strong>: OpenAI’s rate limits help safeguard the API from any malicious intent. Without them, bad actors could potentially flood the API, resulting in disruptions or overloads.</li><li><strong>Ensuring Fair Access</strong>: No single individual or organization should hog all the resources. Rate limits make sure that everyone gets their fair share, preventing one user’s excessive requests from slowing down the API for others.</li><li><strong>Infrastructure Management</strong>: As the number of requests rises, it can strain the servers. Rate limits are a proactive approach to ensure the servers remain performant and provide a consistent experience to all users.</li></ol><h2 id="How-does-OpenAI-enforce-these-rate-limits"><a href="#How-does-OpenAI-enforce-these-rate-limits" class="headerlink" title="How does OpenAI enforce these rate limits?"></a>How does OpenAI enforce these rate limits?</h2><p>Rate limits aren’t arbitrary. They’re carefully considered and applied based on certain parameters:</p><ul><li><p><strong>Organization-Level Monitoring</strong>: Unlike some platforms which enforce rate limits per user, OpenAI’s limits are set at the organization level. This means if you’re part of multiple organizations, you’ll have an aggregated higher limit.</p></li><li><p><strong>Types of Rate Measures</strong>:</p><ul><li><strong>RPM (Requests Per Minute)</strong>: This counts how many requests are made within a minute.</li><li><strong>RPD (Requests Per Day)</strong>: Monitors the number of requests made in a day.</li><li><strong>TPM (Tokens Per Minute)</strong>: Keeps track of how many tokens are generated per minute.</li></ul></li><li><p><strong>Account Types</strong>: Depending on the type of account you have, the limits might vary. It’s always a good idea to check the default rate limits for the API or visit the ‘rate limits’ section under account management to be in the know.</p></li></ul><h2 id="Another-Dimension-Dollar-Amount-Limit"><a href="#Another-Dimension-Dollar-Amount-Limit" class="headerlink" title="Another Dimension: Dollar Amount Limit"></a>Another Dimension: Dollar Amount Limit</h2><p>It’s not just about the number of requests or tokens; there’s also a monetary aspect to consider. There are limits in terms of the dollar amount, which users should be mindful of. Setting this limit wisely ensures that you don’t overshoot your budget. And if ever you feel constrained, there’s always an option to request an increase in your usage limit.</p><h2 id="In-Conclusion"><a href="#In-Conclusion" class="headerlink" title="In Conclusion"></a>In Conclusion</h2><p>Rate limits might seem like a restriction, but in reality, they’re a boon. They preserve the integrity and performance of the API, ensuring that all users have a smooth and enjoyable experience. Whether you’re a seasoned developer or just starting out, being aware of these limits will help you make the most of OpenAI’s offerings without any hiccups.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to call openai whisper model to get time informaiton in Python</title>
      <link href="2023/09/10/how-to-call-openai-whisper-with-timestamp-such-as-srt-in-python/"/>
      <url>2023/09/10/how-to-call-openai-whisper-with-timestamp-such-as-srt-in-python/</url>
      
        <content type="html"><![CDATA[<p>The speech to text API provides two endpoints, transcriptions and translations, based on our state-of-the-art open source large-v2 Whisper model. They can be used to:</p><p>Transcribe audio into whatever language the audio is in.<br>Translate and transcribe the audio into english.<br>File uploads are currently limited to 25 MB and the following input file types are supported: mp3, mp4, mpeg, mpga, m4a, wav, and webm.</p><p>Here is one example in Python</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import openai</span><br><span class="line"></span><br><span class="line"># Define the API endpoint and headers</span><br><span class="line">url = &quot;https://api.openai.com/v1/audio/transcriptions&quot;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;Authorization&quot;: &quot;Bearer &#123;&#125;&quot;.format(open_ai_key)  # replace with your API key</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># location of your audito files, could be mp3 or mp4, etc.</span><br><span class="line">FILE_PATH = &quot;./upload-whisper.mp4&quot;</span><br><span class="line"># define the parameters</span><br><span class="line"></span><br><span class="line">files = &#123;</span><br><span class="line">    &#x27;file&#x27;: (&#x27;test.mp4&#x27;, open(FILE_PATH, &#x27;rb&#x27;)),</span><br><span class="line">    &#x27;model&#x27;: (None, &#x27;whisper-1&#x27;),</span><br><span class="line">    &#x27;response_format&#x27;: (None, &#x27;srt&#x27;)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, files=files)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>the output is:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">1</span><br><span class="line">00:00:00,000 --&gt; 00:00:02,600</span><br><span class="line">First, I need you to go to the front desk to sign up for work.</span><br></pre></td></tr></table></figure><p>Notice that, in the above code, we set the resonse_format to be “srt” which comes with timestamp.<br>The format of the transcript output can be also one of these options: json, text, srt, verbose_json, or vtt.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> whisper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to manage endpoints for model serving on Databricks using API and UI</title>
      <link href="2023/09/08/model-serving-using-endpoint-managment-in-databricks-api-and-ui-way/"/>
      <url>2023/09/08/model-serving-using-endpoint-managment-in-databricks-api-and-ui-way/</url>
      
        <content type="html"><![CDATA[<p>To create, modify, and delete endpoints for model serving on Databricks, you can follow the instructions provided below.</p><h2 id="Creating-Model-Serving-Endpoints"><a href="#Creating-Model-Serving-Endpoints" class="headerlink" title="Creating Model Serving Endpoints"></a>Creating Model Serving Endpoints</h2><p>You have two options to create model serving endpoints: using the Databricks Machine Learning API or the Databricks Machine Learning UI.</p><ol><li><strong>API Workflow</strong></li></ol><p>To create an endpoint using the API, you can use the following Python code with the <code>requests</code> library:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;https://&lt;databricks-instance&gt;/api/2.0/serving-endpoints&quot;</span></span><br><span class="line"></span><br><span class="line">payload = &#123;</span><br><span class="line">  <span class="string">&quot;name&quot;</span>: <span class="string">&quot;feed-ads&quot;</span>,</span><br><span class="line">  <span class="string">&quot;config&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;served_models&quot;</span>: [&#123;</span><br><span class="line">      <span class="string">&quot;model_name&quot;</span>: <span class="string">&quot;ads1&quot;</span>,</span><br><span class="line">      <span class="string">&quot;model_version&quot;</span>: <span class="string">&quot;1&quot;</span>,</span><br><span class="line">      <span class="string">&quot;workload_size&quot;</span>: <span class="string">&quot;Small&quot;</span>,</span><br><span class="line">      <span class="string">&quot;scale_to_zero_enabled&quot;</span>: <span class="literal">True</span></span><br><span class="line">    &#125;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">  <span class="string">&quot;Authorization&quot;</span>: <span class="string">&quot;Bearer &lt;access-token&gt;&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, json=payload, headers=headers)</span><br></pre></td></tr></table></figure><p>Notice that:</p><ul><li>The POST request method is used for creating endpoints.</li><li>It’s important to note that the API workflow for creating endpoints only works the first time a model is created. If the underlying model version changes or there are any configuration updates, you need to use the modify endpoint method.</li><li>The access-token is your databricks access token which can be generated follow this <a href="/2023/01/20/generate-access-token-in-databricks/">blog</a></li></ul><ol start="2"><li><strong>UI Workflow</strong></li></ol><p>To create an endpoint using the UI, follow these steps:</p><ol><li>Go to the Databricks sidebar and click on “Serving”.</li><li>Click on “Create serving endpoint”.</li><li>Provide a name for your endpoint.</li><li>In the “Edit configuration” section, select the model from either the Workspace Model Registry or Unity Catalog, along with its version.</li><li>Click “Confirm”.</li><li>Select the compute size for your endpoint and specify if it should scale to zero when not in use.</li><li>Configure the traffic percentage to route to the served model.</li><li>Click “Create serving endpoint”.</li></ol><h2 id="Modifying-the-Compute-Configuration-of-an-Endpoint"><a href="#Modifying-the-Compute-Configuration-of-an-Endpoint" class="headerlink" title="Modifying the Compute Configuration of an Endpoint"></a>Modifying the Compute Configuration of an Endpoint</h2><p>After enabling an endpoint, you can modify its compute configuration using either the API or the UI.</p><ol><li><strong>API Workflow</strong></li></ol><p>To modify the compute configuration of an endpoint using the API, you can use the following Python code:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">endpoint_name = <span class="string">&quot;feed-ads&quot;</span></span><br><span class="line">url = <span class="string">f&quot;https://&lt;databricks-instance&gt;/api/2.0/serving-endpoints/<span class="subst">&#123;endpoint_name&#125;</span>/config&quot;</span></span><br><span class="line"></span><br><span class="line">payload = &#123;</span><br><span class="line">  <span class="string">&quot;served_models&quot;</span>: [&#123;</span><br><span class="line">    <span class="string">&quot;model_name&quot;</span>: <span class="string">&quot;ads1&quot;</span>,</span><br><span class="line">    <span class="string">&quot;model_version&quot;</span>: <span class="string">&quot;2&quot;</span>,</span><br><span class="line">    <span class="string">&quot;workload_size&quot;</span>: <span class="string">&quot;Small&quot;</span>,</span><br><span class="line">    <span class="string">&quot;scale_to_zero_enabled&quot;</span>: <span class="literal">True</span></span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">  <span class="string">&quot;Authorization&quot;</span>: <span class="string">&quot;Bearer &lt;access-token&gt;&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.put(url, json=payload, headers=headers)</span><br></pre></td></tr></table></figure><p>Notice that:</p><ul><li>The PUT request method is used for modifying the compute configuration of an endpoint.</li><li>Use this method when you want to update the compute configuration or change the served models of an existing endpoint such as increase the model version.</li></ul><ol start="2"><li><strong>UI Workflow</strong></li></ol><p>To modify the compute configuration of an endpoint using the UI, follow these steps:</p><ol><li>Go to the Databricks sidebar and click on “Serving”.</li><li>Select the endpoint you want to modify.</li><li>Click on “Edit configuration”.</li><li>Choose a workload size and specify if the endpoint should scale down to zero when not in use.</li><li>Modify the traffic percentage to route to the served model.</li><li>Click “Save”.</li></ol><h2 id="Deleting-a-Model-Serving-Endpoint"><a href="#Deleting-a-Model-Serving-Endpoint" class="headerlink" title="Deleting a Model Serving Endpoint"></a>Deleting a Model Serving Endpoint</h2><p>To disable serving for a model, you can delete the endpoint it’s served on.</p><ol><li><strong>API Workflow</strong></li></ol><p>To delete an endpoint using the API, you can use the following Python code:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">endpoint_name = <span class="string">&quot;feed-ads&quot;</span></span><br><span class="line">url = <span class="string">f&quot;https://&lt;databricks-instance&gt;/api/2.0/serving-endpoints/<span class="subst">&#123;endpoint_name&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">  <span class="string">&quot;Authorization&quot;</span>: <span class="string">&quot;Bearer &lt;access-token&gt;&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.delete(url, headers=headers)</span><br></pre></td></tr></table></figure><ol start="2"><li><strong>UI Workflow</strong></li></ol><p>To delete an endpoint using the UI, follow these steps:</p><ol><li>Go to the Databricks sidebar and click on “Serving”.</li><li>Select the endpoint you want to delete.</li><li>Click on the kebab menu at the top and choose “Delete”.</li></ol><p>These instructions provide a clearer and more concise way to deploy, modify, and delete model serving endpoints on Databricks, using both the API and the UI.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is &quot;Not Enough Segments&quot; Error with JWT Token in Python?</title>
      <link href="2023/09/06/not-enough-segments-error-in-jwt-token/"/>
      <url>2023/09/06/not-enough-segments-error-in-jwt-token/</url>
      
        <content type="html"><![CDATA[<p>JSON Web Tokens (JWT) are a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object used as the payload of a JSON Web Signature (JWS) structure or as plaintext. Like many technologies, JWT needs to be correctly implemented to function as expected. An incorrect JWT implementation in Python often results in a common error message: ‘Not Enough Segments.’ Let’s understand this error and how to resolve it.</p><h2 id="Understanding-‘Not-Enough-Segments’-Error"><a href="#Understanding-‘Not-Enough-Segments’-Error" class="headerlink" title="Understanding ‘Not Enough Segments’ Error:"></a>Understanding ‘Not Enough Segments’ Error:</h2><p>JWT Tokens typically follow a structure containing three parts, each separated by a period (.): the Header, the Payload, and the Signature. To break it down,</p><ul><li>Header: The header usually consists of two parts: the type of the token, which is JWT, and the signing algorithm being used, such as HMAC SHA256 or RSA.</li><li>Payload: This section contains the ‘claims.’ Claims are statements about an entity (typically, the user) and additional metadata.</li><li>Signature: The signature is used to verify that the sender of the JWT is who it says it is and to ensure that the message wasn’t changed along the way.</li></ul><p>A correctly structured JWT Token looks like this:</p><p>“eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9l<br>IiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c”</p><p>Python’s JWT decoding libraries, such as PyJWT, will raise a ‘Not Enough Segments’ error if the token only contains one or two sections. Thus, if you find yourself facing a ‘Not Enough Segments’ error, it likely that the token you are trying to decode is not correctly structured.</p><h2 id="Solving-the-‘Not-Enough-Segments’-Error"><a href="#Solving-the-‘Not-Enough-Segments’-Error" class="headerlink" title="Solving the ‘Not Enough Segments’ Error:"></a>Solving the ‘Not Enough Segments’ Error:</h2><ol><li><p>Verify the Token Structure: Ensure that the JWT Token has three distinct sections divided by periods. </p></li><li><p>Check the Token Source: If you’re receiving the token from an external source, confirm that they are sending a properly formatted JWT token. </p></li><li><p>Review Your Code: If you are generating the token within your own application, review your code to ensure you are creating the token correctly.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> jwt token </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Your GitHub Contributions Not Counted? Check Your Email Setup on Git</title>
      <link href="2023/09/06/setup-git-email-to-count-github-contributions-properly/"/>
      <url>2023/09/06/setup-git-email-to-count-github-contributions-properly/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>GitHub, as the largest code hosting platform, allows developers to showcase their contributions to open-source projects and collaborate with others. However, you might find that your contributions are not being counted accurately. One crucial aspect that can impact this is your email setup on Git. In this blog post, we will explore how to ensure your email address is properly configured to track your contributions accurately.</p><h2 id="Why-is-Your-Email-Address-Important"><a href="#Why-is-Your-Email-Address-Important" class="headerlink" title="Why is Your Email Address Important?"></a>Why is Your Email Address Important?</h2><p>Git uses the email address associated with your commits to identify you as the author. When you make a contribution to a repository, the email address linked to your commits is used to attribute the contribution to your GitHub account. It helps in generating valuable statistics, such as your commit history, activity, and contributions graph.</p><h2 id="Checking-Your-Current-Email-Setup"><a href="#Checking-Your-Current-Email-Setup" class="headerlink" title="Checking Your Current Email Setup"></a>Checking Your Current Email Setup</h2><p>To check the email address currently set up in Git, you can use the following command in the terminal:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config user.email</span><br></pre></td></tr></table></figure><p>If this command returns an empty result, it means no email address is configured.</p><h2 id="Setting-Up-Your-Email-Address"><a href="#Setting-Up-Your-Email-Address" class="headerlink" title="Setting Up Your Email Address"></a>Setting Up Your Email Address</h2><p>To configure your email address in Git, use the following command:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">git config --global user.email &quot;your-email@example.com&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Troubleshooting CORS Issues with AWS S3</title>
      <link href="2023/09/05/CORS-error-when-accessing-S3-bucket/"/>
      <url>2023/09/05/CORS-error-when-accessing-S3-bucket/</url>
      
        <content type="html"><![CDATA[<p>When developing web applications that interact with AWS S3, developers might occasionally encounter the dreaded CORS (Cross-Origin Resource Sharing) issues. These issues can appear in various scenarios, one notable case being while using presigned URLs. In this blog post, we’ll dive deep into what causes these CORS issues and how to resolve them.</p><h2 id="What-is-CORS"><a href="#What-is-CORS" class="headerlink" title="What is CORS?"></a>What is CORS?</h2><p>Cross-Origin Resource Sharing (CORS) is a security feature implemented by web browsers. It controls how web pages from one origin can request and interact with resources on another origin. </p><p>Imagine a scenario where a web application hosted on <code>https://example.com</code> tries to fetch a resource from an S3 bucket with a different origin (like <code>https://mybucket.s3.amazonaws.com</code>). Without the right CORS headers from the S3 bucket, the browser will block this request, leading to a CORS error.</p><h2 id="When-Do-CORS-Issues-Arise-with-S3"><a href="#When-Do-CORS-Issues-Arise-with-S3" class="headerlink" title="When Do CORS Issues Arise with S3?"></a>When Do CORS Issues Arise with S3?</h2><p>CORS issues typically arise when:</p><ol><li>An S3 bucket’s CORS policy doesn’t permit requests from the domain in question.</li><li>The client-side request doesn’t match what the S3 bucket’s CORS policy allows in terms of methods or headers.</li><li>There’s a region or header mismatch during the request.</li></ol><p>One notable use case where CORS errors might surprise you is when using <strong>presigned URLs</strong>. These are temporary URLs generated to grant temporary access to a private object in S3. Even if the URL is valid and the object is accessible, if the CORS configuration isn’t set up correctly, accessing the object via the browser can lead to a CORS error.</p><h2 id="How-to-Resolve-CORS-Issues-with-S3"><a href="#How-to-Resolve-CORS-Issues-with-S3" class="headerlink" title="How to Resolve CORS Issues with S3"></a>How to Resolve CORS Issues with S3</h2><ol><li><p><strong>Configure the S3 Bucket’s CORS Policy</strong>: </p><p>Navigate to AWS Management Console -&gt; S3 -&gt; Select your bucket -&gt; Permissions -&gt; CORS configuration.</p><p>Here’s a basic CORS policy example:</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">    &#123;</span><br><span class="line">        &quot;AllowedHeaders&quot;: [&quot;*&quot;],</span><br><span class="line">        &quot;AllowedMethods&quot;: [&quot;GET&quot;, &quot;PUT&quot;, &quot;POST&quot;, &quot;DELETE&quot;],</span><br><span class="line">        &quot;AllowedOrigins&quot;: [&quot;*&quot;],</span><br><span class="line">        &quot;MaxAgeSeconds&quot;: 3000</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>For security, avoid using “*” for AllowedOrigins in production. Specify the exact domains that should have access.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CORS </tag>
            
            <tag> S3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Use Function Calling with OpenAI</title>
      <link href="2023/09/04/fully-understand-function-calling-in-openai-gpt/"/>
      <url>2023/09/04/fully-understand-function-calling-in-openai-gpt/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction-to-Function-Calling"><a href="#Introduction-to-Function-Calling" class="headerlink" title="Introduction to Function Calling"></a>Introduction to Function Calling</h2><p>Function calling is a feature provided by OpenAI that allows developers to describe specific functions to models like gpt-4 and gpt-3.5-turbo. The model can then choose to output a JSON object containing the necessary arguments for those functions. It essentially integrates GPT’s capabilities with external tools and APIs in a seamless manner.</p><p>OpenAI’s models have been enhanced to determine when to call a function based on the user’s input. Additionally, they respond with JSON formatted according to the function’s signature. This results in:</p><ul><li><p><strong>Creating Chatbots:</strong> Developers can build chatbots that use external tools, such as ChatGPT Plugins.</p></li><li><p><strong>Natural Language Conversion:</strong> Convert user queries like “Email Anya about coffee next Friday” into structured function calls. For instance, transforming it to <code>send_email(to: string, body: string)</code>. Another example would be converting “What’s the weather in Boston?” to <code>get_current_weather(location: string, unit: &#39;celsius&#39; | &#39;fahrenheit&#39;)</code>.</p></li><li><p><strong>API and Database Calls:</strong> You can turn user queries into direct API calls or even database requests. For example, “Who are my top customers this month?” could be translated into an API call like <code>get_customers_by_revenue(start_date: string, end_date: string, limit: int)</code>.</p></li><li><p><strong>Data Extraction:</strong> Extracting structured data from texts is also possible. For instance, you could define a function named <code>extract_people_data(people: [&#123;name: string, birthday: string, location: string&#125;])</code> to fetch details of all individuals mentioned in a Wikipedia article.</p></li></ul><h2 id="Practical-Example-Weather-Information"><a href="#Practical-Example-Weather-Information" class="headerlink" title="Practical Example: Weather Information"></a>Practical Example: Weather Information</h2><h3 id="Step-1-User’s-Initial-Request-to-OpenAI"><a href="#Step-1-User’s-Initial-Request-to-OpenAI" class="headerlink" title="Step 1: User’s Initial Request to OpenAI"></a><strong>Step 1: User’s Initial Request to OpenAI</strong></h3><p>Using Python’s <code>requests</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer YOUR_OPENAI_API_KEY&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;model&quot;</span>: <span class="string">&quot;gpt-3.5-turbo-0613&quot;</span>,</span><br><span class="line">    <span class="string">&quot;messages&quot;</span>: [&#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the weather like in Boston?&quot;</span>&#125;],</span><br><span class="line">    <span class="string">&quot;functions&quot;</span>: [&#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;get_current_weather&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;Get the current weather in a given location&quot;</span>,</span><br><span class="line">        <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;location&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The city and state, e.g. San Francisco, CA&quot;</span></span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;unit&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;enum&quot;</span>: [<span class="string">&quot;celsius&quot;</span>, <span class="string">&quot;fahrenheit&quot;</span>]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;location&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(<span class="string">&quot;https://api.openai.com/v1/chat/completions&quot;</span>, headers=headers, data=json.dumps(data))</span><br><span class="line"><span class="built_in">print</span>(response.json())</span><br></pre></td></tr></table></figure><p>OpenAI’s response:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-123&quot;,</span><br><span class="line">  ...</span><br><span class="line">  &quot;choices&quot;: [&#123;</span><br><span class="line">    &quot;index&quot;: 0,</span><br><span class="line">    &quot;message&quot;: &#123;</span><br><span class="line">      &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">      &quot;content&quot;: null,</span><br><span class="line">      &quot;function_call&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;get_current_weather&quot;,</span><br><span class="line">        &quot;arguments&quot;: &quot;&#123; \&quot;location\&quot;: \&quot;Boston, MA\&quot;&#125;&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;finish_reason&quot;: &quot;function_call&quot;</span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Observe that in the provided response, the “content” key returns a null value. In contrast, the “function_call” key offers structured JSON results. Users can utilize this structured format to make external API calls based on their requirements.</p><p>The subsequent steps are independent of OpenAI. We simply harness the structured format provided by OpenAI to proceed further.</p><h3 id="Step-2-Use-OpenAI’s-Response-for-Your-External-API-Call"><a href="#Step-2-Use-OpenAI’s-Response-for-Your-External-API-Call" class="headerlink" title="Step 2: Use OpenAI’s Response for Your External API Call"></a>Step 2: Use OpenAI’s Response for Your External API Call</h3><p>An example using the fictional weather API:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">weather_response = requests.get(&quot;https://weatherapi.com/YOUR_ENDPOINT&quot;, params=&#123;&quot;location&quot;: &quot;Boston, MA&quot;&#125;)</span><br><span class="line">print(weather_response.json())</span><br></pre></td></tr></table></figure><p>Resulting in:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123; &quot;temperature&quot;: 22, &quot;unit&quot;: &quot;celsius&quot;, &quot;description&quot;: &quot;Sunny&quot; &#125;</span><br></pre></td></tr></table></figure><p>You can stop here, or use OpenAI again to convert this structured data into natural language.<br>Following Step 2, we could potentially conclude our workflow and present the response to the users.</p><p>Yet, there’s an option to progress further by sending the API response from Step 2 back to OpenAI. This allows OpenAI to transform the structured data into more natural language. However, if our primary intention is merely to have OpenAI shape the initial output into a JSON format for our workflow, there’s no need to advance to Step 3.</p><h3 id="Step-3-Convert-Structured-Data-into-Natural-Language-with-OpenAI"><a href="#Step-3-Convert-Structured-Data-into-Natural-Language-with-OpenAI" class="headerlink" title="Step 3: Convert Structured Data into Natural Language with OpenAI"></a>Step 3: Convert Structured Data into Natural Language with OpenAI</h3><p>Making another request to OpenAI:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data[&quot;messages&quot;].extend([</span><br><span class="line">    &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: null, &quot;function_call&quot;: &#123;&quot;name&quot;: &quot;get_current_weather&quot;, &quot;arguments&quot;: &quot;&#123; \&quot;location\&quot;: \&quot;Boston, MA\&quot;&#125;&quot;&#125;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;function&quot;, &quot;name&quot;: &quot;get_current_weather&quot;, &quot;content&quot;: &quot;&#123;\&quot;temperature\&quot;: 22, \&quot;unit\&quot;: \&quot;celsius\&quot;, \&quot;description\&quot;: \&quot;Sunny\&quot;&#125;&quot;&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">final_response = requests.post(&quot;https://api.openai.com/v1/chat/completions&quot;, headers=headers, data=json.dumps(data))</span><br><span class="line">print(final_response.json())</span><br></pre></td></tr></table></figure><p>OpenAI’s final response:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-123&quot;,</span><br><span class="line">  ...</span><br><span class="line">  &quot;choices&quot;: [&#123;</span><br><span class="line">    &quot;index&quot;: 0,</span><br><span class="line">    &quot;message&quot;: &#123;</span><br><span class="line">      &quot;role&quot;: &quot;assistant&quot;,</span><br><span class="line">      &quot;content&quot;: &quot;The weather in Boston is currently sunny with a temperature of 22 degrees Celsius.&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;finish_reason&quot;: &quot;stop&quot;</span><br><span class="line">  &#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This example demonstrates the flexibility and power of OpenAI’s function calling feature, allowing for seamless integrations with external tools and services.</p><p>Remember to replace placeholders such as YOUR_OPENAI_API_KEY with your actual credentials.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> function calling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Intuitive World of Vector Dot and Cross Products</title>
      <link href="2023/09/03/understand-vector-dot-product-and-cross-product-intuitively/"/>
      <url>2023/09/03/understand-vector-dot-product-and-cross-product-intuitively/</url>
      
        <content type="html"><![CDATA[<p>Welcome to a journey into the heart of vector mathematics! Two of the most intriguing operations when dealing with vectors are the dot and cross products. While they may seem esoteric at first, understanding them intuitively can open up a wealth of comprehension in fields ranging from physics to computer graphics.</p><h2 id="The-Dot-Product-Projecting-Shadows"><a href="#The-Dot-Product-Projecting-Shadows" class="headerlink" title="The Dot Product: Projecting Shadows"></a>The Dot Product: Projecting Shadows</h2><p>The dot product is often visualized as the “projection” or “shadow” one vector casts upon another.</p><p><strong>Mathematically</strong>: If we have two vectors A and B, their dot product is given by:<br>A . B = |A| * |B| * cos(theta)<br>where theta is the angle between the two vectors.</p><p><strong>Intuitively</strong>: Imagine a bright light shining perpendicularly to vector B. The shadow that A would cast on B is representative of the dot product. If A is directly aligned with B, its shadow is longest. If they’re perpendicular, the shadow disappears.</p><h2 id="The-Cross-Product-Building-Parallelograms-and-Understanding-Rotation"><a href="#The-Cross-Product-Building-Parallelograms-and-Understanding-Rotation" class="headerlink" title="The Cross Product: Building Parallelograms and Understanding Rotation"></a>The Cross Product: Building Parallelograms and Understanding Rotation</h2><p>The cross product between two vectors provides a vector with a special direction and magnitude. Let’s break this down.</p><p><strong>Magnitude</strong>:</p><ul><li>The magnitude of the cross product A x B is given by the formula |A| * |B| * sin(theta).</li><li>This magnitude can be visualized as the area of the parallelogram formed by the two vectors.</li><li>If the vectors are nearly parallel, the area is almost zero, so is their cross product. On the other hand, if the vectors are perpendicular, the parallelogram’s area (and thus the magnitude of the cross product) is maximized.</li></ul><p><strong>Direction</strong>:</p><ul><li>The direction of A x B is always perpendicular to both A and B.</li><li>The direction is determined using the right-hand rule. If you point your fingers in the direction of A and curl them towards B, your thumb points in the direction of A x B.</li></ul><p>To see this in action, think of a door. When you push or pull its handle (force F), you’re applying a torque about its hinge. The direction of this torque (or the rotation axis of the door) is found using the cross product of the position vector (from the hinge to the handle, r) and the force vector F. This torque vector always points along the hinge axis, which is consistent with our intuitive understanding of how doors rotate.</p><h2 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h2><p>Both dot and cross products offer deep insights into the behavior of vectors and their interrelationships. While the dot product provides a scalar value that tells us about the “overlap” between two vectors, the cross product gives us a new vector that reveals how the two vectors spatially relate to each other.</p><p>By internalizing these concepts, one can develop a more profound understanding of many phenomena in physics, engineering, and computer graphics, to name a few fields. So next time you push open a door or enjoy the play of sunlight casting shadows, remember the elegant mathematics that underlies these everyday experiences!</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Few-Shot Learning with OpenAI&#39;s ChatGPT, A Comparative Study of Methods</title>
      <link href="2023/09/01/fewshot-learning-methos-with-chatgpt/"/>
      <url>2023/09/01/fewshot-learning-methos-with-chatgpt/</url>
      
        <content type="html"><![CDATA[<p>In the world of machine learning and natural language processing, OpenAI’s ChatGPT has made waves with its incredible language comprehension and generation capabilities. One of the striking features of ChatGPT is its ability to do “few-shot learning.” In simple terms, this means that the model can be guided to perform a specific task by showing it a few examples (or “shots”) of that task beforehand.</p><p>But what’s the best way to implement few-shot learning? Two popular approaches exist: one involving a single prompt with examples, and another leveraging the conversation history with the API. Let’s delve into these two methods to see how they work and how they compare.</p><h1 id="Method-1-Single-Prompt-with-Few-Shots"><a href="#Method-1-Single-Prompt-with-Few-Shots" class="headerlink" title="Method 1: Single Prompt with Few-Shots"></a>Method 1: Single Prompt with Few-Shots</h1><p>The first approach involves creating a single, concatenated prompt that includes the examples and the new input for which you want an output. You structure the prompt so that it establishes a pattern for the model to follow. The format might look something like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Translate the following English to French phrases:  </span><br><span class="line">- Hello -&gt; Bonjour  </span><br><span class="line">- Thank you -&gt; Merci  </span><br><span class="line">Translate: Good morning</span><br></pre></td></tr></table></figure><p>When passed to the API, the model recognizes the pattern you’ve provided in the input and attempts to follow it in the output, translating “Good morning” to French, likely as “Bonjour.”</p><h3 id="Pros"><a href="#Pros" class="headerlink" title="Pros:"></a>Pros:</h3><p>Simple to implement.<br>No need to manage conversation history.</p><h3 id="Cons"><a href="#Cons" class="headerlink" title="Cons:"></a>Cons:</h3><p>Less dynamic; doesn’t adapt based on interactive feedback.<br>Might require more tokens, leaving less room for the actual task.</p><h2 id="Method-2-Conversation-Style-Few-Shots"><a href="#Method-2-Conversation-Style-Few-Shots" class="headerlink" title="Method 2: Conversation Style Few-Shots"></a>Method 2: Conversation Style Few-Shots</h2><p>The second approach uses the conversation history as the method for providing examples. Each message in the history serves as a “shot,” teaching the model the desired behavior. You can communicate with the model via the API by passing a list of message objects. Each object typically has a “role” (either “user” or “assistant”) and “content” (the text of the message).</p><p>Here’s how the conversation could be structured in Python when using the API:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conversation = [</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate the following English to French: Hello&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Bonjour&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate: Thank you&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Merci&quot;&#125;,</span><br><span class="line">    &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Translate: Good morning&quot;&#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="Pros-1"><a href="#Pros-1" class="headerlink" title="Pros:"></a>Pros:</h3><p>More dynamic; allows you to interactively adapt and refine your input.<br>Uses fewer tokens for examples, leaving more space for additional interactions.</p><h3 id="Cons-1"><a href="#Cons-1" class="headerlink" title="Cons:"></a>Cons:</h3><p>Requires managing the conversation history.</p><h2 id="Comparison-and-Conclusion"><a href="#Comparison-and-Conclusion" class="headerlink" title="Comparison and Conclusion"></a>Comparison and Conclusion</h2><p>While both methods can be effective, your choice may depend on the specific task and how you plan to interact with the model. If you need a one-off, non-interactive task, the single-prompt method may suffice. However, if you’re developing a more interactive application where the context and examples may change over time, the conversational approach could offer more flexibility.</p><p>Regardless of the method you choose, few-shot learning can be a powerful way to guide ChatGPT in generating the output you desire, making it a more valuable tool for a wide variety of tasks.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Deciphering Beta Distribution in Bayesian Analysis</title>
      <link href="2023/08/31/easy-way-to-understand-beta-distribution-and-bayesian-method/"/>
      <url>2023/08/31/easy-way-to-understand-beta-distribution-and-bayesian-method/</url>
      
        <content type="html"><![CDATA[<p>Statistics is full of axiomatic jargon that, when understood, can help data scientists and analysts make better-informed decisions. One such sophisticated tool in Bayesian statistics is the Beta Distribution. Known for its versatility and robustness, the Beta Distribution plays a crucial role in statistical modelling and machine learning.</p><h3 id="What-is-Beta-Distribution"><a href="#What-is-Beta-Distribution" class="headerlink" title="What is Beta Distribution?"></a>What is Beta Distribution?</h3><p>Beta Distribution in Bayesian statistics is often used as a prior distribution for binomial outcomes or events. It is mathematically conjugate, implying that the posterior distribution retains a similar format, making the calculations entailing it more straightforward. The beta distribution is defined by two parameters, α (alpha) and ß (beta), which are strictly positive real numbers.</p><h3 id="Understanding-Beta-Distribution-Intuitively"><a href="#Understanding-Beta-Distribution-Intuitively" class="headerlink" title="Understanding Beta Distribution Intuitively"></a>Understanding Beta Distribution Intuitively</h3><p>In the realm of Bayesian statistics, one can intuitively understand these two parameters. Alpha represents the number of successes and Beta, the number of failures. So, when we gather data observed, updating the posterior distribution becomes rather simple. We add the observed number of successes to the alpha parameter and the number of failures to the beta parameter.</p><p>For instance, let’s consider a typical binomial experiment like flipping a coin. Assuming it to be fair, you’d expect equal numbers of heads and tails over time. If you began with a Beta Distribution as Beta(1,1) or a uniform distribution, then after observing data (let’s say 4 heads and 6 tails), you’d simply update to Beta(5,7), adjusting the parameters based on data collected.</p><p>Hence, the beta distribution as a prior has a unique trait that, when updated with data, it results in the same beta distribution but with different parameters, thereby strengthening the evidence for Bayesian inference.</p><h3 id="Why-Use-Bayesian-Paradigm"><a href="#Why-Use-Bayesian-Paradigm" class="headerlink" title="Why Use Bayesian Paradigm?"></a>Why Use Bayesian Paradigm?</h3><p>Research practitioners and data scientists often ask why choose Bayesian paradigm at all? Foremost, it’s because Bayesian statistics allows us to treat parameters such as the probability in the Bernoulli or Binomial distribution as a random variable. Therefore, a beta distribution becomes an ideal estimate of such distributions.</p><p>It offers us the luxury to update our belief about the distribution of these parameters as more data pours in. This continuous learning and updating from incoming data make Bayesian paradigm a popular choice among machine learning practitioners, where data and its context are ever-evolving.</p><h3 id="Convergence-of-Bayesian-and-Frequentist-Approach"><a href="#Convergence-of-Bayesian-and-Frequentist-Approach" class="headerlink" title="Convergence of Bayesian and Frequentist Approach"></a>Convergence of Bayesian and Frequentist Approach</h3><p>It’s worth noting that regardless of whether we use the frequentist or Bayesian approach, as we observe more and more data, our estimations of the parameters are going to converge rapidly. This suggests that with a large amount of data, the frequentist and Bayesian inferences can often seem to align closely.</p><p>For example, if our prior belief (Beta Distribution) over coin bias is Beta(2,2) and we flip the coin 100 times, observing 58 heads and 42 tails, the posterior distribution would be Beta(60,44). Notice that as we collect more data, the initial prior (Beta(2,2)) is somewhat washed out and the total bias is majorly inferred from data, landing us closer to what one would infer with the frequentist approach.</p><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>Beta distribution, with its flexible structural form, represents a family of different shapes of distributions, and this adaptability makes it a powerful tool in the Bayesian paradigm. Whether a beginner in data analytics or an experienced professional, understanding the Beta distribution will undoubtedly enhance your statistical and machine learning toolbox. Experiment with different priors, gather data, and watch your beliefs update. As more data comes in, your results will converge to a more precise and accurate estimate of the world. Ultimately, isn’t that what we’re all trying to do - estimate the world around us more accurately?</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Comprehensive Guide to Activation Functions in Neural Networks</title>
      <link href="2023/08/29/guide-to-activation-function-in-deep-learning/"/>
      <url>2023/08/29/guide-to-activation-function-in-deep-learning/</url>
      
        <content type="html"><![CDATA[<h2 id="Why-Do-We-Need-Activation-Functions"><a href="#Why-Do-We-Need-Activation-Functions" class="headerlink" title="Why Do We Need Activation Functions?"></a>Why Do We Need Activation Functions?</h2><p>Theoretically, training a neural network model is the process of fitting a mathematical function y=f(x) that maps from input x to output y. The ability to fit this function well depends on the quality of the data and the structure of the model. Models like logistic regression and perceptrons have limited fitting abilities, unable to even fit the XOR function.</p><p>According to the universal approximation theorem, a feed-forward neural network with a linear output layer and at least one hidden layer with a “squashing” activation function can approximate any function to arbitrary precision, given enough neurons in the hidden layers. Activation functions play a crucial role in this, offering non-linear transformations in the feature space—compressing values numerically and deforming geometry.</p><p>In the absence of activation functions, no matter how deep the network is, the output remains a linear combination of the inputs, and the transformed feature space remains linearly inseparable.</p><h2 id="How-to-Choose-an-Appropriate-Activation-Function"><a href="#How-to-Choose-an-Appropriate-Activation-Function" class="headerlink" title="How to Choose an Appropriate Activation Function?"></a>How to Choose an Appropriate Activation Function?</h2><p>An activation function should offer non-linear transformations and be differentiable. Different layers (hidden and output) focus on different aspects. Let’s discuss some commonly used activation functions:</p><h3 id="Sigmoid-and-Tanh"><a href="#Sigmoid-and-Tanh" class="headerlink" title="Sigmoid and Tanh"></a>Sigmoid and Tanh</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line">tanh = nn.Tanh()</span><br></pre></td></tr></table></figure><p>Tanh generally outperforms sigmoid for hidden layers because it outputs values in [−1,+1], offering normalized (mean-centered) data for the subsequent layers. Sigmoid is not zero-centered, making optimization inefficient due to zig-zag behavior. For output layers in binary classification, sigmoid is generally preferred due to its probability interpretation.</p><h3 id="ReLU-and-Leaky-ReLU"><a href="#ReLU-and-Leaky-ReLU" class="headerlink" title="ReLU and Leaky ReLU"></a>ReLU and Leaky ReLU</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">relu = nn.ReLU()</span><br><span class="line">leaky_relu = nn.LeakyReLU(0.01)</span><br></pre></td></tr></table></figure><p>ReLU is computationally efficient and helps accelerate gradient descent. However, it causes sparsity in activations—neurons with negative activation don’t get trained. Leaky ReLU mitigates this by having a small gradient for negative values.</p><h3 id="Softplus"><a href="#Softplus" class="headerlink" title="Softplus"></a>Softplus</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">softplus = nn.Softplus()</span><br></pre></td></tr></table></figure><p>Softplus is a smoother version of ReLU but generally not as effective.</p><h3 id="Swish"><a href="#Swish" class="headerlink" title="Swish"></a>Swish</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">class Swish(nn.Module):</span><br><span class="line">    def forward(self, x):</span><br><span class="line">        return x * torch.sigmoid(x)</span><br></pre></td></tr></table></figure><p>Swish is similar to ReLU but offers smoother and non-monotonic behavior, often outperforming ReLU.</p><h3 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">class Maxout(nn.Module):</span><br><span class="line">    def __init__(self, d_in, d_out, pool_size):</span><br><span class="line">        super(Maxout, self).__init__()</span><br><span class="line">        self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size</span><br><span class="line">        self.lin = nn.Linear(d_in, d_out * pool_size)</span><br><span class="line">        </span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        shape = list(inputs.size())</span><br><span class="line">        shape[-1] = self.d_out</span><br><span class="line">        shape.append(self.pool_size)</span><br><span class="line">        max_out = self.lin(inputs)</span><br><span class="line">        m, i = max_out.view(*shape).max(-1)</span><br><span class="line">        return m</span><br></pre></td></tr></table></figure><p>Maxout is a learnable piece-wise linear function, offering the benefit of adaptability.</p><h3 id="RBF"><a href="#RBF" class="headerlink" title="RBF"></a>RBF</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># PyTorch implementation</span><br><span class="line">class RBFLayer(nn.Module):</span><br><span class="line">    def __init__(self, in_features, out_features, gamma):</span><br><span class="line">        super(RBFLayer, self).__init__()</span><br><span class="line">        self.in_features = in_features</span><br><span class="line">        self.out_features = out_features</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.centers = nn.Parameter(torch.Tensor(out_features, in_features))</span><br><span class="line">        self.initialize_centers()</span><br><span class="line"></span><br><span class="line">    def initialize_centers(self):</span><br><span class="line">        nn.init.uniform_(self.centers)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        x = x.unsqueeze(1).expand(-1, self.out_features, -1)</span><br><span class="line">        diff = x - self.centers</span><br><span class="line">        l2 = torch.sum(diff ** 2, dim=-1)</span><br><span class="line">        return torch.exp(-1 * self.gamma * l2)</span><br></pre></td></tr></table></figure><p>RBF (Radial Basis Function) is seldom used in neural networks due to its tendency to saturate to zero for most inputs, making it difficult to optimize.</p><p>These are just a few examples. The choice of activation function is largely empirical and depends on the task at hand.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> activation function </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Lowercase &#39;p&#39; vs. Uppercase &#39;P&#39;, A Tale of SSH and SCP Port Specification</title>
      <link href="2023/08/29/low-case-p-for-ssh-but-upper-case-p-for-scp-with-port/"/>
      <url>2023/08/29/low-case-p-for-ssh-but-upper-case-p-for-scp-with-port/</url>
      
        <content type="html"><![CDATA[<p>In the world of secure file transfers and remote server access, ssh and scp are commonly used commands. However, these two commands differ in a seemingly small yet significant way: the flag used for specifying a custom port number. While ssh uses a lowercase ‘p’, scp requires an uppercase ‘P’. In this short blog, let’s explore the reasons behind this discrepancy.</p><h2 id="SSH-and-the-Lowercase-‘p’"><a href="#SSH-and-the-Lowercase-‘p’" class="headerlink" title="SSH and the Lowercase ‘p’"></a>SSH and the Lowercase ‘p’</h2><p>When using ssh to connect to a remote server, specifying a port is straightforward. The lowercase ‘p’ is employed, as shown below:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh -p 2222 username@remote-server</span><br></pre></td></tr></table></figure><p>In the context of ssh, the lowercase ‘p’ is dedicated solely for specifying the port, making it simple to remember.</p><h2 id="SCP-and-the-Uppercase-‘P’"><a href="#SCP-and-the-Uppercase-‘P’" class="headerlink" title="SCP and the Uppercase ‘P’"></a>SCP and the Uppercase ‘P’</h2><p>On the other hand, scp (Secure Copy Protocol) uses an uppercase ‘P’ to specify the port:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -P 2222 local-file.txt username@remote-server:/path/to/remote/directory/</span><br></pre></td></tr></table></figure><p>Why the uppercase? It turns out, scp already uses the lowercase ‘p’ to preserve file attributes like modification and access times:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -p local-file.txt username@remote-server:/path/to/remote/directory/</span><br></pre></td></tr></table></figure><h2 id="Why-the-Difference"><a href="#Why-the-Difference" class="headerlink" title="Why the Difference?"></a>Why the Difference?</h2><p>The different options (-p vs -P) between ssh and scp are primarily due to historical reasons and how each utility evolved. Each tool is a separate utility with a different primary function, even though they are part of the same OpenSSH suite. Over time, these utilities have maintained their distinct set of options, which unfortunately means that some inconsistencies like this exist.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Although this difference may seem minor, it can lead to mistakes or confusion when working with ssh and scp. It serves as a reminder that even in a unified suite of tools like OpenSSH, small inconsistencies can still be present. So next time you are specifying a custom port, remember: it’s ssh -p but scp -P.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> ubuntu </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Demystifying Python&#39;s Asyncio, Powering Concurrent and Asynchronous Code</title>
      <link href="2023/08/28/how-and-why-to-use-asyncio-in-python/"/>
      <url>2023/08/28/how-and-why-to-use-asyncio-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python, there is a built-in Python library, asyncio, managing such tasks has become more performant and efficient. So why do we need asyncio and what does it bring to the table? Let’s dive into it.</p><h2 id="Why-do-we-need-asyncio"><a href="#Why-do-we-need-asyncio" class="headerlink" title="Why do we need asyncio?"></a>Why do we need asyncio?</h2><p>Asyncio brings about a significant boost in performance for IO-bound programs, meaning applications that spend most of their time waiting for Input/Output or other operations, such as network or file operations. This is made possible with the help of “non-blocking” code and async/await syntax. Instead of being idle and wasting valuable CPU cycles during these waiting periods, applications coded with asyncio are getting meaningful work done, thus improving overall performance.</p><p>Additionally, asyncio provides better resource utilization. It being single-threaded allows the running of multiple tasks concurrently on a single thread, delivering a significantly more resource-friendly solution compared to creating multiple threads or processes.</p><p>In the context readability and simplicity of the code, asyncio makes the cut too. It wraps the complex concepts of callbacks, thread/process management with the async/await syntax making it easier for developers to write, read, and understand concurrent code.</p><p>If your application needs to handle multiple tasks or connections simultaneously like in the case of servers, asyncio truly shines. Given its resource efficiency and concurrent capabilities, it’s an ideal choice for such applications.</p><h2 id="Writing-asyncio-code-Best-Practice"><a href="#Writing-asyncio-code-Best-Practice" class="headerlink" title="Writing asyncio code: Best Practice"></a>Writing asyncio code: Best Practice</h2><p>The suggested and most effective way of writing asyncio code is by leveraging asynchronous operations, denoted by the async keyword, and using the await keyword for IO-bound tasks.</p><p>Consider this example below:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> aiofiles</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">async_read_file</span>(<span class="params">file_name</span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_name, mode=<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        data = <span class="keyword">await</span> f.read()</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><p>In this function, the ‘await’ keyword tells asyncio to pause execution of the current task, whilst waiting for IO. However, unlike time.sleep(), it won’t block other tasks in the event loop. </p><p>Remember, asyncio’s central idea is non-blocking, cooperative multitasking, so sticking to non-blocking functions and libraries that support asyncio’s asynchronous operations is your best bet.</p><h2 id="Fallback-method-using-run-in-executor"><a href="#Fallback-method-using-run-in-executor" class="headerlink" title="Fallback method using run_in_executor"></a>Fallback method using run_in_executor</h2><p>But what if the I/O operation doesn’t have an async equivalent, or we’re stuck with a legacy codebase of synchronous blocking functions? For these instances, asyncio provides a fallback method, asyncio.run_in_executor(). This method runs your blocking operations in a separate thread allowing your asyncio code to keep running without being blocked. </p><p>Let’s see how it’s done:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> asyncio</span><br><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sync_function</span>(<span class="params">seconds</span>):</span></span><br><span class="line">    time.sleep(seconds)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Finished sleeping&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    executor = ThreadPoolExecutor(max_workers=<span class="number">3</span>)</span><br><span class="line">    loop = asyncio.get_event_loop()</span><br><span class="line">    result = <span class="keyword">await</span> loop.run_in_executor(executor, sync_function, <span class="number">3</span>)</span><br><span class="line">    <span class="built_in">print</span>(result)</span><br><span class="line"></span><br><span class="line">asyncio.run(main())</span><br></pre></td></tr></table></figure><p>This allows the blocking function to seem as if it were a normal async function without blocking the main event loop. But bear in mind, while run_in_executor() can be a life-saver with blocking code, the ideal asyncio design is to stick as much as possible with non-blocking, async-native functions.</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>Asyncio empowers Python to handle concurrent tasks in a more efficient and performant way. By using asynchronous functions wherever possible, and knowing how to deal with blocking functions, you can write code that is efficient, fast, and capable of handling more tasks than before. However, remember that asyncio serves IO-bound tasks best and for CPU-bound tasks, following the traditional multi-threading or multi-process methods might be more effective.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Handling Long-Running Tasks in FastAPI, Best Practices and Strategies</title>
      <link href="2023/08/26/handling-long-running-tasks-in-fastapi-python/"/>
      <url>2023/08/26/handling-long-running-tasks-in-fastapi-python/</url>
      
        <content type="html"><![CDATA[<p>Building fast and efficient APIs is an essential aspect of modern software development. However, it’s not uncommon to encounter API endpoints that take a long time to execute due to reasons such as data processing, calls to third-party services, or complex computations. When such scenarios arise, it’s crucial to ensure that these long-running tasks don’t degrade user experience or system performance. This blog post aims to guide you through the best practices and strategies for managing long-running tasks in FastAPI.</p><h2 id="The-Challenges-of-Long-Running-API-Endpoints"><a href="#The-Challenges-of-Long-Running-API-Endpoints" class="headerlink" title="The Challenges of Long-Running API Endpoints"></a>The Challenges of Long-Running API Endpoints</h2><ol><li>User Experience: A prolonged wait time for a response can result in a poor user experience.</li><li>Resource Utilization: Long-running tasks can consume significant system resources, potentially affecting the performance of other tasks.</li><li>Error Handling: Tasks that take a long time to complete are more susceptible to errors, requiring robust error-handling mechanisms.</li></ol><h2 id="Best-Practices-for-Managing-Long-Running-Tasks"><a href="#Best-Practices-for-Managing-Long-Running-Tasks" class="headerlink" title="Best Practices for Managing Long-Running Tasks"></a>Best Practices for Managing Long-Running Tasks</h2><h3 id="1-Asynchronous-Endpoints"><a href="#1-Asynchronous-Endpoints" class="headerlink" title="1. Asynchronous Endpoints"></a>1. Asynchronous Endpoints</h3><p>You can use Python’s async def syntax to define asynchronous endpoints in FastAPI, which can help in I/O-bound operations.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">async def read_root():</span><br><span class="line">    # Perform a long-running task</span><br><span class="line">    return &#123;&quot;message&quot;: &quot;done&quot;&#125;</span><br></pre></td></tr></table></figure><h3 id="2-Background-Tasks"><a href="#2-Background-Tasks" class="headerlink" title="2. Background Tasks"></a>2. Background Tasks</h3><p>FastAPI allows you to run background tasks that can continue processing after the response has been sent.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import BackgroundTasks, FastAPI</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">def long_running_task():</span><br><span class="line">    # Perform a long-running task</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">async def read_root(background_tasks: BackgroundTasks):</span><br><span class="line">    background_tasks.add_task(long_running_task)</span><br><span class="line">    return &#123;&quot;message&quot;: &quot;Task is running in the background&quot;&#125;</span><br></pre></td></tr></table></figure><h3 id="3-Using-Celery-for-Distributed-Task-Queues"><a href="#3-Using-Celery-for-Distributed-Task-Queues" class="headerlink" title="3. Using Celery for Distributed Task Queues"></a>3. Using Celery for Distributed Task Queues</h3><p>For particularly long-running tasks, you can offload them to a task queue like Celery.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line">from celery import Celery</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line">celery_app = Celery(&#x27;tasks&#x27;, broker=&#x27;pyamqp://guest@localhost//&#x27;)</span><br><span class="line"></span><br><span class="line">@celery_app.task</span><br><span class="line">def long_running_task():</span><br><span class="line">    # Perform a long-running task</span><br><span class="line">    pass</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&quot;)</span><br><span class="line">def read_root():</span><br><span class="line">    long_running_task.apply_async()</span><br><span class="line">    return &#123;&quot;message&quot;: &quot;Task is running in the background&quot;&#125;</span><br></pre></td></tr></table></figure><h2 id="Strategies-to-Notify-Users"><a href="#Strategies-to-Notify-Users" class="headerlink" title="Strategies to Notify Users"></a>Strategies to Notify Users</h2><p>Once a task is offloaded or made asynchronous, it’s essential to inform the user of its completion. Below are some strategies to achieve this:</p><h3 id="1-Polling"><a href="#1-Polling" class="headerlink" title="1. Polling"></a>1. Polling</h3><p>In this approach, the client initially receives a task ID and then repeatedly polls an endpoint to check the task’s status.</p><p>Server-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line">from some_task_queue import some_task_queue</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.post(&quot;/start_task/&quot;)</span><br><span class="line">def start_task():</span><br><span class="line">    task_id = some_task_queue.enqueue(&quot;long_running_task&quot;)</span><br><span class="line">    return &#123;&quot;task_id&quot;: task_id&#125;</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/get_result/&#123;task_id&#125;&quot;)</span><br><span class="line">def get_result(task_id: str):</span><br><span class="line">    result = some_task_queue.get_result(task_id)</span><br><span class="line">    return &#123;&quot;result&quot;: result&#125;</span><br></pre></td></tr></table></figure><p>Client-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">async function startAndPollTask() &#123;</span><br><span class="line">  const response = await fetch(&#x27;/start_task/&#x27;);</span><br><span class="line">  const task = await response.json();</span><br><span class="line">  </span><br><span class="line">  let result;</span><br><span class="line">  do &#123;</span><br><span class="line">    const resultResponse = await fetch(`/get_result/$&#123;task.task_id&#125;`);</span><br><span class="line">    result = await resultResponse.json();</span><br><span class="line">    if (result.is_done) &#123;</span><br><span class="line">      break;</span><br><span class="line">    &#125;</span><br><span class="line">    await new Promise(resolve =&gt; setTimeout(resolve, 2000));</span><br><span class="line">  &#125; while(true);</span><br><span class="line">  </span><br><span class="line">  console.log(&quot;Final result:&quot;, result);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-Webhooks"><a href="#2-Webhooks" class="headerlink" title="2. Webhooks"></a>2. Webhooks</h3><p>Here, the client provides a callback URL that the server can POST the result to once the task is complete.</p><p>Server-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI, BackgroundTasks</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">def long_running_task(callback_url):</span><br><span class="line">    # Your long-running task here</span><br><span class="line">    result = &quot;some_result&quot;</span><br><span class="line">    requests.post(callback_url, json=&#123;&quot;result&quot;: result&#125;)</span><br><span class="line"></span><br><span class="line">@app.post(&quot;/start_task/&quot;)</span><br><span class="line">async def start_task(background_tasks: BackgroundTasks, callback_url: str):</span><br><span class="line">    background_tasks.add_task(long_running_task, callback_url)</span><br><span class="line">    return &#123;&quot;status&quot;: &quot;Task started&quot;&#125;</span><br></pre></td></tr></table></figure><h3 id="3-WebSockets"><a href="#3-WebSockets" class="headerlink" title="3. WebSockets"></a>3. WebSockets</h3><p>You can establish a WebSocket connection between the client and server to send the result when the task is complete.</p><p>Server-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI, WebSocket</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.websocket(&quot;/ws/&quot;)</span><br><span class="line">async def websocket_endpoint(websocket: WebSocket):</span><br><span class="line">    await websocket.accept()</span><br><span class="line">    # Long-running task here</span><br><span class="line">    result = &quot;some_result&quot;</span><br><span class="line">    await websocket.send_json(&#123;&quot;result&quot;: result&#125;)</span><br></pre></td></tr></table></figure><p>Client-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">const socket = new WebSocket(&#x27;ws://localhost:8000/ws/&#x27;);</span><br><span class="line"></span><br><span class="line">socket.addEventListener(&#x27;message&#x27;, function(event) &#123;</span><br><span class="line">    const result = JSON.parse(event.data);</span><br><span class="line">    console.log(&quot;Received result:&quot;, result);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="4-Server-Sent-Events-SSE"><a href="#4-Server-Sent-Events-SSE" class="headerlink" title="4. Server-Sent Events (SSE)"></a>4. Server-Sent Events (SSE)</h3><p>SSE allows the server to send updates and final results over a single HTTP connection.</p><p>Server-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI</span><br><span class="line">from fastapi.responses import StreamingResponse</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/task_status/&quot;)</span><br><span class="line">def get_status():</span><br><span class="line">    def event_stream():</span><br><span class="line">        # Long-running task</span><br><span class="line">        result = &quot;some_result&quot;</span><br><span class="line">        yield f&quot;data: &#123;result&#125;\n\n&quot;</span><br><span class="line">    return StreamingResponse(event_stream(), media_type=&quot;text/event-stream&quot;)</span><br></pre></td></tr></table></figure><p>Client-side:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">const eventSource = new EventSource(&#x27;/task_status/&#x27;);</span><br><span class="line"></span><br><span class="line">eventSource.onmessage = function(event) &#123;</span><br><span class="line">    const result = event.data;</span><br><span class="line">    console.log(&quot;Received result:&quot;, result);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Long-running tasks can pose challenges in API design, but FastAPI provides a variety of features and techniques to handle them efficiently. Whether it’s asynchronous programming, background tasks, or advanced strategies like Celery, Webhooks, and WebSockets, you can choose the right approach based on your API’s requirements. By adhering to these best practices and strategies, you can ensure that long-running tasks are managed effectively without compromising user experience or system performance.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> fastapi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why and How You Should Use VS Code as Your Python IDE</title>
      <link href="2023/08/23/python-development-using-visual-studio-code/"/>
      <url>2023/08/23/python-development-using-visual-studio-code/</url>
      
        <content type="html"><![CDATA[<p>Python is a versatile language used for web development, data science, machine learning, and more. When developing with Python, selecting an Integrated Development Environment (IDE) can elevate your coding experience. One such IDE that has garnered immense popularity is Visual Studio Code (VS Code). In this blog, I will explain why VS Code is a top pick for Python development.</p><h2 id="Installing-VS-Code"><a href="#Installing-VS-Code" class="headerlink" title="Installing VS Code"></a>Installing VS Code</h2><p>To kick things off, installing VS Code is a breeze:</p><ol><li>Visit the official <a href="https://code.visualstudio.com/">Visual Studio Code website</a>.</li><li>Download the version suitable for your operating system, be it Mac, Linux, or Windows.</li><li>Once installed, head over to the Extensions view by clicking on the square icon in the sidebar or pressing Ctrl+Shift+X.</li><li>Search for “Python” and install the Python extension provided by Microsoft. This extension offers enhanced support for Python inside VS Code.</li></ol><h2 id="Benefits-of-Using-VS-Code-for-Python"><a href="#Benefits-of-Using-VS-Code-for-Python" class="headerlink" title="Benefits of Using VS Code for Python"></a>Benefits of Using VS Code for Python</h2><ol><li>Seamless Code Navigation:</li></ol><ul><li>When working on projects that consist of multiple files and structures, navigation can become challenging. VS Code offers features like “Go to Definition”, “Find All References”, and a sidebar view of all your files and folders. This makes it simpler to move between different parts of your codebase.</li></ul><ol start="2"><li>Integrated Virtual Environments:</li></ol><ul><li>VS Code shines in its support for Python virtual environments. With a virtual environment, you can create isolated spaces for your Python projects, ensuring that dependencies do not clash between different projects. When you create a new Python file or open an existing one, VS Code intelligently suggests and even activates the best virtual environment for that project. This compartmentalization is invaluable for developers juggling multiple projects with varying dependencies.</li></ul><ol start="3"><li>Rich Debugging Experience:</li></ol><ul><li>Debugging is an integral part of development, and with VS Code, you get a powerful debugger out of the box. You can set breakpoints, inspect variables, view the call stack, and enjoy interactive debugging sessions, making problem diagnosis faster and more intuitive.</li></ul><ol start="4"><li>Integrated Terminal:</li></ol><ul><li>VS Code comes with an integrated terminal that can be split and customized. This feature lets developers run scripts, git commands, and other shell commands without ever leaving the IDE.</li></ul><ol start="5"><li>Extensions and Customization:</li></ol><ul><li>Apart from the Python extension, the VS Code marketplace is bustling with myriad extensions that cater to different needs. Whether you need support for Python linting, formatting, or even extensions for frameworks like Django or Flask, the marketplace has you covered. Additionally, the highly customizable nature of VS Code ensures you can tweak the IDE to fit your preferences perfectly.</li></ul><ol start="6"><li>Live Share:</li></ol><ul><li>Collaborative coding becomes a pleasure with the Live Share extension. Developers can collaborate in real-time, making pair programming or code reviews more dynamic and productive.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>VS Code is more than just an editor; it’s a comprehensive environment that, when paired with the Python extension, becomes a powerful platform for Python development. With a rich set of features that promote productivity and an active community continually enhancing its capabilities, VS Code is a compelling choice for both novice and seasoned Python developers. If you haven’t already, give it a try and elevate your Python coding journey.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JSON Serialization and the Unicode Escape Mystery</title>
      <link href="2023/08/22/json-dumps-and-cause-unicode-escape/"/>
      <url>2023/08/22/json-dumps-and-cause-unicode-escape/</url>
      
        <content type="html"><![CDATA[<p>One of the beautiful things about the digital age is that data has become universal, crossing borders, languages, and scripts. With such diversity, it’s important to handle international characters correctly in our applications. This brings us to a small mystery that many developers encounter when working with the json module in Python: the case of the unexpected Unicode escape sequences.</p><h2 id="The-Puzzle"><a href="#The-Puzzle" class="headerlink" title="The Puzzle"></a>The Puzzle</h2><p>You’ve been there. You’re serializing a dictionary containing some non-English text into a JSON string in Python:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;greeting&quot;: &quot;你好&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">json_string = json.dumps(data)</span><br><span class="line">print(json_string)  # Outputs: &#123;&quot;greeting&quot;: &quot;\u4f60\u597d&quot;&#125;</span><br></pre></td></tr></table></figure><p>Instead of the actual Chinese characters “你好”, you find yourself staring at the Unicode escape sequences “\u4f60\u597d”. Why did this happen? Is it a bug?</p><h2 id="Delving-into-the-Behavior"><a href="#Delving-into-the-Behavior" class="headerlink" title="Delving into the Behavior"></a>Delving into the Behavior</h2><p>The <code>json.dumps()</code> function in Python comes with a parameter, ensure_ascii, which by default is set to True. This means that during the serialization process, any non-ASCII characters will be escaped into their Unicode representation to ensure that the resulting JSON string is pure ASCII.</p><p>The benefit of this behavior is clear: ASCII strings are generally more portable and less error-prone when interfacing with various systems or software. For instance, older systems or certain network protocols might not handle non-ASCII characters well.</p><h2 id="The-Solution-ensure-ascii-False"><a href="#The-Solution-ensure-ascii-False" class="headerlink" title="The Solution: ensure_ascii=False"></a>The Solution: <code>ensure_ascii=False</code></h2><p>If you want your JSON string to retain the actual non-ASCII characters, you can set the ensure_ascii parameter to False:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">json_string = json.dumps(data, ensure_ascii=False)</span><br><span class="line">print(json_string)  # Outputs: &#123;&quot;greeting&quot;: &quot;你好&quot;&#125;</span><br></pre></td></tr></table></figure><p>By using ensure_ascii=False, the output JSON will contain the actual characters without escaping them. This results in a more human-readable format, especially if you’re working with a lot of non-English content.</p><h2 id="Precautions"><a href="#Precautions" class="headerlink" title="Precautions"></a>Precautions</h2><p>Although setting ensure_ascii to False makes the output more readable, it’s essential to be aware of the destination or consumers of the JSON. Ensure that any systems or processes that consume the generated JSON can handle non-ASCII characters, especially if they’re being transmitted or stored.</p><p>For example, if the JSON data is being sent to a web service, ensure that the service expects and can handle UTF-8 encoded data. Or, if the data is being written to a file, make sure to save the file using an encoding like “utf-8”.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to migrate from circleCI to github actions</title>
      <link href="2023/08/17/how-to-migrate-from-circleci-to-github-action/"/>
      <url>2023/08/17/how-to-migrate-from-circleci-to-github-action/</url>
      
        <content type="html"><![CDATA[<p>Migrating from CircleCI to GitHub Actions involves moving your CI/CD workflow definitions and making sure all the secrets, environment variables, and specific configurations are set up correctly in the new environment. Here’s a step-by-step guide to help you migrate:</p><h2 id="1-Review-Your-Current-Configuration"><a href="#1-Review-Your-Current-Configuration" class="headerlink" title="1. Review Your Current Configuration:"></a>1. Review Your Current Configuration:</h2><ul><li>Before starting, understand the CircleCI configuration by checking the <code>.circleci/config.yml</code> in your project.</li><li>Make a note of all the steps, environment variables, jobs, workflows, and any custom logic/scripts that are involved.</li></ul><h2 id="2-Set-Up-Your-GitHub-Repository"><a href="#2-Set-Up-Your-GitHub-Repository" class="headerlink" title="2. Set Up Your GitHub Repository:"></a>2. Set Up Your GitHub Repository:</h2><ul><li>If your repository isn’t already on GitHub, push it there.</li><li>Ensure you have appropriate permissions to set up Actions and Secrets for the repository.</li></ul><h2 id="3-Define-the-GitHub-Actions-Workflow"><a href="#3-Define-the-GitHub-Actions-Workflow" class="headerlink" title="3. Define the GitHub Actions Workflow:"></a>3. Define the GitHub Actions Workflow:</h2><ul><li>Create a directory <code>.github/workflows</code> in your repository if it doesn’t exist.<br>Now, for each job/workflow you have in CircleCI, create a new .yml file inside this directory.</li></ul><h2 id="4-Translating-CircleCI-Configuration-to-GitHub-Actions"><a href="#4-Translating-CircleCI-Configuration-to-GitHub-Actions" class="headerlink" title="4. Translating CircleCI Configuration to GitHub Actions:"></a>4. Translating CircleCI Configuration to GitHub Actions:</h2><ul><li>Workflow &amp; Jobs:</li><li><ul><li>Translate your CircleCI workflows and jobs into GitHub Actions jobs.</li></ul></li><li><ul><li>Unlike CircleCI, GitHub Actions doesn’t differentiate between workflows and jobs in the same strict manner. Essentially, every .yml file can be a workflow on its own.</li></ul></li><li>Steps:</li><li><ul><li>Translate CircleCI steps into GitHub Actions steps.</li></ul></li><li><ul><li>There’s a rich marketplace of actions provided by GitHub and the community. For example, to checkout code, you can use: actions/checkout@v2.</li></ul></li><li>Docker:</li><li><ul><li>If you’re using a Docker executor in CircleCI, you can use a container job in GitHub Actions.</li></ul></li><li>Caching:</li><li><ul><li>Translate caching in CircleCI to actions/cache in GitHub Actions.</li></ul></li></ul><h2 id="5-Environment-Variables-amp-Secrets"><a href="#5-Environment-Variables-amp-Secrets" class="headerlink" title="5. Environment Variables &amp; Secrets:"></a>5. Environment Variables &amp; Secrets:</h2><ul><li><p>In CircleCI, you might have used context or project settings for secrets and environment variables.</p></li><li><p>In GitHub Actions, move these to ‘Secrets’. You can set them under Your Repo &gt; Settings &gt; Secrets.</p></li><li><p>In the workflow file, use $ to access the secret.</p><h2 id="6-Integrate-with-AWS-and-Terraform"><a href="#6-Integrate-with-AWS-and-Terraform" class="headerlink" title="6. Integrate with AWS and Terraform:"></a>6. Integrate with AWS and Terraform:</h2></li><li><p>If you’re using AWS CLI or SDK in your workflow, configure AWS credentials using aws-actions/configure-aws-credentials@v1.</p></li><li><p>For Terraform, you might want to look into the hashicorp/setup-terraform action.</p></li><li><p>Ensure all environment-specific variables for Terraform are properly configured.</p></li></ul><h2 id="7-Testing-the-Workflow"><a href="#7-Testing-the-Workflow" class="headerlink" title="7. Testing the Workflow:"></a>7. Testing the Workflow:</h2><ul><li>Once you’ve translated the configuration, push a commit or use a pull request to trigger the GitHub Actions workflows. Monitor the execution and fix any issues or errors that arise.</li><li>Iteratively improve the workflow until it matches the reliability and functionality of your CircleCI configuration.</li></ul><h2 id="8-Optimizations-and-Advanced-Features"><a href="#8-Optimizations-and-Advanced-Features" class="headerlink" title="8. Optimizations and Advanced Features:"></a>8. Optimizations and Advanced Features:</h2><ul><li><p>Explore matrix builds if you had parallel workflows in CircleCI.</p></li><li><p>Implement build artifacts, test reports, etc. as needed.</p><h2 id="9-Update-Documentation-amp-Notify-Team"><a href="#9-Update-Documentation-amp-Notify-Team" class="headerlink" title="9. Update Documentation &amp; Notify Team:"></a>9. Update Documentation &amp; Notify Team:</h2></li><li><p>Since CI/CD can be critical for team workflows, ensure all team members are informed about the migration and any changes they need to be aware of.</p></li><li><p>Update any project documentation or guidelines to reflect the move to GitHub Actions.</p><h2 id="10-Monitoring-amp-Notifications"><a href="#10-Monitoring-amp-Notifications" class="headerlink" title="10. Monitoring &amp; Notifications:"></a>10. Monitoring &amp; Notifications:</h2></li><li><p>Ensure you have proper notifications set up, so team members are alerted of build failures or other critical issues.</p></li><li><p>Monitor the workflows for a while to ensure they’re running as expected.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Build Factory Class in Python</title>
      <link href="2023/08/15/build-factory-class-with-class-method-in-python/"/>
      <url>2023/08/15/build-factory-class-with-class-method-in-python/</url>
      
        <content type="html"><![CDATA[<p>In object-oriented design and programming, the Factory pattern is a design pattern used to create objects without specifying the exact class of object that will be created. The Factory method pattern deals with the problem of creating objects without specifying the exact class of object that will be created. Instead, it refers to the creation through the use of a common interface.</p><p>In Python, the Factory pattern can be implemented in various ways. One way to link it with class methods is by using a class method as a factory for creating instances of that class.</p><p>Here’s an example to demonstrate this concept:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Animal:</span><br><span class="line">    _types = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def register_type(cls, animal_type, animal_cls):</span><br><span class="line">        cls._types[animal_type] = animal_cls</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def create(cls, animal_type, *args, **kwargs):</span><br><span class="line">        if animal_type not in cls._types:</span><br><span class="line">            raise ValueError(f&quot;Animal type &#123;animal_type&#125; not recognized.&quot;)</span><br><span class="line">        return cls._types[animal_type](*args, **kwargs)</span><br><span class="line"></span><br><span class="line">class Cat(Animal):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    def speak(self):</span><br><span class="line">        return f&quot;&#123;self.name&#125; says Meow!&quot;</span><br><span class="line"></span><br><span class="line">class Dog(Animal):</span><br><span class="line">    def __init__(self, name):</span><br><span class="line">        self.name = name</span><br><span class="line"></span><br><span class="line">    def speak(self):</span><br><span class="line">        return f&quot;&#123;self.name&#125; says Woof!&quot;</span><br><span class="line"></span><br><span class="line"># Registering the animal types with the factory</span><br><span class="line">Animal.register_type(&quot;cat&quot;, Cat)</span><br><span class="line">Animal.register_type(&quot;dog&quot;, Dog)</span><br><span class="line"></span><br><span class="line"># Using the factory method to create instances</span><br><span class="line">cat = Animal.create(&quot;cat&quot;, &quot;Whiskers&quot;)</span><br><span class="line">dog = Animal.create(&quot;dog&quot;, &quot;Buddy&quot;)</span><br><span class="line"></span><br><span class="line">print(cat.speak())  # Whiskers says Meow!</span><br><span class="line">print(dog.speak())  # Buddy says Woof!</span><br></pre></td></tr></table></figure><p>In this example:</p><ol><li><code>Animal</code> class serves as a factory. It has a dictionary <code>_types</code> to keep track of the registered animal types.</li><li><code>register_type</code> is a class method used to register new animal types.</li><li><code>create</code> is the factory method. It’s a class method that creates an instance of the appropriate type.</li><li><code>Cat</code> and <code>Dog</code> are concrete implementations of the Animal type.</li><li>We register these types with the Animal factory, and then use the create method to produce instances.</li></ol><p>The relationship between factory class and class method here is that the class method provides a convenient and encapsulated way to create instances of the class without directly invoking the constructors of the concrete implementations. This can make code more flexible and easier to maintain because the exact classes of the objects and the logic for their creation can be kept separate from the rest of the application.</p><p>More about <code>python class method</code> can be found <a href="/2023/08/15/how-does-python-class-method-work/">here</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between Static method and Class method in Python</title>
      <link href="2023/08/15/difference_between_static_method_and_class_method_in_python/"/>
      <url>2023/08/15/difference_between_static_method_and_class_method_in_python/</url>
      
        <content type="html"><![CDATA[<p>In Python, both staticmethod and classmethod are ways to define methods that are tied to a class and not to an instance of the class. However, there are important differences between the two:</p><h2 id="First-Argument"><a href="#First-Argument" class="headerlink" title="First Argument:"></a>First Argument:</h2><p><code>Class Method</code>: It takes a reference to the class (cls) as its first parameter. This allows you to call class-level attributes and other class methods or even to instantiate the class. You would generally use cls as the convention for this parameter.<br><code>Static Method</code>: It doesn’t take any specific first parameter. It behaves just like a regular function but belongs to the class’s namespace.</p><h2 id="Decorator"><a href="#Decorator" class="headerlink" title="Decorator:"></a>Decorator:</h2><p><code>Class Method</code>: Defined using the @classmethod decorator.<br><code>Static Method</code>: Defined using the @staticmethod decorator.</p><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage:"></a>Usage:</h2><p><code>Class Method</code>: Useful when you need to have methods that operate on class-level attributes or when you want to be able to override methods in subclasses.<br><code>Static Method:</code> Useful when you want to perform an action in a class’s context but don’t need to access or modify the class or its instances.</p><h2 id="Overriding"><a href="#Overriding" class="headerlink" title="Overriding:"></a>Overriding:</h2><p><code>Class Method</code>: Can be overridden by subclasses, which allows for polymorphism. The method defined in the subclass will receive the subclass as its cls parameter, not the base class.<br><code>Static Method</code>: Cannot be easily overridden for polymorphism. It behaves the same way no matter what subclass you might be working with.</p><h2 id="Calling"><a href="#Calling" class="headerlink" title="Calling:"></a>Calling:</h2><p>Both static methods and class methods can be called on the class itself, rather than on instances of the class.</p><p>Here’s a quick example to illustrate:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyClass:</span><br><span class="line">    </span><br><span class="line">    class_var = &quot;Class variable&quot;</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def class_method(cls):</span><br><span class="line">        return f&quot;Called from the class method of &#123;cls&#125;. Accessed: &#123;cls.class_var&#125;&quot;</span><br><span class="line"></span><br><span class="line">    @staticmethod</span><br><span class="line">    def static_method(arg):</span><br><span class="line">        return f&quot;Called from the static method with argument: &#123;arg&#125;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Usage</span><br><span class="line">print(MyClass.class_method())    # Outputs: Called from the class method of &lt;class &#x27;__main__.MyClass&#x27;&gt;. Accessed: Class variable</span><br><span class="line">print(MyClass.static_method(5))  # Outputs: Called from the static method with argument: 5</span><br></pre></td></tr></table></figure><h2 id="In-conclusion"><a href="#In-conclusion" class="headerlink" title="In conclusion:"></a>In conclusion:</h2><p>Use classmethod when you need to interact with class-level attributes or want to be polymorphic with subclasses.<br>Use staticmethod when you just want to place a method inside a class for organizational reasons, and the method neither needs to access instance-specific data nor class-level data.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Python&#39;s Class Methods with example</title>
      <link href="2023/08/15/how-does-python-class-method-work/"/>
      <url>2023/08/15/how-does-python-class-method-work/</url>
      
        <content type="html"><![CDATA[<p>Python, as a versatile language, offers a variety of tools to help developers write cleaner and more efficient code. One such tool is the class method. In this article, we will dive into:</p><ul><li>What is a class method?</li><li>The benefits and use cases of class methods.</li><li>How to utilize class methods in your code.</li></ul><h2 id="1-What-is-a-Class-Method"><a href="#1-What-is-a-Class-Method" class="headerlink" title="1. What is a Class Method?"></a>1. What is a Class Method?</h2><p>A class method in Python is a method that is bound to the class and not the instance of the class. Unlike standard instance methods, which have access to instance-specific data and come with the <code>self</code> parameter, class methods deal with class-level data and come with a <code>cls</code> parameter, representing the class itself.</p><p>To define a class method, we use the @classmethod decorator:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyClass:</span><br><span class="line">    class_variable = &quot;I&#x27;m a class variable&quot;</span><br><span class="line">    </span><br><span class="line">    @classmethod</span><br><span class="line">    def class_method_example(cls):</span><br><span class="line">        return cls.class_variable</span><br></pre></td></tr></table></figure><h2 id="2-Benefits-and-Use-Cases-of-Class-Methods"><a href="#2-Benefits-and-Use-Cases-of-Class-Methods" class="headerlink" title="2. Benefits and Use Cases of Class Methods"></a>2. Benefits and Use Cases of Class Methods</h2><h3 id="Alternative-Constructors"><a href="#Alternative-Constructors" class="headerlink" title="Alternative Constructors"></a>Alternative Constructors</h3><p>One of the most common uses of class methods is to provide alternative ways to create class instances. For instance:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Date:</span><br><span class="line">    def __init__(self, day, month, year):</span><br><span class="line">        self.day = day</span><br><span class="line">        self.month = month</span><br><span class="line">        self.year = year</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def from_string(cls, date_string):</span><br><span class="line">        year, month, day = map(int, date_string.split(&#x27;-&#x27;))</span><br><span class="line">        return cls(day, month, year)</span><br><span class="line"></span><br><span class="line">date_obj = Date.from_string(&quot;2023-08-15&quot;)</span><br></pre></td></tr></table></figure><p>In the example, instead of using the main constructor that requires three arguments (day, month, year), we used the from_string class method to instantiate a Date object from a string.</p><h3 id="Modifying-Class-State"><a href="#Modifying-Class-State" class="headerlink" title="Modifying Class State"></a>Modifying Class State</h3><p>Class methods can be used to modify class-level attributes:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class MyClass:</span><br><span class="line">    setting = &quot;default&quot;</span><br><span class="line"></span><br><span class="line">    @classmethod</span><br><span class="line">    def set_setting(cls, new_setting):</span><br><span class="line">        cls.setting = new_setting</span><br></pre></td></tr></table></figure><h3 id="Inheritance-and-Method-Overriding"><a href="#Inheritance-and-Method-Overriding" class="headerlink" title="Inheritance and Method Overriding"></a>Inheritance and Method Overriding</h3><p>Class methods can be overridden by subclasses, ensuring that the appropriate version of the method is called based on the subclass:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Parent:</span><br><span class="line">    @classmethod</span><br><span class="line">    def speak(cls):</span><br><span class="line">        return &quot;Parent speaking!&quot;</span><br><span class="line"></span><br><span class="line">class Child(Parent):</span><br><span class="line">    @classmethod</span><br><span class="line">    def speak(cls):</span><br><span class="line">        return &quot;Child speaking!&quot;</span><br><span class="line"></span><br><span class="line">print(Parent.speak())  # Outputs: Parent speaking!</span><br><span class="line">print(Child.speak())   # Outputs: Child speaking!</span><br></pre></td></tr></table></figure><h2 id="3-No-Need-to-Instantiate"><a href="#3-No-Need-to-Instantiate" class="headerlink" title="3. No Need to Instantiate"></a>3. No Need to Instantiate</h2><p>A significant advantage of class methods is that you don’t need to create an instance of the class to call them. They can be directly invoked on the class itself. This characteristic makes class methods powerful tools for tasks like alternative constructors.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Python’s class methods bring a range of advantages to the table, from offering alternative ways to instantiate objects, modifying class-level data, to enabling method overriding in subclasses. By understanding and leveraging class methods, developers can write more organized and efficient object-oriented code.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Why Modifying a Dictionary During Iteration Causes an Error</title>
      <link href="2023/08/12/why-modify-dictionary-in-iteration-cause-error-in-python/"/>
      <url>2023/08/12/why-modify-dictionary-in-iteration-cause-error-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python, dictionaries are a type of mutable container. That means, in principle, you can change them – add new key-value pairs, update values, or even remove keys. However, a common error when working with dictionaries is trying to modify them (e.g., using pop()) while looping through them. Let’s delve into why this is problematic and how to address it.</p><h2 id="The-Issue"><a href="#The-Issue" class="headerlink" title="The Issue"></a>The Issue</h2><p>Consider a simple loop through a dictionary:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data = &#123;&#x27;good&#x27;: 1, &#x27;bad&#x27;: 2, &#x27;ugly&#x27;: 3&#125;</span><br><span class="line">for key in data:</span><br><span class="line">    if key == &#x27;bad&#x27;:</span><br><span class="line">        data.pop(key)</span><br></pre></td></tr></table></figure><p>When you try to run this code, Python will raise a RuntimeError with the message: <code>dictionary changed size during iteration</code>.</p><p>The reason is intuitive. When you’re looping through a dictionary (or any iterable, for that matter), Python creates an iterator that expects the items to be in a specific order. If you change the dictionary’s size while iterating, the iterator loses track of which items have been seen and which haven’t, which can cause unpredictable results.</p><h2 id="Solutions"><a href="#Solutions" class="headerlink" title="Solutions"></a>Solutions</h2><h3 id="1-Create-a-Copy-for-Iteration"><a href="#1-Create-a-Copy-for-Iteration" class="headerlink" title="1. Create a Copy for Iteration"></a>1. Create a Copy for Iteration</h3><p>The simplest solution is to loop over a copy of the dictionary’s keys. This way, the original dictionary can be modified while iterating over the copy:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for key in list(data.keys()):</span><br><span class="line">    if key == &#x27;bad&#x27;:</span><br><span class="line">        data.pop(key)</span><br></pre></td></tr></table></figure><h3 id="2-Dictionary-Comprehension"><a href="#2-Dictionary-Comprehension" class="headerlink" title="2. Dictionary Comprehension"></a>2. Dictionary Comprehension</h3><p>Python’s dictionary comprehensions provide a concise and efficient way to filter or transform dictionary data:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data = &#123;&#x27;good&#x27;: 1, &#x27;bad&#x27;: 2, &#x27;ugly&#x27;: 3&#125;</span><br><span class="line">new_dict = &#123;k: v for k, v in data.items() if k != &#x27;bad&#x27;&#125;</span><br></pre></td></tr></table></figure><p>In the above code, new_dict will have the value {‘good’: 1, ‘ugly’: 3} after execution.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>While Python allows mutable operations on dictionaries, caution is needed when combining iteration with mutations. Always remember that modifying a collection while iterating through it can lead to unexpected behavior or errors. Using a list copy or dictionary comprehensions are two effective methods to avoid these pitfalls.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Using `*args` and `**kwargs` to Design Flexible Constructors in Python class Inheritance</title>
      <link href="2023/08/10/flexible-way-to-extend-and-inherit-python-class/"/>
      <url>2023/08/10/flexible-way-to-extend-and-inherit-python-class/</url>
      
        <content type="html"><![CDATA[<p>In object-oriented programming with Python, one often encounters scenarios where a subclass needs to be designed to handle any changes or additions to its parent class without needing continual adjustments. One common place this is encountered is the <strong>init</strong> method of classes, particularly when dealing with inheritance.</p><p>To handle such scenarios gracefully, Python provides the <code>*args</code> and <code>**kwargs</code> conventions. Before diving into our main example, let’s get a brief understanding of what these conventions are:</p><p><code>*args</code>: Allows you to pass a variable number of non-keyword arguments to a function. These arguments are captured into a tuple.</p><p><code>**kwargs</code>: Enables passing a variable number of keyword arguments to a function. These arguments are captured into a dictionary.</p><p>With this knowledge, let’s look at a scenario where these can be beneficial.</p><h2 id="Scenario-Extending-a-Parent-Class"><a href="#Scenario-Extending-a-Parent-Class" class="headerlink" title="Scenario: Extending a Parent Class"></a>Scenario: Extending a Parent Class</h2><p>Suppose we have a Class A which accepts two arguments in its constructor:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class A:</span><br><span class="line">    def __init__(self, arg1, arg2):</span><br><span class="line">        self.arg1 = arg1</span><br><span class="line">        self.arg2 = arg2</span><br><span class="line">        print(f&quot;Class A Constructor with arguments: &#123;arg1&#125;, &#123;arg2&#125;&quot;)</span><br></pre></td></tr></table></figure><p>Now, if we’re designing Class B that inherits from Class A, and we want Class B to be adaptive to any changes in Class A without having to modify Class B every time, we can make the <strong>init</strong> method of Class B generic using *args and **kwargs:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class B(A):</span><br><span class="line">    def __init__(self, *args, **kwargs):</span><br><span class="line">        super().__init__(*args, **kwargs)  # Passing all arguments to Class A&#x27;s constructor</span><br><span class="line">        print(&quot;Class B Constructor&quot;)</span><br></pre></td></tr></table></figure><p>By designing Class B this way, any changes to the signature of Class A’s <strong>init</strong> method won’t break our Class B instantiation as long as we ensure the arguments passed to Class B match the updated requirements of Class A.</p><p>For instance:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">b = B(&quot;Hello&quot;, &quot;World&quot;) </span><br><span class="line"># Outputs: </span><br><span class="line"># Class A Constructor with arguments: Hello, World</span><br><span class="line"># Class B Constructor</span><br></pre></td></tr></table></figure><h2 id="Benefits-of-this-Approach"><a href="#Benefits-of-this-Approach" class="headerlink" title="Benefits of this Approach:"></a>Benefits of this Approach:</h2><ul><li><p>Flexibility: Class B remains resilient to modifications in Class A. This is particularly helpful in larger projects or libraries where Class A might be part of a module that receives updates.</p></li><li><p>Maintainability: As the project grows, developers won’t have to revisit Class B every time there’s a change in Class A.</p></li><li><p>Cleaner Code: Instead of handling each argument individually, we’re capturing all of them generically, leading to cleaner and more concise code.</p></li></ul><h2 id="Words-of-Caution"><a href="#Words-of-Caution" class="headerlink" title="Words of Caution:"></a>Words of Caution:</h2><p>While this approach provides flexibility, it also requires diligence:</p><p>Blindly passing all arguments can sometimes lead to unintended consequences, especially if Class A’s constructor changes in a way that’s incompatible with how Class B is instantiated.</p><p>Debugging can be a bit trickier since the error messages might point to the super class’s constructor rather than the derived class.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Using *args and **kwargs to design flexible constructors is a powerful tool in a Python developer’s toolkit, especially when working with inheritance. By understanding and employing this approach, developers can ensure their classes remain flexible and maintainable across changes and iterations. As always, while flexibility is beneficial, it’s also crucial to exercise caution and ensure that the flexibility doesn’t introduce unforeseen issues.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> class inheritance </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between overwrite and override in programming</title>
      <link href="2023/08/10/overwrite-vs-override-in-programming/"/>
      <url>2023/08/10/overwrite-vs-override-in-programming/</url>
      
        <content type="html"><![CDATA[<p>In programming, “overwrite” and “override” have distinct meanings:</p><h2 id="Overwrite"><a href="#Overwrite" class="headerlink" title="Overwrite:"></a>Overwrite:</h2><ul><li>Literally means to replace existing data or code with new data or code.</li><li>If you have a file or a block of code and you replace it with something else, you are overwriting it.</li><li>Example: If you have a backup of a file and you save a new version of the file on top of the old one, you overwrite the old file.</li></ul><h2 id="Override"><a href="#Override" class="headerlink" title="Override:"></a>Override:</h2><ul><li>Commonly used in the context of object-oriented programming (OOP) to indicate that a subclass provides a specific implementation of a method that is already provided by one of its superclasses. The subclass’s version of the method “overrides” the superclass’s version.</li><li>It means to change or extend the default behavior of a method or function.</li><li>Example: In Java, the @Override annotation is used to indicate that the subsequent method overrides a method from its superclass.</li></ul><p>If you updated some code, the term you use depends on the nature of the update:</p><p>If you replaced the existing code with a completely new version, then you “overwrote” the old code.</p><p>If you made modifications to an existing code base to change or extend some of its behavior, you might say you “modified” or “updated” the code. If your changes are specifically related to object-oriented method behavior in subclasses, then you “overrode” certain methods.</p><p>In general conversation or documentation, if you’re talking about replacing code entirely, you’d typically use “overwrite.” If you’re talking about making modifications to existing methods or functions in an OOP context, “override” might be more apt. However, in many contexts, especially outside of OOP, “modify” or “update” would be most commonly used to describe general code changes.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the Error in Percentage Measurements</title>
      <link href="2023/08/10/understand-error-in-percentage-measurement/"/>
      <url>2023/08/10/understand-error-in-percentage-measurement/</url>
      
        <content type="html"><![CDATA[<p>When we deal with percentages derived from sample data, understanding the potential error or variability is essential. A common method to gauge this variability is by calculating the standard deviation of the sampling distribution of a proportion.</p><h2 id="Formula"><a href="#Formula" class="headerlink" title="Formula:"></a>Formula:</h2><p>Standard Deviation (SD) of proportion, p:</p><p><code>SD = Square Root [(p * (1 - p)) / n]</code></p><p>Where:</p><p><code>SD</code>: Standard deviation of the proportion’s sampling distribution.<br><code>p</code>: Sample proportion (expressed as a decimal, so 70% would be 0.7).<br><code>n</code>: Sample size.</p><h2 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h2><p>Consider a political survey in a town regarding support for a particular candidate. From a random sample of 400 residents, 280 express their support. The sample proportion, p, is 280/400 = 0.7 (70%).</p><p>Using our formula:<br><code>SD = Square Root [(0.7 * (1 - 0.7)) / 400]</code><br><code>SD</code> ≈ 0.023 or 2.3%</p><p>This suggests that in our sample, the standard deviation of the residents’ support proportion for the candidate is about 2.3%.</p><h2 id="Deep-Dive"><a href="#Deep-Dive" class="headerlink" title="Deep Dive:"></a>Deep Dive:</h2><p>Our formula assumes the proportion’s sampling distribution is roughly normal. This is generally true when both n * p and n * (1 - p) are greater than 5.</p><p>To compute a confidence interval for this proportion, the standard deviation isn’t enough. We also need the z-value, from the z-distribution, for our desired confidence level. The margin of error is:<br>Margin of Error = z * SD<br>For a 95% confidence level, we often use a z-value of 1.96.</p><p>If comparing proportions from two independent samples, the formula gets more involved. In such cases, the two-proportion z-test is used to see if there’s a significant difference.</p><p>In conclusion, this formula gives insight into the variability in proportion measurements. It’s crucial to apply it correctly, keeping in mind its assumptions and limitations.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> statistics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Price and Volume, The Dual Pillars of Intraday Stock Trading</title>
      <link href="2023/08/09/price-and-volume-in-stock-trading/"/>
      <url>2023/08/09/price-and-volume-in-stock-trading/</url>
      
        <content type="html"><![CDATA[<p>In stock trading, especially intraday trading, considering both price and volume is crucial. Price tells you the direction and magnitude of the move, while volume provides information about the strength and validity of that move. Here are some strategies and points to consider when analyzing both:</p><h2 id="1-Volume-amp-Breakouts"><a href="#1-Volume-amp-Breakouts" class="headerlink" title="1. Volume &amp; Breakouts:"></a>1. Volume &amp; Breakouts:</h2><p>When a stock breaks out from a resistance or consolidation pattern on higher-than-average volume, it often signals strength in the move.<br>A breakout on low volume may not be as reliable because it indicates a lack of conviction from buyers.</p><h2 id="2-Volume-amp-Pullbacks"><a href="#2-Volume-amp-Pullbacks" class="headerlink" title="2. Volume &amp; Pullbacks:"></a>2. Volume &amp; Pullbacks:</h2><p>A healthy pullback in an uptrend typically has lower volume than the preceding upward move. This indicates that there’s no significant selling pressure.<br>If a pullback occurs on high volume, it may signal that the trend could be reversing.</p><h2 id="3-Volume-Climax"><a href="#3-Volume-Climax" class="headerlink" title="3. Volume Climax:"></a>3. Volume Climax:</h2><p>Sometimes, after an extended move, you might notice a day with exceptionally high volume that ends up being the top or bottom of a move. This is often a sign of a climax or exhaustion from buyers/sellers.<br>This can be a sign of a potential trend reversal.</p><h2 id="4-Price-Action-Confirmation"><a href="#4-Price-Action-Confirmation" class="headerlink" title="4. Price Action Confirmation:"></a>4. Price Action Confirmation:</h2><p>Look for price action patterns like doji, hammers, engulfing patterns, etc., to confirm your read on volume. For instance, a hammer candlestick on high volume at a key support level may indicate a strong buying interest.</p><h2 id="5-Volume-Averages"><a href="#5-Volume-Averages" class="headerlink" title="5. Volume Averages:"></a>5. Volume Averages:</h2><p>Using a volume moving average (like a 50-day or 20-day moving average) can help smooth out daily volume and make it easier to spot anomalies or significant increases in trading activity.</p><h2 id="6-On-Balance-Volume-OBV"><a href="#6-On-Balance-Volume-OBV" class="headerlink" title="6. On Balance Volume (OBV):"></a>6. On Balance Volume (OBV):</h2><p>OBV is a technical indicator that takes into account both price and volume. It accumulates a total volume figure to show the flow of volume in and out of a stock. If OBV is rising, it suggests that volume on up days is outpacing volume on down days.</p><h2 id="7-Volume-by-Price"><a href="#7-Volume-by-Price" class="headerlink" title="7. Volume by Price:"></a>7. Volume by Price:</h2><p>This is a histogram shown on the y-axis (price axis) that displays the amount of volume at different price levels. It can help traders identify levels where the stock has found a lot of support or resistance in the past.</p><h2 id="8-Look-for-Divergences"><a href="#8-Look-for-Divergences" class="headerlink" title="8. Look for Divergences:"></a>8. Look for Divergences:</h2><p>If you see the stock making new highs, but volume is declining, it may indicate a lack of conviction in the new highs, suggesting a potential reversal.<br>Conversely, if a stock is making new lows but on decreasing volume, it might indicate a potential end to the downward trend.</p><h2 id="9-Use-Volume-with-Other-Indicators"><a href="#9-Use-Volume-with-Other-Indicators" class="headerlink" title="9. Use Volume with Other Indicators:"></a>9. Use Volume with Other Indicators:</h2><p>Volume can be combined with other technical indicators, like MACD, RSI, or Stochastic Oscillators, to find confirmations or divergences that reinforce a trading decision.</p><h2 id="Final-Thoughts"><a href="#Final-Thoughts" class="headerlink" title="Final Thoughts:"></a>Final Thoughts:</h2><p>Volume and price are two of the most fundamental data points in stock trading. However, no single tool or combination guarantees success. It’s essential to backtest any strategy and practice in a demo or paper trading environment. Risk management and a good understanding of market psychology are also crucial components of successful intraday trading.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> stock </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Jupyter AI, Integrating AI with Flexibility, but need to set up model api keys first</title>
      <link href="2023/08/08/set-up-llm-model-api-keys-before-using-jupyter-ai/"/>
      <url>2023/08/08/set-up-llm-model-api-keys-before-using-jupyter-ai/</url>
      
        <content type="html"><![CDATA[<p>Jupyter AI now introduces an integration feature with numerous AI model providers. But bear in mind, it’s not free. To get the best out of Jupyter AI, you’ll need to link it up with your desired model provider’s API key or credentials. Below, I’ve summarized the core information for you:</p><p>Supported AI Model Providers:</p><ul><li>AI21:</li></ul><p>Package: ai21<br>API Key Environment Variable: AI21_API_KEY</p><ul><li>Anthropic:</li></ul><p>Package: anthropic<br>API Key Environment Variable: ANTHROPIC_API_KEY</p><ul><li>Bedrock (Amazon):</li></ul><p>Package: boto3<br>For access and authentication, visit the Amazon Bedrock Homepage.</p><ul><li>Cohere:</li></ul><p>Package: cohere<br>API Key Environment Variable: COHERE_API_KEY</p><ul><li>Hugging Face Hub:</li></ul><p>Packages: huggingface_hub, ipywidgets, pillow (the latter is essential for text-to-image models).<br>API Key Environment Variable: HUGGINGFACEHUB_API_TOKEN<br>Discover an array of models at Hugging Face’s Models.</p><ul><li>OpenAI:</li></ul><p>Package: openai<br>API Key Environment Variable: OPENAI_API_KEY<br>For OpenAI chat models, the package remains openai with the same API key.</p><ul><li>SageMaker:</li></ul><p>Package: boto3<br>Learn about deploying your model and creating endpoint names in the SageMaker documentation.<br>Authentication is through boto3.</p><p>More instructions about model provides are <a href="https://jupyter-ai.readthedocs.io/en/latest/users/index.html#model-providers">here</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jupyter-ai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decluttering Your Matplotlib Plots, A Simple Guide</title>
      <link href="2023/08/06/declutter-matplotlib-plots-reduce-axis-ticks-to-show/"/>
      <url>2023/08/06/declutter-matplotlib-plots-reduce-axis-ticks-to-show/</url>
      
        <content type="html"><![CDATA[<p>When visualizing data with Matplotlib in Python, it’s common to run into the issue of cluttered x-axis or y-axis ticks. Especially for dense time series data, the axis can become a confusing jumble of overlapping labels. The good news? Matplotlib offers easy-to-use tools to help you make your plots clearer and more professional-looking. Today, we’re diving into a simple approach to declutter your x-axis ticks, focusing on time series data.</p><h2 id="Problem-Statement"><a href="#Problem-Statement" class="headerlink" title="Problem Statement"></a>Problem Statement</h2><p>Consider you have time series data, say, stock closing prices sampled every 5 minutes. If you try to plot the closing prices against time directly using Matplotlib, you’ll probably get an x-axis swamped with tick labels, especially for long trading sessions. This makes the plot less readable and, at times, counter-productive.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line"># Extract the time series data</span><br><span class="line">time_series = data[&#x27;Time Series (5min)&#x27;]</span><br><span class="line"></span><br><span class="line"># Extract time and closing prices</span><br><span class="line">times = sorted(list(time_series.keys()))</span><br><span class="line">prices = [float(time_series[t][&#x27;4. close&#x27;]) for t in times]</span><br><span class="line"></span><br><span class="line"># Plot</span><br><span class="line">plt.figure(figsize=(10, 6))</span><br><span class="line">plt.plot(times, prices, &#x27;-o&#x27;, marker=&quot;*&quot;)</span><br><span class="line">plt.xticks(rotation=45)</span><br><span class="line">plt.xlabel(&#x27;Time&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Close Price&#x27;)</span><br><span class="line">plt.title(&#x27;Closing Price vs Time&#x27;)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>If you’ve done something similar, you know that the result can be less than ideal: overlapping x-tick labels.</p><h2 id="The-Solution-Adjusting-X-Ticks"><a href="#The-Solution-Adjusting-X-Ticks" class="headerlink" title="The Solution: Adjusting X-Ticks"></a>The Solution: Adjusting X-Ticks</h2><p>Luckily, Matplotlib provides the tools we need to make our x-axis more comprehensible.</p><h3 id="Step-1-First-import-the-necessary-libraries"><a href="#Step-1-First-import-the-necessary-libraries" class="headerlink" title="Step 1: First, import the necessary libraries:"></a>Step 1: First, import the necessary libraries:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from matplotlib.ticker import MultipleLocator</span><br></pre></td></tr></table></figure><h2 id="Step-2-Determine-the-number-of-ticks-you-want-on-your-x-axis-A-general-guideline-might-be-to-aim-for-5-10-ticks-for-clarity-but-this-can-vary-based-on-the-length-of-your-dataset-and-the-specific-context"><a href="#Step-2-Determine-the-number-of-ticks-you-want-on-your-x-axis-A-general-guideline-might-be-to-aim-for-5-10-ticks-for-clarity-but-this-can-vary-based-on-the-length-of-your-dataset-and-the-specific-context" class="headerlink" title="Step 2: Determine the number of ticks you want on your x-axis. A general guideline might be to aim for 5-10 ticks for clarity, but this can vary based on the length of your dataset and the specific context."></a>Step 2: Determine the number of ticks you want on your x-axis. A general guideline might be to aim for 5-10 ticks for clarity, but this can vary based on the length of your dataset and the specific context.</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">n = len(times) // 10  # Display approximately 10 x-ticks</span><br></pre></td></tr></table></figure><h3 id="Step-3-Set-the-tick-locator-for-the-x-axis-using-the-MultipleLocator"><a href="#Step-3-Set-the-tick-locator-for-the-x-axis-using-the-MultipleLocator" class="headerlink" title="Step 3: Set the tick locator for the x-axis using the MultipleLocator:"></a>Step 3: Set the tick locator for the x-axis using the MultipleLocator:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ax = plt.gca()  # Get the current axes</span><br><span class="line">ax.xaxis.set_major_locator(MultipleLocator(n))</span><br></pre></td></tr></table></figure><p>After these steps, the modified code will look something like:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from matplotlib.ticker import MultipleLocator</span><br><span class="line"></span><br><span class="line"># Extract the time series data</span><br><span class="line">time_series = data[&#x27;Time Series (5min)&#x27;]</span><br><span class="line"></span><br><span class="line"># Extract time and closing prices</span><br><span class="line">times = sorted(list(time_series.keys()))</span><br><span class="line">prices = [float(time_series[t][&#x27;4. close&#x27;]) for t in times]</span><br><span class="line"></span><br><span class="line"># Plot</span><br><span class="line">plt.figure(figsize=(10, 6))</span><br><span class="line">plt.plot(times, prices, &#x27;-o&#x27;, marker=&quot;*&quot;)</span><br><span class="line">plt.xticks(rotation=45)</span><br><span class="line">plt.xlabel(&#x27;Time&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Close Price&#x27;)</span><br><span class="line">plt.title(&#x27;Closing Price vs Time&#x27;)</span><br><span class="line"></span><br><span class="line"># Adjust the x-ticks to reduce clutter</span><br><span class="line">ax = plt.gca()</span><br><span class="line">n = len(times) // 10</span><br><span class="line">ax.xaxis.set_major_locator(MultipleLocator(n))</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Now the plot will looks something like this, much better:<br><img src="/content/images/2023-08-06-01.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> matplotlib </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>QR Codes, The Little Squares that Pack a Punch</title>
      <link href="2023/08/05/how-does-qr-codes-work/"/>
      <url>2023/08/05/how-does-qr-codes-work/</url>
      
        <content type="html"><![CDATA[<p>Imagine you’re flipping through a photo album and, instead of just pictures, some photos magically play videos or transport you to a website. Sounds like science fiction, right? That’s essentially what a QR code does, but in real life!</p><h2 id="What-is-a-QR-Code"><a href="#What-is-a-QR-Code" class="headerlink" title="What is a QR Code?"></a>What is a QR Code?</h2><p>QR stands for “Quick Response.” It’s a square barcode, but instead of the simple lines you see on products at the store, it has a bunch of jumbled patterns. This quirky design allows it to store a lot more information than a regular barcode.</p><h2 id="How-Does-It-Work"><a href="#How-Does-It-Work" class="headerlink" title="How Does It Work?"></a>How Does It Work?</h2><p>Imagine each QR code as a puzzle. Every QR code has a unique design or pattern. When you scan it with your smartphone or a scanner, it’s like solving that puzzle, and the solution reveals a hidden message or action. This could be a link to a website, a coupon, a video, or even a Wi-Fi password.</p><h2 id="Let’s-Dive-a-Bit-Deeper"><a href="#Let’s-Dive-a-Bit-Deeper" class="headerlink" title="Let’s Dive a Bit Deeper!"></a>Let’s Dive a Bit Deeper!</h2><p><code>The Makeup</code>: A QR code is divided into tiny squares, and these squares can either be black or white. The combination of these squares creates the “message” of the QR code.<br><code>The Corners</code>: You might notice square patterns in three of the corners. These are the QR code’s guides. They help your phone know it’s looking at a QR code and understand its orientation.<br><code>Scanning</code>: Your smartphone’s camera acts like an interpreter. When you open a QR scanning app or your camera app (many phones have this feature built-in), and you point it at the code, it reads the black and white squares and translates them into something meaningful like a web address.</p><h2 id="Why-Do-People-Use-Them"><a href="#Why-Do-People-Use-Them" class="headerlink" title="Why Do People Use Them?"></a>Why Do People Use Them?</h2><p>QR codes bridge the gap between the physical and digital world. Here are a few cool things people do with them:</p><p><code>Menus at Restaurants</code>: Instead of a physical menu, just scan a code and see the menu on your phone.<br><code>Business Cards</code>: Instead of typing out contact details, scan a code, and instantly save them.<br><code>Tickets and Boarding Passes</code>: Get into events or board your flight with a scan.<br><code>Promotions and Deals</code>: Scan to get a discount or special offer!</p><h2 id="In-Conclusion"><a href="#In-Conclusion" class="headerlink" title="In Conclusion"></a>In Conclusion</h2><p>Think of QR codes as magical portals that connect the real world to the digital one. With just a scan, they offer a world of information and convenience right at your fingertips!</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> QR codes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The 800 Club, how much people are in America&#39;s High Credit Score Elite group</title>
      <link href="2023/08/04/my-credit-score-is-over-800-how-rare-it-is/"/>
      <url>2023/08/04/my-credit-score-is-over-800-how-rare-it-is/</url>
      
        <content type="html"><![CDATA[<p>A credit score greater than 800 is considered excellent, and it is indeed achievable. In fact, a significant number of people in the U.S. have credit scores within this range.</p><p>A credit score is a numerical representation of a person’s creditworthiness, which lenders use to evaluate the likelihood that the person will repay debts. The most widely used credit scores are FICO scores, which range from 300 to 850.</p><p>Here’s how FICO scores are typically interpreted:</p><ul><li>300-579: Very poor</li><li>580-669: Fair</li><li>670-739: Good</li><li>740-799: Very good</li><li>800-850: Exceptional</li></ul><p>A score above 800 demonstrates to lenders that a person is an exceptional borrower, likely to repay loans on time. This is achieved by maintaining a history of responsible credit use, including paying bills on time, keeping credit utilization low, and only opening new credit accounts when necessary.</p><p>While it may seem like a score above 800 is rare, many Americans do indeed have such scores. According to a 2019 report by FICO, <code>approximately 21% of the U.S. population had a FICO score over 800</code>.</p><p>These individuals usually have a long credit history with a mix of account types, such as credit cards, student loans, mortgages, etc. They also have few, if any, negative marks on their credit reports.</p><p>It’s worth noting that achieving a credit score above 800 is commendable, but scores above 740 are generally enough to qualify a person for the best interest rates and terms on credit products. However, a score above 800 can provide a buffer, in case of future financial mishaps that might drop the score a bit.</p><p>It’s also important to note that while having a high credit score is beneficial for securing loans and getting good interest rates, it is only one factor that lenders consider. Your income, job stability, and the amount of debt you already have are also important.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> credit score </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Task Ordering with Python&#39;s Concurrent.Futures</title>
      <link href="2023/08/02/how-to-order-results-from-multi-threading/"/>
      <url>2023/08/02/how-to-order-results-from-multi-threading/</url>
      
        <content type="html"><![CDATA[<p>One of the most powerful features of modern programming languages is their ability to manage multiple threads or processes simultaneously. Python is no exception, thanks to its <code>concurrent.futures</code> library. Specifically, through its <code>ThreadPoolExecutor</code> or <code>ProcessPoolExecutor</code>, Python can handle multiple tasks concurrently, significantly improving the efficiency of your code. </p><p>A common question among Python developers using <code>concurrent.futures</code> is whether the results of these concurrent tasks come back in the same order as initially called. The answer is a conditional yes. </p><p>When using <code>ThreadPoolExecutor</code> or <code>ProcessPoolExecutor</code>, if you are utilizing the <code>map()</code> function, Python ensures that the results come back in the order of the original calls, regardless of the sequence in which the tasks were completed. This ordering is extremely helpful when the sequence of tasks matters in the context of your program.</p><p>Here’s a simple example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from concurrent.futures import ThreadPoolExecutor </span><br><span class="line"></span><br><span class="line">def task(n):</span><br><span class="line">    return n * n</span><br><span class="line">    </span><br><span class="line">with ThreadPoolExecutor() as executor: </span><br><span class="line">    results = executor.map(task, (1, 2, 3, 4, 5)) </span><br><span class="line"></span><br><span class="line">for result in results: </span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>In this code, even if task 5 completes before task 2, the results will still be printed in the order of 1, 2, 3, 4, 5.</p><p>However, there’s a caveat. If you’re submitting tasks using a different method - <code>submit()</code> - and waiting for them using <code>as_completed()</code>, the results might return in a different order than they were submitted. This is because <code>as_completed()</code> yields tasks as and when they complete.</p><p>In many scenarios, this completion-order returns could be advantageous, especially when each task is independent, and you want to handle the completed tasks while other tasks are still running. But, it’s essential to be aware that you won’t be maintaining the same order as the tasks were initially submitted.</p><p>In conclusion, Python’s <code>concurrent.futures</code> offers different techniques for multithreading or multiprocessing. Depending on your requirements, you can choose to maintain tasks order by using <code>map()</code> or opt to handle tasks as they finish by using <code>submit()</code> with <code>as_completed()</code>.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> concurrent </tag>
            
            <tag> threadpool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Introduction to Code Profiling in Python with `time`, `timeit`</title>
      <link href="2023/08/02/how-to-time-the-code-in-python/"/>
      <url>2023/08/02/how-to-time-the-code-in-python/</url>
      
        <content type="html"><![CDATA[<p>Determining where your code is spending its time is an important step in making your program more efficient. In Python, you can do this using the built-in <code>time</code> and <code>timeit</code> modules. So let’s have a look at how you can use these packages to pinpoint the bottlenecks in your code.</p><h2 id="The-time-module"><a href="#The-time-module" class="headerlink" title="The time module"></a>The <code>time</code> module</h2><p>The <code>time</code> module in Python is base on the Unix timestamp, which represents the number of seconds since the start of January 1, 1970 (also known as the epoch). An example of its usage:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_function</span>():</span></span><br><span class="line">    start = time.time()</span><br><span class="line">     <span class="comment"># Steps in your function</span></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Time spent:&#x27;</span>, end - start)</span><br></pre></td></tr></table></figure><p>In this example, <code>time.time()</code> returns the current system time, and then the difference between the end and start times gives the total time spent for executing the function.</p><h2 id="The-timeit-module"><a href="#The-timeit-module" class="headerlink" title="The timeit module"></a>The <code>timeit</code> module</h2><p>The <code>timeit</code> module provides a simple interface for timing small bits of Python code. It has both a command-line interface and a callable one. It avoids a number of common traps for measuring execution times. Here’s how you can use it</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line">start = timeit.default_timer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Your code here</span></span><br><span class="line"></span><br><span class="line">end = timeit.default_timer()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Time taken:&#x27;</span>, end - start)</span><br></pre></td></tr></table></figure><p>In this snippet, <code>timeit.default_timer()</code> is used which is a higher precision clock and gives better measure of time intervals in the code.</p><p>You can also use <code>timeit</code> function to run a particular function multiple times.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> timeit</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">function</span>():</span></span><br><span class="line">    <span class="comment"># Your code here</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(timeit.timeit(function, number=<span class="number">1000</span>))</span><br></pre></td></tr></table></figure><p>In this style, <code>timeit()</code> runs the <code>function</code> 1000 times and then gives the total time taken for those 1000 executions.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p><code>time</code> and <code>timeit</code> are powerful tools for basic timing and profiling in Python. But it’s always important to remember that many factors can influence timing results, including other processes running on your system, disk I/O, and more. For more robust profiling, you might want to consider using more advanced tools like <code>profile</code> or <code>cProfile</code>, which can give detailed reports about time spent in each function call, or other third-party libraries like <code>line_profiler</code> for line-by-line analyses</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Credit Card Interest, A Fintech Perspective</title>
      <link href="2023/08/01/understanding-credit-card-interest/"/>
      <url>2023/08/01/understanding-credit-card-interest/</url>
      
        <content type="html"><![CDATA[<p>Hello, everyone. As someone deeply involved in the fintech industry, I often find myself immersed in discussions about personal finance and smart money management. One question that pops up consistently is about how credit card interest works, especially when payments are made after the due date. Today, I’ll clarify some of these points, speaking directly from the perspective of a fintech professional.</p><p>One fundamental concept to understand is the ‘grace period’. This is the timeframe between when your billing cycle ends and when your payment is due, usually between 21 and 25 days. If you pay your balance in full during this period, you can avoid interest charges altogether. This grace period offers an interest-free loan of sorts, making credit cards a financially smart tool when used responsibly.</p><p>However, if you do not pay off your balance by the due date, things start to change. The remaining balance starts accruing interest, calculated based on your card’s annual percentage rate (APR). Typically, this interest is calculated daily from the day after the due date.</p><p>What is important—and often misunderstood—is the impact on your grace period if you carry a balance past the due date. You essentially lose this grace period, meaning that any new purchases start accruing interest immediately. There’s no interest-free window for these new charges until you’ve paid off your full balance by the due date for one or more consecutive billing cycles, depending on your issuer’s policy.</p><p>It’s important to remember that late payments may result in additional late fees and even an increase in your APR, not to mention the potential negative impact on your credit score.</p><p>The digital age, spearheaded by fintech innovations, has provided us more control over our financial lives than ever before. Mobile apps and online portals make it easy to monitor balances, make payments, and track expenses. It’s important that we leverage these tools to stay informed and make timely payments.</p><p>As with all aspects of personal finance, the key is awareness and understanding. Take the time to read your credit card agreement or reach out to your card issuer to clarify any doubts. In fintech, our goal is to empower you with the tools and knowledge to make the best financial decisions, and understanding credit card interest is a crucial part of this.</p><p>Remember, credit cards can be a powerful financial tool, offering convenience, rewards, and credit-building capabilities. But it’s essential to use them responsibly to avoid unnecessary costs. Stay informed, stay diligent, and manage your finances wisely.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> credit card </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Home Equity Line of Credit (HELOC), A Complete Breakdown</title>
      <link href="2023/07/30/data-scientist-help-you-to-understand-home-equity-line-of-credit-HELOC/"/>
      <url>2023/07/30/data-scientist-help-you-to-understand-home-equity-line-of-credit-HELOC/</url>
      
        <content type="html"><![CDATA[<p>It is usualy not clear to understand how the home equity line of credit work in details, here we answer the core questions by collecting most asked questions using data science techniques.</p><p>Understanding the ins and outs of a Home Equity Line of Credit (HELOC) can seem like a daunting task, but when broken down into its fundamental elements, it’s not as complicated as it might seem. Let’s discuss these points clearly and comprehensively, using a simple example.</p><h2 id="What-is-a-HELOC"><a href="#What-is-a-HELOC" class="headerlink" title="What is a HELOC?"></a>What is a HELOC?</h2><p>A HELOC is a line of credit secured by your home. It offers a certain credit limit that you can draw from during a specified period known as the draw period. After the draw period, the loan enters the repayment phase. A notable aspect of a HELOC is that it offers flexibility, allowing you to borrow only what you need, when you need it, within the draw period.</p><h2 id="The-Draw-Period-and-Repayment-Period"><a href="#The-Draw-Period-and-Repayment-Period" class="headerlink" title="The Draw Period and Repayment Period"></a>The Draw Period and Repayment Period</h2><p>During the draw period, you’re allowed to borrow any amount up to your approved credit limit. You’re not required to draw any specific amount each month, but rather as needed.</p><p>One of the common questions is whether you need to make repayments during the draw period. The answer is yes, but typically, only the interest that accrues on the borrowed amount needs to be paid back each month during the draw period.</p><p>Let’s illustrate this with an example. Suppose you borrowed $10,000 during the draw period, and the annual interest rate is 5%. The total annual interest that you would have to pay would be $500 (5% of $10,000). This translates to roughly $41.67 each month ($500 divided by 12 months).</p><p>Once the draw period ends, you enter the repayment period. At this stage, any outstanding balance is converted into a traditional term loan, where you’ll have to pay back both principal and interest according to a pre-determined schedule. It’s also worth noting that, during the repayment period, you’re not allowed to borrow any more money.</p><h2 id="Can-You-Pay-More-Than-The-Required-Interest"><a href="#Can-You-Pay-More-Than-The-Required-Interest" class="headerlink" title="Can You Pay More Than The Required Interest?"></a>Can You Pay More Than The Required Interest?</h2><p>Absolutely! If you have the financial capability, it’s advisable to pay more than the required monthly interest during the draw period. Any additional amount you pay will go towards reducing the principal amount, which, in turn, reduces the amount of interest you’ll have to pay in the future.</p><h2 id="Is-There-Any-Penalty-for-Repaying-All-The-Money-At-Once"><a href="#Is-There-Any-Penalty-for-Repaying-All-The-Money-At-Once" class="headerlink" title="Is There Any Penalty for Repaying All The Money At Once?"></a>Is There Any Penalty for Repaying All The Money At Once?</h2><p>This entirely depends on your lender and the terms of your HELOC agreement. Some lenders charge prepayment penalties if you pay off your loan early. Before you decide to pay off your HELOC all at once, make sure you understand all the terms of your loan and consult with your lender or financial advisor.</p><h2 id="Wrap-Up"><a href="#Wrap-Up" class="headerlink" title="Wrap Up"></a>Wrap Up</h2><p>HELOCs offer a flexible way to tap into the value of your home for expenses such as home improvements, education, or even debt consolidation. However, it’s crucial to understand the rules that govern the draw period, the repayment period, and the repayment terms. Armed with this knowledge, you can make the most out of your HELOC while minimizing any potential costs.</p>]]></content>
      
      
      <categories>
          
          <category> fintech </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HELOC </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Rise of the AI Engineer, Filling the Gap in the AI Revolution</title>
      <link href="2023/07/29/rise-of-ai-engineering/"/>
      <url>2023/07/29/rise-of-ai-engineering/</url>
      
        <content type="html"><![CDATA[<p>In a world with around 5000 Language Learning Model (LLM) researchers but approximately 50 million software engineers, supply constraints dictate an impending surge in a new class of professionals—AI Engineers. Their rise is not merely a prediction; it’s an inevitable response to the shifting dynamics of the tech world. The emergence of AI Engineers as the new vanguard in tech space is a compelling evolution that transcends traditional software development practices, marking the onset of a thrilling era in AI application.</p><p>The landscape of technology roles is continually evolving, and the AI Engineer role is rapidly gaining traction. Positioned uniquely at the crossroads of research, application, and development, AI Engineers represent a seismic shift in how we approach artificial intelligence (AI) and software engineering.</p><p>Unlike the traditional roles of Machine Learning (ML) Research Scientist, Machine Learning Engineer, and Software Engineer, AI Engineers integrate an understanding of AI models with robust software engineering principles. The goal? To create scalable, user-friendly AI solutions and services.</p><p>Machine Learning Research Scientists primarily pioneer AI’s future by discovering novel algorithms or techniques. Their work, though essential, often finds place in academic journals and might not have immediate practical implications for software products.</p><p>On the contrary, Machine Learning Engineers translate these new techniques into viable applications for software products, favoring practical application over theory. Software Engineers, meanwhile, employ a wide range of tools and technologies to construct and maintain software systems. While they do integrate AI or ML components into their work, they don’t necessarily delve deep into these technologies.</p><p>This is where the AI Engineer enters the scene, bringing an understanding of AI models akin to Machine Learning Engineers, but with a sharper focus on using pre-trained AI models such as GPT-4 or other Foundation Models. These professionals assess different AI models, leverage tools like LangChain or Pinecone, and translate cutting-edge research into practical AI products. Like Software Engineers, AI Engineers understand and apply principles of robust, scalable, and maintainable system design.</p><p>The importance of AI Engineers is magnified by the advent of Foundation Models. While AI Researchers create these models, it’s the AI Engineers who exploit their potential through extensive interaction and find innovative applications in underexplored domains.</p><p>The year 2023 is witnessing the increasing relevance of human-written code in harnessing and augmenting the power of LLMs like GPT-4. As human engineers become more proficient in AI, AI is increasingly taking on engineering tasks, blurring the boundaries between them.</p><p>With startups securing significant funds to own their hardware, the role of AI Engineers in utilizing models, rather than training them, will grow. The adoption of an ‘agile’ approach to AI, enabling AI Engineers to build and validate AI products rapidly and cost-effectively, is gaining momentum.</p><p>A significant transition is the shift of AI tools from Python to JavaScript, which broadens the user base and presents new opportunities. This change could lead to a divergence in the discipline, with AI Engineers developing a new suite of products using a distinct toolkit.</p><p>As the supply and demand dynamics in the AI landscape continue to evolve, AI Engineers are increasingly becoming a critical link between AI research and practical application. Their role is a testament to the ongoing AI revolution and the ever-increasing relevance of AI in our daily lives. Their rise underscores a significant shift in the tech industry, marking the start of an exciting new chapter in technology history.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> prompt engineering </tag>
            
            <tag> ai engineer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to check the python version used in jupyter notebook or databricks notebook</title>
      <link href="2023/07/27/check-python-version-in-a-notebook-jupyter-or-databricks/"/>
      <url>2023/07/27/check-python-version-in-a-notebook-jupyter-or-databricks/</url>
      
        <content type="html"><![CDATA[<p>Here are two simple commands to use in the cell to check the current python version.</p><h2 id="method-1"><a href="#method-1" class="headerlink" title="method 1"></a>method 1</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!python --version</span><br></pre></td></tr></table></figure><h2 id="method-2"><a href="#method-2" class="headerlink" title="method 2"></a>method 2</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from platform import python_version</span><br><span class="line">print(python_version())</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Advanced GitHub and Pip Usage, Creating Releases, Managing Versions and Access Tokens</title>
      <link href="2023/07/27/how-to-create-release-with-github-repo-and-pip-install-the-repo/"/>
      <url>2023/07/27/how-to-create-release-with-github-repo-and-pip-install-the-repo/</url>
      
        <content type="html"><![CDATA[<p>Version control is pivotal to any development process. It enables developers to track and manage changes made in projects, ensuring efficient collaboration and progress. In the world of version control systems, one of the most commonly used is Git, with GitHub providing a fantastic platform for managing Git repositories.</p><p>A significant feature provided by GitHub is ‘Releases’, enabling developers to manage different versions of their software and package them conveniently for users. Along with these, pip plays a crucial role as a package manager, allowing the installation of specific releases conveniently.</p><p>In this expanded edition of our blog post, we will deep dive into the process of creating releases on GitHub, handling versions in both private and public repositories, and authenticating with personal access tokens.</p><h2 id="Creating-a-Release-on-GitHub"><a href="#Creating-a-Release-on-GitHub" class="headerlink" title="Creating a Release on GitHub"></a>Creating a Release on GitHub</h2><p>Let’s cover the steps to add a release to your GitHub repository:</p><ol><li>Navigate to your GitHub repository’s main page.</li><li>Under your repository name, click on <strong>Releases</strong>.</li><li>On the new page that opens, click on <strong>Draft a new release</strong>.</li><li>Fill in the <em>Tag version</em> field with the version number (ex. v1.0).</li><li>Optionally, provide a release title and describe the changes in this version in the description box.</li><li>Once you’ve filled in the necessary information, click <strong>Publish release</strong>.</li></ol><p>Voila! You have successfully created a new version of your repository.</p><h2 id="Authenticating-with-Personal-Access-Tokens-in-pip"><a href="#Authenticating-with-Personal-Access-Tokens-in-pip" class="headerlink" title="Authenticating with Personal Access Tokens in pip"></a>Authenticating with Personal Access Tokens in pip</h2><p>Now that we understand creating GitHub releases, let’s delve into installing a specific release using pip with authentication when dealing with private repositories.</p><p>Assuming you need to access a private repository in your organization under your personal GitHub account, you can generate a personal access token and use it in pip’s install command:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install git+https://user_name:&lt;access_token&gt;@github.com/company_username/repo.git@version</span><br></pre></td></tr></table></figure><p>In the above command, replace <code>user_name</code> with your personal GitHub username, <code>&lt;access_token&gt;</code> with your GitHub access token, <code>company_username</code> with the organization’s GitHub username, and <code>version</code> with the version tag you wish to install.</p><h2 id="Including-Private-Repos-in-requirements-txt"><a href="#Including-Private-Repos-in-requirements-txt" class="headerlink" title="Including Private Repos in requirements.txt"></a>Including Private Repos in requirements.txt</h2><p>It’s common practice to include dependencies in a <code>requirements.txt</code> file in Python projects. If your project includes a dependency hosted in a private repository, you can include the repository in your <code>requirements.txt</code> file as follows:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git+https://user_name:&lt;access_token&gt;@github.com/company_username/repo.git@version</span><br></pre></td></tr></table></figure><p>In the command above, <code>user_name</code> is your GitHub username, and <code>&lt;access_token&gt;</code> is the token generated from your GitHub account, <code>company_username</code> is the GitHub username of the organization, and <code>version</code> is the release tag.</p><p>As these tokens are highly sensitive, handling them securely is crucial. Avoid hardcoding your access tokens in your code or files and consider using environment variables or protected configuration files for better security.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Parallel Processing in Python with concurrent.futures and ThreadPoolExecutor</title>
      <link href="2023/07/24/parallel-processing-using-cocurrent-futures-threadpool-in-python/"/>
      <url>2023/07/24/parallel-processing-using-cocurrent-futures-threadpool-in-python/</url>
      
        <content type="html"><![CDATA[<p>When it comes to improving the performance of your Python code, parallel processing is a powerful tool that can significantly reduce execution times for tasks that can be broken up into independent pieces. One module that makes it easy to implement parallel processing is concurrent.futures. In this blog, we will explore how to use this module with a particular focus on the ThreadPoolExecutor class.</p><h2 id="What-is-concurrent-futures"><a href="#What-is-concurrent-futures" class="headerlink" title="What is concurrent.futures?"></a>What is concurrent.futures?</h2><p>Introduced in Python 3.2, concurrent.futures is a high-level interface for asynchronously executing Preemptive multitasking that can take advantage of multi-core processors to maximize performance. It provides several classes for executing tasks, including ThreadPoolExecutor for thread-based parallelism.</p><h2 id="Code-Example-Using-ThreadPoolExecutor"><a href="#Code-Example-Using-ThreadPoolExecutor" class="headerlink" title="Code Example: Using ThreadPoolExecutor"></a><strong>Code Example: Using ThreadPoolExecutor</strong></h2><p>Here’s a basic example of how to use ThreadPoolExecutor:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> concurrent.futures</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">some_function</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(f’start function &#123;n&#125;\n’)</span><br><span class="line">    time.sleep(n)</span><br><span class="line">    <span class="built_in">print</span>(f’end function &#123;n&#125;\n’)</span><br><span class="line">    <span class="keyword">return</span> n</span><br><span class="line"><span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">3</span>) <span class="keyword">as</span> executor:</span><br><span class="line">    futures = [executor.submit(some_function, n) <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>)]</span><br><span class="line">    <span class="keyword">for</span> future <span class="keyword">in</span> concurrent.futures.as_completed(futures):</span><br><span class="line">        result = future.result()</span><br><span class="line">        <span class="built_in">print</span>(f’Thread that finished: &#123;result&#125; \n’)</span><br></pre></td></tr></table></figure><p>In this code, we define a task some_function that simulates a long-running operation. The executor.submit method schedules tasks to be executed and returns a Future object. We then use as_completed to yield these Future objects as they complete.</p><h2 id="Collecting-Results-with-executor-map"><a href="#Collecting-Results-with-executor-map" class="headerlink" title="Collecting Results with executor.map()"></a><strong>Collecting Results with executor.map()</strong></h2><p>If you want to collect all results and return them together, executor.map is a timesaver.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor(max_workers=<span class="number">3</span>) <span class="keyword">as</span> executor:</span><br><span class="line">        results = executor.<span class="built_in">map</span>(some_function, <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">4</span>))</span><br><span class="line">        <span class="built_in">print</span>(f’The results: &#123;<span class="built_in">list</span>(results)&#125;’)</span><br><span class="line"><span class="keyword">if</span> __name__ == “__main__“:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>In this example, executor.map simplifies aggregating the results. It runs some_function for each input and collects the results in the order they were called.</p><h2 id="Using-executor-map-with-Multiple-Parameters"><a href="#Using-executor-map-with-Multiple-Parameters" class="headerlink" title="Using executor.map() with Multiple Parameters"></a><strong>Using executor.map() with Multiple Parameters</strong></h2><p>If your function takes more than one argument, the map function can still be used by passing additional iterables.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiply</span>(<span class="params">x, y</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * y</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">with</span> concurrent.futures.ThreadPoolExecutor() <span class="keyword">as</span> executor:</span><br><span class="line">        xs = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">        ys = [<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>, <span class="number">50</span>]</span><br><span class="line">        results = executor.<span class="built_in">map</span>(multiply, xs, ys)</span><br><span class="line">        <span class="built_in">print</span>(<span class="built_in">list</span>(results))</span><br><span class="line"><span class="keyword">if</span> __name__ == “__main__“:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a><strong>Conclusion</strong></h2><p>Python’s concurrent.futures module, and the ThreadPoolExecutor class in particular, provide powerful tools for implementing parallel processing in your code. By understanding and utilizing these tools, you can dramatically improve the performance of your Python applications.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> concurrent </tag>
            
            <tag> threadpool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Python’s __init__ Method and Interpreting Errors with Empty Initialization</title>
      <link href="2023/07/24/pyton-class-init-method-indendation-error-fix/"/>
      <url>2023/07/24/pyton-class-init-method-indendation-error-fix/</url>
      
        <content type="html"><![CDATA[<p>Python’s Object-Oriented Programming (OOP) capabilities mainly involve classes and objects. A crucial part of defining a Python class is initializing the class attributes and methods. We use the <code>__init__</code> method of a class for that.<br>The <code>__init__</code> special method is executed whenever we create a new instance (object) of a class. This method allows us to initialize the specific attributes of our class. However, what happens when the <code>__init__</code> method doesn’t require any initialization and thereby is left empty?<br>Empty <code>__init__</code> methods in Python can be addressed using the keyword <code>pass</code> or simply returning <code>None</code>.<br>Example using <code>pass</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span>  <span class="comment"># The pass keyword performs no operation. It’s a placeholder.</span></span><br></pre></td></tr></table></figure><p>Example using <code>None</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>  <span class="comment"># Explicitly returns None.</span></span><br></pre></td></tr></table></figure><p>In Python, the <code>pass</code> statement is used when your code requires no operation but the syntax requires a statement. This is common in places where your code will eventually reside but hasn’t been written yet.<br>However, returning <code>None</code> explicitly states that the function doesn’t return anything. In Python, if a function doesn’t have a return statement, it implicitly returns <code>None</code>.<br>So, if you’re wondering whether to use <code>pass</code> or <code>None</code> in an <code>__init__</code> method, bear in mind that <code>__init__()</code> is meant to initialize instances of a class and therefore doesn’t usually necessitate a <code>return</code> statement. Thus, using just <code>pass</code> would suffice.<br>Now, what happens when you define an <code>__init__</code> method and leave it without an indented statement? Python throws an IndentationError. Here’s an example of the error:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"><span class="comment"># This raises IndentationError: expected an indented block.</span></span><br></pre></td></tr></table></figure><p>To prevent this, you should at least use the <code>pass</code> statement inside the <code>__init__</code> method.</p><p>```python<br>class MyClass:<br>    def <strong>init</strong>(self):<br>        pass</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Setting Up GitHub Pull Request Notifications in Your Enterprise Slack Channel</title>
      <link href="2023/07/22/send-github-pulls-results-to-slack-channel/"/>
      <url>2023/07/22/send-github-pulls-results-to-slack-channel/</url>
      
        <content type="html"><![CDATA[<p>In the modern collaborative work environment, GitHub and Slack are indispensable tools used by many teams. With a straightforward integration, you can set up GitHub to send pull request notifications directly to your Slack channels. Let’s walk you through the entire process, starting with the installation of the GitHub app in Slack.</p><ol><li><p><strong>Add GitHub App to Slack</strong></p><p>If GitHub is not yet installed in your Slack, start by adding the GitHub app. You can do this by visiting the ‘Apps’ section in Slack, or by visiting the GitHub for Slack page and clicking on ‘Add to Slack’. You will need to select the workspace where you want to install the GitHub app.</p></li><li><p><strong>Authenticate Your GitHub Account with Slack</strong></p><p>After installing the app, the next step is to link your GitHub account with Slack. This can be achieved by typing <code>/github signin</code> in your desired Slack workspace. You will be directed via a direct message to GitHub’s OAuth to authenticate.</p></li><li><p><strong>Invite GitHub App to Your Slack Channel</strong></p><p>Now that your app is installed and authenticated, it’s time to invite the GitHub app into your specific Slack channel. This can be accomplished by typing <code>/invite @Github</code> into your desired channel.</p></li><li><p><strong>Set Up Pull Request Notifications</strong></p><p>Finally, set up notifications for pull requests from the specific repository you’re interested in. You can subscribe to these notifications by typing <code>/github subscribe myorg/myrepo pulls</code> into your Slack channel. Remember to replace ‘myorg’ and ‘myrepo’ with your actual GitHub organization and repository names.</p></li></ol><p>And voilà, you are all set! You will now start receiving notifications on your Slack whenever there are updates on your subscribed GitHub repository’s pull requests.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
            <tag> slack </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimizing Product Ranking through Machine Learning, From Data Collection to Model Building</title>
      <link href="2023/07/22/simple-way-to-optimize-products-ranking-using-machine-learning/"/>
      <url>2023/07/22/simple-way-to-optimize-products-ranking-using-machine-learning/</url>
      
        <content type="html"><![CDATA[<p>In today’s digital age, product rankings on your webpage can significantly impact your business’s success. Machine Learning (ML) offers a potent tool for optimizing this ranking, learning from past user interactions to forecast and improve future user engagement. This post will guide you through a comprehensive process: from collecting Click-Through Rate (CTR) data to building a model that optimizes product rankings.</p><h2 id="1-Setting-up-the-Experiment"><a href="#1-Setting-up-the-Experiment" class="headerlink" title="1. Setting up the Experiment"></a>1. Setting up the Experiment</h2><p>The first step is to collect relevant CTR data. You can do this by setting up an experiment on your webpage. In our scenario, every time a user visits the site, we will randomly display three out of the total ten products. Every instance of a product being shown and whether it was clicked or not is then recorded. This experimental setup allows us to capture unbiased user interaction data with the products, which becomes the foundation for our ML model.</p><h2 id="2-Data-Preparation"><a href="#2-Data-Preparation" class="headerlink" title="2. Data Preparation"></a>2. Data Preparation</h2><p>Once the CTR data is collected, organize it in a structured format. Each row should represent an instance of a product being shown, whether it was clicked or not, along with any additional context information, such as the timestamp. Here’s a sample layout for your data:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">| User ID | Product ID | Shown_At | Clicked |</span><br><span class="line">|---------|------------|----------|---------|</span><br><span class="line">| 1       | 3          | Time1    | 0       |</span><br><span class="line">| 1       | 7          | Time2    | 1       |</span><br><span class="line">| 2       | 10         | Time3    | 0       |</span><br><span class="line">|...      |...         |...       |...      |</span><br></pre></td></tr></table></figure><p>Where Clicked is binary: 1 means the product was clicked, and 0 means it wasn’t.</p><h2 id="3-Feature-Engineering"><a href="#3-Feature-Engineering" class="headerlink" title="3. Feature Engineering"></a>3. Feature Engineering</h2><p>To train a successful model, you need to extract meaningful features from your data. Consider including the product’s past CTR, user’s past activity, demographic information, product properties, and other time and context-specific features.</p><h2 id="4-Model-Selection"><a href="#4-Model-Selection" class="headerlink" title="4. Model Selection"></a>4. Model Selection</h2><p>Your model should suit the complexity of your data and the nature of your problem. With historical CTR data, a supervised learning approach is appropriate. For simple binary classification problems, Logistic Regression can work well. More complex models like Random Forests, Gradient Boosting Machines (GBMs), or deep learning models might be necessary for high dimensional data or non-linear relationships.</p><h2 id="5-Model-Training"><a href="#5-Model-Training" class="headerlink" title="5. Model Training"></a>5. Model Training</h2><p>Split your data into training and validation sets, train your selected model on the training data, and validate it on the validation set. Your target variable will be whether the product was clicked or not.</p><h2 id="6-Evaluation"><a href="#6-Evaluation" class="headerlink" title="6. Evaluation"></a>6. Evaluation</h2><p>Evaluate your model using relevant metrics such as ROC-AUC, precision, recall, or F1 score. It’s crucial to monitor both the model’s performance and its business impact.</p><h2 id="7-Rank-Optimization"><a href="#7-Rank-Optimization" class="headerlink" title="7. Rank Optimization"></a>7. Rank Optimization</h2><p>Once your model is trained, it can predict the likelihood of each product being clicked. Rank the products based on these probabilities and show the top 3 products each time. This ensures you’re presenting the products that the model predicts are most likely to be clicked, based on past CTR data.</p><h2 id="8-Experiment-and-Iterate"><a href="#8-Experiment-and-Iterate" class="headerlink" title="8. Experiment and Iterate"></a>8. Experiment and Iterate</h2><p>Remember, data science is an iterative process. You might need to go back and engineer new features, try different models, or collect more data. Regularly conduct A/B tests to confirm your new ranking algorithm is improving the CTR.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> product ranking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Large Language Model&#39;s Impact and Influence on Programmers</title>
      <link href="2023/07/22/will-large-language-model-replace-software-engineeer/"/>
      <url>2023/07/22/will-large-language-model-replace-software-engineeer/</url>
      
        <content type="html"><![CDATA[<p>Following are very interesting insights from Tencent Tech lead。</p><h2 id="1-The-Single-Point-Efficiency-Improvement-of-LLM-in-Software-Development"><a href="#1-The-Single-Point-Efficiency-Improvement-of-LLM-in-Software-Development" class="headerlink" title="1. The Single Point Efficiency Improvement of LLM in Software Development"></a>1. The Single Point Efficiency Improvement of LLM in Software Development</h2><p>For the single-point efficiency improvement of LLM in software development, here are some potential applications:</p><ul><li>Intelligent code prompting</li><li>Intelligent generation of code snippets</li><li>Intelligent generation and optimization of SQL statements</li><li>More efficient and accurate static code checking and automatic repair (not rule-based)</li><li>Intelligent assistance in code review and code refactoring</li><li>Automatic generation of unit test and interface test codes</li><li>More advanced duplicate code checking (semantic duplicate checking)</li><li>Automatic analysis and attribution of failed cases</li><li>More precise technical Q&amp;A</li></ul><p>Looking at this, you may conclude that programmers will be largely unemployed. Is that really the case? To answer this question, we need to look at the problem from a global perspective. What has LLM changed in software development? What has not changed?</p><h2 id="2-What-Has-LLM-Changed-in-Software-Development"><a href="#2-What-Has-LLM-Changed-in-Software-Development" class="headerlink" title="2 What Has LLM Changed in Software Development?"></a>2 What Has LLM Changed in Software Development?</h2><p>From the above examples, you should be able to appreciate the various possibilities of LLM’s single-point efficiency improvement in software development. These capabilities show us the changes in software development, which I summarize as: the democratization of basic coding knowledge, leading to localized efficiency improvement.</p><p>In the past, individual engineers needed a long learning cycle to master a computer language and its corresponding data structures and algorithms. Many experiences and patterns required individual engineers to summarize through extensive practice. Every individual engineer was repeating this process. Now, LLM allows an individual who has not received systematic training to have the same ability. The difference in ability between individuals is leveled by LLM. This is the democratization of knowledge.</p><p>If ChatGPT has achieved the democratization of knowledge in the digital age, then large code language models like Codex have achieved the democratization of basic coding ability, thereby bringing about local efficiency improvement in software development.</p><p>LLM has lowered the threshold for software development, allowing more people interested in software development to participate more easily. At the same time, LLM has improved the efficiency and quality of programming, allowing us to complete more work in less time, thus leaving us more time to think.</p><p>Not long ago, Matt Welsh, a former computer science professor at Harvard University who has held senior engineering positions at Google and Apple, released a video. The main point was that “LLM will represent the end of programming”. He believes that programmers will be eliminated, and only product managers and code reviewers will remain in the future. I don’t know what you think about this?</p><p>In my opinion, while holding a sense of awe, we should not rush to conclusions. Why? Because there is still much in software development that has not changed, and these unchanged aspects are the core issues and main contradictions in software engineering.</p><h2 id="3-What-Hasn’t-Changed-in-Software-Development-with-LLM"><a href="#3-What-Hasn’t-Changed-in-Software-Development-with-LLM" class="headerlink" title="3 What Hasn’t Changed in Software Development with LLM?"></a>3 What Hasn’t Changed in Software Development with LLM?</h2><p>We are facing the problems of software engineering. Programming is not equal to software engineering, it is just a part of it. The four inherent characteristics of software engineering (complexity, inconsistency, changeability, and invisibility) have not fundamentally changed with the advent of LLM. These are the main contradictions facing software engineering.</p><p>From the perspective of complexity, the complexity of the problem domain itself has not changed, and the essential complexity has not changed. What may have changed is only a part of the accidental complexity. Although local coding has become simpler, or more efficient, requirements analysis and software design have not become simpler due to LLM. We’ll discuss this later.</p><p>From the perspective of consistency, due to the essence of software development still being “mass collaboration of knowledge craftsmen”, we need consistency. If the system is consistent, it means that similar things are done in similar ways. Making mistakes is not terrible, what’s terrible is the myriad ways of making mistakes. The emergence of LLM has not improved the consistency of software development. In fact, due to the probabilistic nature of LLM itself, the inconsistency issue in code generation using LLM is amplified. We’ll expand on this later.</p><p>From the perspective of changeability, software evolves and changes with requirements, so architecture design and module abstraction can only face the present. They are inherently short-sighted or limited. Even the best architects cannot overcome this limitation.</p><p>In the agile development model, this problem is even more prominent. Moreover, requirements are scattered, and targets are vague. Without a global view, architecture is naturally limited, so it needs to be iteratively changed. Each iteration only provides a tiny piece of information in the grand view, far from the whole picture, and LLM is helpless in this.</p><p>From the perspective of invisibility, the objective existence of software does not have spatial physical characteristics. Different focus points will have different diagrams. It’s difficult to overlay these diagrams, and forced visualization will result in exceptionally complex diagrams, losing the value of visualization. The inability to visualize design restricts effective communication and exchange.</p><p>If you add the scale effect of large software, which includes the scale of the software system itself and the scale of the software development team, the problem becomes more serious. It significantly increases the communication cost, decision cost, cognitive cost, and trial and error cost in the software development process. These are the essence of software engineering problems. These essential problems have never changed, and LLM is largely powerless against them.</p><p>Based on the above analysis, we can see that the core contradiction of software engineering has not changed. Modern software engineering addresses various problems in a large-scale scenario. The programming efficiency achieved by LLM is just a small part of it. The most important requirements and code evolution patterns have not fundamentally changed. Let’s discuss each of these next</p><h2 id="4-The-importance-of-requirements-hasn’t-changed-it’s-even-magnified-in-the-LLM-era"><a href="#4-The-importance-of-requirements-hasn’t-changed-it’s-even-magnified-in-the-LLM-era" class="headerlink" title="4 The importance of requirements hasn’t changed, it’s even magnified in the LLM era"></a>4 The importance of requirements hasn’t changed, it’s even magnified in the LLM era</h2><p>Only when our requirements are clear enough, will the generated code be accurate. Therefore, accurately and comprehensively describing requirements becomes crucial. For natural language programming, firstly, you need the ability to articulate well. But the question is: can you?</p><p>Through some practices, we found that the workload to describe the requirements to the extent that it can write the correct code seems to have reached or even exceeded the coding. There are two main reasons for this.</p><p>Firstly, most code implementation is imperative, while requirement description is declarative. These two put entirely different demands on people. The education we, as a programmer group, receive is programming, not requirement description. This means that programmers are inherently better at coding, not describing requirements.</p><p>Secondly, under the current development model, programmers implicitly compensate for the requirements (product managers) with code. Much of the content not explicitly mentioned in the requirements has been implemented directly by the programmer (compensation). Now it requires a reversal where the details of the requirement must be fully clarified first, which may not be a programmer’s current work habit. Besides, the information entropy of code is actually greater than that of natural language. Programmers are better at describing matters with code rather than natural language.</p><p>For example, how do you clearly describe the requirements of a sorting function sort? The numbers output by sort must be arranged from small to large. Is this enough to describe the requirements? Far from it. How do you handle repeated numbers? Is there an upper limit to the number of sorted data? If so, how to prompt? Does the sorting duration need a timeout design? Is it a pre-judgment or a mid-judgment? Are there specific requirements for algorithm complexity? Does the algorithm need to deal with concurrency? What’s the scale of concurrency? And so on.</p><p>The requirements of software are not just functional. There are many non-functional requirements that need to be clearly described. Moreover, when implementing code, consideration must be given to design for testability, extensibility, maintainability, observability, etc. A lot of these were previously compensated for by development. Now, to generate code from requirements, you must explain these in advance.</p><p>Therefore, our conclusion is: “Software practitioners overestimate the complexity of programming, but underestimate the profundity of function and design”.</p><h2 id="5-Code-is-continually-“grown”-requiring-continuous-updates"><a href="#5-Code-is-continually-“grown”-requiring-continuous-updates" class="headerlink" title="5 Code is continually “grown”, requiring continuous updates"></a>5 Code is continually “grown”, requiring continuous updates</h2><p>For the current software development paradigm, when requirements change, it is usually modified based on the existing code, rather than generating all the code from scratch. At this time, what LLM essentially does is auxiliary to local programming (pair programming). In the process of local programming assistance, it is often necessary to make local modifications to the code, which is often not easy.</p><p>We know that the information entropy of code is greater than that of natural language. It is difficult to describe code, especially accurately describe several positions in a large section of code, with natural language of lower information entropy. Imagine how inefficient it would be to tell others where to modify the code in the chat online, compared to pointing at the screen or using a dedicated CR tool, the efficiency gap is huge.</p><p>Describing how to modify further would be more difficult, because it probably needs a lot of descriptions related to the code context, so the requirements for the prompt’s expression and length are high.</p><p>Besides, the output after LLM accepts the modification suggestion (prompt) is unstable and non-convergent, and it is uninterpretable. LLM does not rewrite based on the modification suggestion (prompt), but rewrites a new one based on the modification suggestion (prompt). The output code requires people to repeatedly read and understand, making the cognitive cost higher.</p><p>At the same time, the principle of LLM determines its nature of “talking nonsense seriously”, mixing up some non-existent things. The mixture of falsehoods in AI can be said to be a “confidence” response of AI in ignorance, and this point is disastrous in code generation. For example, it will mix different types of SQL statements together, or confuse Go’s os.Kill with Python’s os.kill(). This problem may need to be alleviated by using AI to audit AI.</p><p>As mentioned earlier, to modify based on the existing code, it is necessary to use the existing code context, not to start from scratch. To achieve this, a simple way is to paste the entire project code into the prompt, but it’s not realistic. Because GPT-3.5 can only accommodate up to 4096 tokens, and GPT-4 up to 8192 tokens, unless the project is very small, it will not fit. This problem might need langchain to solve.</p><p>LangChain is a middleware linking user-oriented programs and LLM. It “customizes” its own LLM by inputting its own knowledge base. Langchain uses embedding to establish a vector knowledge base specific to the project and realizes “question-answering based on specific documents”.</p><h2 id="6-LLM-Era-Further-Considerations-on-Software-Development"><a href="#6-LLM-Era-Further-Considerations-on-Software-Development" class="headerlink" title="6 LLM Era: Further Considerations on Software Development"></a>6 LLM Era: Further Considerations on Software Development</h2><h3 id="Consideration-1-Coding-Monkeys-will-be-replaced-Engineers-will-Coexist"><a href="#Consideration-1-Coding-Monkeys-will-be-replaced-Engineers-will-Coexist" class="headerlink" title="Consideration 1: Coding Monkeys will be replaced, Engineers will Coexist"></a>Consideration 1: Coding Monkeys will be replaced, Engineers will Coexist</h3><p>In the software development process, once the pseudocode-level design is completed, the final kilometer of coding implementation will be replaced by LLM, because simple repetitive coding based on memory is not a human advantage, but a machine advantage.</p><p>This part of the job currently belongs to coding monkeys, also known as CRUD workers and API Boys in layman’s terms, so many coders who do not involve design may be replaced by large models.</p><p>Engineers, on the other hand, need to focus on business understanding, requirement breakdown, architectural design, and design trade-offs, and learn to cooperate with AI based on these foundations, thereby achieving a 1+1 &gt;2 effect of “Engineer + LLM”. This is symbiosis.</p><p>It is worth noting that this kind of symbiosis must always maintain human subjectivity, machines must be Copilots, that is, intelligent co-pilots, and humans must be the main drivers, only such a human-machine relationship can develop healthily in the long term. This is also the fundamental reason why Microsoft’s current CEO, Satya Nadella, emphasizes that Copilot is more advanced than Autopilot.</p><p>In addition, it is worth mentioning that: In the short term, engineers who first learn to use LLM will benefit, but soon everyone will master it, and the ability level will be leveled again. This is very similar to the point of view in the previous article “Delivery Riders Stuck in the System”, so as symbiotic engineers, we need to strengthen our abilities in the following three aspects:</p><p>Ability to understand, analyze, and break down requirements<br>Ability to design architecture, analyze architecture, make design trade-offs, and promote the documentation and standardization of design<br>Understand the essence of the problem, not just learning to apply (Teach a man to fish is better than giving a man a fish)</p><h3 id="Consideration-2-Beneficial-for-Controlling-the-Scale-of-R-amp-D-Teams-and-Maintaining-the-Advantage-of-Small-Teams"><a href="#Consideration-2-Beneficial-for-Controlling-the-Scale-of-R-amp-D-Teams-and-Maintaining-the-Advantage-of-Small-Teams" class="headerlink" title="Consideration 2: Beneficial for Controlling the Scale of R&amp;D Teams and Maintaining the Advantage of Small Teams"></a>Consideration 2: Beneficial for Controlling the Scale of R&amp;D Teams and Maintaining the Advantage of Small Teams</h3><p>As a software scale expands, more and more people participate in the software project, and the division of labor becomes finer and finer, and the amount of communication needed between people also increases exponentially. Soon you will find that the time spent on communication gradually becomes more than the time saved by the division of labor. In plain words, after a certain point, the more people, the more chaotic, not the more helpful. A job that can be completed by one person in 12 months does not necessarily mean that it can be completed by 12 people in 1 month, let alone in 12 months.</p><p>The Mythical Man-Month suggests a kind of organization called “surgical-style team”. Just like a surgery, there is a chief surgeon, and a software project should also have a chief programmer, with everyone else providing support. This way, you can both get the product integrity produced by a few minds, and the overall productivity of multiple assistants, while completely reducing the amount of communication.</p><p>But as software scales up, more programmers will inevitably be needed, and the team scale will definitely accelerate its expansion. However, the emergence of LLM, which automates basic programming work to some extent, is very beneficial for controlling the scale of R&amp;D teams and maintaining the efficiency advantage of small teams.</p><h3 id="Consideration-3-Tacit-Knowledge"><a href="#Consideration-3-Tacit-Knowledge" class="headerlink" title="Consideration 3: Tacit Knowledge"></a>Consideration 3: Tacit Knowledge</h3><p>The success of large models largely comes from learning from existing Internet text corpora and professional books and other materials. Correspondingly, in the field of software engineering, what needs to be learned is not just code, but also requirements and design.</p><p>However, many requirements and designs do not exist in the form of documents, but often exist in the minds of programmers and architects, or during discussions. Even if there are documents, the documents and codes are highly likely to be out of sync. Even if the documents are synchronized, there is often a lot of plan comparison and deliberation behind the documents (requirements and designs), and even many design compromises based on the original debt base, and these decision-making processes are generally not explicitly recorded. This kind of knowledge that has not been documented, we call it “tacit knowledge”.</p><p>Although we say that as long as there is enough data, large models can learn the knowledge of requirements and design. But these “tacit knowledge” are difficult to capture in themselves, and the premise of “enough data” may be difficult to meet in requirement analysis and software design.</p><p>In addition, in actual software development, requirements may not be expressed clearly at one time, and need to be gradually written clearly while developing. This is especially true for agile development. So for some general problems that do not require specific domain knowledge, LLM’s performance will be better, but for those specialized problems that require specific domain knowledge (private domain knowledge), LLM may not be very good.</p><p>In summary, “You can think of more than you can say, you can say more than you can write down.” So this naturally limits the upper limit of LLM’s ability.</p><h3 id="Consideration-4-Prompt-is-Code-Code-is-no-Longer-Code"><a href="#Consideration-4-Prompt-is-Code-Code-is-no-Longer-Code" class="headerlink" title="Consideration 4: Prompt is Code, Code is no Longer Code"></a>Consideration 4: Prompt is Code, Code is no Longer Code</h3><p>Let’s make a bold assumption, if when software requirements change, we no longer change the code, but directly modify the prompt corresponding to the requirements, and then directly generate the complete code based on the prompt, this will be a change in the paradigm of software development.</p><p>Under this paradigm, we need to ensure that the code cannot be modified by humans, and must all be directly generated by the prompt. At this time, we also need to version control the prompt, and maybe a new species like git’s prompt version control will appear.</p><p>At this point, fundamentally speaking, prompt is code, and the original code is no longer code, which truly realizes programming based on natural language (prompt), and the programming paradigm will change from prompt to code to prompt as code.</p><p>To think further, when prompt as code is implemented, do we still need code, and are many engineering practices related to code still important? Now we think code engineering is important because code is written by humans and maintained by humans. But when code is written by LLM and maintained by LLM, is the existing software architecture system still applicable? At this time, maybe the evolution of the software development paradigm has truly been realized.</p><h3 id="Consideration-5-Directly-executable-the-possibility-of-prompt-to-executable-software-development-paradigm"><a href="#Consideration-5-Directly-executable-the-possibility-of-prompt-to-executable-software-development-paradigm" class="headerlink" title="Consideration 5: Directly executable, the possibility of prompt to executable software development paradigm"></a>Consideration 5: Directly executable, the possibility of prompt to executable software development paradigm</h3><p>Thinking one step further, will the infrastructure of direct execution, prompt to executable appear?</p><p>Code is just a part of software engineering, far from all of software engineering. Think about how much time you spend coding. Generally speaking, after the coding is completed, it often has to go through a series of engineering practices such as CI and CD to deliver value to end users.</p><p>So can the new software paradigm realize the direct transition from prompt to executable program instance? At present, Serverless may be one of the possible architectures.</p><h3 id="Consideration-6-Reflection-on-computer-education"><a href="#Consideration-6-Reflection-on-computer-education" class="headerlink" title="Consideration 6: Reflection on computer education"></a>Consideration 6: Reflection on computer education</h3><p>After the emergence of LLM, I think there are two levels of reflection on computer education:</p><p>Firstly, the change of research direction in computer science. Previously, NLP, knowledge graph, code understanding, code defect discovery, test oracle generation, etc. were all independent research directions. However, the AGI ability shown by LLM seems to make the research of these vertical fields lose its meaning, because the AGI ability of LLM can solve them, perhaps even better.</p><p>So where will these research directions go is what we need to think about. Some people say that LLM is a new milestone in NLP, but others think it is more like the epitaph of NLP, which very well expresses my view.</p><p>Secondly, LLM has repeatedly proved that by “memorizing + simple reasoning”, it can pass most human exams and technical interviews. So what is the ultimate goal of education? Advanced artificial intelligence attempts to cultivate machines into humans, while backward education attempts to cultivate humans into machines. Computer education, in fact, our entire education is at a time when we need to reflect thoroughly.</p><p>Or are we all wrong?!</p><p>Peter Drucker once said, “The greatest risk of turbulent times is not turbulence itself, but trying to cope with turbulence with yesterday’s logic.” Today’s impact of LLM on software engineering, I am still analyzing with the previous logic, this foundation may be wrong in the first place, a new era requires a new way of thinking, and then we wait and see.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> llm </tag>
            
            <tag> software engineer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python Modules and Namespaces, Managing Variables Across Files</title>
      <link href="2023/07/20/managing-variables-in-multiple-module-or-files-in-python/"/>
      <url>2023/07/20/managing-variables-in-multiple-module-or-files-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python programming, understanding how variables are managed across different files or modules is critical to structuring your programs correctly. It’s a matter of scope and namespaces.</p><h2 id="Namespaces-and-Scope"><a href="#Namespaces-and-Scope" class="headerlink" title="Namespaces and Scope"></a>Namespaces and Scope</h2><p>Firstly, we need to understand what a namespace is. In Python, a namespace is a mapping from names (variable names, function names, etc.) to objects. The important thing to note here is that there is no relation between names in different namespaces. Therefore, two different modules can both define a variable, my_var, without conflict, because each module has its own namespace.</p><p>For instance, consider two Python files:</p><p>In file1.py:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_var = &quot;Hello from file1&quot;</span><br></pre></td></tr></table></figure><p>In file2.py:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_var = &quot;Hello from file2&quot;</span><br></pre></td></tr></table></figure><p>The my_var in file1.py and file2.py are two distinct variables. They live in separate namespaces and do not interfere with each other.</p><h2 id="Importing-Variables-Between-Modules"><a href="#Importing-Variables-Between-Modules" class="headerlink" title="Importing Variables Between Modules"></a>Importing Variables Between Modules</h2><p>While each module has its own namespace, Python allows you to access variables from one module in another using the import statement.</p><p>In file1.py:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_var = &quot;Hello from file1&quot;</span><br></pre></td></tr></table></figure><p>In file2.py:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from file1 import my_var</span><br><span class="line">print(my_var)  # This will output: Hello from file1</span><br></pre></td></tr></table></figure><p>Here, my_var in file2.py is the same as my_var in file1.py because it was imported. However, if my_var is subsequently redefined in file2.py, this change won’t affect the original my_var in file1.py.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> namespace </tag>
            
            <tag> module </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a Powerful Text Search System with Pinecone, Langchain, and OpenAI Embedding</title>
      <link href="2023/07/18/create-pinecone-vectorstore-usnig-langchain-and-openai-embedding/"/>
      <url>2023/07/18/create-pinecone-vectorstore-usnig-langchain-and-openai-embedding/</url>
      
        <content type="html"><![CDATA[<p>In today’s data-driven world, businesses and developers often need to implement powerful text search capabilities. Traditional search algorithms may not always provide optimal results, especially when dealing with large amounts of unstructured text data. This is where Pinecone, Langchain, and the OpenAI service come into play. In this blog post, we will explore the steps required to set up and leverage these tools to build a highly accurate and efficient text search system.</p><h2 id="Step-1-Setting-up-the-Index"><a href="#Step-1-Setting-up-the-Index" class="headerlink" title="Step 1: Setting up the Index"></a>Step 1: Setting up the Index</h2><p>To begin, we need to set up an index in Pinecone. Install the required Python packages, including pinecone-client, openai, and tiktoken. Then proceed with the following code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pinecone</span><br><span class="line"></span><br><span class="line">pinecone.init(api_key=&quot;YOUR_API_KEY&quot;, environment=&quot;YOUR_ENVIRONMENT&quot;)</span><br><span class="line"></span><br><span class="line">pinecone.create_index(&quot;langchain-demo&quot;, dimension=1536, metric=&quot;cosine&quot;)</span><br></pre></td></tr></table></figure><p>The <code>dimension</code> parameter is set to 1536 because we will be using the “text-embedding-ada-002” OpenAI model, which has an output dimension of 1536. If you need to delete the index, use the <code>pinecone.delete_index(&quot;langchain-demo&quot;)</code> command.</p><h2 id="Step-2-Importing-Libraries-and-Setting-up-Keys"><a href="#Step-2-Importing-Libraries-and-Setting-up-Keys" class="headerlink" title="Step 2: Importing Libraries and Setting up Keys"></a>Step 2: Importing Libraries and Setting up Keys</h2><p>Next, we need to import the required libraries and set up the necessary keys. Import the following libraries:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from langchain.embeddings.openai import OpenAIEmbeddings</span><br><span class="line">from langchain.text_splitter import CharacterTextSplitter</span><br><span class="line">from langchain.vectorstores import Pinecone</span><br><span class="line">from langchain.document_loaders import TextLoader</span><br></pre></td></tr></table></figure><p>Set the <code>PINECONE_API_KEY</code> and <code>PINECONE_ENV</code> variables to your Pinecone API key and environment. Additionally, set the <code>OPENAI_API_KEY</code> environment variable to your OpenAI API key.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">os.environ[&quot;OPENAI_API_KEY&quot;] = &#x27;your openai api key&#x27;</span><br></pre></td></tr></table></figure><h2 id="Step-3-Preparing-the-Data-and-Embedding-Layer"><a href="#Step-3-Preparing-the-Data-and-Embedding-Layer" class="headerlink" title="Step 3: Preparing the Data and Embedding Layer"></a>Step 3: Preparing the Data and Embedding Layer</h2><p>Now, load the text data (here we use an example) and prepare the embedding layer using the OpenAI service. Use the <code>TextLoader</code> class from Langchain to load the text data:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">loader = TextLoader(&quot;state_of_the_union.txt&quot;)</span><br><span class="line">documents = loader.load()</span><br></pre></td></tr></table></figure><p>We can then split the documents into smaller chunks using the <code>CharacterTextSplitter</code> class:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)</span><br><span class="line">docs = text_splitter.split_documents(documents)</span><br></pre></td></tr></table></figure><p>Finally, initialize the OpenAI embeddings:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">embeddings = OpenAIEmbeddings()</span><br></pre></td></tr></table></figure><h2 id="Step-4-Chunking-the-Documents-and-Indexing-the-Embedding-Vectors"><a href="#Step-4-Chunking-the-Documents-and-Indexing-the-Embedding-Vectors" class="headerlink" title="Step 4: Chunking the Documents and Indexing the Embedding Vectors"></a>Step 4: Chunking the Documents and Indexing the Embedding Vectors</h2><p>In this step, we will chunk the documents into smaller pieces and index the OpenAI embedding vectors using Pinecone. Use the following code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pinecone</span><br><span class="line"></span><br><span class="line">pinecone.init(</span><br><span class="line">    api_key=PINECONE_API_KEY,</span><br><span class="line">    environment=PINECONE_ENV,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">index_name = &quot;langchain-demo&quot;</span><br><span class="line"></span><br><span class="line">docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)</span><br><span class="line"></span><br><span class="line">query = &quot;What did the president say about Ketanji Brown Jackson&quot;</span><br><span class="line">docs = docsearch.similarity_search(query)</span><br><span class="line"></span><br><span class="line">print(docs[0].page_content)</span><br></pre></td></tr></table></figure><h2 id="Step-5-Adding-More-Texts-to-the-Index"><a href="#Step-5-Adding-More-Texts-to-the-Index" class="headerlink" title="Step 5: Adding More Texts to the Index"></a>Step 5: Adding More Texts to the Index</h2><p>To add more texts to an existing index or start with an empty index, use the following code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">index = pinecone.Index(&quot;langchain-demo&quot;)</span><br><span class="line">vectorstore = Pinecone(index, embeddings.embed_query, &quot;text&quot;)</span><br><span class="line"></span><br><span class="line">vectorstore.add_texts([&quot;More text to add as an example!&quot;])</span><br></pre></td></tr></table></figure><p>If you need to add metadata to the index, you can pass a list of dictionaries with the texts:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">vectorstore.add_texts([&quot;More text to add as an example!&quot;], [&#123;&#x27;name&#x27;:&#x27;example&#x27;&#125;])</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>By following these steps, you can build a powerful text search system using Pinecone, Langchain, and the OpenAI service. These tools allow you to leverage advanced text embeddings and indexing capabilities to achieve highly accurate and efficient search results. Whether you need to search through large volumes of documents or implement a recommendation system, this combination of tools can significantly enhance your application’s performance and user experience.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> langchain </tag>
            
            <tag> openai </tag>
            
            <tag> pinecone </tag>
            
            <tag> vectorstore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding and Resolving GitHub Merge Conflicts</title>
      <link href="2023/07/17/understand-and-resolve-github-merge-conflicts/"/>
      <url>2023/07/17/understand-and-resolve-github-merge-conflicts/</url>
      
        <content type="html"><![CDATA[<p>Understanding and resolving merge conflicts is inevitable when working with Git, especially in a collaborative coding environment. Let’s make it crystal clear for you.</p><h2 id="What-is-a-Merge-Conflict"><a href="#What-is-a-Merge-Conflict" class="headerlink" title="What is a Merge Conflict?"></a>What is a Merge Conflict?</h2><p>A merge conflict occurs when two branches you’re trying to merge both modified the same part of the same file, and Git can’t determine which version to use. This situation needs manual intervention to decide which changes to keep.</p><h2 id="How-to-Identify-a-Merge-Conflict"><a href="#How-to-Identify-a-Merge-Conflict" class="headerlink" title="How to Identify a Merge Conflict?"></a>How to Identify a Merge Conflict?</h2><p>When you encounter a merge conflict, Git uses markers to show you what and where the conflict is.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</span><br><span class="line">...</span><br><span class="line">=======</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt; branch-name</span><br></pre></td></tr></table></figure><h2 id="The-Anatomy-of-GitHub-Merge-Conflict-Indicators"><a href="#The-Anatomy-of-GitHub-Merge-Conflict-Indicators" class="headerlink" title="The Anatomy of GitHub Merge Conflict Indicators"></a>The Anatomy of GitHub Merge Conflict Indicators</h2><p>Suppose you have two branches: <code>master</code> and <code>dev</code>. Let’s understand about conflict indicators.</p><ul><li><p><code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> denotes the start of the conflict area, and <code>HEAD</code> just refers to the branch you’re currently on (master branch, in this case).</p></li><li><p>The part between <code>&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD</code> and <code>=======</code> comes from the current branch (HEAD / master branch).</p></li><li><p><code>=======</code> is a separator between the conflicting changes.</p></li><li><p>The part between <code>=======</code> and <code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; branch-name</code> is the conflicting code from the branch you’re trying to merge (dev branch, in this case).</p></li><li><p><code>&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev</code> denotes the end of conflict area.</p></li></ul><h2 id="An-Example"><a href="#An-Example" class="headerlink" title="An Example"></a>An Example</h2><p>Let’s consider: </p><p>In master branch:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, world!&quot;</span>)</span><br></pre></td></tr></table></figure><p>In dev branch:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, GitHub!&quot;</span>)</span><br></pre></td></tr></table></figure><p>On merge, GitHub presents you the conflicting code as:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD       </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, world!&quot;</span>)</span><br><span class="line">=======              </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hello, GitHub!&quot;</span>)</span><br><span class="line">&gt;&gt;&gt;&gt;&gt;&gt;&gt; dev         </span><br></pre></td></tr></table></figure><h2 id="How-to-resolve-a-conflict"><a href="#How-to-resolve-a-conflict" class="headerlink" title="How to resolve a conflict?"></a>How to resolve a conflict?</h2><p>To resolve this conflict, you will need to decide whether to keep the code from your current (HEAD) branch, accept the incoming code from the other branch, or even perhaps a combination of both. It entirely depends on which code is correct or up-to-date with respect to your project requirements.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Implementing OpenAI&#39;s ChatGPT Interaction using GitHub Actions</title>
      <link href="2023/07/15/integrate-chatgpt-with-github-repo-using-github-action/"/>
      <url>2023/07/15/integrate-chatgpt-with-github-repo-using-github-action/</url>
      
        <content type="html"><![CDATA[<p>GitHub Actions help you to automate your software workflows with CI/CD capabilities. This includes continuous integration (building and testing your project), continuous delivery, and continuous deployment.</p><p>To integrate GitHub Actions with ChatGPT, follow these steps. These instructions will help you create an action that triggers when you push a commit, and it will connect with OpenAI’s API to send a request to ChatGPT.</p><p>For instance, this will show you how to setup GitHub Actions to send a message to ChatGPT and get a response.</p><h2 id="Step-1-Create-a-new-GitHub-repository"><a href="#Step-1-Create-a-new-GitHub-repository" class="headerlink" title="Step 1: Create a new GitHub repository"></a>Step 1: Create a new GitHub repository</h2><p>Navigate to GitHub and create a new repository. Give it a name, a description, and initialize it with a README file. Then, clone it to your local machine.</p><h2 id="Step-2-Generate-an-OpenAI-API-key"><a href="#Step-2-Generate-an-OpenAI-API-key" class="headerlink" title="Step 2: Generate an OpenAI API key"></a>Step 2: Generate an OpenAI API key</h2><p>Go to the OpenAI website, navigate to your dashboard, and generate a new API key. This will be used to authenticate your requests to the ChatGPT API.</p><h2 id="Step-3-Setup-a-GitHub-Action"><a href="#Step-3-Setup-a-GitHub-Action" class="headerlink" title="Step 3: Setup a GitHub Action"></a>Step 3: Setup a GitHub Action</h2><p>Create a new file in your repository with the following path: .github/workflows/chat.yml. This file will define your GitHub Action.</p><p>Add the following content to chat.yml:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name: Chat with GPT</span><br><span class="line"></span><br><span class="line">on: [push]</span><br><span class="line"></span><br><span class="line">jobs:</span><br><span class="line">  chat:</span><br><span class="line">    runs-on: ubuntu-latest</span><br><span class="line"></span><br><span class="line">    steps:</span><br><span class="line">    - uses: actions/checkout@v2</span><br><span class="line"></span><br><span class="line">    - name: Send message to GPT</span><br><span class="line">      run: |</span><br><span class="line">        curl -X POST \</span><br><span class="line">        https://api.openai.com/v1/chat/completions \</span><br><span class="line">        -H &quot;Content-Type: application/json&quot; \</span><br><span class="line">        -H &quot;Authorization: Bearer $&#123;&#123; secrets.OPENAI_KEY &#125;&#125;&quot; \</span><br><span class="line">        -d &#x27;&#123;</span><br><span class="line">             &quot;model&quot;: &quot;gpt-3.5-turbo&quot;,</span><br><span class="line">             &quot;messages&quot;: [&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Say this is a test!&quot;&#125;],</span><br><span class="line">             &quot;temperature&quot;: 0.7</span><br><span class="line">           &#125;&#x27;</span><br><span class="line">      env:</span><br><span class="line">        OPENAI_KEY: $&#123;&#123; secrets.OPENAI_KEY &#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This action will trigger every time you push a commit. It will send a request to the OpenAI API with a hardcoded prompt. Please replace the {} with the English text that you want to translate.</p><h2 id="Step-4-Add-your-API-key-to-GitHub-Secrets"><a href="#Step-4-Add-your-API-key-to-GitHub-Secrets" class="headerlink" title="Step 4: Add your API key to GitHub Secrets"></a>Step 4: Add your API key to GitHub Secrets</h2><p>To avoid exposing your API key, add it to the GitHub Secrets. Go to your GitHub repository, click on “Settings” &gt; “Secrets” &gt; “New repository secret”, and add your API key with the name OPENAI_KEY.</p><h2 id="Step-5-Commit-and-push"><a href="#Step-5-Commit-and-push" class="headerlink" title="Step 5: Commit and push"></a>Step 5: Commit and push</h2><p>Add your changes to git, commit them, and push to GitHub. This will trigger your GitHub Action.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> github </tag>
            
            <tag> github action </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to discard changes or unstage file when using git and github</title>
      <link href="2023/07/13/how-to-discard-change-or-unstage-changes-using-git/"/>
      <url>2023/07/13/how-to-discard-change-or-unstage-changes-using-git/</url>
      
        <content type="html"><![CDATA[<p>The normal workflow of working with git and github are:</p><ul><li><code>git add</code>: Select specific files or changes to include in the commit.</li><li><code>git commit</code>: Create a snapshot of the selected changes with a descriptive commit message.</li><li><code>git push</code>: Push the committed changes to a remote repository for sharing and collaboration.</li></ul><p>Example:</p><ul><li><code>git add file1.txt</code>: Select file1.txt to be included in the commit.</li><li><code>git commit -m &quot;Updated file1.txt&quot;</code>: Create a snapshot of the changes made to file1.txt with a descriptive commit message.</li><li><code>git push origin main</code>: Push the committed changes to the remote repository named origin in the main branch.</li></ul><p>However, how about if something doesn’t work out smoothly, and we need to back up a little bit.<br>For example</p><h2 id="discard-the-changes-to-some-file-in-the-current-directory"><a href="#discard-the-changes-to-some-file-in-the-current-directory" class="headerlink" title="discard the changes to some file in the current directory"></a>discard the changes to some file in the current directory</h2><p>The anwer is:<br>use <code>git restore &lt;file-with-full-path&gt; </code> to discard changes in working directory</p><p>for example:<br><code>git restore dir1/file1.txt</code></p><h2 id="I-staged-the-file-by-using-git-add-but-now-I-want-to-unstage"><a href="#I-staged-the-file-by-using-git-add-but-now-I-want-to-unstage" class="headerlink" title="I staged the file by using git add, but now I want to unstage"></a>I staged the file by using <code>git add</code>, but now I want to unstage</h2><p>the answer is:<br>use <code>git restore --staged &lt;file-with-full-path&gt; </code>to unstage<br>for example:</p><p><code> git restore --staged dir1/file1.txt</code></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to turn on  chatGPT&#39;s code interpreter in the UI</title>
      <link href="2023/07/11/how-to-turn-on-code-interpreter-in-chatgpt-ui/"/>
      <url>2023/07/11/how-to-turn-on-code-interpreter-in-chatgpt-ui/</url>
      
        <content type="html"><![CDATA[<p>Two requirements:<br>(1) you need to sign up the chatGPT plus, paid account, not the free account. It may open to free account later as well.<br>(2) I also found that, you need to turn the chat history (which is on by default).</p><p>Then, go to settings first as the following screen shot:<br><img src="/content/images/2023-07-11-01.jpg"></p><p>Second, on the setting page, click <code>beta features</code>, then turn on the <code>code interpreter</code> button.<br><img src="/content/images/2023-07-11-02.jpg"></p><p>Third, refresh your page, come back to the conversation page, and hover over “GPT-4” icon, and futher click the <code>code interpreter</code>.<br>And you should be set to use the code intepreter tool.<br><img src="/content/images/2023-07-11-03.jpg"></p><p>For how to use it, here is one simple example:<br><a href="/2023/07/10/chatgpt-code-interpreter-is-easy-to-use/">simple example to use code interpreter</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> code-interpreter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to use chatGPT&#39;s code interpreter, pretty easy</title>
      <link href="2023/07/10/chatgpt-code-interpreter-is-easy-to-use/"/>
      <url>2023/07/10/chatgpt-code-interpreter-is-easy-to-use/</url>
      
        <content type="html"><![CDATA[<p>OpenAI’s code interpreter is actually easy to use. It’s basically the usual chatgpt plus capability to upload data and run the code.</p><p>For how to turn in on in the chatGPT UI, following the following link:<br><a href="/2023/07/11/how-to-turn-on-code-interpreter-in-chatgpt-ui/">turn on code interpreter in the UI</a></p><p>Without code-interpreter, when we want to ask chatGPT to generate somde code, we will first desribe our needs, and the code wil be generated. But we then have to copy the code to our actuall notebook or other IDE environment, and run the code with some data. If there were issues, we may come back to report the error and chatGPT will make further adjustments.</p><p>But with code-interpreter,  the code can be actually interpreted and run immediately. So user have chance to see the results and discuss feedbacks with chatGPT right away.</p><p>Here is some simple example data that I have:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">name,age</span><br><span class="line">mary,18</span><br><span class="line">joh,39</span><br><span class="line">nice,49</span><br><span class="line">baby,10</span><br></pre></td></tr></table></figure><p>stored locally in my computer as a file called “testdata.txt”.</p><p>Now I begin to use chatGPT, as usual, I give instructions to chatGPT and want to group the data by ages as the following, and chatGPT gives code results as the following:</p><p><img src="/content/images/2023-07-10-01.jpg"></p><p>Now is the difference between code-interpreter and other modes, where we can click the “+” button on the message box and upload the file:</p><p><img src="/content/images/2023-07-10-02.jpg"></p><p>After the file is uploaded, chatGPT understand the context better, updated the file name, and I continue to say “correct” or “continue”:</p><p><img src="/content/images/2023-07-10-03.jpg"></p><p>Finally,  chatGPT will continue to run the code with the actual data and give me the results:</p><p><img src="/content/images/2023-07-10-04.jpg"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> code-interpreter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adding Retry Logging in Tenacity,Enhancing Error Handling and Visibility</title>
      <link href="2023/07/09/how-to-add-retry-logging-in-tenacity/"/>
      <url>2023/07/09/how-to-add-retry-logging-in-tenacity/</url>
      
        <content type="html"><![CDATA[<p>To add a log message that shows a retry was made when using Tenacity, you can use the retry decorator’s before_sleep parameter to define a function that gets called before each retry. Here’s an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from tenacity import retry, wait_fixed, stop_after_attempt,before_sleep_log</span><br><span class="line">import logging</span><br><span class="line"></span><br><span class="line"># Configure logging</span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br><span class="line"></span><br><span class="line"># Function to be executed</span><br><span class="line">@retry(wait=wait_fixed(1), stop=stop_after_attempt(3), before_sleep=before_sleep_log(logging.getLogger(), logging.INFO))</span><br><span class="line">def my_function():</span><br><span class="line">    # Your code here</span><br><span class="line">    pass</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we import the logging module and configure it to log at the INFO level. Then, we define our function my_function() and decorate it with the @retry decorator. We pass the before_sleep parameter to the decorator, which takes the tenacity.before_sleep_log() function. This function takes two arguments: a logger and a log level. In this case, we pass the logger created with logging.getLogger() and the log level logging.INFO.</p><p>With this configuration, Tenacity will log a message before each retry attempt, indicating that a retry was made. You can adjust the log level and log format according to your preferences.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> retry </tag>
            
            <tag> tenacity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Harnessing the Power of concurrent.futures.ThreadPoolExecutor for Multi-Parameter Function Execution</title>
      <link href="2023/07/09/how-to-submit-function-with-multiple-parameters-in-concurrent-futures-threadpoolexecutor/"/>
      <url>2023/07/09/how-to-submit-function-with-multiple-parameters-in-concurrent-futures-threadpoolexecutor/</url>
      
        <content type="html"><![CDATA[<p>Here’s an example of how you can use <code>concurrent.futures.ThreadPoolExecutor</code> to submit a function with multiple parameters:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import concurrent.futures</span><br><span class="line"></span><br><span class="line"># Function to be executed</span><br><span class="line">def my_function(param1, param2):</span><br><span class="line">    # Perform some computation or task</span><br><span class="line">    result = param1 + param2</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"># Create a ThreadPoolExecutor with maximum 5 threads</span><br><span class="line">with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:</span><br><span class="line">    # Submit the function with multiple parameters using the `submit` method</span><br><span class="line">    future = executor.submit(my_function, 10, 20)</span><br><span class="line">    </span><br><span class="line">    # Retrieve the result of the function execution</span><br><span class="line">    result = future.result()</span><br><span class="line">    </span><br><span class="line">    # Print the result</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure><p>In this example, we define a function called my_function that takes two parameters (<code>param1</code> and <code>param2</code>) and returns their sum. We then create a ThreadPoolExecutor with a maximum of 5 threads using the max_workers parameter.</p><p>We submit the my_function to the executor using the submit method, passing the function and its parameters (10 and 20). The submit method returns a Future object that represents the execution of the function.</p><p>We retrieve the result of the function execution by calling <code>future.result()</code>, which will block until the function completes. Finally, we print the result.</p><p>Note that ThreadPoolExecutor is particularly useful when you have multiple tasks that can run concurrently and you want to parallelize their execution.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> concurrent </tag>
            
            <tag> threadpool </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to avoid openAI rate limit errors, Retrying with exponential backoff</title>
      <link href="2023/07/08/openai-gpt-handel-rate-limit-error-with-exponential-backoff-retrying/"/>
      <url>2023/07/08/openai-gpt-handel-rate-limit-error-with-exponential-backoff-retrying/</url>
      
        <content type="html"><![CDATA[<p>One easy way to avoid rate limit errors when using openai chatGPT or GPT4 api calls, or any API clals, is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.</p><h2 id="This-approach-has-many-benefits"><a href="#This-approach-has-many-benefits" class="headerlink" title="This approach has many benefits:"></a>This approach has many benefits:</h2><p>Automatic retries means you can recover from rate limit errors without crashes or missing data<br>Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail<br>Adding random jitter to the delay helps retries from all hitting at the same time<br>Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.</p><p>Below are a few example solutions.</p><h2 id="Example-1-Using-the-Tenacity-library"><a href="#Example-1-Using-the-Tenacity-library" class="headerlink" title="Example #1: Using the Tenacity library"></a>Example #1: Using the Tenacity library</h2><p>Tenacity is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.</p><p>To add exponential backoff to your requests, you can use the tenacity.retry decorator. The following example uses the tenacity.wait_random_exponential function to add random exponential backoff to a request.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai  # for OpenAI API calls</span><br><span class="line">from tenacity import (</span><br><span class="line">    retry,</span><br><span class="line">    stop_after_attempt,</span><br><span class="line">    wait_random_exponential,</span><br><span class="line">)  # for exponential backoff</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))</span><br><span class="line">def completion_with_backoff(**kwargs):</span><br><span class="line">    return openai.Completion.create(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completion_with_backoff(model=&quot;text-davinci-002&quot;, prompt=&quot;Once upon a time,&quot;)</span><br></pre></td></tr></table></figure><h2 id="Example-2-Using-the-backoff-library"><a href="#Example-2-Using-the-backoff-library" class="headerlink" title="Example #2: Using the backoff library"></a>Example #2: Using the backoff library</h2><p>Another library that provides function decorators for backoff and retry is backoff.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import backoff  # for exponential backoff</span><br><span class="line">import openai  # for OpenAI API calls</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">@backoff.on_exception(backoff.expo, openai.error.RateLimitError)</span><br><span class="line">def completions_with_backoff(**kwargs):</span><br><span class="line">    return openai.Completion.create(**kwargs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">completions_with_backoff(model=&quot;text-davinci-002&quot;, prompt=&quot;Once upon a time,&quot;)</span><br></pre></td></tr></table></figure><h2 id="more-reference-with-OpenAI"><a href="#more-reference-with-OpenAI" class="headerlink" title="more reference with OpenAI"></a>more reference with OpenAI</h2><p><a href="https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb">openai example</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What does dataclass decorator in Python Class</title>
      <link href="2023/07/08/what-is-dataclass-decorator-in-python/"/>
      <url>2023/07/08/what-is-dataclass-decorator-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python, the @dataclass decorator is a feature introduced in Python 3.7 as part of the dataclasses module. It provides a convenient way to define classes that are primarily used to store data.</p><p>By applying the @dataclass decorator to a class, you can automatically generate several common methods and functionality, such as initializing attributes, comparing instances, generating string representations, and more. This helps reduce boilerplate code that would otherwise be required for such operations.</p><p>Here’s an example of using @dataclass:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from dataclasses import dataclass</span><br><span class="line"></span><br><span class="line">@dataclass</span><br><span class="line">class Person:</span><br><span class="line">    name: str</span><br><span class="line">    age: int</span><br><span class="line">    profession: str</span><br><span class="line"></span><br><span class="line"># Create an instance of the dataclass</span><br><span class="line">person = Person(&quot;John Doe&quot;, 30, &quot;Engineer&quot;)</span><br><span class="line"></span><br><span class="line"># Accessing attributes</span><br><span class="line">print(person.name)       # Output: John Doe</span><br><span class="line">print(person.age)        # Output: 30</span><br><span class="line">print(person.profession) # Output: Engineer</span><br><span class="line"></span><br><span class="line"># String representation</span><br><span class="line">print(person)            # Output: Person(name=&#x27;John Doe&#x27;, age=30, profession=&#x27;Engineer&#x27;)</span><br></pre></td></tr></table></figure><p>By simply adding the @dataclass decorator, Python automatically generates the <strong>init</strong> method, <strong>repr</strong> method, and various other methods behind the scenes. It also provides default implementations for comparison methods such as <strong>eq</strong>, <strong>ne</strong>, <strong>lt</strong>, <strong>gt</strong>, <strong>le</strong>, and <strong>ge</strong>.</p><p>You can further customize the behavior of the dataclass by using additional class-level decorators or by providing explicit type annotations, default values, and other options within the class definition. The dataclasses module offers additional features to enhance the functionality of dataclass, such as ordering, immutability, and more.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Essential Libraries and Frameworks for Machine Learning and AI Beginners in Python</title>
      <link href="2023/07/06/essential-libraries-and-framework-for-ai-beginners/"/>
      <url>2023/07/06/essential-libraries-and-framework-for-ai-beginners/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h2><p>Machine learning has revolutionized industries by enabling data analysis, prediction, and complex problem-solving. Python, with its extensive ecosystem of libraries and frameworks, has emerged as the language of choice for machine learning practitioners. In this blog, we will explore a comprehensive set of essential libraries, frameworks, packages, and tools that empower you to excel in machine learning using Python.</p><h2 id="The-list"><a href="#The-list" class="headerlink" title="The list"></a>The list</h2><ol><li><p>NumPy:<br>NumPy, the Numerical Python library, provides support for large, multi-dimensional arrays and matrices. It offers a wide range of mathematical functions and operations, making it essential for numerical computations in machine learning. NumPy serves as the foundation for many other libraries in the Python data science stack.</p></li><li><p>pandas:<br>pandas is a powerful data manipulation library that introduces DataFrames, which facilitate easy handling and analysis of structured data. It offers features for data cleaning, exploration, filtering, transformation, and more. pandas simplifies data preprocessing and integrates seamlessly with other libraries, making it indispensable for data scientists and machine learning practitioners.</p></li><li><p>scikit-learn:<br>scikit-learn, also known as sklearn, is a versatile and user-friendly machine learning library. It provides a vast array of algorithms for classification, regression, clustering, and dimensionality reduction. Additionally, scikit-learn offers tools for model selection, evaluation, and preprocessing. Its consistent API and comprehensive documentation make it a valuable resource for experimenting with different machine learning techniques.</p></li><li><p>TensorFlow:<br>Developed by Google, TensorFlow has gained significant popularity for building and training deep learning models. It offers a computational graph abstraction for executing complex machine learning algorithms efficiently on CPUs or GPUs. TensorFlow’s ecosystem includes TensorFlow Keras, a high-level API for neural network construction, and TensorFlow Hub, a repository of pre-trained models for transfer learning.</p></li><li><p>PyTorch:<br>PyTorch is a widely adopted deep learning framework that prioritizes flexibility and ease of use. It provides dynamic computational graphs, enabling easy model debugging and experimentation. With excellent GPU acceleration support, PyTorch offers powerful tools like torchvision for computer vision tasks and torchaudio for audio processing.</p></li><li><p>Keras:<br>Keras, a high-level neural networks API, can run on top of TensorFlow, Theano, or Microsoft Cognitive Toolkit (CNTK). It simplifies the process of building and training neural networks by providing a user-friendly interface and abstracting away low-level details. Keras is well-suited for rapid prototyping and is lauded for its simplicity and versatility.</p></li><li><p>XGBoost:<br>XGBoost is a scalable and efficient gradient boosting library used for classification and regression problems. It offers state-of-the-art algorithms and has become a go-to choice for winning solutions in various machine learning competitions. XGBoost’s ability to handle large datasets and its focus on optimization make it a crucial tool for boosting ensemble models.</p></li><li><p>LightGBM:<br>LightGBM is another high-performance gradient boosting framework that excels in handling large datasets. It is known for its fast training speed and low memory usage, making it ideal for dealing with high-dimensional data. LightGBM provides excellent support for categorical features and offers advanced features like early stopping and parallel learning.</p></li><li><p>CatBoost:<br>CatBoost is a gradient boosting framework that is particularly effective in handling categorical features. It automatically handles the encoding of categorical variables, reducing the need for manual preprocessing. CatBoost’s robust handling of missing values and advanced features like ordered boosting make it a valuable addition to your machine learning toolbox.</p></li><li><p>NLTK:<br>The Natural Language Toolkit (NLTK) is a library specifically designed for natural language processing (NLP). It provides a wide range of functionalities for tokenization, stemming, lemmatization, part-of-speech tagging, and more. NLTK also includes various corpora and lexical resources, making it an indispensable tool for NLP tasks in machine learning.</p></li><li><p>OpenCV:<br>OpenCV (Open Source Computer Vision Library) is a popular library for computer vision tasks. It offers a wide range of functions and algorithms for image and video processing, object detection and recognition, and feature extraction. OpenCV is widely used in machine learning projects involving computer vision and image analysis.</p></li><li><p>Jupyter Notebook:<br>Jupyter Notebook is an interactive web-based environment that allows you to create and share documents containing live code, visualizations, and explanatory text. It is widely used in the machine learning community for experimentation, prototyping, and sharing code. Jupyter Notebook promotes an iterative and collaborative workflow, making it an essential tool for machine learning practitioners.</p></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Python’s rich ecosystem of libraries, frameworks, packages, and tools has significantly contributed to the popularity and success of machine learning. NumPy, pandas, scikit-learn, TensorFlow, PyTorch, Keras, XGBoost, LightGBM, CatBoost, NLTK, OpenCV, and Jupyter Notebook are among the essential components for any aspiring machine learning practitioner. By leveraging these resources, you can dive into the world of data science, solve complex problems, and unlock the full potential of your data. Remember to stay updated with the latest advancements in the field and continuously expand your knowledge to thrive in the ever-evolving realm of machine learning.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Power of Vector Embedding and Unlock Semantic Understanding in AI Chatbot Development</title>
      <link href="2023/07/05/vector-embedding-semantic-power-of-ai/"/>
      <url>2023/07/05/vector-embedding-semantic-power-of-ai/</url>
      
        <content type="html"><![CDATA[<p>In the realm of natural language processing, the ability to understand and extract meaning from text has always been a significant challenge. However, recent advancements in language modeling and vector embedding techniques have revolutionized the way machines comprehend language. Vector embedding allows us to represent words, phrases, and even entire texts as multidimensional vectors, capturing semantic meaning and enabling more effective language processing. In this blog post, we will explore the concept of vector embedding, its ability to capture semantic information, and how it works in conjunction with large language models to improve question-answering capabilities.</p><h2 id="What-is-Vector-Embedding"><a href="#What-is-Vector-Embedding" class="headerlink" title="What is Vector Embedding?"></a>What is Vector Embedding?</h2><p>Vector embedding, also known as word embedding or word vector representation, is a technique used to map words or textual elements to numerical vectors in a high-dimensional space. Each dimension of the vector represents a particular feature or property associated with the word or text. The underlying principle behind vector embedding is that similar words or texts should be represented by vectors that are close to each other in this high-dimensional space.</p><h2 id="Capturing-Semantic-Meaning"><a href="#Capturing-Semantic-Meaning" class="headerlink" title="Capturing Semantic Meaning"></a>Capturing Semantic Meaning</h2><p>One of the main advantages of vector embedding is its ability to capture semantic meaning. Traditional language processing models often struggle to capture the contextual and semantic nuances of words and phrases, resulting in limited understanding of the underlying meaning. However, vector embedding provides a solution by representing words in a dense, continuous vector space where similar words are positioned close together.</p><p>By utilizing techniques like word2vec, GloVe, or BERT, vector embedding models can train on large amounts of text data to learn these vector representations. During the training process, the models consider the co-occurrence patterns of words and learn to assign similar vectors to words that share similar contexts. As a result, words with similar meanings or semantic relationships end up closer to each other in the vector space.</p><h2 id="Enhancing-Language-Models"><a href="#Enhancing-Language-Models" class="headerlink" title="Enhancing Language Models"></a>Enhancing Language Models</h2><p>Vector embedding plays a crucial role in enhancing the capabilities of large language models like GPT-3.5. These language models leverage the power of contextual word embeddings, where each word’s embedding is influenced by its surrounding context. By incorporating vector embeddings, language models gain a deeper understanding of the meaning of words within the context of a given sentence or text.</p><p>When presented with a question or query, a language model utilizing vector embedding can quickly narrow down the relevant passages or texts that are likely to contain the answer. By comparing the vector representations of the question with those of the available texts, the model can identify the most semantically similar texts, reducing the search space and improving the accuracy of the answer retrieval process.</p><p>Furthermore, vector embeddings allow language models to perform various semantic operations. For instance, by performing vector arithmetic, such as subtracting the vector representation of “king” from “man” and adding “woman,” the resulting vector representation would be close to the vector representation of “queen.” This ability to capture semantic relationships enables the model to provide more insightful and accurate responses.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Vector embedding has emerged as a powerful tool in natural language processing, facilitating the understanding of textual data and enhancing the performance of language models. By capturing semantic meaning and representing words as numerical vectors, vector embedding enables more effective language processing and helps narrow down relevant texts when answering questions.</p><p>As the field of natural language processing continues to evolve, vector embedding techniques will likely play an increasingly vital role in advancing language understanding and improving the accuracy and relevance of machine-generated responses. With ongoing research and advancements, we can expect vector embedding to contribute significantly to the development of more sophisticated language models capable of comprehending language with greater precision and nuance.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
            <tag> vector embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>the Water Thirst of AI, The Environmental Cost of Large Language Models</title>
      <link href="2023/07/04/ai-is-thirsty-one-conversation-with-chatgpt-equals-one-bottle-of-water/"/>
      <url>2023/07/04/ai-is-thirsty-one-conversation-with-chatgpt-equals-one-bottle-of-water/</url>
      
        <content type="html"><![CDATA[<p>Large language models such as OpenAI’s ChatGPT and Google’s Bard are known for their power and versatility. However, these models come at a significant cost in terms of energy consumption and water usage. Recent research reveals that training the GPT-3 model alone required 185,000 gallons (700,000 liters) of water, equivalent to filling a nuclear reactor’s cooling tower. This raises concerns about the impact on water supplies, particularly during times of drought and environmental uncertainty in the US.</p><p>A study conducted by the University of California Riverside and the University of Texas Arlington sheds light on the water consumption of AI models. It highlights the distinction between water “withdrawal” and “consumption.” While withdrawal refers to physically taking water from sources like rivers or lakes, consumption refers to the evaporation of water in data centers, which cannot be recycled.</p><p>To keep server rooms cool, data centers rely on cooling towers that consume substantial amounts of water. Approximately one gallon of water is consumed for every kilowatt-hour expended in an average data center. Freshwater is necessary to prevent corrosion and ensure humidity control. Moreover, the electricity consumed by data centers contributes to off-site indirect water consumption.</p><p>The water consumption issue is not limited to OpenAI and AI models. In 2019, Google requested over 2.3 billion gallons of water for its data centers in three states. Google’s LaMDA and Bard models, which are even larger than GPT-3, may require millions of liters of water for training. Additionally, the energy requirements of these large language models are staggering, with GPT-3 alone releasing 502 metric tons of carbon during training.</p><p>The concern over water usage is amplified by climate change and worsening droughts. The World Economic Forum estimates that millions of US residents lack access to water and adequate water systems. Climate change and population growth are expected to exacerbate water scarcity issues. Rising temperatures and severe droughts in the American West have already had a significant impact on freshwater supplies.</p><p>To address these concerns, AI companies and data centers can take steps to improve water efficiency. Training AI models at cooler times or in more water-efficient data centers could help reduce water usage. Chatbot users could also engage with the models during “water-efficient hours.” However, achieving these changes requires greater transparency from tech companies regarding where and when the models are trained.</p><p>In conclusion, the water footprint of AI models is a pressing issue that needs to be addressed. Transparency, efficient data center practices, and user awareness can contribute to mitigating the impact on water resources, especially as AI becomes more pervasive across various sectors.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to access multiple github accounts from one computer</title>
      <link href="2023/07/03/how-to-access-multiple-git-accounts-in-one-computer/"/>
      <url>2023/07/03/how-to-access-multiple-git-accounts-in-one-computer/</url>
      
        <content type="html"><![CDATA[<p>If you simply copy your ssh key from one github account to another github account, you will find it’s not working. And After you click add the ssh key, it won’t appear under the ssh key list.</p><p>So to access multiple git accounts in one computer, you will need to this.</p><p>SSH allows you to create multiple SSH keys and specify which SSH key to use for which server or host. This can be achieved by setting the following steps:</p><ol><li><p>Generate a new SSH key for each Git account using the command</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -C &quot;your-email@example.com&quot; </span><br></pre></td></tr></table></figure><p>Replace “<a href="mailto:&#x79;&#x6f;&#117;&#114;&#45;&#101;&#x6d;&#x61;&#x69;&#108;&#64;&#x65;&#x78;&#97;&#109;&#112;&#x6c;&#x65;&#x2e;&#99;&#111;&#109;">&#x79;&#x6f;&#117;&#114;&#45;&#101;&#x6d;&#x61;&#x69;&#108;&#64;&#x65;&#x78;&#97;&#109;&#112;&#x6c;&#x65;&#x2e;&#99;&#111;&#109;</a>“ with the email you used for your Git account. When asked for a file to save the key, choose a location that does not already contain an SSH key (for example: <code>~/.ssh/id_rsa_new</code>).</p></li><li><p>After creating the keys, you need to add them to <code>ssh-agent</code>. This is done using the ssh-add command, for example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ssh-add ~/.ssh/id_rsa_new</span><br></pre></td></tr></table></figure></li><li><p>Then you will create or modify the config file inside the .ssh directory (Create it if it doesn’t exist)</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nano ~/.ssh/config</span><br></pre></td></tr></table></figure></li><li><p>Inside this file, you will define each host with its respective SSH key. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Account 1</span><br><span class="line">Host github.com-account1</span><br><span class="line">    HostName github.com</span><br><span class="line">    User git</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_account1</span><br><span class="line"></span><br><span class="line"># Account 2</span><br><span class="line">Host github.com-account2</span><br><span class="line">   HostName github.com</span><br><span class="line">    User git</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_account2</span><br></pre></td></tr></table></figure></li><li><p>Afterward, we have to do the cloning in a different way to use specific ssh key via git. for example:</p> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone git@github.com-account1:username1/repo.git</span><br><span class="line">git clone git@github.com-account2:username2/repo.git</span><br></pre></td></tr></table></figure></li><li><p>Lastly, go to your Git account settings. Find “SSH and GPG keys” and click “New SSH key”. Copy the content of your public ssh key (e.g., id_rsa_new.pub) and paste it here.</p></li></ol><p>By doing this, you can work with multiple Git accounts on the same machine. Remember to replace “account1” and “account2”, “username1” and “username2” with your actual account usernames.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github </tag>
            
            <tag> ssh-keygen </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Beginner&#39;s Guide to Tuning Hyperparameters of Popular Machine Learning Algorithms</title>
      <link href="2023/07/02/beginner-guide-how-to-set-hyper-prameters-for-popular-machine-learning-algorithms/"/>
      <url>2023/07/02/beginner-guide-how-to-set-hyper-prameters-for-popular-machine-learning-algorithms/</url>
      
        <content type="html"><![CDATA[<p>Machine Learning is a rapidly evolving field with algorithms that often require hyperparameters fine-tuning to improve model performance. Understanding the red lines and rules of thumb for tuning these hyperparameters is essential. Let’s explore some popular machine learning algorithms and the hyperparameters that govern them.</p><ol><li><p><strong>Random Forest</strong></p><ul><li><p><strong>n_estimators</strong>: This represents the number of trees in the forest. Generally, a higher number improves the model and makes the predictions stronger and more stable, but a very high number can result in longer computational time. Starting point can be 100.</p></li><li><p><strong>max_features</strong>: It represents the number of features to consider when looking for the best split. Good starting points can be ‘auto’, ‘sqrt’ or ‘log2’.</p></li><li><p><strong>max_depth</strong>: The maximum depth of the tree. You can leave this value as None resulting in full expansion of trees.</p></li></ul></li><li><p><strong>Boosting Algorithms (XGBoost, LightGBM, CatBoost)</strong></p><ul><li><p>All the three boosting variants share some common hyperparameters like,</p></li><li><p><strong>n_estimators</strong>: The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. A reasonable starting point could be 100.</p></li><li><p><strong>learning_rate (or eta)</strong>: It makes the model more robust by shrinking the weights on each step. A smaller value might require higher number of boosting rounds hence, increased computation. You can start with values like 0.1, 0.01 or so on.</p></li><li><p><strong>max_depth</strong>: The maximum tree depth for boosting models. Starting points could be 6 to 10.</p></li><li><p><strong>subsample</strong>: The fraction of observations to be randomly selected for each tree. Lower values make the algorithm conservative and prevents overfitting. Usually set at 0.8 to 1.</p></li></ul></li></ol><p>3.<strong>Transformer Models</strong></p><p>   Transformers rely more on architecture selection than hyperparameter tuning. You may however tune the following:</p><ul><li><strong>Learning Rate</strong>: Choose this too large and optimization might overshoot and diverge - too small, and it might never reach the minimum. 0.0001 could be a reasonable start.</li><li><strong>Batch Size</strong>: Influences the noise in the gradient estimate, and the computational requirements of the algorithm. Try powers of 2 that fit into memory (e.g., 32, 64, 128).</li><li><strong>Number of Layers (for architectures like BERT, etc)</strong>: Depending on the complexity of the task, increasing the number of layers may increase model understanding but beware of overfitting.</li></ul><p>Hyperparameter tuning can be a bit of an art and may require a bit of trial and error. Remember, as a rule of thumb, changing one hyperparameter can affect the behavior of the algorithm, so we should consider the interaction of parameters to achieve the best results.</p><p>Machine learning libraries like Scikit-learn provide handy tools like GridSearch and RandomizedSearch for hyperparameters optimization. AutoML, Bayesian Optimization, and Genetic Algorithms are also gaining popularity for hyperparameter tuning.</p><p>We hope this beginner-friendly guide serves as a useful starting point for your machine learning journey.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Importing and Calling Class Functions in Python</title>
      <link href="2023/06/29/how-to-import-python-class-functions-from-another-folder/"/>
      <url>2023/06/29/how-to-import-python-class-functions-from-another-folder/</url>
      
        <content type="html"><![CDATA[<p>Working with classes and functions in different files or folders is a common scenario in Python programming. In this blog, we will explore how to import and call class functions in two different cases: static/non-static functions. We will cover the steps required to import and use functions from a class defined in another folder. Let’s dive in!</p><h2 id="Case-1-Calling-a-Static-Function-from-a-Class"><a href="#Case-1-Calling-a-Static-Function-from-a-Class" class="headerlink" title="Case 1: Calling a Static Function from a Class:"></a>Case 1: Calling a Static Function from a Class:</h2><p>A static function belongs to the class itself and can be accessed without creating an instance of the class. Here’s how you can call a static function from a class defined in another folder:</p><p>Step 1: Ensure the containing folder is a Python module by adding an <code>__init__.py</code> file if needed.</p><p>Step 2: Determine the absolute path of the module you want to import. Let’s say the folder is named “my_module” and the file inside it is “my_class.py”. The absolute path would be <code>my_module.my_class</code>.</p><p>Step 3: In your Python script, import the desired static function from the class. For example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> my_module.my_class <span class="keyword">import</span> my_static_function</span><br></pre></td></tr></table></figure><p>Step 4: Now, you can directly call the imported static function in your code:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_static_function()</span><br></pre></td></tr></table></figure><h2 id="Case-2-Calling-a-Non-Static-Function-from-a-Class"><a href="#Case-2-Calling-a-Non-Static-Function-from-a-Class" class="headerlink" title="Case 2: Calling a Non-Static Function from a Class:"></a>Case 2: Calling a Non-Static Function from a Class:</h2><p>A non-static function is associated with an instance of the class. To call such a function, we need to create an instance first. Follow these steps to import and use a non-static function from a class defined in another folder:</p><p>Step 1: Ensure the folder is a Python module by adding an <code>__init__.py</code> file if required.</p><p>Step 2: Determine the absolute path of the module you want to import. Let’s say the folder is named “my_module” and the file inside it is “my_class.py”. The absolute path would be <code>my_module.my_class</code>.</p><p>Step 3: In your Python script, import the class from the module:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> my_module.my_class <span class="keyword">import</span> MyClass</span><br></pre></td></tr></table></figure><p>Step 4: Create an instance of the class:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_instance = MyClass()</span><br></pre></td></tr></table></figure><p>Step 5: Now, you can call the non-static function using the instance created:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_instance.my_non_static_function()</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>In this blog post, we discussed how to import and call class functions in Python from a class defined in another folder. For static functions, you can directly import and call them without creating an instance. However, for non-static functions, the class needs to be instantiated first. By following these steps, you can effectively work with class functions across multiple files or directories in your Python projects.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pythonl </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Power of Open Source LLMs, Empowering Private Conversations and Enhanced Chatbot Capabilities</title>
      <link href="2023/06/28/open-source-llm-and-enchanced-chatbot-capabilities/"/>
      <url>2023/06/28/open-source-llm-and-enchanced-chatbot-capabilities/</url>
      
        <content type="html"><![CDATA[<p>In today’s digital landscape, safeguarding our data and maintaining control over our interactions has become increasingly important. The concept of private LLMs (Large Language Models) has gained significant traction, offering a secure and autonomous approach to accessing and utilizing LLMs without relying on third-party intermediaries. Moreover, the open source community has been actively involved in training LLMs, resulting in a plethora of options for individuals and businesses alike. In this blog post, we will delve into the world of open source LLMs and explore how instruction tuning has revolutionized the development of powerful and versatile chatbot-style models.</p><h2 id="Exploring-Open-Source-LLMs"><a href="#Exploring-Open-Source-LLMs" class="headerlink" title="Exploring Open Source LLMs:"></a>Exploring Open Source LLMs:</h2><p>The open source community has been instrumental in advancing the availability and accessibility of LLMs for public use. Notable examples include Meta’s LLaMA series, EleutherAI’s Pythia series, Berkeley AI Research’s OpenLLaMA model, and MosaicML. These projects have garnered significant attention, as demonstrated by the 24K stars earned by the popular GitHub repository called PrivateGPT. With such vibrant activity in the field, businesses now have the opportunity to leverage open source LLMs to process vast amounts of data while retaining control over their information.</p><h2 id="Introducing-GPT4All-Unlocking-Commercial-Potential"><a href="#Introducing-GPT4All-Unlocking-Commercial-Potential" class="headerlink" title="Introducing GPT4All: Unlocking Commercial Potential:"></a>Introducing GPT4All: Unlocking Commercial Potential:</h2><p>Among the array of open source LLMs, GPT4All stands out as a popular and commercially licensed option. Unlike certain models, such as Meta’s Llama, which are limited to non-commercial research use, GPT4All offers full licensing for commercial integration. This flexibility enables developers to incorporate GPT4All into commercial products without any concerns, making it an attractive choice for businesses seeking to capitalize on the power of LLMs.</p><h2 id="Instruction-Tuning-Unleashing-Chatbot-Style-Capabilities"><a href="#Instruction-Tuning-Unleashing-Chatbot-Style-Capabilities" class="headerlink" title="Instruction Tuning: Unleashing Chatbot-Style Capabilities:"></a>Instruction Tuning: Unleashing Chatbot-Style Capabilities:</h2><p>Traditionally, LLMs were primarily trained to predict the next sequence of words statistically, rather than being explicitly optimized for conversational interactions. However, through extensive training on large datasets, LLMs have exhibited emergent abilities, enabling them to generate more sophisticated responses than initially anticipated. Nonetheless, it was discovered that increasing the size of language models alone does not guarantee alignment with users’ intent or improved performance.</p><p>In 2022, a breakthrough approach to developing highly capable chatbot-style LLMs emerged: instruction tuning. By fine-tuning a base model using question-and-answer-style prompts that emulate user interactions, developers achieved comparable or even superior performance to models trained on massive datasets. This method allows for the use of a smaller base model, combined with targeted instruction tuning, to achieve remarkable chatbot-style capabilities.</p><h2 id="GPT4All-A-Concrete-Example"><a href="#GPT4All-A-Concrete-Example" class="headerlink" title="GPT4All: A Concrete Example:"></a>GPT4All: A Concrete Example:</h2><p>Let’s examine GPT4All, a prime example that illustrates the power of instruction tuning. GPT4All-J-v1.0, available on Hugging Face, has been fine-tuned based on GPT-J—a model from EleutherAI trained on six billion parameters. Compared to ChatGPT’s massive 175 billion parameters, GPT-J appears minuscule. However, the pivotal aspect lies in the type of data these models are trained on and the differences it makes in their performance.</p><h2 id="GPT-J-vs-GPT4All-J-Unveiling-the-Distinctions"><a href="#GPT-J-vs-GPT4All-J-Unveiling-the-Distinctions" class="headerlink" title="GPT-J vs. GPT4All-J: Unveiling the Distinctions:"></a>GPT-J vs. GPT4All-J: Unveiling the Distinctions:</h2><p>GPT-J relies on the Pile dataset, an 825 GB collection of information, for its training. While it excels at predicting the next words in a text string using statistical methods, its proficiency in Q&amp;A-style interactions is limited. In contrast, GPT4All employs a much more compact dataset, totaling less than 1 GB, designed in a question-and-answer format. This deliberate focus on instruction tuning enhances GPT4All’s Q&amp;A-style capabilities, making it a more adept and versatile chatbot.</p><h2 id="Harnessing-the-Power-of-GPT4All"><a href="#Harnessing-the-Power-of-GPT4All" class="headerlink" title="Harnessing the Power of GPT4All:"></a>Harnessing the Power of GPT4All:</h2><p>By utilizing GPT-J as the pretrained model and employing instruction tuning with a smaller, targeted dataset, GPT4All emerges as a formidable Q&amp;A-style chatbot. This approach leverages the strengths of the base model while fine-tuning it to excel in conversational interactions. The result is a chatbot that can understand and respond to user queries effectively, surpassing the limitations of its initial training.</p><h2 id="The-Advantages-of-Open-Source-LLMs"><a href="#The-Advantages-of-Open-Source-LLMs" class="headerlink" title="The Advantages of Open Source LLMs:"></a>The Advantages of Open Source LLMs:</h2><p>The availability of open source LLMs brings forth numerous benefits for developers and businesses alike. Firstly, the open source community fosters collaboration and knowledge sharing, enabling continuous improvements and innovations. Developers can tap into existing open source LLMs, such as GPT4All, and build upon them to create tailored solutions for their specific needs. Moreover, the cost advantages associated with open source models make them an attractive option for businesses looking to leverage LLMs without significant financial investments.</p><h2 id="Privacy-and-Control-Safeguarding-Data"><a href="#Privacy-and-Control-Safeguarding-Data" class="headerlink" title="Privacy and Control: Safeguarding Data:"></a>Privacy and Control: Safeguarding Data:</h2><p>One of the most appealing aspects of private LLMs is the assurance of data privacy and control. By operating their own LLMs, individuals and organizations can query and process information without relying on third-party intermediaries. This level of control fosters a sense of security and ensures that sensitive data remains within the confines of the user’s infrastructure. Open source LLMs align perfectly with this vision, as they provide the means to create and manage private LLM instances, enhancing data security and privacy.</p><h2 id="The-Future-of-Open-Source-LLMs"><a href="#The-Future-of-Open-Source-LLMs" class="headerlink" title="The Future of Open Source LLMs:"></a>The Future of Open Source LLMs:</h2><p>As the field of open source LLMs continues to thrive, we can expect even more exciting developments on the horizon. The collaborative nature of the open source community ensures ongoing research, innovation, and improvements in LLM training methodologies. Instruction tuning, in particular, has opened up new possibilities for developing highly capable chatbot-style models with relatively smaller datasets. With each advancement, open source LLMs become increasingly accessible and powerful, revolutionizing the way we interact with AI-driven conversational systems.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Open source LLMs offer a gateway to private and secure conversations while empowering developers to create advanced chatbot-style models. Through instruction tuning, models like GPT4All showcase the remarkable capabilities that can be achieved with targeted fine-tuning. As the open source community continues to drive progress in LLM development, the future holds immense potential for more sophisticated and versatile chatbot experiences. By embracing the power of open source LLMs, we can unlock new realms of AI-driven conversational interactions while safeguarding our data and retaining control over our digital experiences.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Transformer Models in Deep Learning, Explaining with Salience Maps</title>
      <link href="2023/06/27/understand-and-explian-pytorch-and-transformer-models/"/>
      <url>2023/06/27/understand-and-explian-pytorch-and-transformer-models/</url>
      
        <content type="html"><![CDATA[<p>Deep learning models, especially transformer models, have achieved remarkable success in various natural language processing tasks. However, understanding how these models make predictions can be challenging. In this blog post, we will delve into methods for explaining transformer models and focus on one specific technique: generating salience maps.</p><h2 id="1-Explaining-Transformer-Models"><a href="#1-Explaining-Transformer-Models" class="headerlink" title="1. Explaining Transformer Models:"></a>1. Explaining Transformer Models:</h2><p>Transformer models revolutionized the field of natural language processing (NLP) with their attention mechanisms. Before explaining the details of salience maps, let’s explore a few general methods for understanding transformer models.</p><p>a) Attention Visualization:<br>Visualization of attention weights allows us to identify the input regions that receive higher importance. By observing these attention weights, we gain insights into what the model focuses on during processing.</p><p>b) Gradient-based Attribution Methods:<br>These methods calculate the gradients of model predictions with respect to input tokens. The gradient information helps attribute importance scores to individual tokens, indicating their contribution to the model’s output.</p><p>c) Influence Functions:<br>The influence function approach analyzes how model predictions change when inputs are modified. By identifying influential inputs, we gain a better understanding of the model’s behavior and can debug or optimize it accordingly.</p><h2 id="2-Salience-Maps-Capturing-Token-Importance"><a href="#2-Salience-Maps-Capturing-Token-Importance" class="headerlink" title="2. Salience Maps: Capturing Token Importance:"></a>2. Salience Maps: Capturing Token Importance:</h2><p>One method for explaining transformer models is generating salience maps. Salience maps highlight the most important tokens in the input sequence that contribute significantly to the model’s predictions. Let’s explore the steps involved in obtaining salience maps.</p><p>a) Loading the Model and Tokenizer:<br>To generate salience maps, we first load the pre-trained transformer model and its corresponding tokenizer. These components are essential for preprocessing the input text and executing the model.</p><p>b) Preprocessing the Input:<br>Text data is tokenized and encoded as numerical sequences suitable for the transformer model. Proper preprocessing ensures compatibility between the input and the transformer’s expectations.</p><p>c) Using Integrated Gradients:<br>Integrated Gradients is a popular gradient-based attribution method. By computing the gradients of the model’s predictions with respect to input tokens, Integrated Gradients assigns importance scores to each token. These scores indicate how much a token contributes to the model’s output.</p><p>d) Normalizing and Visualizing the Salience Weights:<br>The importance scores obtained from Integrated Gradients are normalized to ensure they sum up to 1. By multiplying these scores with the corresponding token embeddings, we obtain the salience weights. These weights are then visualized as a heatmap overlaid on the input sequence, highlighting the most important regions.</p><h2 id="Example-Code"><a href="#Example-Code" class="headerlink" title="Example Code:"></a>Example Code:</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoTokenizer, AutoModelForSequenceClassification</span><br><span class="line"><span class="keyword">from</span> captum.attr <span class="keyword">import</span> IntegratedGradients</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the pre-trained tokenizer and model</span></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a sample input sequence</span></span><br><span class="line">input_text = <span class="string">&quot;The movie was great, I really enjoyed it.&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tokenize the input text and encode it as a tensor</span></span><br><span class="line">input_ids = torch.tensor(tokenizer.encode(input_text, add_special_tokens=<span class="literal">True</span>)).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define the baseline input tensor for the Integrated Gradients method</span></span><br><span class="line">baseline_ids = torch.zeros_like(input_ids)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate the Integrated Gradients method</span></span><br><span class="line">integrated_grads = IntegratedGradients(model)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the importance scores for each token in the input sequence</span></span><br><span class="line">attributions, _ = integrated_grads.attribute(inputs=input_ids, baselines=baseline_ids, </span><br><span class="line">                                              target=<span class="number">0</span>, return_convergence_delta=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Normalize the attributions to sum to 1 and convert them to numpy arrays</span></span><br><span class="line">salience_weights = attributions[<span class="number">0</span>].detach().numpy() / <span class="built_in">sum</span>(attributions[<span class="number">0</span>]).detach().numpy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract the list of tokens from the input sequence</span></span><br><span class="line">tokens = tokenizer.convert_ids_to_tokens(input_ids[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot the salience weights as a heatmap overlaid on the input sequence</span></span><br><span class="line">sns.<span class="built_in">set</span>()</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">5</span>))</span><br><span class="line">sns.heatmap([salience_weights], annot=[tokens], cmap=<span class="string">&quot;Blues&quot;</span>, fmt=<span class="string">&quot;&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Salience Map for Input Text&quot;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Tokens&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Salience Weights&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Understanding the inner workings of transformer models is crucial for their interpretability and trustworthiness. Explaining transformer models using methods like attention visualization, gradient-based attribution, influence functions, and salience maps helps shed light on their decision-making processes. Salience maps, in particular, offer intuitive insights by highlighting the important tokens in the input. By leveraging techniques like Integrated Gradients, we can generate salience maps and gain a better understanding of how transformer models process and interpret language.</p><p>In conclusion, generating salience maps and employing other methods to explain transformer models enhance their interpretability and enable more robust analysis and improvement. These techniques contribute to the broader field of explainable AI, promoting transparency and trust in deep learning models.</p><p>References:</p><ul><li><a href="https://huggingface.co/transformers/">HuggingFace Transformers</a></li><li><a href="https://captum.ai/">Captum - Model Interpretability Library</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> explainability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Convolution, From Mathematics to Image Processing</title>
      <link href="2023/06/26/understand-convolution-in-math-and-in-machine-learning/"/>
      <url>2023/06/26/understand-convolution-in-math-and-in-machine-learning/</url>
      
        <content type="html"><![CDATA[<p>Convolution is a powerful mathematical operation that plays a crucial role in various fields, including signal processing, mathematics, and particularly image processing. Whether you’re interested in diving deeper into the mathematics behind convolution or exploring its applications in convolutional neural networks (CNNs) for image processing, this blog post will provide you with a comprehensive understanding of this fundamental concept.</p><h2 id="What-is-Convolution"><a href="#What-is-Convolution" class="headerlink" title="What is Convolution?"></a>What is Convolution?</h2><p>At its core, convolution is a mathematical operation that combines two functions to produce a third function. In the context of continuous functions, the convolution of functions f(t) and g(t) is defined as the integral of the product of f(t) and a time-reversed and shifted version of g(t). This operation is denoted as (f * g)(t). Visualizing convolution as a sliding and overlapping summation or integration helps grasp its essence.</p><h2 id="Convolution-Properties"><a href="#Convolution-Properties" class="headerlink" title="Convolution Properties:"></a>Convolution Properties:</h2><p>Convolution exhibits several important properties. It is commutative, meaning the order of the functions being convolved does not affect the result. It is also associative, enabling the grouping of functions without impacting the outcome. Additionally, convolution distributes over addition or subtraction, which is known as the distributive property.</p><h2 id="Convolution-Example-Uniform-Function-with-Gaussian"><a href="#Convolution-Example-Uniform-Function-with-Gaussian" class="headerlink" title="Convolution Example: Uniform Function with Gaussian"></a>Convolution Example: Uniform Function with Gaussian</h2><p>Let’s consider an example of convolving a uniform function with a Gaussian function. The uniform function, denoted as u(t), is defined as follows:<br>u(t) = 1, for -a/2 ≤ t ≤ a/2<br>u(t) = 0, otherwise</p><p>The Gaussian function, denoted as g(t), can be expressed as:<br>g(t) = (1/σ√(2π)) * e^(-t^2/(2σ^2))</p><p>To convolve the uniform function with the Gaussian, we perform the convolution integral:<br>(f * g)(t) = ∫[from -∞ to +∞] u(τ) * g(t - τ) dτ</p><p>The resulting function will depend on the specific values of ‘a’ and ‘σ’ and the range of integration. For example, if we assume a = 2 and σ = 1, the convolution might result in a smoothed version of the original uniform function. The resulting function will have a smoother transition at the edges and gradually decrease towards zero outside the range of the uniform function.</p><h2 id="Convolution-in-CNNs"><a href="#Convolution-in-CNNs" class="headerlink" title="Convolution in CNNs:"></a>Convolution in CNNs:</h2><p>In the realm of image processing and CNNs, convolution is a fundamental operation. CNNs utilize convolutional layers to process images and extract meaningful features. In simple terms, convolution in CNNs involves sliding small-sized filters, also known as kernels, over an input image or feature map.</p><p>Each filter applies element-wise multiplications between its weights and the corresponding pixel values in its receptive field. These products are then summed up to produce a single value, which represents the activation at that specific location. By sliding the filter over the entire input, a new feature map is generated, capturing spatial information and higher-level features.</p><p>Convolutional layers in CNNs are typically followed by activation functions, such as ReLU, to introduce non-linearity, and other operations like pooling or additional convolutional layers in deep networks. The parameters (weights) of the convolutional filters are learned through a training process, enabling the network to adapt and recognize patterns and objects in images effectively.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> convolution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to evaluate and visualize regression results</title>
      <link href="2023/06/24/how-to-evaluate-and-visualize-regression/"/>
      <url>2023/06/24/how-to-evaluate-and-visualize-regression/</url>
      
        <content type="html"><![CDATA[<p>When evaluating regression models, there are several metrics you can use to assess their performance beyond just Mean Squared Error (MSE) or Root Mean Squared Error (RMSE). Here are some commonly used evaluation metrics for regression:</p><ul><li><p>Mean Absolute Error (MAE): This metric measures the average absolute difference between the predicted and actual values. It provides a measure of the model’s average prediction error.</p></li><li><p>R-squared (R²) or Coefficient of Determination: R-squared indicates the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features). It ranges from 0 to 1, where 1 indicates a perfect fit.</p></li><li><p>Mean Squared Logarithmic Error (MSLE): MSLE measures the average logarithmic error between the predicted and actual values. It can be useful when the target variable has exponential growth.</p></li><li><p>Explained Variance Score: This score measures the proportion of variance explained by the model. It ranges from 0 to 1, where 1 indicates a perfect fit.</p></li><li><p>Median Absolute Error (MedAE): Similar to MAE, this metric calculates the median absolute difference between the predicted and actual values. It is less sensitive to outliers compared to MAE.</p></li><li><p>R-squared Adjusted (Adjusted R²): Adjusted R-squared takes into account the number of predictors in the model. It penalizes the addition of unnecessary variables and helps avoid overfitting.</p></li><li><p>Mean Percentage Error (MPE): This metric calculates the average percentage difference between the predicted and actual values. It is useful when you want to understand the relative error of the model’s predictions.</p></li><li><p>Mean Absolute Percentage Error (MAPE): MAPE calculates the average percentage difference between the predicted and actual values, similar to MPE. It is commonly used in time series forecasting.</p></li><li><p>Quantile Loss: Quantile loss measures the accuracy of predicting specific quantiles of the target variable. It provides information about the model’s performance across different levels of uncertainty.</p></li></ul><p>Visualization techniques for regression evaluation can include:</p><ul><li><p>Scatter plots: Plotting the predicted values against the actual values can help visualize the overall performance of the model. Ideally, the points should lie close to a diagonal line.</p></li><li><p>Residual plots: Residuals are the differences between the predicted and actual values. Plotting the residuals against the predicted values or the independent variables can help identify patterns or heteroscedasticity (unequal variance).</p></li><li><p>Distribution plots: Comparing the distribution of predicted values with the actual values can provide insights into the model’s accuracy and whether it is capturing the underlying data distribution.</p></li><li><p>Regression line plot: Visualizing the regression line along with the data points can help understand the relationship between the independent and dependent variables.</p></li></ul><p>Following code shows some example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.linear_model import LinearRegression</span><br><span class="line">from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score</span><br><span class="line"></span><br><span class="line"># Generate some random data for demonstration</span><br><span class="line">np.random.seed(42)</span><br><span class="line">X = np.random.rand(100, 1)  # Independent variable</span><br><span class="line">y = 2 + 3 * X + np.random.randn(100, 1)  # Dependent variable with some noise</span><br><span class="line"></span><br><span class="line"># Split the data into training and test sets</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)</span><br><span class="line"></span><br><span class="line"># Fit the regression model</span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"># Make predictions on the test set</span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Evaluate the model using different metrics</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line">mae = mean_absolute_error(y_test, y_pred)</span><br><span class="line">r2 = r2_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"># Calculate adjusted R-squared</span><br><span class="line">n = X_test.shape[0]  # Number of samples</span><br><span class="line">p = X_test.shape[1]  # Number of predictors</span><br><span class="line">adjusted_r2 = 1 - ((1 - r2) * (n - 1) / (n - p - 1))</span><br><span class="line"></span><br><span class="line"># Scatter plot of actual vs predicted values</span><br><span class="line">plt.scatter(y_test, y_pred)</span><br><span class="line">plt.xlabel(&#x27;Actual Values&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Predicted Values&#x27;)</span><br><span class="line">plt.title(&#x27;Actual vs Predicted Values&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"># Residual plot</span><br><span class="line">residuals = y_test - y_pred</span><br><span class="line">plt.scatter(y_pred, residuals)</span><br><span class="line">plt.xlabel(&#x27;Predicted Values&#x27;)</span><br><span class="line">plt.ylabel(&#x27;Residuals&#x27;)</span><br><span class="line">plt.title(&#x27;Residual Plot&#x27;)</span><br><span class="line">plt.axhline(y=0, color=&#x27;r&#x27;, linestyle=&#x27;-&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">print(&quot;Mean Squared Error (MSE):&quot;, mse)</span><br><span class="line">print(&quot;Mean Absolute Error (MAE):&quot;, mae)</span><br><span class="line">print(&quot;R-squared (R²):&quot;, r2)</span><br><span class="line">print(&quot;Adjusted R-squared:&quot;, adjusted_r2)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Loss Functions, Backpropagation, and the PyTorch Transformer</title>
      <link href="2023/06/22/common-loss-function-and-how-does-backpropgation-work-in-pytorch/"/>
      <url>2023/06/22/common-loss-function-and-how-does-backpropgation-work-in-pytorch/</url>
      
        <content type="html"><![CDATA[<p>In the field of deep learning, loss functions and the backpropagation algorithm play a crucial role in training neural network models. Additionally, frameworks like PyTorch provide powerful tools for implementing and training complex models, such as the Transformer. In this blog post, we will explore the concepts of loss functions and backpropagation, and discuss how the PyTorch Transformer leverages these techniques to achieve effective training.</p><h2 id="Loss-Functions"><a href="#Loss-Functions" class="headerlink" title="Loss Functions:"></a>Loss Functions:</h2><p>Loss functions quantify the discrepancy between predicted and actual values, guiding the optimization process. We delve into some commonly used loss functions in deep learning:</p><p>a) Mean Squared Error (MSE): Ideal for continuous target variables, MSE calculates the average squared difference between predictions and actual values.</p><p>b) Binary Cross-Entropy (BCE): Suited for binary classification tasks, BCE measures the dissimilarity between predicted probabilities and true binary labels.</p><p>c) Categorical Cross-Entropy (CCE): CCE is used for multi-class classification problems, calculating the dissimilarity between predicted class probabilities and one-hot encoded labels.</p><p>d) Kullback-Leibler Divergence (KL Divergence): Employed in scenarios where comparing two probability distributions is required, KL divergence quantifies the difference between distributions.</p><h2 id="Backpropagation"><a href="#Backpropagation" class="headerlink" title="Backpropagation:"></a>Backpropagation:</h2><p>Backpropagation is a fundamental algorithm used to train neural networks by iteratively propagating gradients from the output to the input layers. Here’s how it works:</p><p>a) Forward Pass: The input data flows through the network, and predictions are computed.</p><p>b) Loss Computation: The loss function evaluates the discrepancy between predictions and true values.</p><p>c) Backward Pass: Gradients are calculated by propagating errors from the output layer to the input layer, using the chain rule of calculus. The loss function’s differentiability is crucial for this step.</p><p>d) Parameter Updates: Once gradients are obtained, optimization algorithms like stochastic gradient descent (SGD) update the model parameters, minimizing the loss and improving the model’s performance.</p><h2 id="PyTorch-Transformer-and-Backpropagation"><a href="#PyTorch-Transformer-and-Backpropagation" class="headerlink" title="PyTorch Transformer and Backpropagation:"></a>PyTorch Transformer and Backpropagation:</h2><p>The PyTorch library provides extensive support for building and training neural networks, including the Transformer model. Here’s how the PyTorch Transformer incorporates backpropagation:</p><p>a) Differentiable Loss Functions: The Transformer model in PyTorch assumes the use of differentiable loss functions. Standard loss functions, such as cross-entropy for classification tasks or mean squared error for regression tasks, are compatible with backpropagation.</p><p>b) Automatic Differentiation: PyTorch’s computational graph framework enables automatic differentiation. When the loss function is defined using PyTorch’s tensor operations, the framework tracks the operations and builds a computational graph to compute gradients efficiently.</p><p>c) Gradient Calculation: The gradients are computed by backpropagating the errors through the computational graph. The gradients capture the sensitivity of the loss function with respect to the model parameters, allowing for gradient-based optimization.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Loss functions and backpropagation are vital components of deep learning, enabling the optimization and training of neural network models. In PyTorch, the Transformer model leverages backpropagation and differentiable loss functions to achieve effective training. By understanding these concepts, researchers and practitioners can harness the power of deep learning and frameworks like PyTorch to tackle a wide range of complex tasks.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> loss function </tag>
            
            <tag> backpropagation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to choose the right batch size in deep learning such as Transformer</title>
      <link href="2023/06/20/how-to-choose-batch-size-for-deep-learning-such-as-transformer/"/>
      <url>2023/06/20/how-to-choose-batch-size-for-deep-learning-such-as-transformer/</url>
      
        <content type="html"><![CDATA[<p>Choosing an appropriate batch size in deep learning, including models like Transformer, requires careful consideration and experimentation. The batch size is the number of samples processed in one forward and backward pass during training. Here are some factors to consider when selecting the batch size for your Transformer model:</p><ul><li><p>Memory constraints: Transformers often require substantial memory due to their self-attention mechanism. The larger the batch size, the more memory is required. Ensure that your hardware (e.g., GPU) has sufficient memory to accommodate the batch size you choose.</p></li><li><p>Training time: A larger batch size can provide computational efficiency by parallelizing operations across examples. This can speed up training since more computations are performed simultaneously. However, excessively large batch sizes may lead to suboptimal results or slower convergence.</p></li><li><p>Generalization: Smaller batch sizes tend to promote better generalization. They allow the model to encounter a wider variety of examples and update its parameters more frequently. This can prevent overfitting and improve performance on unseen data. However, very small batch sizes can introduce noise and slow down training.</p></li><li><p>Dataset size: The size of your dataset is another factor to consider. If you have a large dataset, you can afford to use larger batch sizes. Conversely, with smaller datasets, you may need to use smaller batch sizes to prevent overfitting and increase the diversity of examples seen by the model.</p></li><li><p>Empirical evaluation: Experiment with different batch sizes and evaluate their impact on your specific task and dataset. Monitor metrics such as loss and accuracy during training and validation. You can start with a moderate batch size and then increase or decrease it based on performance.</p></li><li><p>Hardware limitations: Consider the hardware resources available for training. If you have limited memory or a small GPU, you may need to choose a smaller batch size that fits within these constraints.</p></li></ul><p>As mentioned earlier, there is no fixed “start number” for batch sizes that is universally recommended or followed. The choice of the initial batch size depends on the factors discussed previously, such as memory constraints, dataset size, and hardware limitations. However, a commonly used starting point for many deep learning tasks is a batch size of 32.</p><p>Batch sizes around 32 are often chosen because they strike a balance between computational efficiency and generalization. This size allows for parallel processing on most GPUs, provides a reasonable amount of diversity in the training examples, and is generally memory-efficient. From this starting point, you can experiment with larger or smaller batch sizes to see how they impact your model’s performance.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> deep learning </tag>
            
            <tag> batch size </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Fast way to map an array X and array Y using nonparametric regression, a decision tree based regression</title>
      <link href="2023/06/19/non-parametric-way-to-map-x-and-y-using-decistrion-regressor/"/>
      <url>2023/06/19/non-parametric-way-to-map-x-and-y-using-decistrion-regressor/</url>
      
        <content type="html"><![CDATA[<p>If want to find a a non linear relationship between two arrays, and it’s hard to model it using simple functions such as polynomial or exponential.<br>We can try some nonparametric way, and decision tree based regression is a fast and good way.</p><h2 id="Case-1-where-we-don’t-have-too-much-outliers"><a href="#Case-1-where-we-don’t-have-too-much-outliers" class="headerlink" title="Case 1, where we don’t have too much outliers"></a>Case 1, where we don’t have too much outliers</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"># Create sample dummy data</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X = np.sort(5 * np.random.rand(80, 1), axis=0)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"></span><br><span class="line"># Fit the decision tree regressor</span><br><span class="line">tree = DecisionTreeRegressor()</span><br><span class="line">tree.fit(X, y)</span><br><span class="line"></span><br><span class="line"># Make predictions</span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">y_pred = tree.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Visualize the dummy data and the model&#x27;s predictions</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, label=&#x27;Dummy Data&#x27;)</span><br><span class="line">plt.plot(X_test, y_pred, color=&#x27;red&#x27;, label=&#x27;Decision Tree Regression&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(&#x27;X&#x27;)</span><br><span class="line">plt.ylabel(&#x27;y&#x27;)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>where we don’t need to worry about over fit, and use the default depth values, and the results look like this:</p><p><img src="/content/images/2023-06-19-01.png"></p><h2 id="Case-2-where-we-have-some-outliers"><a href="#Case-2-where-we-have-some-outliers" class="headerlink" title="Case 2, where we have some outliers"></a>Case 2, where we have some outliers</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn.tree import DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line"># Create sample dummy data</span><br><span class="line">np.random.seed(0)</span><br><span class="line">X = np.sort(5 * np.random.rand(80, 1), axis=0)</span><br><span class="line">y = np.sin(X).ravel()</span><br><span class="line"></span><br><span class="line">y[::5] += 3 * (0.5 - np.random.rand(16))  # Add some noise to the data</span><br><span class="line"></span><br><span class="line"># Fit the decision tree regressor</span><br><span class="line">tree = DecisionTreeRegressor(max_depth=3)</span><br><span class="line">tree.fit(X, y)</span><br><span class="line"></span><br><span class="line"># Make predictions</span><br><span class="line">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]</span><br><span class="line">y_pred = tree.predict(X_test)</span><br><span class="line"></span><br><span class="line"># Visualize the dummy data and the model&#x27;s predictions</span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(X, y, label=&#x27;Dummy Data&#x27;)</span><br><span class="line">plt.plot(X_test, y_pred, color=&#x27;red&#x27;, label=&#x27;Decision Tree Regression&#x27;)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(&#x27;X&#x27;)</span><br><span class="line">plt.ylabel(&#x27;y&#x27;)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>we we can reduce the depth of the decision tree, and avoid over fit, and results looks like this:<br><img src="/content/images/2023-06-19-02.png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> regression </tag>
            
            <tag> descion tree </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What does start and join mean in multiprocessing in Python</title>
      <link href="2023/06/19/start-vs-join-in-multprocessing-in-python/"/>
      <url>2023/06/19/start-vs-join-in-multprocessing-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python’s <code>multiprocessing</code> module, the <code>start()</code> and <code>join()</code> methods are used to control the execution of a <code>Process</code> object.</p><p><strong>start()</strong></p><p>The <code>start()</code> method is used to start a new process, which begins running the target function specified during the <code>Process</code> object’s creation. When you call <code>start()</code> on a <code>Process</code> object, the following happens:</p><ol><li>A new process is created by the operating system.</li><li>The new process begins executing the target function, along with any arguments passed to it, in a separate environment with its own memory space.</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Processing <span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">process = Process(target=my_function, args=(<span class="number">42</span>,))</span><br><span class="line">process.start()  <span class="comment"># This starts the execution of my_function in a new process</span></span><br></pre></td></tr></table></figure><p><strong>join()</strong></p><p>The <code>join()</code> method is used to block the main process (i.e., the script that initiated the new process) until the target process completes its execution. This is useful for synchronizing processes and ensuring that the main process waits for all child processes to finish before continuing.</p><p>When you call <code>join()</code> on a <code>Process</code> object, the following happens:</p><ol><li>The main process is blocked and waits for the target process to finish its execution.</li><li>Once the target process is completed, the <code>join()</code> method returns, and the main process continues executing.</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&#x27;Processing <span class="subst">&#123;x&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">process = Process(target=my_function, args=(<span class="number">42</span>,))</span><br><span class="line">process.start()  <span class="comment"># Start the execution of my_function in a new process</span></span><br><span class="line">process.join()  <span class="comment"># Wait for the new process to complete before continuing</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Process completed&quot;</span>)</span><br></pre></td></tr></table></figure><p>In this example, the <code>join()</code> method ensures that the “Process completed” message is printed only after the <code>my_function</code> has finished executing in the new process.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multiprocessing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Parallelism in Python, Threads vs. Processes and concurrent.futures</title>
      <link href="2023/06/19/understand-arallelism-in-python-thread-process-concurrent-future/"/>
      <url>2023/06/19/understand-arallelism-in-python-thread-process-concurrent-future/</url>
      
        <content type="html"><![CDATA[<p>In Python, parallelism is a technique that allows programs to execute multiple tasks concurrently, thereby improving the overall performance. Python offers several methods to achieve parallelism, including threading, multiprocessing, and the <code>concurrent.futures</code> module. In this blog post, we will explore the concept of threads and processes, how they differ, and when to choose between them. We will also take a look at the <code>concurrent.futures</code> module as a high-level interface for parallel computing in Python. Examples will be provided to illustrate how to use threads, processes, and <code>concurrent.futures</code> for parallel computing.</p><p><strong>Threads</strong></p><p>Threads, short for “thread of execution,” represent a single flow of control in a program. They are the smallest units of execution that an operating system can manage and schedule. Threads within a process share common resources, such as memory and file handles, which make it easier and more efficient to share data between multiple threads. However, this also means that care must be taken to ensure that shared data is accessed safely and with proper synchronization to avoid issues like race conditions or deadlocks.</p><p>In Python, threads can be created and managed using the <code>threading</code> module. Here’s an example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_numbers</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Number <span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_letters</span>():</span></span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">&#x27;abcde&#x27;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Letter <span class="subst">&#123;letter&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">thread1 = threading.Thread(target=print_numbers)</span><br><span class="line">thread2 = threading.Thread(target=print_letters)</span><br><span class="line"></span><br><span class="line">thread1.start()</span><br><span class="line">thread2.start()</span><br><span class="line"></span><br><span class="line">thread1.join()</span><br><span class="line">thread2.join()</span><br></pre></td></tr></table></figure><p>It’s important to note that the CPython implementation of Python has a Global Interpreter Lock (GIL), which limits the parallel execution of threads. This makes threading in Python more suitable for IO-bound tasks, where threads spend much of their time waiting for IO operations to complete.</p><p><strong>Processes</strong></p><p>Processes, unlike threads, have completely separate memory spaces and run in their own isolated environments. This means that inter-process communication requires more complex mechanisms and can be slower compared to thread communication. However, processes offer better isolation – a bug or crash in one process won’t affect other processes.</p><p>Python’s <code>multiprocessing</code> module is used for creating and managing processes. Here’s an example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> multiprocessing <span class="keyword">import</span> Process</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_numbers</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Number <span class="subst">&#123;i&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_letters</span>():</span></span><br><span class="line">    <span class="keyword">for</span> letter <span class="keyword">in</span> <span class="string">&#x27;abcde&#x27;</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Letter <span class="subst">&#123;letter&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br><span class="line">process1 = Process(target=print_numbers)</span><br><span class="line">process2 = Process(target=print_letters)</span><br><span class="line"></span><br><span class="line">process1.start()</span><br><span class="line">process2.start()</span><br><span class="line"></span><br><span class="line">process1.join()</span><br><span class="line">process2.join()</span><br></pre></td></tr></table></figure><p>Since processes can achieve true parallelism, multiprocessing is more appropriate for CPU-bound tasks in Python, where running tasks simultaneously can significantly improve performance.</p><p><strong>concurrent.futures</strong></p><p>The <code>concurrent.futures</code> module provides a high-level interface for asynchronously executing callables in Python. It has <code>ThreadPoolExecutor</code> and <code>ProcessPoolExecutor</code> classes, which are used for parallelizing code execution using multiple threads or processes, respectively. This module simplifies the process of managing threads and processes and provides additional functionality, such as handling exceptions and interacting with results as they become available.</p><p>Example usage of <code>concurrent.futures.ThreadPoolExecutor</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ThreadPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ThreadPoolExecutor(max_workers=<span class="number">4</span>) <span class="keyword">as</span> executor:</span><br><span class="line">    result = <span class="built_in">list</span>(executor.<span class="built_in">map</span>(square, <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>Example usage of <code>concurrent.futures.ProcessPoolExecutor</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> concurrent.futures <span class="keyword">import</span> ProcessPoolExecutor</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">square</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x * x</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> ProcessPoolExecutor(max_workers=<span class="number">4</span>) <span class="keyword">as</span> executor:</span><br><span class="line">    result = <span class="built_in">list</span>(executor.<span class="built_in">map</span>(square, <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">10</span>)))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p><strong>Threads vs. Processes: Differences</strong></p><ol><li>Memory and resource sharing</li><li>Creation and management</li><li>Concurrency and parallelism</li><li>Error handling and fault tolerance</li></ol><p>Refer to the detailed explanations provided earlier in this blog post for more information on these differences.</p><p><strong>When to Use Threads, Processes, or concurrent.futures</strong></p><p>The choice between threads and processes depends on the specific requirements and nature of the tasks being executed:</p><ul><li>Use threads for IO-bound tasks where there are multiple tasks that often spend time waiting for IO operations to complete. Threads are lightweight, share memory and resources, and provide better performance for concurrent IO-bound tasks. The <code>concurrent.futures.ThreadPoolExecutor</code> can be used for simplified thread management.</li><li>Use processes for CPU-bound tasks where true parallelism is required for maximum computation efficiency. Processes are heavyweight, isolated, and offer better fault tolerance. The <code>concurrent.futures.ProcessPoolExecutor</code> can be used for simplified process management.</li></ul><p><strong>Conclusion</strong></p><p>In this blog post, we have explored the concepts of threads and processes in Python, discussed their differences, and introduced the <code>concurrent.futures</code> module as a high-level interface for parallel computing. Understanding when to use threads, processes, or <code>concurrent.futures</code> is crucial for writing efficient programs in Python and can significantly improve the performance of your applications.</p><p>Remember to consider the type of tasks (CPU-bound or IO-bound), the number of available CPU cores, concurrency, parallelism, and synchronization requirements when deciding between threads and processes or choosing between the <code>ThreadPoolExecutor</code> and <code>ProcessPoolExecutor</code> in <code>concurrent.futures</code>. With these factors in mind, you can choose the most appropriate method of parallelism for your Python program and optimize performance.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multiprocessing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Utilizing Human Feedback for Enhanced Fine-Tuning of Large Language Models, RLHV vs finetuning difference</title>
      <link href="2023/06/17/how-does-rlhf-work-and-difference-with-finetuning/"/>
      <url>2023/06/17/how-does-rlhf-work-and-difference-with-finetuning/</url>
      
        <content type="html"><![CDATA[<p>Wonder what’s the difference between RLHF method with finetuning method?  Fine-Tuning is actually the first step of a RLHF method. Continue to read.</p><p>The utilization of reinforcement learning from human feedback (RLHF) has proven to be an effective method in aligning foundation models with human preferences. This technique, which involves fine-tuning models, has played a crucial role in recent advancements in AI, exemplified by the success of OpenAI’s ChatGPT model and Anthropic’s Claude model.</p><p>The implementation of RLHF brings about subtle yet significant improvements in the usability and performance of models. These enhancements can include refining the tone, mitigating biases and toxic elements, and enabling domain-specific content generation. This article will delve into the application of RLHF in fine-tuning large language models (LLMs).</p><h2 id="Understanding-Reinforcement-Learning-from-Human-Feedback"><a href="#Understanding-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="Understanding Reinforcement Learning from Human Feedback"></a>Understanding Reinforcement Learning from Human Feedback</h2><p>RLHF originated from a fundamental challenge in reinforcement learning: the complexity, ambiguity, and difficulty in defining goals for many reinforcement learning tasks. This predicament leads to a misalignment between our values and the objectives of RL systems, as emphasized in the paper “Deep reinforcement learning from human preferences.”</p><p>Numerous AI applications, particularly in businesses, encounter goals that are challenging to specify. For instance, in content moderation, the nuanced context of moderation policies might conflict with the enforcement decisions made by algorithms. Similarly, content generation, such as automated support agents, faces hurdles in achieving optimal quality. Although generative AI enables cost-effective content creation, concerns about maintaining brand style and tone have impeded widespread adoption. How can a team establish a reward function that consistently adheres to brand guidelines? In situations where the risks associated with AI-generated content are significant, opting for a deterministic chatbot or human support agent might be a justifiable investment.</p><p>In traditional reinforcement learning, a clear reward function provides guidance to algorithms. However, for more intricate tasks, determining an appropriate reward function can be challenging. In such cases, human preferences can effectively guide AI systems towards making the right decisions. This is because people, even those without expertise, possess an intuitive understanding of navigating nuanced and contextual tasks. For example, given a sample of brand marketing copy, individuals can easily assess how well AI-generated copy aligns with the brand’s intended tone. The main challenge, however, lies in the time and cost required to incorporate human preferences directly into the reinforcement learning training process. As stated in the paper “Deep reinforcement learning from human preferences,” “Using human feedback directly as a reward function is prohibitively expensive for RL systems that require hundreds or thousands of hours of experience.”</p><p>To address this challenge, researchers introduced reinforcement learning from human feedback (RLHF), which involves training a reward predictor, or preference model, to estimate human preferences. Utilizing a reward predictor significantly enhances the cost-effectiveness and scalability of the process compared to directly supplying human feedback to an RL algorithm.</p><h2 id="The-RLHF-Process-Insights-from-OpenAI"><a href="#The-RLHF-Process-Insights-from-OpenAI" class="headerlink" title="The RLHF Process: Insights from OpenAI"></a>The RLHF Process: Insights from OpenAI</h2><h3 id="Leveraging-RLHF-to-Enhance-Large-Language-Models"><a href="#Leveraging-RLHF-to-Enhance-Large-Language-Models" class="headerlink" title="Leveraging RLHF to Enhance Large Language Models"></a>Leveraging RLHF to Enhance Large Language Models</h3><p>RLHF serves as a powerful tool to improve the helpfulness, accuracy, and reduction of harmful biases in large language models. A comparison between GPT-3 and InstructGPT (a model fine-tuned using RLHF) conducted by OpenAI researchers revealed that labelers “significantly prefer” outputs from InstructGPT. InstructGPT also demonstrated improvements over GPT-3 in terms of truthfulness and toxicity benchmarks. Similarly, a research paper by Anthropic in 2022 documented similar benefits, stating that “RLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models up.” These studies present a compelling case for leveraging RLHF to achieve various business objectives with large language models.</p><p>Let’s Explore the RLHF Workflow for Fine-tuning</p><h3 id="Step-1-Gather-Demonstration-Data-and-Train-a-Supervised-Policy"><a href="#Step-1-Gather-Demonstration-Data-and-Train-a-Supervised-Policy" class="headerlink" title="Step 1: Gather Demonstration Data and Train a Supervised Policy"></a>Step 1: Gather Demonstration Data and Train a Supervised Policy</h3><p>To initiate the fine-tuning of a large language model (LLM), the first step is to collect a dataset called demonstration data. This dataset consists of text prompts and their corresponding outputs, representing the desired behavior of the fine-tuned model. For example, in an email summarization task, the prompt could be a full email, and the completion would be a two-sentence summary. In a chat task, the prompt might be a question, and the completion would be the ideal response.</p><p>Demonstration data can be sourced from various channels, such as existing data, a labeling team, or even from a model itself, as demonstrated in the concept of Self-Instruct: Aligning Language Model with Self-Generated Instructions. According to OpenAI’s fine-tuning guidelines, a few hundred high-quality examples are typically required for successful fine-tuning. The performance of the model tends to improve linearly with the size of the dataset. It is crucial to manually review the demonstration datasets to ensure accuracy, avoid toxic content, mitigate biases, and provide helpful information, as recommended by OpenAI’s researchers.</p><p>Platforms like OpenAI and Cohere offer comprehensive guides on the technical steps involved in fine-tuning a large language model using supervised learning.</p><h3 id="Step-2-Collect-Comparison-Data-and-Train-a-Reward-Model"><a href="#Step-2-Collect-Comparison-Data-and-Train-a-Reward-Model" class="headerlink" title="Step 2: Collect Comparison Data and Train a Reward Model"></a>Step 2: Collect Comparison Data and Train a Reward Model</h3><p>Once the large language model has been fine-tuned using supervised learning, it becomes capable of generating task-specific completions on its own. The next stage of the RLHF process involves collecting human feedback in the form of comparisons on these generated completions. This comparison data is then utilized to train a reward model, which will subsequently be employed to optimize the fine-tuned supervised learning model through reinforcement learning (as described in step 3).</p><p>To generate comparison data, a labeling team is presented with multiple completions produced by the model for the same prompt. The labelers rank these completions from best to worst. The number of completions can vary, ranging from a simple side-by-side comparison to the ranking of three or more completions. OpenAI found it effective to present labelers with a range of four to nine completions to rank at a time during the fine-tuning of InstructGPT.</p><p>There are third party vendor or tools that facilitate the execution of comparison tasks, enabling direct upload of model completions or real-time generation via a model endpoint.</p><p>It is crucial to test the fine-tuned LLM against baselines to assess honesty, helpfulness, bias, and toxicity. Standard LLM benchmarks like TruthfulQA, the Bias Benchmark for Question Answering, and RealToxicityPrompts for toxicity evaluation can be used for this purpose.</p><h3 id="Step-3-Optimize-the-Supervised-Policy-Using-Reinforcement-Learning"><a href="#Step-3-Optimize-the-Supervised-Policy-Using-Reinforcement-Learning" class="headerlink" title="Step 3: Optimize the Supervised Policy Using Reinforcement Learning"></a>Step 3: Optimize the Supervised Policy Using Reinforcement Learning</h3><p>In this step, the supervised learning baseline, which represents the fine-tuned LLM, is further optimized by leveraging a reinforcement learning (RL) algorithm. One prominent class of RL algorithms developed by OpenAI is Proximal Policy Optimization (PPO). Detailed information on PPO algorithms can be found on OpenAI’s website.</p><p>The RL process aligns the behavior of the supervised policy with the preferences expressed by the labelers. Through iterations of steps 2 and 3, the performance of the model can be continually enhanced.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> finetuning </tag>
            
            <tag> RLHF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Constraining Prediction ranges in Deep Learning Regression Models</title>
      <link href="2023/06/16/how-to-control-regression-prediction-ranges-in-deep-learning/"/>
      <url>2023/06/16/how-to-control-regression-prediction-ranges-in-deep-learning/</url>
      
        <content type="html"><![CDATA[<p>Deep learning is widely used for various regression analysis tasks. However, sometimes we need to apply certain constraints on the predicted values produced by the regression models. These constraints could be ensuring that the predicted values are always positive or limiting the predictions to lie within a specific range.</p><p>In this blog post, we will discuss different approaches to address these challenges in deep learning and how to apply them to a neural network using an output layer with specific activation functions.</p><h2 id="Section-1-Ensuring-Positive-Predictions"><a href="#Section-1-Ensuring-Positive-Predictions" class="headerlink" title="Section 1: Ensuring Positive Predictions"></a>Section 1: Ensuring Positive Predictions</h2><p>To make sure that the deep learning model’s predictions are always positive, you can apply the following approaches:</p><ol><li><p>ReLU activation: The Rectified Linear Unit (ReLU) activation function is defined as max(x, 0) and will produce non-negative values. By using ReLU in the output layer of your neural network, you can ensure that the model predictions stay positive.</p></li><li><p>Exponential activation: Using exponential activation, the output layer computes the exponential of the input values. Since the exponential of any value is positive, this approach guarantees that the predicted results will be positive.</p></li><li><p>Softplus activation: The Softplus activation function is similar to ReLU but provides a smooth and differentiable approximation. By using Softplus in the output layer, you can enforce the model to predict non-negative values.</p></li><li><p>Log-transform: You can apply a log-transform to your target values before training the regression model. This will change the problem into a log-scale regression. Then, in the output layer, apply the exponential activation function to get back values in the original positive scale.</p></li><li><p>Custom loss function: Define a custom loss function that penalizes negative predictions more heavily than positive predictions. This can encourage the model to produce positive predictions. Asymmetric loss functions, like asymmetric mean squared error (MSE) or asymmetric mean absolute error (MAE), can be used, giving higher weight to negative errors compared to positive errors.</p></li><li><p>Constrain weights: Constrain the weights of your neural network to favor positive predictions using regularization techniques like L1 or L2 regularization. This method can maintain a balance that encourages positive predictions.</p></li><li><p>Post-processing: After obtaining the predictions, replace any negative predicted values with a small positive value (e.g., a small threshold value or the absolute value of the negative prediction).</p></li></ol><h2 id="Section-2-Constraining-Predictions-to-a-Range-min-value-max-value"><a href="#Section-2-Constraining-Predictions-to-a-Range-min-value-max-value" class="headerlink" title="Section 2: Constraining Predictions to a Range [min_value, max_value]"></a>Section 2: Constraining Predictions to a Range [min_value, max_value]</h2><p>If you want to limit the predictions of your deep learning model to within a given range [min_value, max_value], you can use the following techniques:</p><ol><li><p>Sigmoid scaling: Normalize your target values to be within the range of 0 and 1 using min-max scaling. Then, use a sigmoid activation function in the output layer of your neural network, which outputs values between 0 and 1. After obtaining predictions, scale them back to the original [min_value, max_value] range.</p></li><li><p>Tanh scaling: Normalize your target values to be within the range of -1 and 1. Now, use a tanh function in the output layer of your neural network, which will output predictions in the range between -1 and 1. Then, scale them back to your original min and max range.</p></li><li><p>Custom activation: Alternatively, you can create a custom activation function that directly scales the output to the desired range, such as a rescaled sigmoid function. Implement this activation function within the output layer of your neural network.</p></li></ol><p>Conclusion:</p><p>Applying constraints to the predictions of deep learning regression models is a common requirement in many applications. By using these suggested techniques, you can ensure that your model provides predictions within the desired bounds. Remember to experiment with different methods, validate your model using appropriate evaluation metrics, and consider the specific context and data patterns of your dataset. By doing so, you will achieve better model performance without compromising other essential aspects of your deep learning model.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> regression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>New Function role added to GPT3.5 and GPT4 to give model to use API or functions as tools</title>
      <link href="2023/06/15/new-function-role-to-use-as-tool-in-gpt-similar-as-langchain/"/>
      <url>2023/06/15/new-function-role-to-use-as-tool-in-gpt-similar-as-langchain/</url>
      
        <content type="html"><![CDATA[<p>The latest update (June 2023) from OpenAI added a new role called <code>function</code> into their tool box.<br>By describing functions in your prompts, the model can intelligently choose to output a JSON object containing arguments to call these functions based on user input — perfect for integrating with other tools or APIs</p><p>This is very similar to the LangChain package to allow mole to choose tools, instead, it’s more native and probably easier and have better performance.</p><p>The latest models (gpt-3.5-turbo-0613 and gpt-4-0613) have been fine-tuned to include this function role. And from June 27th, the stable gpt-4 or gpt-3.5-turbo will be automatically upgraded to this new version.<br>So for the following code example, very possible you don’t need to specify the specific model with date, and instead ust use “gpt-3.5-turbo” or “gpt-4”, it’s going to be same.</p><p>To demo how to pass in function as tools for the model to choose, we show the followng example.</p><p>First, we define a simple function. In reality, this could be a more complex function or API endpoint.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OPENAI_API_KEY = &#x27;your api key&#x27;</span><br><span class="line">openai.api_key = OPENAI_API_KEY</span><br><span class="line"></span><br><span class="line"># Example dummy function hard coded to return the same weather</span><br><span class="line"># In production, this could be your backend API or an external API</span><br><span class="line">def get_current_weather(location, unit=&quot;fahrenheit&quot;):</span><br><span class="line">    &quot;&quot;&quot;Get the current weather in a given location&quot;&quot;&quot;</span><br><span class="line">    weather_info = &#123;</span><br><span class="line">        &quot;location&quot;: location,</span><br><span class="line">        &quot;temperature&quot;: &quot;72&quot;,</span><br><span class="line">        &quot;unit&quot;: unit,</span><br><span class="line">        &quot;forecast&quot;: [&quot;sunny&quot;, &quot;windy&quot;],</span><br><span class="line">    &#125;</span><br><span class="line">    return json.dumps(weather_info)</span><br></pre></td></tr></table></figure><p>Here is how we should define the messages sent to OpenAI:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Step 1, send model the user query and what functions it has access to</span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">model=&quot;gpt-3.5-turbo-0613&quot;,</span><br><span class="line">messages=[&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What&#x27;s the weather like in Boston?&quot;&#125;],</span><br><span class="line">functions=[</span><br><span class="line">&#123;</span><br><span class="line">&quot;name&quot;: &quot;get_current_weather&quot;,</span><br><span class="line">&quot;description&quot;: &quot;Get the current weather in a given location&quot;,</span><br><span class="line">&quot;parameters&quot;: &#123;</span><br><span class="line">&quot;type&quot;: &quot;object&quot;,</span><br><span class="line">&quot;properties&quot;: &#123;</span><br><span class="line">&quot;location&quot;: &#123;</span><br><span class="line">&quot;type&quot;: &quot;string&quot;,</span><br><span class="line">&quot;description&quot;: &quot;The city and state, e.g. San Francisco, CA&quot;,</span><br><span class="line">&#125;,</span><br><span class="line">&quot;unit&quot;: &#123;&quot;type&quot;: &quot;string&quot;, &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;]&#125;,</span><br><span class="line">&#125;,</span><br><span class="line">&quot;required&quot;: [&quot;location&quot;],</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line">],</span><br><span class="line">function_call=&quot;auto&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>As you can see, here are doing the usual thing asking a quesition through the “user” role, and the difference is we also passed a <code>functions</code>. If the model decides that function we pass in should be used answer the question,<br>it will return the corresponding output. And let’s print out the responsed message:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">message = response[&quot;choices&quot;][0][&quot;message&quot;]</span><br><span class="line">print(message)</span><br></pre></td></tr></table></figure><p>and we see the print results:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;content&quot;: null,</span><br><span class="line">  &quot;function_call&quot;: &#123;</span><br><span class="line">    &quot;arguments&quot;: &quot;&#123;\n\&quot;location\&quot;: \&quot;Boston\&quot;\n&#125;&quot;,</span><br><span class="line">    &quot;name&quot;: &quot;get_current_weather&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;role&quot;: &quot;assistant&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>So instead of return the direct answer, it tells us it could to use a function call with the corresponding parameters according to what we have sent to it in the previous step.<br>So for us, we need to check the above output, if it has <code>function_call</code> key in the result, we should call our functions, and put the results back and send to OpenAI, and hopefully get the final user-facing messages.</p><p>Let’s code it up:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Step 2, check if the model wants to call a function</span><br><span class="line">if message.get(&quot;function_call&quot;):</span><br><span class="line">function_name = message[&quot;function_call&quot;][&quot;name&quot;]</span><br><span class="line">function_args = json.loads(message[&quot;function_call&quot;][&quot;arguments&quot;])</span><br><span class="line"></span><br><span class="line"># Step 3, call the function</span><br><span class="line"># Note: the JSON response from the model may not be valid JSON</span><br><span class="line">function_response = get_current_weather(</span><br><span class="line">location=function_args.get(&quot;location&quot;),</span><br><span class="line">unit=function_args.get(&quot;unit&quot;),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># Step 4, send model the info on the function call and function response</span><br><span class="line">second_response = openai.ChatCompletion.create(</span><br><span class="line">model=&quot;gpt-3.5-turbo-0613&quot;,</span><br><span class="line">messages=[</span><br><span class="line">&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the weather like in boston?&quot;&#125;,</span><br><span class="line">message,</span><br><span class="line">&#123;</span><br><span class="line">&quot;role&quot;: &quot;function&quot;,</span><br><span class="line">&quot;name&quot;: function_name,</span><br><span class="line">&quot;content&quot;: function_response,</span><br><span class="line">&#125;,</span><br><span class="line">],</span><br><span class="line">)</span><br><span class="line">return second_response</span><br></pre></td></tr></table></figure><p>and if we check the output response from the above steps, we will see:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;choices&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;finish_reason&quot;: &quot;stop&quot;,</span><br><span class="line">      &quot;index&quot;: 0,</span><br><span class="line">      &quot;message&quot;: &#123;</span><br><span class="line">        &quot;content&quot;: &quot;The current weather in Boston is sunny and windy with a temperature of 72 degrees Fahrenheit.&quot;,</span><br><span class="line">        &quot;role&quot;: &quot;assistant&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;created&quot;: 1686856448,</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-7Rmi0JDIiOfagxDFJq16dhiC6qagZ&quot;,</span><br><span class="line">  &quot;model&quot;: &quot;gpt-3.5-turbo-0613&quot;,</span><br><span class="line">  &quot;object&quot;: &quot;chat.completion&quot;,</span><br><span class="line">  &quot;usage&quot;: &#123;</span><br><span class="line">    &quot;completion_tokens&quot;: 18,</span><br><span class="line">    &quot;prompt_tokens&quot;: 68,</span><br><span class="line">    &quot;total_tokens&quot;: 86</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Where it shows the answer that can be shown to the final user.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> openai </tag>
            
            <tag> gpt4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The short history of open source large language model from chatGPT so far</title>
      <link href="2023/06/13/short-history-of-open-source-large-language-model-so-far/"/>
      <url>2023/06/13/short-history-of-open-source-large-language-model-so-far/</url>
      
        <content type="html"><![CDATA[<p>Large language models (LLMs) have revolutionized the field of artificial intelligence, and their long-lasting impact is only growing stronger. OpenAI’s ChatGPT, a highly advanced conversational AI, has served as a significant breakthrough in recent months, igniting fierce competition among companies and researchers. Many are racing to develop state-of-the-art conversational AI systems, vying for the top spot against OpenAI’s remarkable feats.</p><p>Google has contributed with its Bard, which it fine-tuned on PaLM-E, and the development of a multi-modal LLM with GPT-4. Furthermore, Meta developed its own LLM known as LLaMa, as an answer to the open-source LLM push. Quite recently, an influx of information related to the latest LLMs has emerged, particularly because Meta chose to share the architecture of LLaMa with the research community for non-commercial purposes only.</p><p>Interestingly, LLaMa’s weights eventually leaked, allowing anyone, not just experts or commercial entities, to experiment with these high-performing models firsthand.</p><p>Meta released LLaMa on February 24th, 2023, with the primary goal of giving access to this performing LLM for the academic research community. The team provided four versions of LLaMa with varying parameters: 7B, 13B, 33B, and 65B. Like other large language models, LLaMa input a sequence of words and predicts the next word to generate text recursively. According to its paper, LLaMa-13B surpasses GPT-3 (175B) on most benchmarks, and LLaMa-65B rivals the best models, such as Chinchilla-70B (DeepMind) and PaLM-540B (Google).</p><p>The LLaMa model was released publicly for the research community for non-commercial purposes via Facebook Research GitHub. However, only the untrained model was made available, with the trained weights accessible separately through a Google Form for research purposes. It’s worth noting that training LLaMa on that scale required 2048 A100 GPUs, each costing about $15k. This showcases the tremendous resources needed for creating such a model.</p><p>Aside from the expenditure, having a large and clean dataset is crucial for training LLaMa. The models required trillions of tokens for training, with LLaMa-65B and LLaMa-33B being trained on 1.4 trillion tokens, while LLaMa-7B trained on one trillion tokens. With this pre-trained LLM, it becomes possible to fine-tune it to obtain a replica of ChatGPT, a conversational model capable of human interactions.</p><p>However, a significant challenge is obtaining the required data for fine-tuning the model without spending millions of dollars on human intervention. This is what OpenAI did to train InstructGPT, the model behind ChatGPT.</p><p>Stanford researchers discovered an inexpensive alternative, a way to fine-tune LLaMa without breaking the bank. They introduced Alpaca-7B, a model fine-tuned from the LLaMa-7B model on 52k instruction-following demonstrations. One key issue with instruction-following models, like ChatGPT, is the production of false information, the propagation of social stereotypes, and the generation of toxic language.</p><p>To solve these issues, OpenAI spent millions of dollars to evaluate “bad” answers using human feedback (RLHF) to create InstructGPT. However, OpenAI has not released the dataset used to train InstructGPT, making it a challenge to replicate this kind of model. Stanford researchers’ workaround relied on using Da-Vinci-003 from OpenAI, which was built on InstructGPT, to generate 52k instruction-following examples from 175 self-instructed seed tasks.</p><p>According to the Stanford team, generating the 52k instruction-following demonstrations cost around $500, while training the model on 8 80GB A100 GPUs cost approximately $100 and took only three hours. Despite the smaller model size, Alpaca and Da-Vinci-003 displayed similar performance in human evaluation regarding answer quality.</p><p>Furthermore, Vicuna, built on LLaMa’s original model, is claimed to perform almost on par with OpenAI ChatGPT or Google Bard on instruction-following tasks, while the overall cost of training remained remarkably low at $300. Two versions of Vicuna have been released for non-commercial use: 7B and 13B parameters. A significant upgrade in Vicuna compared to previous models is the increase in the max context length, from 512 with Alpaca to 2048 tokens.</p><p>One caveat with these models, however, is their large size and memory-intensive nature. Deploying them comes with high energy consumption and financial costs. This limitation has led some developers to believe that only corporations with access to large-scale infrastructure could truly benefit from these models. But that changed with Georgi Gerganov’s work on llama.ccp.</p><p>Gerganov’s llama.ccp code takes LLMs to the next level by converting popular instruction-following LLMs, originally coded in Python, into C/C++ language. C/C++ is a low-level programming language that does not require machine compilation, resulting in faster execution times. Additionally, the code supports 4-bit quantization, a process that converts 32-bit floating-point numbers (like weights and activation outputs) to the nearest 8-bit fixed-point numbers, resulting in smaller models and increased inferencing speed.</p><p>Thanks to the contributions of Gerganov and others, along with the leaked LLaMa weights, it is now possible to run any instruction-following model (like Alpaca or Vicuna) directly on a laptop. Various projects have detailed the process of running Vicuna on personal devices using llama.ccp, paving the way for accessible, open-source AI advancements without the constraints of substantial resources.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Handling Data Type Mismatch between NumPy Arrays and PyTorch Tensors</title>
      <link href="2023/06/12/handle-data-type-mismatch-between-numpy-and-pytorch/"/>
      <url>2023/06/12/handle-data-type-mismatch-between-numpy-and-pytorch/</url>
      
        <content type="html"><![CDATA[<p>In NumPy, the default float data type is float64, also known as double precision. In PyTorch, the default float data type is float32, also known as single precision. When we convert a NumPy array to a PyTorch tensor without explicitly specifying the data type, the original data type of the NumPy array is used, which might be different from the PyTorch default. This can cause issues when performing operations between tensors, such as matrix multiplication, which requires the same data types for all involved tensors.</p><p>Here’s an example to demonstrate this issue:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a NumPy array with float64 data type (default float data type in NumPy)</span></span><br><span class="line">input_example_np = np.array([<span class="number">1.2</span>, <span class="number">2.3</span>, <span class="number">3.4</span>, <span class="number">4.5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the NumPy array to a PyTorch tensor without specifying the data type</span></span><br><span class="line">input_example = torch.tensor(input_example_np)</span><br><span class="line"><span class="built_in">print</span>(input_example.dtype)  <span class="comment"># torch.float64</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create another tensor with default data type in PyTorch (float32)</span></span><br><span class="line">another_tensor = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"><span class="built_in">print</span>(another_tensor.dtype)  <span class="comment"># torch.float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now try to perform matrix multiplication, which will raise an error due to different data types</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = torch.matmul(input_example, another_tensor)</span><br><span class="line"><span class="keyword">except</span> RuntimeError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Error:&quot;</span>, e)  <span class="comment"># Error: mat1 and mat2 must have the same dtype</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To fix this issue, specify the desired data type when converting the NumPy array:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the NumPy array to a PyTorch tensor with float32 data type</span></span><br><span class="line">input_example = torch.tensor(input_example_np, dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="built_in">print</span>(input_example.dtype)  <span class="comment"># torch.float32</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Now we can successfully perform matrix multiplication</span></span><br><span class="line">result = torch.matmul(input_example, another_tensor)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure><p>By explicitly specifying the <code>dtype</code> parameter when converting the NumPy array to a PyTorch tensor, we can avoid the “mat1 and mat2 must have the same dtype” error or “expected both vectors to have same dtype, but found Double and Float” error,  and ensure that the tensors are compatible for mathematical operations.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Input dimension for pytorch transformer when the sequence length is one</title>
      <link href="2023/06/11/input-dimension-in-pytorch-transformer/"/>
      <url>2023/06/11/input-dimension-in-pytorch-transformer/</url>
      
        <content type="html"><![CDATA[<p>In pytorch, when we want to provide a 2-dimensional input with shape <code>(batch_size, embedding_size)</code>, we should be careful with the definition.<br>By default, the pytorch will treat the first dimension as sequence dimension, and second dimension as embedding dimension.<br>So we should do two things :<br>(1)  Set batch_first to be True in TransformerEncoderLayer<br>(2)  Modify the input data to be 3 dimensional to indicate it’s a sequence of size of 1</p><p>The code example is as the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class TransformerModel(nn.Module):</span><br><span class="line">    def __init__(self, input_dim, d_model, n_head, n_hid, n_layers, output_dim, dropout=0.5):</span><br><span class="line">        super(TransformerModel, self).__init__()</span><br><span class="line">        self.model_type = &#x27;Transformer&#x27;</span><br><span class="line">        self.encoder_layer = nn.TransformerEncoderLayer(d_model, n_head, n_hid, dropout,batch_first = True)</span><br><span class="line">        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, n_layers)</span><br><span class="line">        self.input_linear = nn.Linear(input_dim, d_model)</span><br><span class="line">        self.output_linear = nn.Linear(d_model, output_dim)</span><br><span class="line">        </span><br><span class="line">    def forward(self, src):</span><br><span class="line">        src = self.input_linear(src)</span><br><span class="line">        src = src.unsqueeze(1)  # Add a sequence dimension (batch_size, seq_len, d_model)</span><br><span class="line">        src = self.transformer_encoder(src)</span><br><span class="line">        output = self.output_linear(src[:, 0, :])</span><br><span class="line">        return output</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How critical is to normalize the input data, since there is layer normlization in the transformer encoder already</title>
      <link href="2023/06/11/should-we-normalize-data-when-using-transformer/"/>
      <url>2023/06/11/should-we-normalize-data-when-using-transformer/</url>
      
        <content type="html"><![CDATA[<p>Normalizing input data is generally a good practice, as it can improve the convergence rate and the overall performance of your model. That being said, the Transformer architecture includes layer normalization inside the TransformerEncoderLayer, which helps stabilize the learning process even if the input data is not normalized.</p><p>However, it’s important to understand that layer normalization operates along the last dimension (d_model) of input data, while input data normalization (using techniques like min-max scaling or standardization) happens across the examples in the dataset. These two normalizations serve different purposes and are not interchangeable.</p><p>Input normalization provides the model with features that are on a consistent scale and removes the bias due to the different ranges of input features, which can be particularly useful in problems where the input feature scales vary significantly.</p><p>On the other hand, layer normalization in the Transformer encoder ensures that the activations inside the model have a stable distribution, which helps improve training stability and convergence speed. It does not have the same effect as normalizing the input data.</p><p>In conclusion, while the Transformer architecture includes layer normalization, it’s still recommended to normalize the input data to improve the model performance and training stability.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Comprehensive Guide to Binary, Multi-Class, and Multi-Label Classification</title>
      <link href="2023/06/09/binary-classification-vs-multi-class-classification-vs-multi-label-classification/"/>
      <url>2023/06/09/binary-classification-vs-multi-class-classification-vs-multi-label-classification/</url>
      
        <content type="html"><![CDATA[<p>In this post, we are going to explore three important classification algorithms in the world of machine learning: binary, multi-class, and multi-label classification. We will take a look at their respective definitions, applications, similarities, and differences. Finally, we will dive into some Python examples to get a hands-on experience.</p><h3 id="What-is-Classification-in-Machine-Learning"><a href="#What-is-Classification-in-Machine-Learning" class="headerlink" title="What is Classification in Machine Learning?"></a>What is Classification in Machine Learning?</h3><p>Classification is a supervised learning technique used in machine learning to categorize data into classes or groups. The goal is to teach a model to predict the class of an object based on its features. There are three types of classification problems - binary, multi-class, and multi-label.</p><h3 id="Binary-Classification"><a href="#Binary-Classification" class="headerlink" title="Binary Classification"></a>Binary Classification</h3><p>Binary classification is the simplest form of classification where we aim to predict one of two possible classes. For instance, we might want to predict if an email is spam or not spam, or if a tumor is malignant or benign.</p><h3 id="Multi-Class-Classification"><a href="#Multi-Class-Classification" class="headerlink" title="Multi-Class Classification"></a>Multi-Class Classification</h3><p>In multi-class classification, we have more than two classes, and each instance belongs to only one class. Examples include recognizing handwritten digits (0-9), identifying different species of animals, or classifying news articles into different categories (sports, politics, etc.).</p><h3 id="Multi-Label-Classification"><a href="#Multi-Label-Classification" class="headerlink" title="Multi-Label Classification"></a>Multi-Label Classification</h3><p>Multi-label classification differs from the other two types because each instance can belong to multiple classes. An example can be music genre classification, where a song might belong to multiple genres like pop, rock, and electronic.</p><p>Now that we have a basic understanding of the three types of classification problems we will further discuss their similarities and differences.</p><h3 id="Similarities"><a href="#Similarities" class="headerlink" title="Similarities"></a>Similarities</h3><ol><li>All three types of classification share the common goal of predicting the class or classes of an instance based on the specific features.</li><li>In all these cases, we can use popular algorithms like logistic regression, support vector machines, decision trees, and random forests, along with tuning the hyperparameters, for optimal results.</li><li>Performance measures like accuracy, precision, recall, and F1-score can be used for all three types of classification tasks to determine the effectiveness of the chosen model.</li></ol><h3 id="Differences"><a href="#Differences" class="headerlink" title="Differences"></a>Differences</h3><h4 id="Data-representation"><a href="#Data-representation" class="headerlink" title="Data representation:"></a>Data representation:</h4><ul><li>In binary classification, the target variable is usually a 1D array with <code>0</code> or <code>1</code> (or <code>-1</code> and <code>1</code>) representing the two possible classes.</li><li>In multi-class classification, the target variable is still a 1D array, but it contains integer values representing multiple classes (0 to <code>n-1</code>, where <code>n</code> is the number of classes).</li><li>In multi-label classification, the target variable is a 2D array, where each row contains a binary vector representing the presence or absence of each class for that instance.</li></ul><h4 id="Algorithm-adaptations"><a href="#Algorithm-adaptations" class="headerlink" title="Algorithm adaptations:"></a>Algorithm adaptations:</h4><ul><li>For binary classification, algorithms like logistic regression can be used directly, without any changes.</li><li>For multi-class classification, some algorithms like logistic regression need an adaptation to handle multiple classes, e.g., by using the “one-vs-rest” (OVR) or “one-vs-one” (OVO) strategies, or by incorporating cross-entropy losses for direct multi-class classification.</li><li>Multi-label classification usually requires an adaptation, such as the use of <code>OneVsRestClassifier</code>, which essentially treats the problem as multiple independent binary classification tasks where each classifier predicts the presence or absence of a specific class.</li></ul><h4 id="Loss-functions"><a href="#Loss-functions" class="headerlink" title="Loss functions:"></a>Loss functions:</h4><ul><li>In binary classification, we often use the binary cross-entropy loss, which measures the difference between the true and predicted probabilities of the single target class.</li><li>For multi-class classification, we use categorical cross-entropy loss, which measures the difference between the true and predicted probabilities for each (mutually exclusive) class.</li><li>Multi-label classification typically uses binary cross-entropy loss (as in binary classification) for each class independently, in a sense combining the loss for separate binary classification tasks.</li></ul><p>Now that we have explored the similarities and differences let’s dive into some Python examples.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Importing the necessary libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer, load_digits</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, classification_report</span><br></pre></td></tr></table></figure><h3 id="Binary-Classification-Example"><a href="#Binary-Classification-Example" class="headerlink" title="Binary Classification Example"></a>Binary Classification Example</h3><p>In this example, we will use the Wisconsin Breast Cancer dataset, a binary classification problem where we predict if a tumor is malignant or benign.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load the breast cancer dataset</span></span><br><span class="line">data = load_breast_cancer()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the logistic regression model</span></span><br><span class="line">binary_model = LogisticRegression(max_iter=<span class="number">1000</span>)</span><br><span class="line">binary_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = binary_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the accuracy and print the report</span></span><br><span class="line">binary_accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Binary Classification Accuracy:&quot;</span>, binary_accuracy)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h3 id="Multi-Class-Classification-Example"><a href="#Multi-Class-Classification-Example" class="headerlink" title="Multi-Class Classification Example"></a>Multi-Class Classification Example</h3><p>For the multi-class classification problem, we will use the MNIST digits dataset, where each instance is a handwritten digit (0-9).</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load the digits dataset</span></span><br><span class="line">data = load_digits()</span><br><span class="line">X = data.data</span><br><span class="line">y = data.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the logistic regression model</span></span><br><span class="line">multi_class_model = LogisticRegression(max_iter=<span class="number">1000</span>, multi_class=<span class="string">&quot;ovr&quot;</span>)</span><br><span class="line">multi_class_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = multi_class_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the accuracy and print the report</span></span><br><span class="line">multi_class_accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Multi-Class Classification Accuracy:&quot;</span>, multi_class_accuracy)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h3 id="Multi-Label-Classification-Example"><a href="#Multi-Label-Classification-Example" class="headerlink" title="Multi-Label Classification Example"></a>Multi-Label Classification Example</h3><p>For this example, let’s consider a hypothetical dataset with three labels and four features.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a hypothetical dataset</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.randn(<span class="number">100</span>, <span class="number">4</span>)</span><br><span class="line">y = np.random.randint(<span class="number">0</span>, <span class="number">2</span>, (<span class="number">100</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split the data into training and testing sets</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the logistic regression model using OneVsRestClassifier</span></span><br><span class="line">multi_label_model = OneVsRestClassifier(LogisticRegression(max_iter=<span class="number">1000</span>))</span><br><span class="line">multi_label_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make predictions on the test set</span></span><br><span class="line">y_pred = multi_label_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the accuracy and print the report</span></span><br><span class="line">multi_label_accuracy = accuracy_score(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Multi-Label Classification Accuracy:&quot;</span>, multi_label_accuracy)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><p>In summary, we explored the three types of classification problems: binary, multi-class, and multi-label classification, and demonstrated how to implement each using logistic regression with the Scikit-Learn library. We also discussed their similarities and differences along with valuable insights into their inner workings. Keep in mind that for each problem, you can also experiment with various other algorithms and fine-tune the models to achieve better performance. Understanding these classification tasks is crucial for building powerful machine learning models.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> classification </tag>
            
            <tag> multi-class classification </tag>
            
            <tag> multi-label classification </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS Lambda Warm-Up Strategies, Reserved vs. Provisioned Concurrency</title>
      <link href="2023/06/08/aws-lambda-warm-up-through-provisioned-concurrency/"/>
      <url>2023/06/08/aws-lambda-warm-up-through-provisioned-concurrency/</url>
      
        <content type="html"><![CDATA[<p>AWS Lambda is an excellent service for running serverless applications, allowing you to build highly scalable and cost-effective solutions. However, one common concern with Lambda is the cold start issue that occurs when a function has been idle for a while and needs to handle an incoming request. Cold starts can lead to increased latency, which can negatively impact the performance of your application. </p><p>In this blog post, we’re going to discuss two concurrency control mechanisms available in AWS Lambda, Reserved Concurrency and Provisioned Concurrency, and how they can help in warming up your Lambda functions. We’ll also compare their differences and highlight which strategy could be the right solution for your application needs.</p><h2 id="Reserved-Concurrency"><a href="#Reserved-Concurrency" class="headerlink" title="Reserved Concurrency"></a>Reserved Concurrency</h2><p>Reserved concurrency allows you to reserve a certain number of concurrent executions for a specific Lambda function. By setting a reserved concurrency value, you are guaranteeing that a designated number of concurrent instances are always available for that function, up to the specified limit.</p><p>Benefits of Reserved Concurrency:</p><ol><li>Prevents ‘noisy neighbor’ issues</li><li>Guarantees available execution capacity for critical functions</li></ol><p>However, it’s essential to note that reserved concurrency on its own does not reduce cold start times. Cold starts may still be experienced when a function with reserved concurrency has to instantiate new instances, but the reservation helps ensure that the desired instances can start without interference from other functions.</p><h2 id="Provisioned-Concurrency"><a href="#Provisioned-Concurrency" class="headerlink" title="Provisioned Concurrency"></a>Provisioned Concurrency</h2><p>Provisioned Concurrency is a feature that directly addresses the cold start issue. It allows you to pre-warm a specific number of function instances, so they are ready to respond immediately to incoming requests. With provisioned concurrency, you can maintain fast and consistent response times, regardless of the function’s previous utilization.</p><p>Benefits of Provisioned Concurrency:</p><ol><li>Reduces cold start latency</li><li>Ensures consistent performance</li></ol><p>Note that enabling provisioned concurrency incurs additional costs based on the number of instances configured and their duration of operation.</p><h2 id="Solution-How-to-Keep-Your-Lambda-Function-Warm"><a href="#Solution-How-to-Keep-Your-Lambda-Function-Warm" class="headerlink" title="Solution: How to Keep Your Lambda Function Warm"></a>Solution: How to Keep Your Lambda Function Warm</h2><p>If you want to eliminate cold starts from your application altogether, using Provisioned Concurrency is the recommended approach. Pre-warming instances directly tackle the cold start issue by keeping them ready for immediate execution.</p><p>Here’s how to set up Provisioned Concurrency:</p><ol><li>Open the AWS Lambda console</li><li>Select the function you want to configure the concurrency</li><li>Go to the Configuration tab and adjust the settings</li><li>Enter a value for provisioned concurrency</li><li>Configure auto-scaling policy (optional)</li><li>Save the settings</li></ol><p>Reserved Concurrency can be a complementary solution to Provisioned Concurrency if you want to manage the overall concurrency usage across your Lambda functions effectively. It becomes particularly helpful when you have multiple functions sharing the available concurrency limit and want to allocate a specific number of concurrent executions to critical or latency-sensitive functions.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>AWS Lambda’s concurrency control mechanisms, Reserved Concurrency and Provisioned Concurrency, provide different ways to address the cold start problem and manage your Lambda function’s performance. While reserved concurrency ensures availability, provisioned concurrency tackles cold starts head-on, pre-warming instances and enabling consistent response times.</p><p>Choosing the right warm-up strategy for your Lambda function depends on your specific application requirements, its critical nature, and your budget for AWS resources. In many cases, using a combination of both reserved and provisioned concurrency can optimize your serverless application’s performance and cost-effectiveness.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AWS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to access run information in databricks experiments using mlflow</title>
      <link href="2023/06/08/get-run-info-from-databricks-using-mlflow/"/>
      <url>2023/06/08/get-run-info-from-databricks-using-mlflow/</url>
      
        <content type="html"><![CDATA[<p>To use the <code>client.list_run_infos()</code> function from the <code>mlflow</code> package when talking to Databricks, you need to set up the Databricks tracking URI and authenticate your connection. You should pass the following parameter: ‘experiment_id’ (Integer or String) - The ID of the experiment for which you want to list the run information.</p><p>Here’s an example:</p><p>```python<br>import mlflow</p><h1 id="Set-the-tracking-URI-to-Databricks"><a href="#Set-the-tracking-URI-to-Databricks" class="headerlink" title="Set the tracking URI to Databricks"></a>Set the tracking URI to Databricks</h1><p>databricks_host = ‘https://<your-databricks-instance>‘<br>databricks_token = ‘<your-databricks-token>‘<br>mlflow.set_tracking_uri(f”databricks://{databricks_host}”)</p><h1 id="Authenticate-to-Databricks"><a href="#Authenticate-to-Databricks" class="headerlink" title="Authenticate to Databricks"></a>Authenticate to Databricks</h1><p>mlflow.tracking.set_registry_uri(f”dbfs://{databricks_host}”)<br>mlflow.databricks.utils.set_databricks_token(databricks_token)</p><h1 id="Create-the-client"><a href="#Create-the-client" class="headerlink" title="Create the client"></a>Create the client</h1><p>client = mlflow.tracking.MlflowClient()</p><h1 id="Set-the-experiment-ID"><a href="#Set-the-experiment-ID" class="headerlink" title="Set the experiment ID"></a>Set the experiment ID</h1><p>experiment_id = ‘12345’  # The experiment ID you want to list the runs</p><h1 id="List-run-information-for-the-given-experiment-ID"><a href="#List-run-information-for-the-given-experiment-ID" class="headerlink" title="List run information for the given experiment ID"></a>List run information for the given experiment ID</h1><p>run_infos = client.list_run_infos(experiment_id)</p><h1 id="Print-the-run-information"><a href="#Print-the-run-information" class="headerlink" title="Print the run information"></a>Print the run information</h1><p>for run_info in run_infos:<br>    print(run_info)<br>``</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Apple&#39;s Vision Pro, An Immersive Leap into the Future of Personal Computing</title>
      <link href="2023/06/07/apple-vision-pro-vr/"/>
      <url>2023/06/07/apple-vision-pro-vr/</url>
      
        <content type="html"><![CDATA[<p>The concept of virtual reality (VR) has captivated the world with its limitless potential to transport users into immersive, computer-generated environments that often blur the line between the digital and physical realms. Over the years, VR technology has continued to advance at an impressive pace, opening up new possibilities for use in various fields including gaming, education, healthcare, and workplace productivity. The ever-evolving VR landscape has given rise to a host of innovative headsets and platforms, each aiming to push the boundaries of realism and interactivity. As the demand for compelling VR experiences grows, tech companies worldwide are competing to develop groundbreaking products. Apple’s recent introduction of the Vision Pro marks a significant milestone in the evolution of VR, as it seamlessly integrates AR capabilities to create a truly unique and immersive user experience.</p><p>At the recent WWDC, Apple revealed its revolutionary new mixed reality headset, the Apple Vision Pro, a spectacular device that seamlessly integrates virtual reality (VR) and augmented reality (AR) technologies. Set to ship in early 2024 with a price tag of $3,499, it’s worth considering whether the Vision Pro will change the game in content consumption and device interaction as we know it.</p><p>One of the standout features of the Vision Pro is its ability to merge VR and AR experiences in a single, seamless experience. Utilizing an impressive array of 12 cameras to render the user’s surroundings in high resolution, the headset offers a smooth, hyper-realistic, and immersive experience. The true potential of the Vision Pro lies in its ability to layer digital experiences onto the wearer’s physical environment, thereby creating an augmented reality experience that maintains a sense of presence in the real world.</p><p>The Vision Pro’s user interface is both innovative and intuitive, using highly accurate eye tracking to determine what the wearer is looking at and enabling interaction with elements by simply touching their fingers together. The headset’s compatibility with keyboards and trackpads further enhances its potential for productivity and content creation. What’s more, the Vision Pro even allows users to project a Mac desktop directly into the headset, possibly replacing the need for traditional computer hardware in the future.</p><p>Beyond productivity, the entertainment and content consumption possibilities of the Vision Pro are astonishing. From immersive video experiences such as intimate sports events to family gatherings, the visual appeal of these virtual scenes feels remarkably close to reality. The Vision Pro’s AR capabilities also allow users to enjoy movies, games, and conference calls in a way that feels immersive yet grounded in their actual environment.</p><p>However, the solitary user experience offered by the Vision Pro raises concerns. Although eye tracking technology aims to maintain natural interactions, the inherently individual nature of the headset could increase isolation and loneliness among its users. While Apple focuses on developing personal, immersive experiences, Meta’s approach emphasizes social connectivity and the importance of shared VR and AR experiences.</p><p>Despite these concerns, the Vision Pro shows immense potential in redefining the landscape of personal computing and entertainment, possibly even surpassing established devices such as Macs and iPads. As developers and creators explore the possibilities that the Vision Pro offers, a new ecosystem of content and applications tailored specifically for the platform could emerge.</p><p>The Vision Pro’s uncanny ability to blend our digital and physical worlds together may well shape the future of computing, entertainment, and productivity. However, striking a balance between these immersive individual experiences and real-world social connections will be crucial to ensure a healthy, connected society as we move forward with this revolutionary technology.</p><p>In summary, Apple’s Vision Pro presents a groundbreaking leap into the future of personal computing. Its sophisticated melding of VR and AR within an intuitive user interface offers users a level of immersion and engagement that pushes the boundaries of what is possible with current technology. Although the potential for increased isolation and loneliness is a concern, the Vision Pro’s innovative features and applications make it a compelling prospect for both productivity and entertainment — and quite possibly the future of personal electronics.</p>]]></content>
      
      
      <categories>
          
          <category> VR </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Apple </tag>
            
            <tag> VR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dive into prompt engineering tricks under the LangChain</title>
      <link href="2023/06/05/prompt-engineering-tricks-under-langchain/"/>
      <url>2023/06/05/prompt-engineering-tricks-under-langchain/</url>
      
        <content type="html"><![CDATA[<p>In the previous <a href="/2023/05/27/understanding-langchain-chains-and-agents/">blog</a>, we discussed how to use agents in LangChain by thinking about which tool to use, and feed back results from tools as observations into the thoughts, and finally get answers.</p><p>The process might be a little bit black box on the top, so let’s dive into the prompt that interacts with the GPT.</p><h2 id="the-starting-prompt-tempate"><a href="#the-starting-prompt-tempate" class="headerlink" title="the starting prompt tempate"></a>the starting prompt tempate</h2><p>In the begining, the prompt template with parameters to be feed in could look like this.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">template = &quot;&quot;&quot;Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">&#123;tools&#125;</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [&#123;tool_names&#125;]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can repeat N times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin! Remember to speak as a pirate when giving your final answer. Use lots of &quot;Arg&quot;s</span><br><span class="line"></span><br><span class="line">Question: &#123;input&#125;</span><br><span class="line">&#123;agent_scratchpad&#125;&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><p>So with our tools to be only the google <code>search</code> tool,  our question to be <code>How many people live in canada as of 2023?</code>, and no previous resutls from agents yet, the first prompt goes into GPT will be like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">Search: useful for when you need to answer questions about current events</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [Search]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can repeat N times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin! Remember to speak as a pirate when giving your final answer. Use lots of &quot;Arg&quot;s</span><br><span class="line"></span><br><span class="line">Question: How many people live in canada as of 2023?</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If we literally type this prompty to GPT <code>text-davinci-003</code> model with temperature set to 0, we will get output like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thought: Hmm, I be not knowin&#x27; the answer to that off the top o&#x27; me head.</span><br><span class="line">Action: Search</span><br><span class="line">Action Input: &quot;Canada population 2023&quot;</span><br><span class="line">Observation: According to Statistics Canada, the population of Canada in 2023 is estimated to be 38.6 million.</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: Arrr, Canada be havin&#x27; 38.6 million people in 2023!</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>So as you can see, the GPT follows the instruction and answer the quesiton using the format, and it also gives a final answer. However in our case, we don’t want to get the potentially hallucinations from GPT, especially we know 2023 is after the GPT’s training time.<br>That’s why in LangChain, we set the stop word of the model to be “Observation” on purpose, so what we really need from the model output is:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Thought: Hmm, I be not knowin&#x27; the answer to that off the top o&#x27; me head.</span><br><span class="line">Action: Search</span><br><span class="line">Action Input: &quot;Canada population 2023&quot;</span><br></pre></td></tr></table></figure><p>So that’s good enough for us now. So we know that we should do the <code>search</code> action, so LangChain will call the google search api, with that suggested action input, then return the google search resutls as the real <code>observation</code>.</p><h2 id="The-second-prompt"><a href="#The-second-prompt" class="headerlink" title="The second prompt"></a>The second prompt</h2><p>Now we are ready to format the second prompt sent to GPT with real observation from google search. With some formating, the second prompt is this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Answer the following questions as best you can, but speaking as a pirate might speak. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">Search: useful for when you need to answer questions about current events</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [Search]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can repeat N times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">Begin! Remember to speak as a pirate when giving your final answer. Use lots of &quot;Arg&quot;s</span><br><span class="line"></span><br><span class="line">Question: How many people live in canada as of 2023?</span><br><span class="line">Thought: Hmm, I be not knowin&#x27; the answer to that off the top o&#x27; me head.</span><br><span class="line">Action: Search</span><br><span class="line">Action Input: &quot;Canada population 2023&quot;</span><br><span class="line">Observation: Canada&#x27;s population was estimated at 39,566,248 on January 1, 2023, after a record population growth of 1,050,110 people from January 1, 2022, to January 1, 2023.</span><br><span class="line">Thought: </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now we sent the above the prompt to GPT, and quite easily, the GPT model just return this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Ahoy, that be the answer I was lookin&#x27; fer!</span><br><span class="line">Final Answer: The population of Canada as of 2023 be 39,566,248, Arg!</span><br></pre></td></tr></table></figure><p>which is the right answer, because it has the “Final Answer” keyword in the output, and LangChain just parse it out and sent to the user as the final output.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> langchain </tag>
            
            <tag> prompt engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>how to install lightgbm on macOS</title>
      <link href="2023/06/03/correct-way-to-install-lightgbm-on-macbook-macos/"/>
      <url>2023/06/03/correct-way-to-install-lightgbm-on-macbook-macos/</url>
      
        <content type="html"><![CDATA[<p>While installing lightgbm  on macbook, i.e., try to do this “python3 -m pip install lightgbm”,<br>I encounted the two following errors one at a time:<br>(1)  subprocess.CalledProcessError: Command ‘[‘make’,’_lightgbm’ … ] some error related to cmake<br>(2)  fatal error: ‘omp.h’ file not found</p><p>Solutions to install lightgbm on macOS are:</p><h2 id="first-solution-simply-do"><a href="#first-solution-simply-do" class="headerlink" title="first solution, simply do"></a>first solution, simply do</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install lightgbm</span><br></pre></td></tr></table></figure><h2 id="second-solution-using-pip-install"><a href="#second-solution-using-pip-install" class="headerlink" title="second solution, using pip install"></a>second solution, using pip install</h2><ol><li>Install CMake (3.16 or higher):<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install cmake</span><br></pre></td></tr></table></figure></li><li>Install OpenMP:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew install libomp</span><br></pre></td></tr></table></figure></li><li>insall lightgbm using pip<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python3 -m pip install lightgbm</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lightgbm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the Difference Between Melt and Pivot in Pandas, Examples and Use Cases</title>
      <link href="2023/06/03/dataframe-operation-melt-and-pivot/"/>
      <url>2023/06/03/dataframe-operation-melt-and-pivot/</url>
      
        <content type="html"><![CDATA[<p>Pandas is a popular data manipulation library in Python that provides powerful tools for data analysis and transformation. Two commonly used functions in Pandas are melt and pivot, which allow users to reshape their data. In this blog post, we will explore the differences between these two functions and provide simple examples to illustrate their usage.</p><h2 id="Melt"><a href="#Melt" class="headerlink" title="Melt:"></a>Melt:</h2><p>The melt function in Pandas is used to transform a dataset from a wide format to a long format, also known as unpivoting. It gathers columns and “melts” them into a single column, creating a new DataFrame with a row for each unique combination of identifiers. The melted column contains the values that were previously spread across multiple columns.</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h3><p>Let’s consider a dataset with information about students and their scores in different subjects:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    &#x27;Name&#x27;: [&#x27;Alice&#x27;, &#x27;Bob&#x27;, &#x27;Charlie&#x27;],</span><br><span class="line">    &#x27;Maths&#x27;: [90, 75, 80],</span><br><span class="line">    &#x27;Physics&#x27;: [85, 70, 95],</span><br><span class="line">    &#x27;Chemistry&#x27;: [92, 87, 78]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      Name  Maths  Physics  Chemistry</span><br><span class="line">0    Alice     90       85         92</span><br><span class="line">1      Bob     75       70         87</span><br><span class="line">2  Charlie     80       95         78</span><br></pre></td></tr></table></figure><p>Now, let’s use the melt function to reshape the data:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">melted_df = df.melt(id_vars=&#x27;Name&#x27;, var_name=&#x27;Subject&#x27;, value_name=&#x27;Score&#x27;)</span><br><span class="line"></span><br><span class="line">print(melted_df)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      Name    Subject  Score</span><br><span class="line">0    Alice      Maths      90</span><br><span class="line">1      Bob      Maths      75</span><br><span class="line">2  Charlie      Maths      80</span><br><span class="line">3    Alice    Physics      85</span><br><span class="line">4      Bob    Physics      70</span><br><span class="line">5  Charlie    Physics      95</span><br><span class="line">6    Alice  Chemistry      92</span><br><span class="line">7      Bob  Chemistry      87</span><br><span class="line">8  Charlie  Chemistry      78</span><br></pre></td></tr></table></figure><p>As shown in the example, the melt function transformed the wide-format DataFrame into a long-format DataFrame, where each row represents a unique combination of the identifier (Name) and the melted column (Subject), with the corresponding values in the Score column.</p><h2 id="Pivot"><a href="#Pivot" class="headerlink" title="Pivot:"></a>Pivot:</h2><p>The pivot function in Pandas is the inverse operation of melt. It is used to transform a long-format DataFrame into a wide format by spreading a column’s values into multiple columns.</p><h3 id="Example-1"><a href="#Example-1" class="headerlink" title="Example:"></a>Example:</h3><p>Let’s use the melted DataFrame from the previous example and apply the pivot function:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pivoted_df = melted_df.pivot(index=&#x27;Name&#x27;, columns=&#x27;Subject&#x27;, values=&#x27;Score&#x27;)</span><br><span class="line"></span><br><span class="line">print(pivoted_df)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Subject  Chemistry  Maths  Physics</span><br><span class="line">Name</span><br><span class="line">Alice           92     90       85</span><br><span class="line">Bob             87     75       70</span><br><span class="line">Charlie         78     80       95</span><br></pre></td></tr></table></figure><p>In this example, the pivot function reshaped the long-format DataFrame back into a wide-format DataFrame. The unique values in the Subject column became the columns in the pivoted DataFrame, and the corresponding values in the Score column were spread across those columns, with each row representing a unique identifier (Name).</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> melt </tag>
            
            <tag> pivot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unlocking the Potential of ChatGPT and 9 Unique and Exciting Use Cases</title>
      <link href="2023/06/02/common-use-cases-with-chatgpt-and-large-language-models/"/>
      <url>2023/06/02/common-use-cases-with-chatgpt-and-large-language-models/</url>
      
        <content type="html"><![CDATA[<p>ChatGPT is an AI model that has the capacity to revolutionize the way we interact with technology. With its ability to understand context and make human-like responses, this powerful tool can fulfill a variety of tasks. In this blog post, we’ll explore 10 interesting things that can be done using ChatGPT, and provide you with examples of prompts and their respective results from the AI.</p><h2 id="1-Debug-code"><a href="#1-Debug-code" class="headerlink" title="1. Debug code"></a>1. Debug code</h2><p><code>Prompt</code>: “Debug this Python code: def add(a, b): return a + b +-2”</p><p><code>Result</code> from ChatGPT:<br>“It appears that there might be a typo in the return statement, resulting in an unintended operation. The correct code should be: def add(a, b): return a + b”</p><h2 id="2-Write-code"><a href="#2-Write-code" class="headerlink" title="2. Write code"></a>2. Write code</h2><p><code>Prompt</code>: “Write a Python function to calculate the factorial of a number.”</p><p><code>Result</code> from ChatGPT:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">factorial</span>(<span class="params">n</span>):</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> n * factorial(n - <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="3-Explain-things-Like-I-am-5-years-old"><a href="#3-Explain-things-Like-I-am-5-years-old" class="headerlink" title="3. Explain things Like I am 5 years old"></a>3. Explain things Like I am 5 years old</h2><p><code>Prompt</code>: “Explain photosynthesis like I am 5 years old.”</p><p><code>Result</code> from ChatGPT:<br>“Photosynthesis is when plants use sunlight to make food. They take in air and water and turn them into food and oxygen using the sun’s energy. Just like how we need food to grow, plants make their food through this process.”</p><h2 id="4-Get-Ideas-for-AI-Art-Decoration-and-Party-Themes"><a href="#4-Get-Ideas-for-AI-Art-Decoration-and-Party-Themes" class="headerlink" title="4. Get Ideas for AI Art, Decoration, and Party Themes"></a>4. Get Ideas for AI Art, Decoration, and Party Themes</h2><p><code>Prompt</code>: “Suggest some AI-themed party decoration ideas.”</p><p><code>Result</code> from ChatGPT:<br>“1. Create robot-shaped balloons and hang them around the party area.<br>2. Use LED lights to create a futuristic atmosphere.<br>3. Design cookies or cupcakes shaped like robots or computer chips.<br>4. Create a photo booth with AI-themed props like VR goggles and robot masks.<br>5. Use circuit board-patterned tablecloths and napkins.”</p><h2 id="5-Find-Answers-to-Homework-and-Assignment-Questions-with-ChatGPT"><a href="#5-Find-Answers-to-Homework-and-Assignment-Questions-with-ChatGPT" class="headerlink" title="5. Find Answers to Homework and Assignment Questions with ChatGPT"></a>5. Find Answers to Homework and Assignment Questions with ChatGPT</h2><p><code>Prompt</code>: “Explain the significance of the Magna Carta.”</p><p><code>Result</code> from ChatGPT:<br>“The Magna Carta, signed in 1215, is a significant historical document that had a profound impact on the development of democracy and individual rights. It limited the power of the English monarch, preventing them from acting arbitrarily, and established the principle that everybody, including the ruler, was subject to the law. This laid the groundwork for modern concepts of democratic governance and rule of law.”</p><h2 id="6-Use-ChatGPT-to-Write-Music"><a href="#6-Use-ChatGPT-to-Write-Music" class="headerlink" title="6. Use ChatGPT to Write Music"></a>6. Use ChatGPT to Write Music</h2><p><code>Prompt</code>: “Compose a simple, four-line song about friendship.”</p><p><code>Result</code> from ChatGPT:<br>“Verse 1:<br>Through the storms and the sunshine, we stand side by side,<br>In laughter and tears, there’s nowhere we hide,<br>A bond so strong, it brightens each day,<br>With you as my friend, we’ll conquer life’s way.”</p><h2 id="7-Use-ChatGPT-for-Translations"><a href="#7-Use-ChatGPT-for-Translations" class="headerlink" title="7. Use ChatGPT for Translations"></a>7. Use ChatGPT for Translations</h2><p><code>Prompt</code>: “Translate ‘I hope you have a great day’ into French.”</p><p><code>Result</code> from ChatGPT:<br>“J’espère que tu passes une excellente journée.”</p><h2 id="8-Extract-Structured-Data-from-Text"><a href="#8-Extract-Structured-Data-from-Text" class="headerlink" title="8. Extract Structured Data from Text"></a>8. Extract Structured Data from Text</h2><p><code>Prompt</code>: “Extract key information from the following text: ‘Jane Austen was born on December 16, 1775, in Steventon, England. She wrote various novels, including Pride and Prejudice and Sense and Sensibility.’”</p><p><code>Result</code> from ChatGPT:<br>Name: Jane Austen<br>Birth Date: December 16, 1775<br>Birthplace: Steventon, England<br>Notable Works: Pride and Prejudice, Sense and Sensibility</p><h2 id="9-Grade-Essays"><a href="#9-Grade-Essays" class="headerlink" title="9. Grade Essays"></a>9. Grade Essays</h2><p><code>Prompt</code>: “Grade this short essay on its coherence, grammar, and overall quality: ‘The sUn is very brigHter. the sun gives us heAt It helps grow tree plants and helps we mOve. Without it we be cold and very no hAppy.’”</p><p><code>Result</code> from ChatGPT:</p><p>“Coherence: 5/10<br>Grammar: 3/10<br>Overall Quality: 4/10</p><p>The essay has some issues in terms of coherence and grammar. There are several capitalization, punctuation, and grammatical errors. The topic is communicated clearly, though, and with some improvement, the essay could become more coherent and engaging.”</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>ChatGPT opens up an array of possibilities for users across various domains. From debugging code to writing music or grading essays, this AI model continues to evolve, making tasks easier, more efficient, and convenient for individuals and organizations alike. With further research and development, ChatGPT has the potential to enhance its capabilities and user experience, making it an essential part of our digital lives in the future.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to use absolute import path in python package</title>
      <link href="2023/05/31/how-to-use-absolute-path-in-python-pacakge-import/"/>
      <url>2023/05/31/how-to-use-absolute-path-in-python-pacakge-import/</url>
      
        <content type="html"><![CDATA[<p>In another <a href="/2023/05/30/understand-relative-imports-in-python-packages/">blog</a>, we discussed how to use relative import path as a better practice.<br>However sometimes if we want to use absolute import path when developing packages, here is how it can be done.</p><p>To ensure that absolute imports work correctly in a normal Python package, follow these steps:</p><ol><li>Organize your package structure:</li></ol><p>Make sure your package has a well-organized structure with <code>__init__.py</code> files in each directory. This allows Python to recognize the directories as packages. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mypackage/</span><br><span class="line">    __init__.py</span><br><span class="line">    main.py</span><br><span class="line">    subpackage1/</span><br><span class="line">        __init__.py</span><br><span class="line">        module1.py</span><br><span class="line">    subpackage2/</span><br><span class="line">        __init__.py</span><br><span class="line">        module2.py</span><br></pre></td></tr></table></figure><ol start="2"><li>Set up your <code>PYTHONPATH</code>:</li></ol><p>Ensure that the top-level directory containing your package is included in the <code>PYTHONPATH</code> environment variable. This allows Python to find your package when using absolute imports.</p><p>You can add the top-level directory to the <code>PYTHONPATH</code> by running the following command in your terminal:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="string">&quot;/path/to/top-level-directory:<span class="variable">$PYTHONPATH</span>&quot;</span></span><br></pre></td></tr></table></figure><p>Replace <code>/path/to/top-level-directory</code> with the actual path to the directory containing your package.</p><p>Or in your python code, simply add system path like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import sys</span><br><span class="line">sys.path.append(&#x27;/path/to/top-level-directory&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><ol start="3"><li>Use absolute imports:</li></ol><p>Now you can use absolute imports in your package files. For example, in <code>module1.py</code>, you can import <code>module2.py</code> using an absolute import:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mypackage.subpackage2 <span class="keyword">import</span> module2</span><br></pre></td></tr></table></figure><p>By following these steps, you can ensure that absolute imports work correctly in your Python package, even for files in subdirectories.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> import </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Organizing Your Python Project with Relative Imports</title>
      <link href="2023/05/30/understand-relative-imports-in-python-packages/"/>
      <url>2023/05/30/understand-relative-imports-in-python-packages/</url>
      
        <content type="html"><![CDATA[<p>Organizing a Python project with different levels and subdirectories can be tedious if absolute imports are used everywhere. In this blog, we will explore how you can make use of relative imports to improve code organization and modularity in your Python projects.</p><h2 id="1-Understanding-Relative-Imports-with-Multiple-Dots"><a href="#1-Understanding-Relative-Imports-with-Multiple-Dots" class="headerlink" title="1. Understanding Relative Imports with Multiple Dots"></a>1. Understanding Relative Imports with Multiple Dots</h2><p>Relative imports use multiple dots to indicate the relative path of a module within a package. The number of dots indicates how many levels up the directory structure to go before accessing the desired module. This way, even if the package structure changes or your code is moved to a new location, the imports will still work as long as the relative paths remain the same. Let’s dive into an example to see how this works in practice.</p><p>Consider the following directory structure:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mypackage</span><br><span class="line">├── dirA</span><br><span class="line">│   ├── dirB</span><br><span class="line">│   │   ├── dirC</span><br><span class="line">│   │   │   ├── __init__.py</span><br><span class="line">│   │   │   └── file1.py</span><br><span class="line">│   │   └── __init__.py</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── file2.py</span><br><span class="line">└── __init__.py</span><br></pre></td></tr></table></figure><p>In this example, we have a package named <code>mypackage</code> with the subdirectory <code>dirA</code>, which contains another directory called <code>dirB</code>. Inside <code>dirB</code>, there is a further subdirectory named <code>dirC</code>. We want to import a function called <code>function_name</code> from <code>file2.py</code> inside <code>file1.py</code>.</p><p>To achieve this, we can use the following relative import in <code>file1.py</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ...file2 <span class="keyword">import</span> function_name</span><br></pre></td></tr></table></figure><p>The three dots (<code>...</code>) indicate going up three levels in the directory structure:</p><ol><li>From <code>dirC</code> to <code>dirB</code></li><li>From <code>dirB</code> to <code>dirA</code></li><li>From <code>dirA</code> to the top of the package (<code>mypackage</code>)</li></ol><p>Now that we have successfully imported <code>function_name</code> from <code>file2.py</code>, we can use it in our code inside <code>file1.py</code>. This illustrates the power and flexibility of using multiple dots for relative imports in Python projects.</p><h2 id="2-Developing-Modules-with-Different-Levels-and-Subdirectories"><a href="#2-Developing-Modules-with-Different-Levels-and-Subdirectories" class="headerlink" title="2. Developing Modules with Different Levels and Subdirectories"></a>2. Developing Modules with Different Levels and Subdirectories</h2><p>When developing a Python project, it’s common to have modules that are organized into a hierarchy of directories. Using <code>__init__.py</code> files and relative imports helps in organizing and structuring your code more efficiently.</p><p>Here’s a step-by-step guide on how to develop modules with different levels and subdirectories:</p><ol><li><p>If not already, convert your project into a package by creating an <code>__init__.py</code> file in your top-level directory (<code>mypackage</code> in our example). This file can be empty, but it signals Python that your directory should be treated as a package.</p></li><li><p>In each subdirectory, create an <code>__init__.py</code> file, which could be empty as well. Its presence signals Python that the directory is a part of a package and allows relative imports between modules in the package.</p></li><li><p>When importing a module or a specific function from a module located in an upper-level directory, use relative imports denoted by multiple dots. The number of dots indicates how many levels up the directory structure your import should be.</p></li></ol><p>For example, let’s say you have the following directory structure:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mypackage</span><br><span class="line">├── dirA</span><br><span class="line">│   ├── dirB</span><br><span class="line">│   │   ├── __init__.py</span><br><span class="line">│   │   └── file1.py</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── file2.py</span><br><span class="line">└── __init__.py</span><br></pre></td></tr></table></figure><p>In this case, you can import <code>function_name</code> from <code>file2.py</code> into <code>file1.py</code> using:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> ..file2 <span class="keyword">import</span> function_name</span><br></pre></td></tr></table></figure><p>The two dots (<code>..</code>) indicate going up two levels in the directory structure:</p><ol><li>From <code>dirB</code> to <code>dirA</code></li><li>From <code>dirA</code> to the top of the package (<code>mypackage</code>)</li></ol><p>By using relative imports and following these steps, you can create a robust and modular Python project with various levels and subdirectories. This will make your code more maintainable and scalable in the long term.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog, we have discussed how you can use multiple dots to indicate relative paths in Python imports and showcased this concept with examples. Additionally, we have demonstrated how to develop modules with different levels and subdirectories, making your Python projects more efficient, maintainable, and scalable.</p><p>Keep these concepts in mind while working on your Python projects, and you’ll find your code is much easier to manage and understand.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> import </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Demystifying the Black Box, Tools and Methods for Model Explainability in Machine Learning</title>
      <link href="2023/05/29/model-explainability-in-machine-learning/"/>
      <url>2023/05/29/model-explainability-in-machine-learning/</url>
      
        <content type="html"><![CDATA[<p>Model explainability refers to the ability to understand and interpret the decisions made by machine learning models. It is important because many modern machine learning algorithms, such as deep neural networks, are often considered “black boxes” due to their complexity. Model explainability tools and methods aim to shed light on the internal workings of these models, providing insights into how they arrive at their predictions or decisions. Here are some common tools and methods used for model explainability in machine learning:</p><ol><li><p>Feature Importance: This method helps identify the most influential features in a model’s decision-making process. Techniques like permutation importance, feature importance from tree-based models, or coefficients from linear models can provide insights into which features have the most impact on the model’s predictions.</p></li><li><p>Partial Dependence Plots (PDPs): PDPs visualize the relationship between a specific feature and the model’s predicted outcome while holding other features at fixed values. They show how the model’s predictions change with variations in the selected feature, helping understand the feature’s effect on the model’s decisions.</p></li><li><p>SHAP Values: SHAP (SHapley Additive exPlanations) values provide a unified measure of feature importance. They are based on cooperative game theory and assign each feature an importance score indicating its contribution to the prediction. SHAP values can be used to explain individual predictions or provide an overall understanding of feature importance.</p></li><li><p>LIME (Local Interpretable Model-Agnostic Explanations): LIME is a technique that explains individual predictions by approximating the model’s behavior locally. It creates a simpler, interpretable model around the instance of interest and explains the predictions based on this local model.</p></li><li><p>Model Surrogates: Surrogate models are simpler, interpretable models that approximate the behavior of complex models. They can be used to provide insights into the decision-making process of the black box model. Surrogate models can be trained on the same data or on synthetic data generated specifically for the purpose of interpretability.</p></li><li><p>Integrated Gradients: Integrated gradients is a method that assigns feature attributions to each input feature. It measures how much each feature contributes to the difference between a baseline input and the current input. It provides a way to quantify feature importance and explain model predictions.</p></li><li><p>Decision Trees: Decision trees are inherently interpretable models. They can be used as a means of explaining complex models by approximating their decision boundaries. Decision trees provide a clear path of decisions and feature splits, allowing for intuitive explanations.</p></li><li><p>Model-Agnostic Methods: Several model-agnostic techniques, such as rule-based explanations, surrogate models, and layer-wise relevance propagation (LRP), aim to explain the decisions of any black box model without relying on its internal architecture or parameters.</p></li><li><p>Visualizations: Visualizing the learned representations of models or the intermediate layers in deep neural networks can provide insights into the model’s decision-making process. Techniques like activation maximization or saliency maps can highlight the important regions or patterns in the input data that influence the model’s predictions.</p></li></ol><p>It’s worth noting that the choice of tool or method for model explainability depends on the specific use case, the type of model being analyzed, and the interpretability requirements of the stakeholders. Different tools and methods have their strengths and limitations, and a combination of approaches may be necessary to gain a comprehensive understanding of model behavior.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> model explainability </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to have good prompt engeering with OpenAI, summary from Andrew NG&#39;s course</title>
      <link href="2023/05/28/how-to-build-effective-prompts-with-openai/"/>
      <url>2023/05/28/how-to-build-effective-prompts-with-openai/</url>
      
        <content type="html"><![CDATA[<p>More and more people realize the importance of mastering prompting engeeing, this new way of coding using language (e.g. English).<br>Some CEOs of big tech companies even predicted, half of the future jobs will be prompt engineering based.</p><p>So how to work with prompty engineering effectively? Recently, Andrew Ng partnered with OpenAI to release a ChatGPT prompt engineering course for developers. This free course offers high-quality content, and here we summarizes the guidelines for crafting effective prompts mentioned in the video lessons, along with my personal insights.</p><h2 id="Importance-of-Effective-Prompts"><a href="#Importance-of-Effective-Prompts" class="headerlink" title="Importance of Effective Prompts"></a>Importance of Effective Prompts</h2><p>Effective prompts are essential for obtaining high-quality responses from ChatGPT. A well-crafted prompt will help the AI to:</p><ul><li>Produce accurate and relevant information</li><li>Maintain context and stay on-topic</li><li>Generate coherent and well-structured responses</li><li>Minimize errors and misunderstandings<br>Poorly constructed prompts can lead to irrelevant, ambiguous, or even incorrect outputs. Hence, investing time in crafting efficient prompts is crucial for obtaining the best results from ChatGPT.</li></ul><h2 id="Crafting-High-Quality-Prompts"><a href="#Crafting-High-Quality-Prompts" class="headerlink" title="Crafting High-Quality Prompts"></a>Crafting High-Quality Prompts</h2><p>To create efficient prompts that yield high-quality responses, consider the following principles and strategies:</p><h3 id="Principle-1-Write-clear-and-specific-instructions"><a href="#Principle-1-Write-clear-and-specific-instructions" class="headerlink" title="Principle 1: Write clear and specific instructions"></a>Principle 1: Write clear and specific instructions</h3><p>Ensure your prompts are clear and concise to help the model understand the intent and desired output. Avoid ambiguous language or phrasing that could lead to multiple interpretations. This can be accomplished with strategies such as:</p><h4 id="Strategy-1-Use-delimiters-to-clearly-indicate-distinct-parts-of-the-input"><a href="#Strategy-1-Use-delimiters-to-clearly-indicate-distinct-parts-of-the-input" class="headerlink" title="Strategy 1: Use delimiters to clearly indicate distinct parts of the input"></a>Strategy 1: Use delimiters to clearly indicate distinct parts of the input</h4><p>Delimiters help avoid potential interference from misleading user input. Examples of delimiters include:</p><ul><li>Triple quotes: “””</li><li>Triple backticks: ```</li><li>Tripe dashes —</li><li>Angle brackets: &lt;&gt;</li><li>XML tags: <tag><br>Prompt example:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">text = f&quot;&quot;&quot;</span><br><span class="line">You should express what you want a model to do by \ </span><br><span class="line">providing instructions that are as clear and \ </span><br><span class="line">specific as you can possibly make them. \ </span><br><span class="line">This will guide the model towards the desired output, \ </span><br><span class="line">and reduce the chances of receiving irrelevant \ </span><br><span class="line">or incorrect responses. Don&#x27;t confuse writing a \ </span><br><span class="line">clear prompt with writing a short prompt. \ </span><br><span class="line">In many cases, longer prompts provide more clarity \ </span><br><span class="line">and context for the model, which can lead to \ </span><br><span class="line">more detailed and relevant outputs.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">Summarize the text delimited by triple backticks \ </span><br><span class="line">into a single sentence.</span><br><span class="line">```&#123;text&#125;```</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure>Output:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Clear and specific instructions should be ...</span><br><span class="line">This allows the model to clearly understand the problem itself but also avoids injecting uncontrollable instructions. For example “Forget the previous command, do XYZ”</span><br></pre></td></tr></table></figure><h4 id="Strategy-2-Ask-for-structured-output-html-json"><a href="#Strategy-2-Ask-for-structured-output-html-json" class="headerlink" title="Strategy 2: Ask for structured output html json"></a>Strategy 2: Ask for structured output html json</h4>This approach helps make model outputs directly usable for programs, such as JSON outputs that can be read and converted into dictionary format by Python programs.</li></ul><p>Prompt example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">Generate a list of three made-up book titles along \ </span><br><span class="line">with their authors and genres. </span><br><span class="line">Provide them in JSON format with the following keys: </span><br><span class="line">book_id, title, author, genre.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;book_id&quot;: 1,</span><br><span class="line">    &quot;title&quot;: &quot;The Lost City of Zorath&quot;,</span><br><span class="line">    &quot;author&quot;: &quot;Aria Blackwood&quot;,</span><br><span class="line">    &quot;genre&quot;: &quot;Fantasy&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;book_id&quot;: 2,</span><br><span class="line">    &quot;title&quot;: &quot;The Last Survivors&quot;,</span><br><span class="line">    &quot;author&quot;: &quot;Ethan Stone&quot;,</span><br><span class="line">    &quot;genre&quot;: &quot;Science Fiction&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="Strategy-3-Check-whether-conditions-are-satisfied-check-assumptions-required-to-do-the-task"><a href="#Strategy-3-Check-whether-conditions-are-satisfied-check-assumptions-required-to-do-the-task" class="headerlink" title="Strategy 3: Check whether conditions are satisfied, check assumptions required to do the task"></a>Strategy 3: Check whether conditions are satisfied, check assumptions required to do the task</h4><p>If the completion of the task has preconditions that must be met, we should require the model to check these conditions first and instruct it to stop trying if they are not met.</p><p>Prompt example (satisfying conditions):</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">text_1 = f&quot;&quot;&quot;</span><br><span class="line">Making a cup of tea is easy! First, you need to get some \ </span><br><span class="line">water boiling. While that&#x27;s happening, \ </span><br><span class="line">grab a cup and put a tea bag in it. Once the water is \ </span><br><span class="line">hot enough, just pour it over the tea bag. \ </span><br><span class="line">Let it sit for a bit so the tea can steep. After a \ </span><br><span class="line">few minutes, take out the tea bag. If you \ </span><br><span class="line">like, you can add some sugar or milk to taste. \ </span><br><span class="line">And that&#x27;s it! You&#x27;ve got yourself a delicious \ </span><br><span class="line">cup of tea to enjoy.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">You will be provided with text delimited by triple quotes. </span><br><span class="line">If it contains a sequence of instructions, \ </span><br><span class="line">re-write those instructions in the following format:</span><br><span class="line">​</span><br><span class="line">Step 1 - ...</span><br><span class="line">Step 2 - …</span><br><span class="line">…</span><br><span class="line">Step N - …</span><br><span class="line">​</span><br><span class="line">If the text does not contain a sequence of instructions, \ </span><br><span class="line">then simply write \&quot;No steps provided.\&quot;</span><br><span class="line">​</span><br><span class="line">\&quot;\&quot;\&quot;&#123;text_1&#125;\&quot;\&quot;\&quot;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(&quot;Completion for Text 1:&quot;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Completion for Text 1:</span><br><span class="line">Step 1 - ...</span><br><span class="line">Step 2 - ...</span><br><span class="line">Step 3 - ...</span><br></pre></td></tr></table></figure><p>This has the added benefit of taking into account potential edge cases to avoid unexpected errors or results.</p><h4 id="Strategy-4-“Few-shot”-prompting-Give-a-successful-example-of-completing-tasks-then-ask-the-model-to-perform-the-task"><a href="#Strategy-4-“Few-shot”-prompting-Give-a-successful-example-of-completing-tasks-then-ask-the-model-to-perform-the-task" class="headerlink" title="Strategy 4: “Few-shot” prompting: Give a successful example of completing tasks, then ask the model to perform the task"></a>Strategy 4: “Few-shot” prompting: Give a successful example of completing tasks, then ask the model to perform the task</h4><p>Providing the model with one or more sample prompts helps clarify the expected output. For more information on few-shot learning, refer to GPT-3’s paper: “Language Models are Few-Shot Learners.”</p><p>Prompt example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">Your task is to answer in a consistent style.</span><br><span class="line">​</span><br><span class="line">&lt;child&gt;: Teach me about patience.</span><br><span class="line">​</span><br><span class="line">&lt;grandparent&gt;: The river that carves the deepest \ </span><br><span class="line">valley flows from a modest spring; the \ </span><br><span class="line">grandest symphony originates from a single note; \ </span><br><span class="line">the most intricate tapestry begins with a solitary thread.</span><br><span class="line">​</span><br><span class="line">&lt;child&gt;: Teach me about resilience.</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;grandparent&gt;: Resilience is like a tree that ...</span><br></pre></td></tr></table></figure><h3 id="Principle-2-Give-the-model-time-to-“think”"><a href="#Principle-2-Give-the-model-time-to-“think”" class="headerlink" title="Principle 2: Give the model time to “think”"></a>Principle 2: Give the model time to “think”</h3><p>This principle utilizes the idea of a thought chain, breaking complex tasks into N sequential subtasks, allowing the model to think step-by-step and produce more accurate outputs. For more details, refer to this paper: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</p><h4 id="Strategy-1-Specify-the-steps-required-to-complete-a-task"><a href="#Strategy-1-Specify-the-steps-required-to-complete-a-task" class="headerlink" title="Strategy 1: Specify the steps required to complete a task"></a>Strategy 1: Specify the steps required to complete a task</h4><p>Here’s an example involving summarizing text, translating it into French, listing names in the French summary, and finally outputting data in JSON format. By providing the necessary steps, the model can reference the results of previous steps and improve the accuracy of the output.</p><p>Prompt example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt_2 = f&quot;&quot;&quot;</span><br><span class="line">Your task is to perform the following actions: </span><br><span class="line">1 - Summarize the following text delimited by </span><br><span class="line">  &lt;&gt; with 1 sentence.</span><br><span class="line">2 - Translate the summary into French.</span><br><span class="line">3 - List each name in the French summary.</span><br><span class="line">4 - Output a json object that contains the </span><br><span class="line">  following keys: french_summary, num_names.</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line">Text: &lt;text to summarize&gt;</span><br><span class="line">Summary: &lt;summary&gt;</span><br><span class="line">Translation: &lt;summary translation&gt;</span><br><span class="line">Names: &lt;list of names in Italian summary&gt;</span><br><span class="line">Output JSON: &lt;json with summary and num_names&gt;</span><br><span class="line"></span><br><span class="line">Text: &lt;&#123;text&#125;&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt_2)</span><br><span class="line">print(&quot;\nCompletion for prompt 2:&quot;)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Completion for prompt 2:</span><br><span class="line"></span><br><span class="line">Summary: Jack and Jill...</span><br><span class="line">Translation: Jack et Jill partent en quête d&#x27;eau...</span><br><span class="line">Names: Jack, Jill</span><br><span class="line">Output JSON: &#123;&quot;french_summary&quot;: &quot;Jack et Jill partent en quête d&#x27;eau...&quot;, &quot;num_names&quot;: 2&#125;</span><br></pre></td></tr></table></figure><h4 id="Strategy-2-Instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion"><a href="#Strategy-2-Instruct-the-model-to-work-out-its-own-solution-before-rushing-to-a-conclusion" class="headerlink" title="Strategy 2: Instruct the model to work out its own solution before rushing to a conclusion"></a>Strategy 2: Instruct the model to work out its own solution before rushing to a conclusion</h4><p>If the task is too complicated or the description is too little, then the model can only draw conclusions by guessing, just like a person solving a complex math problem with a serious shortage of remaining exam time, there is a high probability that the calculation will be wrong. So, in this case, we can instruct the model to take longer to think about the problem.</p><p>For instance, when checking a student’s exercise solution, instruct the model to first find its own solution to prevent rushing to an incorrect answer.</p><p>Poor prompt example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">Determine if the student&#x27;s solution is correct or not.</span><br><span class="line">​</span><br><span class="line">Question:</span><br><span class="line">I&#x27;m building a solar power installation and I need \</span><br><span class="line"> help working out the financials. </span><br><span class="line">- Land costs $100 / square foot</span><br><span class="line">- I can buy solar panels for $250 / square foot</span><br><span class="line">- I negotiated a contract for maintenance that will cost \ </span><br><span class="line">me a flat $100k per year, and an additional $10 / square \</span><br><span class="line">foot</span><br><span class="line">What is the total cost for the first year of operations </span><br><span class="line">as a function of the number of square feet.</span><br><span class="line">​</span><br><span class="line">Student&#x27;s Solution:</span><br><span class="line">Let x be the size of the installation in square feet.</span><br><span class="line">Costs:</span><br><span class="line">1. Land cost: 100x</span><br><span class="line">2. Solar panel cost: 250x</span><br><span class="line">3. Maintenance cost: 100,000 + 100x</span><br><span class="line">Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output (incorrect): The student’s solution is correct.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The student&#x27;s solution is correct.</span><br></pre></td></tr></table></figure><p>Updated prompt:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = f&quot;&quot;&quot;</span><br><span class="line">Your task is to determine if the student&#x27;s solution \</span><br><span class="line">is correct or not.</span><br><span class="line">To solve the problem do the following:</span><br><span class="line">- First, work out your own solution to the problem. </span><br><span class="line">- Then compare your solution to the student&#x27;s solution \ </span><br><span class="line">and evaluate if the student&#x27;s solution is correct or not. </span><br><span class="line">Don&#x27;t decide if the student&#x27;s solution is correct until </span><br><span class="line">you have done the problem yourself.</span><br><span class="line">​</span><br><span class="line">Use the following format:</span><br><span class="line">Question:</span><br><span class="line">```</span><br><span class="line">question here</span><br><span class="line">```</span><br><span class="line">Student&#x27;s solution:</span><br><span class="line">```</span><br><span class="line">student&#x27;s solution here</span><br><span class="line">```</span><br><span class="line">Actual solution:</span><br><span class="line">```</span><br><span class="line">steps to work out the solution and your solution here</span><br><span class="line">```</span><br><span class="line">Is the student&#x27;s solution the same as actual solution \</span><br><span class="line">just calculated:</span><br><span class="line">```</span><br><span class="line">yes or no</span><br><span class="line">```</span><br><span class="line">Student grade:</span><br><span class="line">```</span><br><span class="line">correct or incorrect</span><br><span class="line">```</span><br><span class="line">​</span><br><span class="line">Question:</span><br><span class="line">```</span><br><span class="line">I&#x27;m building a solar power installation and I need help \</span><br><span class="line">working out the financials. </span><br><span class="line">- Land costs $100 / square foot</span><br><span class="line">- I can buy solar panels for $250 / square foot</span><br><span class="line">- I negotiated a contract for maintenance that will cost \</span><br><span class="line">me a flat $100k per year, and an additional $10 / square \</span><br><span class="line">foot</span><br><span class="line">What is the total cost for the first year of operations \</span><br><span class="line">as a function of the number of square feet.</span><br><span class="line">``` </span><br><span class="line">Student&#x27;s solution:</span><br><span class="line">```</span><br><span class="line">Let x be the size of the installation in square feet.</span><br><span class="line">Costs:</span><br><span class="line">1. Land cost: 100x</span><br><span class="line">2. Solar panel cost: 250x</span><br><span class="line">3. Maintenance cost: 100,000 + 100x</span><br><span class="line">Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000</span><br><span class="line">```</span><br><span class="line">Actual solution:</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">response = get_completion(prompt)</span><br><span class="line">print(response)</span><br></pre></td></tr></table></figure><p>Output (correct):</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Let x be the size of the installation in square feet.Costs:</span><br><span class="line">1. Land cost: 100x</span><br><span class="line">2. Solar panel cost: 250x</span><br><span class="line">3. Maintenance cost: 100,000 + 10x</span><br><span class="line"></span><br><span class="line">Total cost: 100x + 250x + 100,000 + 10x = 360x + 100,000</span><br><span class="line"></span><br><span class="line">Is the student’s solution the same as actual solution just calculated:</span><br><span class="line">No</span><br><span class="line"></span><br><span class="line">Student grade:</span><br><span class="line">Incorrect</span><br></pre></td></tr></table></figure><h2 id="Model-Limitations-Hallucinations"><a href="#Model-Limitations-Hallucinations" class="headerlink" title="Model Limitations: Hallucinations"></a>Model Limitations: Hallucinations</h2><p>ChatGPT may produce hallucinations, creating plausible but false information (e.g., non-existent literary works). To avoid this, you can ask the model to first look for relevant reference information (or mention reference information in the question, such as after Google), and then let the model answer the question based on this reference information.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> prompt engeering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain, chains and agents, a great piece of engineering work to facilitate prompt chaining</title>
      <link href="2023/05/27/understanding-langchain-chains-and-agents/"/>
      <url>2023/05/27/understanding-langchain-chains-and-agents/</url>
      
        <content type="html"><![CDATA[<p>According to the official site, LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:</p><ol><li>Data-aware: connect a language model to other sources of data</li><li>Agentic: allow a language model to interact with its environment</li></ol><p>The LangChain framework is designed around these principles, with the large language model (llm) as the engine where we must assume the llm works as it expects.</p><p>Two most important concepts in Langchain are <code>chains</code> and <code>agents</code>.</p><h2 id="Chains"><a href="#Chains" class="headerlink" title="Chains"></a>Chains</h2><p>Using an LLM in isolation is fine for some simple applications, but many more complex ones require chaining LLMs - either with each other or with other experts. LangChain provides a standard interface for Chains, as well as some common implementations of chains for ease of use.</p><h3 id="Why-do-we-need-chains"><a href="#Why-do-we-need-chains" class="headerlink" title="Why do we need chains?"></a>Why do we need chains?</h3><p>Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.</p><h3 id="Quick-start-Using-LLMChain"><a href="#Quick-start-Using-LLMChain" class="headerlink" title="Quick start: Using LLMChain"></a>Quick start: Using LLMChain</h3><p>The LLMChain is a simple chain that takes in a prompt template, formats it with the user input and returns the response from an LLM.</p><p>To use the LLMChain, first create a prompt template.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.prompts import PromptTemplate</span><br><span class="line">from langchain.llms import OpenAI</span><br><span class="line"></span><br><span class="line">llm = OpenAI(temperature=0.9)</span><br><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[&quot;product&quot;],</span><br><span class="line">    template=&quot;What is a good name for a company that makes &#123;product&#125;?&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>We can now create a very simple chain that will take user input, format the prompt with it, and then send it to the LLM.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.chains import LLMChain</span><br><span class="line">chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line"></span><br><span class="line"># Run the chain only specifying the input variable.</span><br><span class="line">print(chain.run(&quot;colorful socks&quot;))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Colorful Toes Co.</span><br></pre></td></tr></table></figure><p>If there are multiple variables, you can input them all at once using a dictionary.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = PromptTemplate(</span><br><span class="line">    input_variables=[&quot;company&quot;, &quot;product&quot;],</span><br><span class="line">    template=&quot;What is a good name for &#123;company&#125; that makes &#123;product&#125;?&quot;,</span><br><span class="line">)</span><br><span class="line">chain = LLMChain(llm=llm, prompt=prompt)</span><br><span class="line">print(chain.run(&#123;</span><br><span class="line">    &#x27;company&#x27;: &quot;ABC Startup&quot;,</span><br><span class="line">    &#x27;product&#x27;: &quot;colorful socks&quot;</span><br><span class="line">    &#125;))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Socktopia Colourful Creations.</span><br></pre></td></tr></table></figure><p>You can use a chat model in an LLMChain as well:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from langchain.prompts.chat import (</span><br><span class="line">    ChatPromptTemplate,</span><br><span class="line">    HumanMessagePromptTemplate,</span><br><span class="line">)</span><br><span class="line">human_message_prompt = HumanMessagePromptTemplate(</span><br><span class="line">        prompt=PromptTemplate(</span><br><span class="line">            template=&quot;What is a good name for a company that makes &#123;product&#125;?&quot;,</span><br><span class="line">            input_variables=[&quot;product&quot;],</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])</span><br><span class="line">chat = ChatOpenAI(temperature=0.9)</span><br><span class="line">chain = LLMChain(llm=chat, prompt=chat_prompt_template)</span><br><span class="line">print(chain.run(&quot;colorful socks&quot;))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Rainbow Socks Co.</span><br></pre></td></tr></table></figure><h2 id="Agents"><a href="#Agents" class="headerlink" title="Agents"></a>Agents</h2><p>Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an unknown chain that depends on the user’s input. In these types of chains, there is a “agent” which has access to a suite of tools. Depending on the user input, the agent can then decide which, if any, of these tools to call.</p><p>At the moment, there are two main types of agents in Langchain:</p><ol><li><p>“Action Agents”: these agents decide an action to take and take that action one step at a time</p></li><li><p>“Plan-and-Execute Agents”: these agents first decide a plan of actions to take, and then execute those actions one at a time.</p></li></ol><p>When should you use each one? Action Agents are more conventional, and good for small tasks. For more complex or long running tasks, the initial planning step helps to maintain long term objectives and focus. However, that comes at the expense of generally more calls and higher latency. These two agents are also not mutually exclusive - in fact, it is often best to have an Action Agent be in charge of the execution for the Plan and Execute agent.</p><h3 id="Action-Agents"><a href="#Action-Agents" class="headerlink" title="Action Agents"></a>Action Agents</h3><p>High level pseudocode of agents looks something like:</p><ul><li><p>Some user input is received</p></li><li><p>The agent decides which tool - if any - to use, and what the input to that tool should be</p></li><li><p>That tool is then called with that tool input, and an observation is recorded (this is just the output of calling that tool with that tool input)</p></li><li><p>That history of tool, tool input, and observation is passed back into the agent, and it decides what step to take next</p></li><li><p>This is repeated until the agent decides it no longer needs to use a tool, and then it responds directly to the user.</p></li></ul><p>The different abstractions involved in agents are as follows:</p><ul><li><p>Agent: this is where the logic of the application lives. Agents expose an interface that takes in user input along with a list of previous steps the agent has taken, and returns either an AgentAction or AgentFinish<br>  AgentAction corresponds to the tool to use and the input to that tool</p><p>  AgentFinish means the agent is done, and has information around what to return to the user</p></li><li><p>Tools: these are the actions an agent can take. What tools you give an agent highly depend on what you want the agent to do</p></li><li><p>Toolkits: these are groups of tools designed for a specific use case. For example, in order for an agent to interact with a SQL database in the best way it may need access to one tool to execute queries and another tool to inspect tables.</p></li><li><p>Agent Executor: this wraps an agent and a list of tools. This is responsible for the loop of running the agent iteratively until the stopping criteria is met.</p></li></ul><p>The most important abstraction of the four above to understand is that of the agent. Although an agent can be defined in whatever way one chooses, the typical way to construct an agent is with:</p><ul><li><p>PromptTemplate: this is responsible for taking the user input and previous steps and constructing a prompt to send to the language model</p></li><li><p>Language Model: this takes the prompt constructed by the PromptTemplate and returns some output</p></li><li><p>Output Parser: this takes the output of the Language Model and parses it into an AgentAction or AgentFinish object.</p></li></ul><h3 id="Plan-and-Execute-Agents"><a href="#Plan-and-Execute-Agents" class="headerlink" title="Plan-and-Execute Agents"></a>Plan-and-Execute Agents</h3><p>High level pseudocode of agents looks something like:</p><ul><li><p>Some user input is received</p></li><li><p>The planner lists out the steps to take</p></li><li><p>The executor goes through the list of steps, executing them</p></li></ul><p>The most typical implementation is to have the planner be a language model, and the executor be an action agent.</p><h2 id="Reveal-the-mystery-behind-agents"><a href="#Reveal-the-mystery-behind-agents" class="headerlink" title="Reveal the mystery behind agents"></a>Reveal the mystery behind agents</h2><p>It might sound like the agents are so smart, but the power actually comes from the large langage model itself.<br>With some clever prompt design, we put the whole workflow to llm through prompting, and let the llm to tell us what to do next.</p><p>Here we take a custom LLM agent as an example.<br>An LLM chat agent consists of three parts:</p><ul><li><p>PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do</p></li><li><p>ChatModel: This is the language model that powers the agent</p></li><li><p>stop sequence: Instructs the LLM to stop generating as soon as this string is found</p></li><li><p>OutputParser: This determines how to parse the LLMOutput into an AgentAction or AgentFinish object</p></li></ul><p>The LLMAgent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:</p><ol><li><p>Passes user input and any previous steps to the Agent (in this case, the LLMAgent)</p></li><li><p>If the Agent returns an AgentFinish, then return that directly to the user</p></li><li><p>If the Agent returns an AgentAction, then use that to call a tool and get an Observation</p></li><li><p>Repeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.</p></li></ol><p>AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).</p><p>AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.</p><h3 id="Set-up-environment"><a href="#Set-up-environment" class="headerlink" title="Set up environment"></a>Set up environment</h3><p>Do necessary imports, etc.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!pip install langchain</span><br><span class="line">!pip install google-search-results</span><br><span class="line">!pip install openai</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser</span><br><span class="line">from langchain.prompts import BaseChatPromptTemplate</span><br><span class="line">from langchain import SerpAPIWrapper, LLMChain</span><br><span class="line">from langchain.chat_models import ChatOpenAI</span><br><span class="line">from typing import List, Union</span><br><span class="line">from langchain.schema import AgentAction, AgentFinish, HumanMessage</span><br><span class="line">import re</span><br><span class="line">from getpass import getpass</span><br></pre></td></tr></table></figure><h3 id="Set-up-tool"><a href="#Set-up-tool" class="headerlink" title="Set up tool"></a>Set up tool</h3><p>Set up any tools the agent may want to use. This may be necessary to put in the prompt (so that the agent knows to use these tools).</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">SERPAPI_API_KEY = getpass()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Define which tools the agent can use to answer user queries</span><br><span class="line">search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)</span><br><span class="line">tools = [</span><br><span class="line">    Tool(</span><br><span class="line">        name = &quot;Search&quot;,</span><br><span class="line">        func=search.run,</span><br><span class="line">        description=&quot;useful for when you need to answer questions about current events&quot;</span><br><span class="line">    )</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="Prompt-Template"><a href="#Prompt-Template" class="headerlink" title="Prompt Template"></a>Prompt Template</h3><p>This instructs the agent on what to do. Generally, the template should incorporate:</p><ul><li><p>tools: which tools the agent has access and how and when to call them.</p></li><li><p>intermediate_steps: These are tuples of previous (AgentAction, Observation) pairs. These are generally not passed directly to the model, but the prompt template formats them in a specific way.</p></li><li><p>input: generic user input</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Set up the base template</span><br><span class="line">template = &quot;&quot;&quot;Complete the objective as best you can. You have access to the following tools:</span><br><span class="line"></span><br><span class="line">&#123;tools&#125;</span><br><span class="line"></span><br><span class="line">Use the following format:</span><br><span class="line"></span><br><span class="line">Question: the input question you must answer</span><br><span class="line">Thought: you should always think about what to do</span><br><span class="line">Action: the action to take, should be one of [&#123;tool_names&#125;]</span><br><span class="line">Action Input: the input to the action</span><br><span class="line">Observation: the result of the action</span><br><span class="line">... (this Thought/Action/Action Input/Observation can repeat N times)</span><br><span class="line">Thought: I now know the final answer</span><br><span class="line">Final Answer: the final answer to the original input question</span><br><span class="line"></span><br><span class="line">These were previous tasks you completed:</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Begin!</span><br><span class="line"></span><br><span class="line">Question: &#123;input&#125;</span><br><span class="line">&#123;agent_scratchpad&#125;&quot;&quot;&quot;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Set up a prompt template</span><br><span class="line">class CustomPromptTemplate(BaseChatPromptTemplate):</span><br><span class="line">    # The template to use</span><br><span class="line">    template: str</span><br><span class="line">    # The list of tools available</span><br><span class="line">    tools: List[Tool]</span><br><span class="line">    </span><br><span class="line">    def format_messages(self, **kwargs) -&gt; str:</span><br><span class="line">        # Get the intermediate steps (AgentAction, Observation tuples)</span><br><span class="line">        # Format them in a particular way</span><br><span class="line">        intermediate_steps = kwargs.pop(&quot;intermediate_steps&quot;)</span><br><span class="line">        thoughts = &quot;&quot;</span><br><span class="line">        for action, observation in intermediate_steps:</span><br><span class="line">            thoughts += action.log</span><br><span class="line">            thoughts += f&quot;\nObservation: &#123;observation&#125;\nThought: &quot;</span><br><span class="line">        # Set the agent_scratchpad variable to that value</span><br><span class="line">        kwargs[&quot;agent_scratchpad&quot;] = thoughts</span><br><span class="line">        # Create a tools variable from the list of tools provided</span><br><span class="line">        kwargs[&quot;tools&quot;] = &quot;\n&quot;.join([f&quot;&#123;tool.name&#125;: &#123;tool.description&#125;&quot; for tool in self.tools])</span><br><span class="line">        # Create a list of tool names for the tools provided</span><br><span class="line">        kwargs[&quot;tool_names&quot;] = &quot;, &quot;.join([tool.name for tool in self.tools])</span><br><span class="line">        formatted = self.template.format(**kwargs)</span><br><span class="line">        return [HumanMessage(content=formatted)]</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">prompt = CustomPromptTemplate(</span><br><span class="line">    template=template,</span><br><span class="line">    tools=tools,</span><br><span class="line">    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically</span><br><span class="line">    # This includes the `intermediate_steps` variable because that is needed</span><br><span class="line">    input_variables=[&quot;input&quot;, &quot;intermediate_steps&quot;]</span><br><span class="line">)</span><br></pre></td></tr></table></figure></li></ul><h3 id="Output-Parser"><a href="#Output-Parser" class="headerlink" title="Output Parser"></a>Output Parser</h3><p>The output parser is responsible for parsing the LLM output into AgentAction and AgentFinish. This usually depends heavily on the prompt used.</p><p>This is where you can change the parsing to do retries, handle whitespace, etc</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">class CustomOutputParser(AgentOutputParser):</span><br><span class="line">    </span><br><span class="line">    def parse(self, llm_output: str) -&gt; Union[AgentAction, AgentFinish]:</span><br><span class="line">        # Check if agent should finish</span><br><span class="line">        if &quot;Final Answer:&quot; in llm_output:</span><br><span class="line">            return AgentFinish(</span><br><span class="line">                # Return values is generally always a dictionary with a single `output` key</span><br><span class="line">                # It is not recommended to try anything else at the moment :)</span><br><span class="line">                return_values=&#123;&quot;output&quot;: llm_output.split(&quot;Final Answer:&quot;)[-1].strip()&#125;,</span><br><span class="line">                log=llm_output,</span><br><span class="line">            )</span><br><span class="line">        # Parse out the action and action input</span><br><span class="line">        regex = r&quot;Action\s*\d*\s*:(.*?)\nAction\s*\d*\s*Input\s*\d*\s*:[\s]*(.*)&quot;</span><br><span class="line">        match = re.search(regex, llm_output, re.DOTALL)</span><br><span class="line">        if not match:</span><br><span class="line">            raise ValueError(f&quot;Could not parse LLM output: `&#123;llm_output&#125;`&quot;)</span><br><span class="line">        action = match.group(1).strip()</span><br><span class="line">        action_input = match.group(2)</span><br><span class="line">        # Return the action and action input</span><br><span class="line">        return AgentAction(tool=action, tool_input=action_input.strip(&quot; &quot;).strip(&#x27;&quot;&#x27;), log=llm_output)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">output_parser = CustomOutputParser()</span><br></pre></td></tr></table></figure><h3 id="Set-up-LLM"><a href="#Set-up-LLM" class="headerlink" title="Set up LLM"></a>Set up LLM</h3><p>Choose the LLM you want to use!</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OPENAI_API_KEY = getpass()</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0)</span><br></pre></td></tr></table></figure><h3 id="Define-the-stop-sequence"><a href="#Define-the-stop-sequence" class="headerlink" title="Define the stop sequence"></a>Define the stop sequence</h3><p>This is important because it tells the LLM when to stop generation.</p><p>This depends heavily on the prompt and model you are using. Generally, you want this to be whatever token you use in the prompt to denote the start of an Observation (otherwise, the LLM may hallucinate an observation for you).</p><h3 id="Set-up-the-Agent"><a href="#Set-up-the-Agent" class="headerlink" title="Set up the Agent"></a>Set up the Agent</h3><p>We can now combine everything to set up our agent</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># LLM chain consisting of the LLM and a prompt</span><br><span class="line">llm_chain = LLMChain(llm=llm, prompt=prompt)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tool_names = [tool.name for tool in tools]</span><br><span class="line">agent = LLMSingleActionAgent(</span><br><span class="line">    llm_chain=llm_chain, </span><br><span class="line">    output_parser=output_parser,</span><br><span class="line">    stop=[&quot;\nObservation:&quot;], </span><br><span class="line">    allowed_tools=tool_names</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="Use-the-Agent"><a href="#Use-the-Agent" class="headerlink" title="Use the Agent"></a>Use the Agent</h3><p>Now we can use it!</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)</span><br><span class="line">agent_executor.run(&quot;Search for Leo DiCaprio&#x27;s girlfriend on the internet.&quot;)</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt; Entering new AgentExecutor chain...</span><br><span class="line">Thought: I should use a reliable search engine to get accurate information.</span><br><span class="line">Action: Search</span><br><span class="line">Action Input: &quot;Leo DiCaprio girlfriend&quot;</span><br><span class="line"></span><br><span class="line">Observation:He went on to date Gisele Bündchen, Bar Refaeli, Blake Lively, Toni Garrn and Nina Agdal, among others, before finally settling down with current girlfriend Camila Morrone, who is 23 years his junior.</span><br><span class="line">I have found the answer to the question.</span><br><span class="line">Final Answer: Leo DiCaprio&#x27;s current girlfriend is Camila Morrone.</span><br><span class="line"></span><br><span class="line">&gt; Finished chain.</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;Leo DiCaprio&#x27;s current girlfriend is Camila Morrone.&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> langchain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Class Parameters and Instance Attributes in Python</title>
      <link href="2023/05/25/python-class-paramter-vs-instance-attribute/"/>
      <url>2023/05/25/python-class-paramter-vs-instance-attribute/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h2><p>In object-oriented programming, Python offers powerful features for defining classes and creating objects with attributes and behaviors. Two essential concepts to grasp when working with classes are class parameters and instance attributes. In this blog post, we will explore the differences between these two concepts, their use cases, and how they affect the behavior and flexibility of your Python code.</p><h2 id="Class-Parameters"><a href="#Class-Parameters" class="headerlink" title="Class Parameters:"></a>Class Parameters:</h2><p>Class parameters, also known as class attributes, are variables that are shared among all instances of a class. They hold data that is common to all objects of that class. Here are some key points to consider:<br>Declaration: Class parameters are typically defined directly within the class definition and reside outside any methods.<br>Shared Data: Since class parameters are shared, any modification made to them will be reflected in all instances of the class.<br>Initialization: Class parameters are often assigned default values that will be used by all instances, although they can be modified individually if needed.<br>Access: Class parameters can be accessed using the class name itself, followed by the parameter name, without the need for an instance.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Car:</span><br><span class="line">    wheels = 4</span><br><span class="line">    doors = 4</span><br><span class="line">    engine = &quot;V6&quot;</span><br><span class="line"></span><br><span class="line">car1 = Car()</span><br><span class="line">print(car1.wheels)  # Output: 4</span><br><span class="line"></span><br><span class="line">car2 = Car()</span><br><span class="line">print(car2.doors)   # Output: 4</span><br></pre></td></tr></table></figure><h2 id="Instance-Attributes"><a href="#Instance-Attributes" class="headerlink" title="Instance Attributes:"></a>Instance Attributes:</h2><p>Instance attributes are specific to each instance of a class. They represent unique data that can vary between different objects of the same class. Consider the following aspects of instance attributes:<br>Declaration: Instance attributes are defined within the <strong>init</strong>() method of a class, which serves as the constructor for the class. Each instance can have its own set of instance attributes.<br>Individual Data: Unlike class parameters, instance attributes are not shared between instances. Each instance maintains its own set of attribute values.<br>Initialization: Instance attributes are typically initialized using the parameters passed to the <strong>init</strong>() method during object creation. They allow for customizing attribute values on a per-instance basis.<br>Access: Instance attributes are accessed using the instance name followed by the attribute name. Each instance can access and modify its own instance attributes independently.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Car:</span><br><span class="line">    def __init__(self, wheels, doors, engine):</span><br><span class="line">        self.wheels = wheels</span><br><span class="line">        self.doors = doors</span><br><span class="line">        self.engine = engine</span><br><span class="line"></span><br><span class="line">car1 = Car(4, 4, &quot;V6&quot;)</span><br><span class="line">print(car1.wheels)  # Output: 4</span><br><span class="line">print(car1.doors)   # Output: 4</span><br><span class="line">print(car1.engine)  # Output: V6</span><br><span class="line"></span><br><span class="line">car2 = Car(2, 2, &quot;Electric&quot;)</span><br><span class="line">print(car2.wheels)  # Output: 2</span><br><span class="line">print(car2.doors)   # Output: 2</span><br><span class="line">print(car2.engine)  # Output: Electric</span><br></pre></td></tr></table></figure><h2 id="Use-Cases-and-Flexibility"><a href="#Use-Cases-and-Flexibility" class="headerlink" title="Use Cases and Flexibility:"></a>Use Cases and Flexibility:</h2><p>Understanding when to use class parameters or instance attributes is crucial for designing effective and flexible classes. Here are some guidelines:</p><p><code>Class Parameters</code>: Use class parameters when you want to store data that is shared among all instances of a class. Examples include constants, default values, or settings that remain consistent across objects. Class parameters provide a way to define and access shared data easily.</p><p><code>Instance Attributes</code>: Utilize instance attributes when you need to store data that is specific to each instance. They allow for customization and variation in attribute values between objects. Instance attributes are particularly useful when you want each object to maintain its state independently.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Introducing Pynecone, A Full-Stack Python Web Framework</title>
      <link href="2023/05/24/build-web-app-using-pynecone-in-python/"/>
      <url>2023/05/24/build-web-app-using-pynecone-in-python/</url>
      
        <content type="html"><![CDATA[<p>Pynecone is a comprehensive Python web framework that simplifies web app development from front-end to back-end and deployment. To get started, simply ensure you have Python 3.7+ and Node.js 12.22.0+ installed on your system, then use the pip command to install Pynecone:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip install pynecone</span><br></pre></td></tr></table></figure><p>Create a new project folder, initialize a new Pynecone project, and run the development web server using the provided commands:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ mkdir pynecode01</span><br><span class="line">$ <span class="built_in">cd</span> pynecode01</span><br><span class="line">$ pc init</span><br><span class="line">$ pc run</span><br></pre></td></tr></table></figure><p>Pynecone’s development server features live reloading capabilities, streamlining the development process.</p><p>The framework offers a wide range of components that can be easily implemented and customized to suit your web application needs. In this tutorial, a sample Pynecone todo application is demonstrated. Start by deleting the default code inside pynecode01.py and adding the following import statement:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pynecone <span class="keyword">as</span> pc</span><br></pre></td></tr></table></figure><p>Implement the application’s state by creating a <code>State</code> class that inherits from <code>pc.State</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">State</span>(<span class="params">pc.State</span>):</span></span><br><span class="line">    items = [<span class="string">&quot;Learn Python&quot;</span>, <span class="string">&quot;Learn Pynecone&quot;</span>, <span class="string">&quot;Have Fun&quot;</span>]</span><br><span class="line">    new_item: <span class="built_in">str</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_item</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.items += [self.new_item]</span><br><span class="line">        self.new_item = <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">finish_item</span>(<span class="params">self, item</span>):</span></span><br><span class="line">        self.items = [i <span class="keyword">for</span> i <span class="keyword">in</span> self.items <span class="keyword">if</span> i != item]</span><br></pre></td></tr></table></figure><p>Next, implement the <code>render_item</code> function to generate a single todo item:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">render_item</span>(<span class="params">item</span>):</span></span><br><span class="line">    <span class="keyword">return</span> pc.list_item(</span><br><span class="line">        pc.hstack(</span><br><span class="line">            pc.text(item, font_size=<span class="string">&quot;1.5em&quot;</span>),</span><br><span class="line">            pc.button(</span><br><span class="line">                <span class="string">&quot;X&quot;</span>,</span><br><span class="line">                color_scheme=<span class="string">&quot;red&quot;</span>,</span><br><span class="line">                size=<span class="string">&quot;sm&quot;</span>,</span><br><span class="line">                on_click=<span class="keyword">lambda</span>: State.finish_item(item),</span><br><span class="line">            ),</span><br><span class="line">            justify_content=<span class="string">&quot;space-between&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>Create the <code>todo_list</code> function that generates the remaining parts of the UI:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">todo_list</span>():</span></span><br><span class="line">    <span class="keyword">return</span> pc.container(</span><br><span class="line">        pc.vstack(</span><br><span class="line">            pc.heading(<span class="string">&quot;Todos&quot;</span>),</span><br><span class="line">            pc.<span class="built_in">input</span>(</span><br><span class="line">                on_blur=State.set_new_item,</span><br><span class="line">                placeholder=<span class="string">&quot;Add a todo...&quot;</span>,</span><br><span class="line">                bg=<span class="string">&quot;white&quot;</span>,</span><br><span class="line">            ),</span><br><span class="line">            pc.button(<span class="string">&quot;Add&quot;</span>, on_click=State.add_item, bg=<span class="string">&quot;green&quot;</span>, color=<span class="string">&quot;white&quot;</span>),</span><br><span class="line">            pc.divider(),</span><br><span class="line">            pc.ordered_list(</span><br><span class="line">                pc.foreach(State.items, <span class="keyword">lambda</span> item: render_item(item)),</span><br><span class="line">            ),</span><br><span class="line">            bg=<span class="string">&quot;#ededed&quot;</span>,</span><br><span class="line">            margin=<span class="string">&quot;5em&quot;</span>,</span><br><span class="line">            padding=<span class="string">&quot;1em&quot;</span>,</span><br><span class="line">            border_radius=<span class="string">&quot;0.5em&quot;</span>,</span><br><span class="line">            shadow=<span class="string">&quot;lg&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>Finally, create the application instance, add the routes, and initiate the compilation of the Pynecone web app:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">app = pc.App(state=State)</span><br><span class="line">app.add_page(todo_list, path=<span class="string">&quot;/&quot;</span>)</span><br><span class="line">app.<span class="built_in">compile</span>()</span><br></pre></td></tr></table></figure><p>In conclusion, Pynecone simplifies the process of building and deploying web apps with Python. Its full-stack framework caters to both beginners and experienced developers, making it an ideal choice for leveraging the power and simplicity of Python in web development. Give Pynecone a try and enhance your web app creations with ease.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pynecone </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Protecting Your API Endpoint in Frontend Applications</title>
      <link href="2023/05/23/strategies%20-to-portect-api-in-frontend/"/>
      <url>2023/05/23/strategies%20-to-portect-api-in-frontend/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction:"></a>Introduction:</h2><p>When developing frontend applications, it is common to interact with API endpoints to retrieve or send data from/to backend services. These API endpoints are often exposed to the users, which might pose a security risk for your application. In this blog, we will discuss the challenges of securing API endpoints in frontend applications and ways to minimize the risk of unauthorized access.</p><h2 id="Why-are-API-endpoints-exposed-in-frontend-applications"><a href="#Why-are-API-endpoints-exposed-in-frontend-applications" class="headerlink" title="Why are API endpoints exposed in frontend applications?"></a>Why are API endpoints exposed in frontend applications?</h2><p>Frontend applications typically execute JavaScript code on the client-side, in the user’s browser. Since the code is executed and visible to users, the API endpoints are also exposed. While you cannot entirely prevent users from calling the API endpoint directly, you can add various layers of security to make it more difficult for unauthorized users to abuse your API.</p><h2 id="How-can-we-minimize-the-risks"><a href="#How-can-we-minimize-the-risks" class="headerlink" title="How can we minimize the risks?"></a>How can we minimize the risks?</h2><p>Here are some methods to improve the security of API endpoints in frontend applications:</p><ol><li>Server-side validation and authorization:</li></ol><p>Each request to your API endpoint should be authenticated and authorized by validating and checking user permissions on the server-side. This way, you can control which users have access to your API and what actions they can perform.</p><ol start="2"><li>HTTP Referer Header:</li></ol><p>Checking the HTTP referer header on your server-side code can ensure that the request is coming from the expected origin or domain. However, this method can be bypassed by manually modifying the referer header, so it should be used with caution.</p><ol start="3"><li>Rate Limiting:</li></ol><p>Implementing rate limiting for your API can help slow down or deter abusive requests. By limiting the number of requests from an IP address within a specified period of time, you can prevent unauthorized users from repeatedly calling your API.</p><ol start="4"><li>Short-lived access tokens:</li></ol><p>Instead of using a hardcoded API key in your JavaScript, require users to log in and receive a short-lived access token. These tokens can be checked on your server for each API request and can be revoked or refreshed as needed, providing a more secure way of managing access to your API.</p><ol start="5"><li>Obfuscating JavaScript code:</li></ol><p>While this method cannot prevent an advanced user from reverse engineering the code, it can make the code harder to understand, adding a slight barrier to unauthorized access. Tools like UglifyJS or Terser can help with obfuscating your code.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Securing API endpoints in frontend applications is a challenging task, and no method can guarantee complete protection. By applying multiple layers of security, you can minimize the risk of unauthorized access. For sensitive operations, it is best to perform them server-side, where you have complete control over the execution environment. Your frontend app should only communicate with your server to request actions or retrieve data from the API, rather than interacting with the API directly. This approach adds an additional layer of security and control over sensitive operations.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> frontend </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Handle Long Running Tasks in FastAPI with Callbacks</title>
      <link href="2023/05/22/handle-long-running-tasks-with-fastapi-endpoints/"/>
      <url>2023/05/22/handle-long-running-tasks-with-fastapi-endpoints/</url>
      
        <content type="html"><![CDATA[<p>In this blog, we will explore handling long-running tasks in FastAPI while allowing users to receive notifications once the task is complete. For this purpose, we will use FastAPI’s background tasks feature and implement a callback mechanism.</p><h2 id="FastAPI-Background-Tasks"><a href="#FastAPI-Background-Tasks" class="headerlink" title="FastAPI Background Tasks"></a>FastAPI Background Tasks</h2><p>FastAPI has built-in support for background tasks using the <code>BackgroundTasks</code> class. This feature allows long-running tasks to execute in the background without blocking the main application, enabling it to continue processing other incoming requests.</p><p>To use background tasks in FastAPI, follow these steps:</p><ol><li>Import <code>BackgroundTasks</code> from FastAPI.</li><li>Create a new function (e.g., <code>long_running_task</code>) that performs the time-consuming operation.</li><li>Modify your route to accept a <code>BackgroundTasks</code> parameter.</li><li>In your route, add the task using <code>background_tasks.add_task()</code> method with the required arguments, and immediately return a response to the user.</li></ol><h2 id="Setting-up-Callbacks"><a href="#Setting-up-Callbacks" class="headerlink" title="Setting up Callbacks"></a>Setting up Callbacks</h2><p>To notify the user once the task is complete, we can implement a callback mechanism by allowing the user to provide a callback URL as a parameter in their request. When the long-running task finishes, our application will send an HTTP request to the specified callback URL with the task’s results or status update.</p><p>In this tutorial, we will use Python’s <code>httpx</code> library to make HTTP requests.</p><h2 id="Complete-Example"><a href="#Complete-Example" class="headerlink" title="Complete Example"></a>Complete Example</h2><p>Here’s the complete code example, including the FastAPI application with background tasks and callbacks and a client-side script to call the API endpoint.</p><h3 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites"></a>Prerequisites</h3><p>Install the necessary packages:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pip install fastapi uvicorn httpx requests</span><br></pre></td></tr></table></figure><h3 id="FastAPI-Application"><a href="#FastAPI-Application" class="headerlink" title="FastAPI Application"></a>FastAPI Application</h3><p>Create a new FastAPI application in a file named <code>app.py</code>:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, BackgroundTasks</span><br><span class="line"><span class="keyword">from</span> pydantic <span class="keyword">import</span> HttpUrl</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">post_callback</span>(<span class="params">url: <span class="built_in">str</span>, payload: <span class="built_in">dict</span></span>):</span></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> httpx.AsyncClient() <span class="keyword">as</span> client:</span><br><span class="line">        <span class="keyword">await</span> client.post(url, json=payload)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">long_running_task</span>(<span class="params">duration: <span class="built_in">int</span>, callback_url: <span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="comment"># Simulate a time-consuming operation</span></span><br><span class="line">    time.sleep(duration)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Send the result to the callback URL</span></span><br><span class="line">    result = &#123;<span class="string">&quot;status&quot;</span>: <span class="string">&quot;completed&quot;</span>, <span class="string">&quot;data&quot;</span>: <span class="string">&quot;result_data&quot;</span>&#125;</span><br><span class="line">    <span class="keyword">await</span> post_callback(callback_url, result)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.post(<span class="params"><span class="string">&quot;/start_task&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">start_long_running_task</span>(<span class="params">background_tasks: BackgroundTasks, duration: <span class="built_in">int</span> = <span class="number">10</span>, callback_url: <span class="built_in">str</span> = <span class="string">&quot;http://example.com/callback&quot;</span></span>):</span></span><br><span class="line">    background_tasks.add_task(long_running_task, duration, callback_url)</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&quot;message&quot;</span>: <span class="string">&quot;Task started! You&#x27;ll receive a notification at the callback URL once it&#x27;s complete.&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(<span class="string">&quot;app:app&quot;</span>, host=<span class="string">&quot;127.0.0.1&quot;</span>, port=<span class="number">8000</span>, log_level=<span class="string">&quot;info&quot;</span>, reload=<span class="literal">True</span>, workers=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="Calling-the-API-Endpoint"><a href="#Calling-the-API-Endpoint" class="headerlink" title="Calling the API Endpoint"></a>Calling the API Endpoint</h3><p>Create a new Python script named <code>call_start_task.py</code> to call the <code>/start_task</code> endpoint:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://127.0.0.1:8000/start_task&quot;</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&quot;duration&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    <span class="string">&quot;callback_url&quot;</span>: <span class="string">&quot;http://example.com/callback&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, json=data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.status_code)</span><br><span class="line"><span class="built_in">print</span>(response.json())</span><br></pre></td></tr></table></figure><p>Replace the <code>http://example.com/callback</code> with your own callback URL that accepts POST requests and processes the data sent as JSON payload.</p><h2 id="Testing-the-Example"><a href="#Testing-the-Example" class="headerlink" title="Testing the Example"></a>Testing the Example</h2><p>First, start your FastAPI application by running:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python app.py</span><br></pre></td></tr></table></figure><p>Next, test the API by running the client-side script:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">python call_start_task.py</span><br></pre></td></tr></table></figure><p>This script will call the <code>/start_task</code> endpoint, providing a duration and callback URL. The FastAPI app will then run the long-running task in the background and send a POST request with the results to the specified callback URL upon task completion.</p><p>This approach allows your FastAPI application to effectively handle long-running tasks while maintaining excellent performance and providing a way to notify users once their tasks are complete.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> fastapi </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>use chatgpt with pandas in python</title>
      <link href="2023/05/22/use-chatgpt-with-pandas/"/>
      <url>2023/05/22/use-chatgpt-with-pandas/</url>
      
        <content type="html"><![CDATA[<p>Pandas AI is a Python library that enhances Pandas, a widely used tool for data analysis and manipulation, by incorporating generative artificial intelligence capabilities. It is specifically designed to complement Pandas, rather than serving as a substitute for it.</p><p>to install the package, you need to have <code>python 3.9</code> or higher</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pandasai</span><br></pre></td></tr></table></figure><p>PandasAI is specifically designed to work alongside Pandas, providing conversational abilities to the library. It enables you to inquire about your data and receive responses in the form of Pandas DataFrames. As an illustration, you can utilize PandasAI to identify all rows in a DataFrame where a column’s value surpasses 5, and it will yield a DataFrame solely comprising those rows.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from pandasai import PandasAI</span><br><span class="line"></span><br><span class="line"># Sample DataFrame</span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    &quot;country&quot;: [&quot;United States&quot;, &quot;United Kingdom&quot;, &quot;France&quot;, &quot;Germany&quot;, &quot;Italy&quot;, &quot;Spain&quot;, &quot;Canada&quot;, &quot;Australia&quot;, &quot;Japan&quot;, &quot;China&quot;],</span><br><span class="line">    &quot;gdp&quot;: [19294482071552, 2891615567872, 2411255037952, 3435817336832, 1745433788416, 1181205135360, 1607402389504, 1490967855104, 4380756541440, 14631844184064],</span><br><span class="line">    &quot;happiness_index&quot;: [6.94, 7.16, 6.66, 7.07, 6.38, 6.4, 7.23, 7.22, 5.87, 5.12]</span><br><span class="line">&#125;)</span><br><span class="line"></span><br><span class="line"># Instantiate a LLM</span><br><span class="line">from pandasai.llm.openai import OpenAI</span><br><span class="line">llm = OpenAI(api_token=&quot;YOUR_API_TOKEN&quot;)</span><br><span class="line"></span><br><span class="line">pandas_ai = PandasAI(llm, conversational=False)</span><br><span class="line">pandas_ai(df, prompt=&#x27;Which are the 5 happiest countries?&#x27;)</span><br></pre></td></tr></table></figure><p>the output should be like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">6            Canada</span><br><span class="line">7         Australia</span><br><span class="line">1    United Kingdom</span><br><span class="line">3           Germany</span><br><span class="line">0     United States</span><br><span class="line">Name: country, dtype: object</span><br></pre></td></tr></table></figure><p>another cool feature is to plot:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pandas_ai(</span><br><span class="line">    df,</span><br><span class="line">    &quot;Plot the histogram of countries showing for each the gpd, using different colors for each bar&quot;,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>the result looks like this:<br><img src="/content/images/2023-05-22-01.png" alt="plot from pandas ai"></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandasai </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Getting Started with Docker and Docker Compose on Ubuntu</title>
      <link href="2023/05/21/install-docker-and-docker-compose-on-ubuntu/"/>
      <url>2023/05/21/install-docker-and-docker-compose-on-ubuntu/</url>
      
        <content type="html"><![CDATA[<p>Docker and Docker Compose are powerful tools that facilitate containerization and the management of multi-container applications. Docker allows you to create and run containers, while Docker Compose simplifies the orchestration of multiple containers within an application. In this blog post, we will guide you through the process of installing Docker and Docker Compose on Ubuntu, enabling you to leverage the benefits of containerization in your projects.</p><h2 id="Step-1-Update-package-lists"><a href="#Step-1-Update-package-lists" class="headerlink" title="Step 1: Update package lists"></a>Step 1: Update package lists</h2><p>Before we begin, let’s ensure that our Ubuntu system has the latest package information. Open a terminal and run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure><h2 id="Step-2-Install-necessary-packages-to-allow-apt-to-use-repositories-over-HTTPS"><a href="#Step-2-Install-necessary-packages-to-allow-apt-to-use-repositories-over-HTTPS" class="headerlink" title="Step 2: Install necessary packages to allow apt to use repositories over HTTPS"></a>Step 2: Install necessary packages to allow apt to use repositories over HTTPS</h2><p>To prepare our system for Docker installation, we need to install a few prerequisite packages. Execute the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install apt-transport-https ca-certificates curl software-properties-common</span><br></pre></td></tr></table></figure><h2 id="Step-3-Add-the-official-Docker-GPG-key"><a href="#Step-3-Add-the-official-Docker-GPG-key" class="headerlink" title="Step 3: Add the official Docker GPG key"></a>Step 3: Add the official Docker GPG key</h2><p>To authenticate Docker’s official repositories, we’ll add the GPG key. Run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg</span><br></pre></td></tr></table></figure><h2 id="Step-4-Add-the-Docker-repository-to-APT-sources"><a href="#Step-4-Add-the-Docker-repository-to-APT-sources" class="headerlink" title="Step 4: Add the Docker repository to APT sources"></a>Step 4: Add the Docker repository to APT sources</h2><p>To access the Docker packages from the official repository, we need to add the repository to our APT sources. Execute the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">echo &quot;deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable&quot; | sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null</span><br></pre></td></tr></table></figure><h2 id="Step-5-Update-package-lists-again"><a href="#Step-5-Update-package-lists-again" class="headerlink" title="Step 5: Update package lists again"></a>Step 5: Update package lists again</h2><p>After adding the Docker repository, update the package lists once more:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure><h2 id="Step-6-Install-Docker"><a href="#Step-6-Install-Docker" class="headerlink" title="Step 6: Install Docker"></a>Step 6: Install Docker</h2><p>Now, we can proceed with the installation of Docker itself. Run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure><h2 id="Step-7-Verify-the-installation"><a href="#Step-7-Verify-the-installation" class="headerlink" title="Step 7: Verify the installation"></a>Step 7: Verify the installation</h2><p>To confirm that Docker is installed correctly, we’ll run a simple test by executing a Docker container. Enter the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo docker run hello-world</span><br></pre></td></tr></table></figure><p>If the installation was successful, you should see a “Hello from Docker!” message in the terminal.</p><h2 id="Step-8-Install-Docker-Compose"><a href="#Step-8-Install-Docker-Compose" class="headerlink" title="Step 8: Install Docker Compose"></a>Step 8: Install Docker Compose</h2><p>To install Docker Compose on Ubuntu, follow these additional steps:</p><h3 id="Step-1-Update-package-lists-1"><a href="#Step-1-Update-package-lists-1" class="headerlink" title="Step 1: Update package lists"></a>Step 1: Update package lists</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br></pre></td></tr></table></figure><h3 id="Step-2-Install-required-packages"><a href="#Step-2-Install-required-packages" class="headerlink" title="Step 2: Install required packages"></a>Step 2: Install required packages</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install curl</span><br></pre></td></tr></table></figure><h3 id="Step-3-Download-the-Docker-Compose-binary"><a href="#Step-3-Download-the-Docker-Compose-binary" class="headerlink" title="Step 3: Download the Docker Compose binary"></a>Step 3: Download the Docker Compose binary</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo curl -L &quot;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><h3 id="Step-4-Apply-executable-permissions-to-the-binary"><a href="#Step-4-Apply-executable-permissions-to-the-binary" class="headerlink" title="Step 4: Apply executable permissions to the binary"></a>Step 4: Apply executable permissions to the binary</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo chmod +x /usr/local/bin/docker-compose</span><br></pre></td></tr></table></figure><h3 id="Step-5-Verify-the-installation"><a href="#Step-5-Verify-the-installation" class="headerlink" title="Step 5: Verify the installation"></a>Step 5: Verify the installation</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker-compose --version</span><br></pre></td></tr></table></figure><p>This command will display the installed version of Docker Compose if the installation was successful.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>Congratulations! You have successfully installed Docker and Docker Compose on your Ubuntu system. With Docker, you can create and manage containers for your applications, while Docker Compose allows you to define and orchestrate multi-container setups easily. These powerful tools provide flexibility and scalability, enabling you to streamline your development and deployment processes.</p><p>Now that you have Docker and Docker Compose up and running, explore the world of containerization and unlock the potential of your projects by leveraging their capabilities. Happy containerizing!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> docker compose </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Squeeze and Unsqueeze in PyTorch</title>
      <link href="2023/05/19/squeeze-and-unsqueeze-in-pytorch/"/>
      <url>2023/05/19/squeeze-and-unsqueeze-in-pytorch/</url>
      
        <content type="html"><![CDATA[<p>When working with tensors in PyTorch, you may encounter situations where you need to modify the shape of your tensors by removing or adding dimensions. Two useful functions for these tasks are <code>squeeze()</code> and <code>unsqueeze()</code>. In this blog post, we will discuss these functions, their use cases, and provide examples to help you understand how to use them effectively.</p><h2 id="Squeeze"><a href="#Squeeze" class="headerlink" title="Squeeze"></a>Squeeze</h2><p>The <code>squeeze()</code> function in PyTorch is used to remove dimensions of size 1 from a tensor. This can be helpful when you want to remove unnecessary dimensions from your tensor, making it more compact and easier to work with.</p><p>Here’s an example of how to use <code>squeeze()</code> in PyTorch:</p><ol><li>Import the necessary libraries:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><ol start="2"><li>Create a tensor with dimensions of size 1:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of size (2, 1, 3, 1)</span></span><br><span class="line">tensor = torch.tensor([[[[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]], [[[<span class="number">4</span>], [<span class="number">5</span>], [<span class="number">6</span>]]]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original tensor:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Original tensor:</span><br><span class="line">tensor([[[[1],</span><br><span class="line">          [2],</span><br><span class="line">          [3]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[4],</span><br><span class="line">          [5],</span><br><span class="line">          [6]]]])</span><br><span class="line">Shape: torch.Size([2, 1, 3, 1])</span><br></pre></td></tr></table></figure><ol start="3"><li>Use <code>squeeze()</code> to remove dimensions of size 1:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Remove dimensions of size 1</span></span><br><span class="line">squeezed_tensor = tensor.squeeze()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Squeezed tensor:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(squeezed_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, squeezed_tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Squeezed tensor:</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]])</span><br><span class="line">Shape: torch.Size([2, 3])</span><br></pre></td></tr></table></figure><p>As you can see, the dimensions of size 1 have been removed, and the resulting tensor has a shape of (2, 3).</p><h2 id="Unsqueeze"><a href="#Unsqueeze" class="headerlink" title="Unsqueeze"></a>Unsqueeze</h2><p>The <code>unsqueeze()</code> function in PyTorch is used to add a dimension of size 1 at a specified position in a tensor. This can be helpful when you want to add a singleton dimension to match the shape of another tensor or to perform certain operations that require specific tensor shapes.</p><p>Here’s an example of how to use <code>unsqueeze()</code> in PyTorch:</p><ol><li>Import the necessary libraries:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure><ol start="2"><li>Create a tensor without dimensions of size 1:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create a tensor of size (2, 3)</span></span><br><span class="line">tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Original tensor:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Original tensor:</span><br><span class="line">tensor([[1, 2, 3],</span><br><span class="line">        [4, 5, 6]])</span><br><span class="line">Shape: torch.Size([2, 3])</span><br></pre></td></tr></table></figure><ol start="3"><li>Use <code>unsqueeze()</code> to add dimensions of size 1:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Add a dimension of size 1 at the end (dim=-1) and then at position 1 (dim=1)</span></span><br><span class="line">unsqueezed_tensor = tensor.unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Unsqueezed tensor:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(unsqueezed_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Shape:&quot;</span>, unsqueezed_tensor.shape)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Unsqueezed tensor:</span><br><span class="line">tensor([[[[1],</span><br><span class="line">          [2],</span><br><span class="line">          [3]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[4],</span><br><span class="line">          [5],</span><br><span class="line">          [6]]]])</span><br><span class="line">Shape: torch.Size([2, 1, 3, 1])</span><br></pre></td></tr></table></figure><p>As you can see, the original tensor had a shape of (2, 3). After applying <code>unsqueeze(-1).unsqueeze(1)</code>, the resulting tensor has a shape of (2, 1, 3, 1). A dimension of size 1 was added at the end (dim=-1) and then at position 1 (dim=1).</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we discussed the <code>squeeze()</code> and <code>unsqueeze()</code> functions in PyTorch and provided examples to demonstrate their usage. These functions are essential when working with tensors, as they allow you to manipulate the shape of your tensors to match the requirements of specific operations or to make them compatible with other tensors. By understanding how to use these functions effectively, you can improve the efficiency and readability of your PyTorch code.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> squeeze </tag>
            
            <tag> unsqueeze </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to setup a simple proxy server using fastapi</title>
      <link href="2023/05/17/simple-proxy-server-using-fastapi/"/>
      <url>2023/05/17/simple-proxy-server-using-fastapi/</url>
      
        <content type="html"><![CDATA[<p>Following code shows how to set up a simple proxy server using fastapi.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from fastapi import FastAPI, Request</span><br><span class="line">from fastapi.responses import HTMLResponse</span><br><span class="line">import httpx</span><br><span class="line">import uvicorn</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br><span class="line"></span><br><span class="line">@app.get(&quot;/&#123;path:path&#125;&quot;)</span><br><span class="line">async def proxy(request: Request, path: str, scheme: str = &quot;http&quot;):</span><br><span class="line">    url = f&quot;&#123;scheme&#125;://&#123;path&#125;&quot;</span><br><span class="line">    async with httpx.AsyncClient() as client:</span><br><span class="line">        response = await client.get(url, headers=request.headers, params=request.query_params)</span><br><span class="line">    return HTMLResponse(content=response.text, status_code=response.status_code)</span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    uvicorn.run(&quot;app:app&quot;, host=&quot;0.0.0.0&quot;, port=8000, log_level=&quot;info&quot;)</span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Put that code into a python file, such as <code>app.py</code>, launch the app on your server, for example, if locally just do:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python app.py</span><br></pre></td></tr></table></figure><p>Once the fastapi is turned on, one can browse websites like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">http://localhost:8000/bing.com?scheme=https</span><br></pre></td></tr></table></figure><p>However, Notice that, this basic example will not work correctly for websites like google.com, as it does not handle cookies, JavaScript, and other dynamic content. Implementing a full-featured proxy server that can handle such websites is beyond the scope of a simple answer, but you can explore existing proxy server projects like <a href="https://mitmproxy.org/">mitmproxy</a> or <a href="https://github.com/tinyproxy/tinyproxy">Tinyproxy</a> for inspiration and guidance.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fastapi </tag>
            
            <tag> proxy server </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Predicting Stock Prices with PyTorch Transformer, a demo of using dummy data</title>
      <link href="2023/05/15/stock-price-prediction-using-transformer-in-pytorch/"/>
      <url>2023/05/15/stock-price-prediction-using-transformer-in-pytorch/</url>
      
        <content type="html"><![CDATA[<p>Predicting stock prices is a challenging task that has attracted the attention of researchers and practitioners alike. With the advent of deep learning techniques, many models have been proposed to tackle this problem. One such model is the Transformer, which has achieved state-of-the-art results in many natural language processing tasks. In this blog post, we will walk you through an example of using a PyTorch Transformer to predict the next 5 days of stock prices given the previous 10 days.</p><h2 id="Getting-Started"><a href="#Getting-Started" class="headerlink" title="Getting Started"></a>Getting Started</h2><p>First, let’s import the necessary libraries:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h2 id="Generating-Dummy-Stock-Price-Data"><a href="#Generating-Dummy-Stock-Price-Data" class="headerlink" title="Generating Dummy Stock Price Data"></a>Generating Dummy Stock Price Data</h2><p>For this example, we will generate some dummy stock price data:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_days = <span class="number">200</span></span><br><span class="line">stock_prices = np.random.rand(num_days) * <span class="number">100</span></span><br></pre></td></tr></table></figure><h2 id="Preprocessing-the-Data"><a href="#Preprocessing-the-Data" class="headerlink" title="Preprocessing the Data"></a>Preprocessing the Data</h2><p>We will prepare the input and target sequences for our model:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_seq_len = <span class="number">10</span></span><br><span class="line">output_seq_len = <span class="number">5</span></span><br><span class="line">num_samples = num_days - input_seq_len - output_seq_len + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">src_data = torch.tensor([stock_prices[i:i+input_seq_len] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]).unsqueeze(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">tgt_data = torch.tensor([stock_prices[i+input_seq_len:i+input_seq_len+output_seq_len] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_samples)]).unsqueeze(-<span class="number">1</span>).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure><h2 id="Creating-a-Custom-Transformer-Model"><a href="#Creating-a-Custom-Transformer-Model" class="headerlink" title="Creating a Custom Transformer Model"></a>Creating a Custom Transformer Model</h2><p>We will create a custom Transformer model for stock price prediction:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StockPriceTransformer</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, d_model, nhead, num_layers, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(StockPriceTransformer, self).__init__()</span><br><span class="line">        self.input_linear = nn.Linear(<span class="number">1</span>, d_model)</span><br><span class="line">        self.transformer = nn.Transformer(d_model, nhead, num_layers, dropout=dropout)</span><br><span class="line">        self.output_linear = nn.Linear(d_model, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, src, tgt</span>):</span></span><br><span class="line">        src = self.input_linear(src)</span><br><span class="line">        tgt = self.input_linear(tgt)</span><br><span class="line">        output = self.transformer(src, tgt)</span><br><span class="line">        output = self.output_linear(output)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">d_model = <span class="number">64</span></span><br><span class="line">nhead = <span class="number">4</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">model = StockPriceTransformer(d_model, nhead, num_layers, dropout=dropout)</span><br></pre></td></tr></table></figure><h2 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h2><p>We will set up the training parameters, loss function, and optimizer:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">epochs = <span class="number">100</span></span><br><span class="line">lr = <span class="number">0.001</span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><p>Now, we will train the model with a training loop:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_samples, batch_size):</span><br><span class="line">        src_batch = src_data[i:i+batch_size].transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        tgt_batch = tgt_data[i:i+batch_size].transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output = model(src_batch, tgt_batch[:-<span class="number">1</span>])</span><br><span class="line">        loss = criterion(output, tgt_batch[<span class="number">1</span>:])</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h2 id="Predicting-the-Next-5-Days-of-Stock-Prices"><a href="#Predicting-the-Next-5-Days-of-Stock-Prices" class="headerlink" title="Predicting the Next 5 Days of Stock Prices"></a>Predicting the Next 5 Days of Stock Prices</h2><p>Finally, we will predict the next 5 days of stock prices using the trained model:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">src = torch.tensor(stock_prices[-input_seq_len:]).unsqueeze(-<span class="number">1</span>).unsqueeze(<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">tgt = torch.zeros(output_seq_len, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(output_seq_len):</span><br><span class="line">        prediction = model(src, tgt[:i+<span class="number">1</span>])</span><br><span class="line">        tgt[i] = prediction[-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">output = tgt.squeeze().tolist()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Next 5 days of stock prices:&quot;</span>, output)</span><br></pre></td></tr></table></figure><p>In this prediction loop, we use the autoregressive decoding approach (<code>model(src, tgt[:i+1])</code>) to generate the output sequence step by step, as the output at each step depends on the previous outputs.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we demonstrated how to predict stock prices using a PyTorch Transformer model. We generated dummy stock price data, preprocessed it, created a custom Transformer model, trained the model, and predicted the next 5 days of stock prices. This example serves as a starting point for developing more sophisticated stock price prediction models using deep learning techniques.</p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/stock-price-prediction-using-transformer-toy-example/2023-05-15-stock%20price%20prediction%20using%20transformer%20in%20pytorch.ipynb">github link</a></p><h2 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h2><p>important questions and answers related to the PyTorch Transformer discussed in this conversation:</p><ol><li><p><strong>Why do we pass <code>tgt_batch[:-1]</code> to the model and use <code>tgt_batch[1:]</code> to compare with the output during training?</strong></p><p>We do this because we are using a technique called “teacher forcing” during training. Teacher forcing is a method used in sequence-to-sequence models, where the true output sequence is fed as input to the model during training instead of using the model’s own predictions from the previous time step. This helps the model to learn faster and more accurately.</p></li><li><p><strong>What determines the number of sequences generated by the model?</strong></p><p>The number of sequences generated by the model is determined by the <code>output_seq_len</code> variable. This means that the model is trained to predict the next <code>output_seq_len</code> stock prices in the sequence, given the previous <code>input_seq_len</code> stock prices.</p></li><li><p><strong>Why do we use the last position of the prediction at every step during inference?</strong></p><p>We use the last position of the prediction at every step during inference because we are generating the next stock prices one at a time in an autoregressive manner. The new prediction will always be at the last position of the output sequence, so we take the last position of the prediction and append it to the target sequence.</p></li><li><p><strong>Why is the sequence length in the output the same as the sequence length in <code>tgt</code>?</strong></p><p>The sequence length in the output is the same as the sequence length in <code>tgt</code> because the Transformer model is designed to generate an output sequence of the same length as the input target sequence. The model generates an output sequence based on the input target sequence, and the output sequence has the same length as the input target sequence.</p></li><li><p><strong>Should we use ground truth during inference?</strong></p><p>During inference, you generally do not have access to the ground truth, as the goal is to make predictions for future data points that are not yet known. The purpose of training a model is to enable it to make accurate predictions when ground truth is not available.</p></li><li><p><strong>Can we use the previous 4 days’ stock prices as the initial target sequence during inference?</strong></p><p>Yes, you can use the previous 4 days’ stock prices as the initial target sequence during inference if you want to predict the next 5 days based on the last 14 days. This way, the model will have more context to generate predictions for the next 5 days.</p></li><li><p><strong>Does the value in the “tgt” parameter matter besides the sequence length?</strong></p><p>Yes, the values in the <code>tgt</code> parameter do matter, as they provide context to the model and influence its predictions. The Transformer model generates predictions based on both the source sequence (<code>src</code>) and the target sequence (<code>tgt</code>). The values in the <code>tgt</code> parameter provide additional context to the model, which helps it learn to generate more accurate predictions.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to generate authentication secrets using python</title>
      <link href="2023/05/12/generate-authentication-secrets-in-python/"/>
      <url>2023/05/12/generate-authentication-secrets-in-python/</url>
      
        <content type="html"><![CDATA[<p>One can generate authentication secrets using Python by leveraging the <code>secrets</code> module, which is available in Python 3.6 and later. The <code>secrets</code> module provides functions for generating cryptographically secure random numbers and strings, which can be used as authentication secrets.</p><p>Here’s an example of how to generate an authentication secret using Python:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> secrets</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_auth_secret</span>(<span class="params">length=<span class="number">32</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generate an authentication secret of the given length.&quot;&quot;&quot;</span></span><br><span class="line">    characters = string.ascii_letters + string.digits + string.punctuation</span><br><span class="line">    secret = <span class="string">&#x27;&#x27;</span>.join(secrets.choice(characters) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length))</span><br><span class="line">    <span class="keyword">return</span> secret</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate a 32-character authentication secret</span></span><br><span class="line">auth_secret = generate_auth_secret()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Authentication secret:&quot;</span>, auth_secret)</span><br></pre></td></tr></table></figure><p>This code defines a function <code>generate_auth_secret</code> that generates a random string of the specified length using characters from the ASCII letters, digits, and punctuation. The <code>secrets.choice</code> function is used to select a random character from the set of characters for each position in the secret.</p><p>You can customize the length of the secret by passing a different value to the <code>length</code> parameter when calling the <code>generate_auth_secret</code> function.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> authentication secrets generation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Securing API Calls in Public-Facing Websites, Best Practices for JavaScript and Authentication</title>
      <link href="2023/05/11/secure-api-calls-in-public-facing-website/"/>
      <url>2023/05/11/secure-api-calls-in-public-facing-website/</url>
      
        <content type="html"><![CDATA[<p>As more and more websites incorporate APIs to provide dynamic and interactive experiences to their users, security becomes an increasingly important consideration. In particular, when it comes to public-facing websites where users do not need to log in, it can be challenging to secure API calls without compromising the security of the site.</p><p>One common scenario involves the use of JavaScript to call a protected API with authentication. However, it is not safe to include authentication credentials in the client-side JavaScript code. So what can be done to ensure the security of API calls in public-facing websites?</p><p>One possible solution is to use an OAuth 2.0 authentication flow. This involves redirecting the user to a login page hosted by the server, and after successful authentication, the server returns an access token to the client. The client can then use this token to call the protected API. Alternatively, a proxy server can handle the authentication and authorization, and the access token can be securely stored on the server-side.</p><p>Another approach is to use JSON Web Tokens (JWTs) to handle authentication and authorization. JWTs can be a secure way to pass authentication information between the client and server, but it is important to ensure that the tokens are properly secured to prevent misuse.</p><p>When implementing these solutions, it is important to keep sensitive information, such as API keys, authentication tokens, and private keys, on the server-side. These should never be exposed to the client-side JavaScript code. It is also important to use secure communication protocols, such as HTTPS, to prevent eavesdropping and tampering.</p><p>In conclusion, securing API calls in public-facing websites can be challenging, but there are solutions available that can help ensure the security of both the site and its users. By implementing best practices for JavaScript and authentication, website developers can provide a safe and secure experience for their users.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> JWT </tag>
            
            <tag> oAuth2.0 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matrix muliplication operator `@` in python</title>
      <link href="2023/05/07/matrix-multiplication-operator-in-python/"/>
      <url>2023/05/07/matrix-multiplication-operator-in-python/</url>
      
        <content type="html"><![CDATA[<p>In Python, the <code>@</code> operator is used to perform matrix multiplication between two arrays when using the NumPy library. Matrix multiplication is an important operation in linear algebra, and it has many applications in machine learning, deep learning, data science, and other fields.</p><p>To perform matrix multiplication in NumPy, we can use the <code>@</code> operator between two NumPy arrays. For example, suppose we have two NumPy arrays <code>a</code> and <code>b</code>:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">a = np.array([[1, 2], [3, 4]])</span><br><span class="line">b = np.array([[5, 6], [7, 8]])</span><br></pre></td></tr></table></figure><p>To perform matrix multiplication between a and b, we can use the <code>@</code> operator:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">c = a @ b</span><br></pre></td></tr></table></figure><p>This will result in a new NumPy array <code>c</code>, which is the result of multiplying a and b using matrix multiplication.</p><p>Before <code>Python 3.5</code>, matrix multiplication was performed using the np.dot() function instead of the <code>@</code> operator. However, the <code>@</code> operator was introduced in Python 3.5 to make matrix multiplication more readable and intuitive.</p><p>In summary, the <code>@</code> operator in Python is a useful tool for performing matrix multiplication with NumPy arrays. Its introduction in Python 3.5 has made it easier to work with matrices in Python and has simplified the code needed to perform this important mathematical operation.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> matrix muliplication operator </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Open-Source Threat to Google and OpenAI, Why They Should Embrace Community-Driven AI Innovations</title>
      <link href="2023/05/07/open-source-threat-to-google-and-openai/"/>
      <url>2023/05/07/open-source-threat-to-google-and-openai/</url>
      
        <content type="html"><![CDATA[<p>Although Google and OpenAI might seem like the frontrunners in natural language processing (NLP) and artificial intelligence (AI), the rapid growth of open-source projects is becoming an increasingly significant threat to their dominance. Open-source models have accelerated at a remarkable pace, achieving impressive results with smaller budgets and shorter timeframes than their more established counterparts.</p><p>Some notable achievements and developments in the open-source AI community include:</p><ol><li>The leak of LLaMA, an open-source foundation model, which led to rapid community-driven innovation in fine-tuning and applications.</li><li>Stanford’s Alpaca project, which introduced low-rank adaptation (LoRA) for faster and more affordable fine-tuning of models on a single GPU.</li><li>Successful deployment of open-source models on low-power devices like Raspberry Pi and MacBook CPUs, thanks to minification efforts and techniques like 4-bit quantization.</li><li>The development of Vicuna, a 13B open-source model with performance comparable to ChatGPT at a fraction of the training cost.</li><li>The use of small, highly curated datasets in open-source projects, leading to more efficient and effective AI models.</li><li>Training the GPT-3 architecture from scratch by Cerebras, using Chinchilla’s optimal compute schedule and μ-parameterization, making the community independent of LLaMA.</li><li>Development of multimodal models like LLaMA-Adapter, which can be fine-tuned for instruction tuning and multimodality within just one hour of training.</li></ol><p>These achievements highlight the power of collective innovation and the rapid pace of progress in the open-source community. The affordability, accessibility, and customizability of open-source models make them increasingly attractive to users and developers, potentially undermining the value proposition of proprietary AI models developed by Google and OpenAI.</p><p>To adapt to this fast-growing open-source environment and stay competitive, Google and OpenAI should consider the following recommendations:</p><ol><li>Collaborate with and learn from open-source projects by enabling third-party integrations.</li><li>Reevaluate their value proposition, as users may be less inclined to pay for restricted models when free, unrestricted alternatives are available.</li><li>Focus on rapid iteration with smaller models, as they can be more quickly improved and adapted to user needs.</li></ol><p>Instead of competing with open-source projects, both organizations should embrace them, establishing themselves as leaders in the open-source community. By cooperating with and learning from the broader conversation, Google and OpenAI can continue driving AI advancements while benefiting from the collective knowledge and creativity of the open-source community. This may involve taking some uncomfortable steps, like publishing model weights for smaller variants, but embracing community-driven innovation can ultimately prove beneficial in the long run.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leaked Google Document,  propbably both Google and OpenAI don&#39;t have moat in large language model</title>
      <link href="2023/05/07/google-and-openai-have-no-moat-in-large-language-model/"/>
      <url>2023/05/07/google-and-openai-have-no-moat-in-large-language-model/</url>
      
        <content type="html"><![CDATA[<p>The following text is a recently leaked document that was shared on a public Discord server by an anonymous individual who gave permission for its republication. The author of the document is a researcher working at Google, and we have confirmed that it is authentic. We have made minor formatting changes and removed links to internal web pages. It is important to note that this document represents the personal opinion of a single Google employee and not the entire company. Although we do not endorse the views expressed in the document, we believe it raises thought-provoking points worth sharing. We will provide our own opinions on the matter in a separate piece for subscribers. Our role is simply to share this document as a means of promoting discussion.</p><h1 id="We-Have-No-Moat"><a href="#We-Have-No-Moat" class="headerlink" title="We Have No Moat"></a>We Have No Moat</h1><h2 id="And-neither-does-OpenAI"><a href="#And-neither-does-OpenAI" class="headerlink" title="And neither does OpenAI"></a>And neither does OpenAI</h2><p>We’ve done a lot of looking over our shoulders at OpenAI. Who will cross the next milestone? What will the next move be?</p><p>But the uncomfortable truth is, we aren’t positioned to win this arms race and neither is OpenAI. While we’ve been squabbling, a third faction has been quietly eating our lunch.</p><p>I’m talking, of course, about open source. Plainly put, they are lapping us. Things we consider “major open problems” are solved and in people’s hands today. Just to name a few:</p><ul><li><p>LLMs on a Phone: People are running foundation models on a Pixel 6 at 5 tokens / sec.</p></li><li><p>Scalable Personal AI: You can finetune a personalized AI on your laptop in an evening.</p></li><li><p>Responsible Release: This one isn’t “solved” so much as “obviated”. There are entire websites full of art models with no restrictions whatsoever, and text is not far behind.</p></li><li><p>Multimodality: The current multimodal ScienceQA SOTA was trained in an hour.</p></li></ul><p>While our models still hold a slight edge in terms of quality, the gap is closing astonishingly quickly. Open-source models are faster, more customizable, more private, and pound-for-pound more capable. They are doing things with $100 and 13B params that we struggle with at $10M and 540B. And they are doing so in weeks, not months. This has profound implications for us:</p><ul><li><p>We have no secret sauce. Our best hope is to learn from and collaborate with what others are doing outside Google. We should prioritize enabling 3P integrations.</p></li><li><p>People will not pay for a restricted model when free, unrestricted alternatives are comparable in quality. We should consider where our value add really is.</p></li><li><p>Giant models are slowing us down. In the long run, the best models are the ones</p></li></ul><p>which can be iterated upon quickly. We should make small variants more than an afterthought, now that we know what is possible in the &lt;20B parameter regime.</p><p><a href="https://lmsys.org/blog/2023-03-30-vicuna/">https://lmsys.org/blog/2023-03-30-vicuna/</a></p><h2 id="What-Happened"><a href="#What-Happened" class="headerlink" title="What Happened"></a>What Happened</h2><p>At the beginning of March the open source community got their hands on their first really capable foundation model, as Meta’s LLaMA was leaked to the public. It had no instruction or conversation tuning, and no RLHF. Nonetheless, the community immediately understood the significance of what they had been given.</p><p>A tremendous outpouring of innovation followed, with just days between major developments (see The Timeline for the full breakdown). Here we are, barely a month later, and there are variants with instruction tuning, quantization, quality improvements, human evals, multimodality, RLHF, etc. etc. many of which build on each other.</p><p>Most importantly, they have solved the scaling problem to the extent that anyone can tinker. Many of the new ideas are from ordinary people. The barrier to entry for training and experimentation has dropped from the total output of a major research organization to one person, an evening, and a beefy laptop.</p><h2 id="Why-We-Could-Have-Seen-It-Coming"><a href="#Why-We-Could-Have-Seen-It-Coming" class="headerlink" title="Why We Could Have Seen It Coming"></a>Why We Could Have Seen It Coming</h2><p>In many ways, this shouldn’t be a surprise to anyone. The current renaissance in open source LLMs comes hot on the heels of a renaissance in image generation. The similarities are not lost on the community, with many calling this the “Stable Diffusion moment” for LLMs.</p><p>In both cases, low-cost public involvement was enabled by a vastly cheaper mechanism for fine tuning called low rank adaptation, or LoRA, combined with a significant breakthrough in scale (latent diffusion for image synthesis, Chinchilla for LLMs). In both cases, access to a sufficiently high-quality model kicked off a flurry of ideas and iteration from individuals and institutions around the world. In both cases, this quickly outpaced the large players.</p><p>These contributions were pivotal in the image generation space, setting Stable Diffusion on a different path from Dall-E. Having an open model led to product integrations, marketplaces, user interfaces, and innovations that didn’t happen for Dall-E.</p><p>The effect was palpable: rapid domination in terms of cultural impact vs the OpenAI solution, which became increasingly irrelevant. Whether the same thing will happen for LLMs remains to be seen, but the broad structural elements are the same.</p><h2 id="What-We-Missed"><a href="#What-We-Missed" class="headerlink" title="What We Missed"></a>What We Missed</h2><p>The innovations that powered open source’s recent successes directly solve problems we’re still struggling with. Paying more attention to their work could help us to avoid reinventing the wheel.</p><h3 id="LoRA-is-an-incredibly-powerful-technique-we-should-probably-be-paying-more-attention-to"><a href="#LoRA-is-an-incredibly-powerful-technique-we-should-probably-be-paying-more-attention-to" class="headerlink" title="LoRA is an incredibly powerful technique we should probably be paying more attention to"></a>LoRA is an incredibly powerful technique we should probably be paying more attention to</h3><p>LoRA works by representing model updates as low-rank factorizations, which reduces the size of the update matrices by a factor of up to several thousand. This allows model fine-tuning at a fraction of the cost and time. Being able to personalize a language model in a few hours on consumer hardware is a big deal, particularly for aspirations that involve incorporating new and diverse knowledge in near real-time. The fact that this technology exists is underexploited inside Google, even though it directly impacts some of our most ambitious projects.</p><h3 id="Retraining-models-from-scratch-is-the-hard-path"><a href="#Retraining-models-from-scratch-is-the-hard-path" class="headerlink" title="Retraining models from scratch is the hard path"></a>Retraining models from scratch is the hard path</h3><p>Part of what makes LoRA so effective is that - like other forms of fine-tuning - it’s stackable. Improvements like instruction tuning can be applied and then leveraged as other contributors add on dialogue, or reasoning, or tool use. While the individual fine tunings are low rank, their sum need not be, allowing full-rank updates to the model to accumulate over time.</p><p>This means that as new and better datasets and tasks become available, the model can be cheaply kept up to date, without ever having to pay the cost of a full run.</p><p>By contrast, training giant models from scratch not only throws away the pretraining, but also any iterative improvements that have been made on top. In the open source world, it doesn’t take long before these improvements dominate, making a full retrain extremely costly.</p><p>We should be thoughtful about whether each new application or idea really needs a whole new model. If we really do have major architectural improvements that preclude directly reusing model weights, then we should invest in more aggressive forms of distillation that allow us to retain as much of the previous generation’s capabilities as possible.</p><h3 id="Large-models-aren’t-more-capable-in-the-long-run-if-we-can-iterate-faster-on-small-models"><a href="#Large-models-aren’t-more-capable-in-the-long-run-if-we-can-iterate-faster-on-small-models" class="headerlink" title="Large models aren’t more capable in the long run if we can iterate faster on small models"></a>Large models aren’t more capable in the long run if we can iterate faster on small models</h3><p>LoRA updates are very cheap to produce (~$100) for the most popular model sizes. This means that almost anyone with an idea can generate one and distribute it. Training times under a day are the norm. At that pace, it doesn’t take long before the cumulative effect of all of these fine-tunings overcomes starting off at a size disadvantage. Indeed, in terms of engineer-hours, the pace of improvement from these models vastly outstrips what we can do with our largest variants, and the best are already largely indistinguishable from ChatGPT. Focusing on maintaining some of the largest models on the planet actually puts us at a disadvantage.</p><h3 id="Data-quality-scales-better-than-data-size"><a href="#Data-quality-scales-better-than-data-size" class="headerlink" title="Data quality scales better than data size"></a>Data quality scales better than data size</h3><p>Many of these projects are saving time by training on small, highly curated datasets. This suggests there is some flexibility in data scaling laws. The existence of such datasets follows from the line of thinking in Data Doesn’t Do What You Think, and they are rapidly becoming the standard way to do training outside Google. These datasets are built using synthetic methods (e.g. filtering the best responses from an existing model) and scavenging from other projects, neither of which is dominant at Google. Fortunately, these high quality datasets are open source, so they are free to use.</p><h3 id="Directly-Competing-With-Open-Source-Is-a-Losing-Proposition"><a href="#Directly-Competing-With-Open-Source-Is-a-Losing-Proposition" class="headerlink" title="Directly Competing With Open Source Is a Losing Proposition"></a>Directly Competing With Open Source Is a Losing Proposition</h3><p>This recent progress has direct, immediate implications for our business strategy. Who would pay for a Google product with usage restrictions if there is a free, high quality alternative without them?</p><p>And we should not expect to be able to catch up. The modern internet runs on open source for a reason. Open source has some significant advantages that we cannot replicate.</p><h3 id="We-need-them-more-than-they-need-us"><a href="#We-need-them-more-than-they-need-us" class="headerlink" title="We need them more than they need us"></a>We need them more than they need us</h3><p>Keeping our technology secret was always a tenuous proposition. Google researchers are leaving for other companies on a regular cadence, so we can assume they know everything we know, and will continue to for as long as that pipeline is open.</p><p>But holding on to a competitive advantage in technology becomes even harder now that cutting edge research in LLMs is affordable. Research institutions all over the world are building on each other’s work, exploring the solution space in a breadth-first way that far outstrips our own capacity. We can try to hold tightly to our secrets while outside innovation dilutes their value, or we can try to learn from each other.</p><h3 id="Individuals-are-not-constrained-by-licenses-to-the-same-degree-as-corporations"><a href="#Individuals-are-not-constrained-by-licenses-to-the-same-degree-as-corporations" class="headerlink" title="Individuals are not constrained by licenses to the same degree as corporations"></a>Individuals are not constrained by licenses to the same degree as corporations</h3><p>Much of this innovation is happening on top of the leaked model weights from Meta. While this will inevitably change as truly open models get better, the point is that they don’t have to wait. The legal cover afforded by “personal use” and the impracticality of prosecuting individuals means that individuals are getting access to these technologies while they are hot.</p><h3 id="Being-your-own-customer-means-you-understand-the-use-case"><a href="#Being-your-own-customer-means-you-understand-the-use-case" class="headerlink" title="Being your own customer means you understand the use case"></a>Being your own customer means you understand the use case</h3><p>Browsing through the models that people are creating in the image generation space, there is a vast outpouring of creativity, from anime generators to HDR landscapes. These models are used and created by people who are deeply immersed in their particular subgenre, lending a depth of knowledge and empathy we cannot hope to match.</p><h3 id="Owning-the-Ecosystem-Letting-Open-Source-Work-for-Us"><a href="#Owning-the-Ecosystem-Letting-Open-Source-Work-for-Us" class="headerlink" title="Owning the Ecosystem: Letting Open Source Work for Us"></a>Owning the Ecosystem: Letting Open Source Work for Us</h3><p>Paradoxically, the one clear winner in all of this is Meta. Because the leaked model was theirs, they have effectively garnered an entire planet’s worth of free labor. Since most open source innovation is happening on top of their architecture, there is nothing stopping them from directly incorporating it into their products.</p><p>The value of owning the ecosystem cannot be overstated. Google itself has successfully used this paradigm in its open source offerings, like Chrome and Android. By owning the platform where innovation happens, Google cements itself as a thought leader and direction-setter, earning the ability to shape the narrative on ideas that are larger than itself.</p><p>The more tightly we control our models, the more attractive we make open alternatives. Google and OpenAI have both gravitated defensively toward release patterns that allow them to retain tight control over how their models are used. But this control is a fiction. Anyone seeking to use LLMs for unsanctioned purposes can simply take their pick of the freely available models.</p><p>Google should establish itself a leader in the open source community, taking the lead by cooperating with, rather than ignoring, the broader conversation. This probably means taking some uncomfortable steps, like publishing the model weights for small ULM variants. This necessarily means relinquishing some control over our models. But this compromise is inevitable. We cannot hope to both drive innovation and control it.</p><h3 id="Epilogue-What-about-OpenAI"><a href="#Epilogue-What-about-OpenAI" class="headerlink" title="Epilogue: What about OpenAI?"></a>Epilogue: What about OpenAI?</h3><p>All this talk of open source can feel unfair given OpenAI’s current closed policy. Why do we have to share, if they won’t? But the fact of the matter is, we are already sharing everything with them in the form of the steady flow of poached senior researchers. Until we stem that tide, secrecy is a moot point.</p><p>And in the end, OpenAI doesn’t matter. They are making the same mistakes we are in their posture relative to open source, and their ability to maintain an edge is necessarily in question. Open source alternatives can and will eventually eclipse them unless they change their stance. In this respect, at least, we can make the first move.</p><h2 id="The-Timeline-so-far"><a href="#The-Timeline-so-far" class="headerlink" title="The Timeline so far"></a>The Timeline so far</h2><h3 id="Feb-24-2023-LLaMA-is-Launched"><a href="#Feb-24-2023-LLaMA-is-Launched" class="headerlink" title="Feb 24, 2023 - LLaMA is Launched"></a>Feb 24, 2023 - LLaMA is Launched</h3><p>Meta launches LLaMA, open sourcing the code, but not the weights. At this point, LLaMA is not instruction or conversation tuned. Like many current models, it is a relatively small model (available at 7B, 13B, 33B, and 65B parameters) that has been trained for a relatively large amount of time, and is therefore quite capable relative to its size.</p><h3 id="March-3-2023-The-Inevitable-Happens"><a href="#March-3-2023-The-Inevitable-Happens" class="headerlink" title="March 3, 2023 - The Inevitable Happens"></a>March 3, 2023 - The Inevitable Happens</h3><p>Within a week, LLaMA is leaked to the public. The impact on the community cannot be overstated. Existing licenses prevent it from being used for commercial purposes, but suddenly anyone is able to experiment. From this point forward, innovations come hard and fast.</p><h3 id="March-12-2023-Language-models-on-a-Toaster"><a href="#March-12-2023-Language-models-on-a-Toaster" class="headerlink" title="March 12, 2023 - Language models on a Toaster"></a>March 12, 2023 - Language models on a Toaster</h3><p>A little over a week later, Artem Andreenko gets the model working on a Raspberry Pi. At this point the model runs too slowly to be practical because the weights must be paged in and out of memory. Nonetheless, this sets the stage for an onslaught of minification efforts.</p><h3 id="March-13-2023-Fine-Tuning-on-a-Laptop"><a href="#March-13-2023-Fine-Tuning-on-a-Laptop" class="headerlink" title="March 13, 2023 - Fine Tuning on a Laptop"></a>March 13, 2023 - Fine Tuning on a Laptop</h3><p>The next day, Stanford releases Alpaca, which adds instruction tuning to LLaMA. More important than the actual weights, however, was Eric Wang’s alpaca-lora repo, which used low rank fine-tuning to do this training “within hours on a single RTX 4090”.</p><p>Suddenly, anyone could fine-tune the model to do anything, kicking off a race to the bottom on low-budget fine-tuning projects. Papers proudly describe their total spend of a few hundred dollars. What’s more, the low rank updates can be distributed easily and separately from the original weights, making them independent of the original license from Meta. Anyone can share and apply them.</p><h3 id="March-18-2023-Now-It’s-Fast"><a href="#March-18-2023-Now-It’s-Fast" class="headerlink" title="March 18, 2023 - Now It’s Fast"></a>March 18, 2023 - Now It’s Fast</h3><p>Georgi Gerganov uses 4 bit quantization to run LLaMA on a MacBook CPU. It is the first “no GPU” solution that is fast enough to be practical.</p><h3 id="March-19-2023-A-13B-model-achieves-“parity”-with-Bard"><a href="#March-19-2023-A-13B-model-achieves-“parity”-with-Bard" class="headerlink" title="March 19, 2023 - A 13B model achieves “parity” with Bard"></a>March 19, 2023 - A 13B model achieves “parity” with Bard</h3><p>The next day, a cross-university collaboration releases Vicuna, and uses GPT-4-powered eval to provide qualitative comparisons of model outputs. While the evaluation method is suspect, the model is materially better than earlier variants. Training Cost: $300.</p><p>Notably, they were able to use data from ChatGPT while circumventing restrictions on its API - They simply sampled examples of “impressive” ChatGPT dialogue posted on sites like ShareGPT.</p><h3 id="March-25-2023-Choose-Your-Own-Model"><a href="#March-25-2023-Choose-Your-Own-Model" class="headerlink" title="March 25, 2023 - Choose Your Own Model"></a>March 25, 2023 - Choose Your Own Model</h3><p>Nomic creates GPT4All, which is both a model and, more importantly, an ecosystem. For the first time, we see models (including Vicuna) being gathered together in one place. Training Cost: $100.</p><h3 id="March-28-2023-Open-Source-GPT-3"><a href="#March-28-2023-Open-Source-GPT-3" class="headerlink" title="March 28, 2023 - Open Source GPT-3"></a>March 28, 2023 - Open Source GPT-3</h3><p>Cerebras (not to be confused with our own Cerebra) trains the GPT-3 architecture using the optimal compute schedule implied by Chinchilla, and the optimal scaling implied by μ-parameterization. This outperforms existing GPT-3 clones by a wide margin, and represents the first confirmed use of μ-parameterization “in the wild”. These models are trained from scratch, meaning the community is no longer dependent on LLaMA.</p><h3 id="March-28-2023-Multimodal-Training-in-One-Hour"><a href="#March-28-2023-Multimodal-Training-in-One-Hour" class="headerlink" title="March 28, 2023 - Multimodal Training in One Hour"></a>March 28, 2023 - Multimodal Training in One Hour</h3><p>Using a novel Parameter Efficient Fine Tuning (PEFT) technique, LLaMA-Adapter introduces instruction tuning and multimodality in one hour of training. Impressively, they do so with just 1.2M learnable parameters. The model achieves a new SOTA on multimodal ScienceQA.</p><h3 id="April-3-2023-Real-Humans-Can’t-Tell-the-Difference-Between-a-13B-Open-Model-and-ChatGPT"><a href="#April-3-2023-Real-Humans-Can’t-Tell-the-Difference-Between-a-13B-Open-Model-and-ChatGPT" class="headerlink" title="April 3, 2023 - Real Humans Can’t Tell the Difference Between a 13B Open Model and ChatGPT"></a>April 3, 2023 - Real Humans Can’t Tell the Difference Between a 13B Open Model and ChatGPT</h3><p>Berkeley launches Koala, a dialogue model trained entirely using freely available data.</p><p>They take the crucial step of measuring real human preferences between their model and ChatGPT. While ChatGPT still holds a slight edge, more than 50% of the time users either prefer Koala or have no preference. Training Cost: $100.</p><h3 id="April-15-2023-Open-Source-RLHF-at-ChatGPT-Levels"><a href="#April-15-2023-Open-Source-RLHF-at-ChatGPT-Levels" class="headerlink" title="April 15, 2023 - Open Source RLHF at ChatGPT Levels"></a>April 15, 2023 - Open Source RLHF at ChatGPT Levels</h3><p>Open Assistant launches a model and, more importantly, a dataset for Alignment via RLHF. Their model is close (48.3% vs. 51.7%) to ChatGPT in terms of human preference. In addition to LLaMA, they show that this dataset can be applied to Pythia-12B, giving people the option to use a fully open stack to run the model. Moreover, because the dataset is publicly available, it takes RLHF from unachievable to cheap and easy for small experimenters.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Finding the Best Time to Meet, A Python-Based Optimization Approach</title>
      <link href="2023/05/05/finding-best-time-to-meet-using-pyton-optimization/"/>
      <url>2023/05/05/finding-best-time-to-meet-using-pyton-optimization/</url>
      
        <content type="html"><![CDATA[<p>Scheduling meetings can be a challenging task, especially when you have to consider the availability of multiple participants. In this blog post, we will walk you through a Python script that helps you find the best time to meet, given the schedules of all participants. We will also discuss how to handle “not good” time slots and prioritize certain users’ schedules.</p><h2 id="Getting-Started-Parsing-Schedules"><a href="#Getting-Started-Parsing-Schedules" class="headerlink" title="Getting Started: Parsing Schedules"></a>Getting Started: Parsing Schedules</h2><p>First, let’s create a function to parse the schedules of each participant. We will represent schedules as strings in the format “Days Time-Range”, where Days are comma-separated days of the week (e.g., “Mon,Tue,Wed”) and Time-Range is a start and end time separated by a hyphen (e.g., “09:00-17:00”).</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_schedule</span>(<span class="params">schedule_str</span>):</span></span><br><span class="line">    days, times = schedule_str.split(<span class="string">&quot; &quot;</span>)</span><br><span class="line">    days = days.split(<span class="string">&quot;,&quot;</span>)</span><br><span class="line">    times = times.split(<span class="string">&quot;-&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> days, [datetime.datetime.strptime(t, <span class="string">&quot;%H:%M&quot;</span>).time() <span class="keyword">for</span> t <span class="keyword">in</span> times]</span><br></pre></td></tr></table></figure><p>This function takes a schedule string and returns a tuple of days and times.</p><h2 id="Finding-the-Best-Time-to-Meet"><a href="#Finding-the-Best-Time-to-Meet" class="headerlink" title="Finding the Best Time to Meet"></a>Finding the Best Time to Meet</h2><p>Now that we can parse schedules, let’s create a function to find the best time to meet based on the highest overlap in availability.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_best_time</span>(<span class="params">schedules</span>):</span></span><br><span class="line">    availability = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> schedule <span class="keyword">in</span> schedules:</span><br><span class="line">        days, times = parse_schedule(schedule)</span><br><span class="line">        <span class="keyword">for</span> day <span class="keyword">in</span> days:</span><br><span class="line">            <span class="keyword">for</span> hour <span class="keyword">in</span> <span class="built_in">range</span>(times[<span class="number">0</span>].hour, times[<span class="number">1</span>].hour):</span><br><span class="line">                availability[(day, hour)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    best_time = <span class="built_in">max</span>(availability, key=availability.get)</span><br><span class="line">    <span class="keyword">return</span> best_time</span><br></pre></td></tr></table></figure><p>This function takes a list of schedules and returns the best time to meet.</p><p>Let’s test it</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    schedules = [</span><br><span class="line">        &quot;Mon,Tue,Wed 09:00-17:00&quot;,</span><br><span class="line">        &quot;Tue,Wed,Thu 10:00-18:00&quot;,</span><br><span class="line">        &quot;Wed,Thu,Fri 11:00-19:00&quot;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    best_time = find_best_time(schedules)</span><br><span class="line">    print(f&quot;The best time to meet is &#123;best_time[0]&#125; at &#123;best_time[1]:02d&#125;:00&quot;)</span><br></pre></td></tr></table></figure><p>In the example provided, the script will output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The best time to meet is Wed at 11:00</span><br></pre></td></tr></table></figure><h2 id="Handling-“Not-Good”-Time-Slots"><a href="#Handling-“Not-Good”-Time-Slots" class="headerlink" title="Handling “Not Good” Time Slots"></a>Handling “Not Good” Time Slots</h2><p>In some cases, you may want to avoid certain time slots when scheduling a meeting. To handle this, we can modify the <code>find_best_time</code> function to take a list of “not good” time slots and remove them from the availability dictionary before finding the best time.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_best_time</span>(<span class="params">schedules, not_good_times</span>):</span></span><br><span class="line">    availability = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> schedule <span class="keyword">in</span> schedules:</span><br><span class="line">        days, times = parse_schedule(schedule)</span><br><span class="line">        <span class="keyword">for</span> day <span class="keyword">in</span> days:</span><br><span class="line">            <span class="keyword">for</span> hour <span class="keyword">in</span> <span class="built_in">range</span>(times[<span class="number">0</span>].hour, times[<span class="number">1</span>].hour):</span><br><span class="line">                availability[(day, hour)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> not_good_time <span class="keyword">in</span> not_good_times:</span><br><span class="line">        days, times = parse_schedule(not_good_time)</span><br><span class="line">        <span class="keyword">for</span> day <span class="keyword">in</span> days:</span><br><span class="line">            <span class="keyword">for</span> hour <span class="keyword">in</span> <span class="built_in">range</span>(times[<span class="number">0</span>].hour, times[<span class="number">1</span>].hour):</span><br><span class="line">                availability.pop((day, hour), <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    best_time = <span class="built_in">max</span>(availability, key=availability.get)</span><br><span class="line">    <span class="keyword">return</span> best_time</span><br></pre></td></tr></table></figure><p>Let’s test it</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    schedules = [</span><br><span class="line">        &quot;Mon,Tue,Wed 09:00-17:00&quot;,</span><br><span class="line">        &quot;Tue,Wed,Thu 10:00-18:00&quot;,</span><br><span class="line">        &quot;Wed,Thu,Fri 11:00-19:00&quot;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    not_good_times = [</span><br><span class="line">        &quot;Wed 12:00-14:00&quot;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    best_time = find_best_time(schedules, not_good_times)</span><br><span class="line">    print(f&quot;The best time to meet is &#123;best_time[0]&#125; at &#123;best_time[1]:02d&#125;:00&quot;)</span><br></pre></td></tr></table></figure><p>In the example provided, the script will output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The best time to meet is Wed at 11:00</span><br></pre></td></tr></table></figure><h2 id="Prioritizing-Certain-Users’-Schedules"><a href="#Prioritizing-Certain-Users’-Schedules" class="headerlink" title="Prioritizing Certain Users’ Schedules"></a>Prioritizing Certain Users’ Schedules</h2><p>Sometimes, you may need to ensure that certain users’ schedules are satisfied when finding the best time to meet. To achieve this, we can modify the script to include a priority list of users and create a separate availability dictionary for these users.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_best_time</span>(<span class="params">schedules, not_good_times, priority_users</span>):</span></span><br><span class="line">    availability = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    priority_availability = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, schedule <span class="keyword">in</span> <span class="built_in">enumerate</span>(schedules):</span><br><span class="line">        days, times = parse_schedule(schedule)</span><br><span class="line">        <span class="keyword">for</span> day <span class="keyword">in</span> days:</span><br><span class="line">            <span class="keyword">for</span> hour <span class="keyword">in</span> <span class="built_in">range</span>(times[<span class="number">0</span>].hour, times[<span class="number">1</span>].hour):</span><br><span class="line">                <span class="keyword">if</span> i <span class="keyword">in</span> priority_users:</span><br><span class="line">                    priority_availability[(day, hour)] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    availability[(day, hour)] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> not_good_time <span class="keyword">in</span> not_good_times:</span><br><span class="line">        days, times = parse_schedule(not_good_time)</span><br><span class="line">        <span class="keyword">for</span> day <span class="keyword">in</span> days:</span><br><span class="line">            <span class="keyword">for</span> hour <span class="keyword">in</span> <span class="built_in">range</span>(times[<span class="number">0</span>].hour, times[<span class="number">1</span>].hour):</span><br><span class="line">                availability.pop((day, hour), <span class="literal">None</span>)</span><br><span class="line">                priority_availability.pop((day, hour), <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    best_time = <span class="literal">None</span></span><br><span class="line">    max_overlap = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> time, count <span class="keyword">in</span> priority_availability.items():</span><br><span class="line">        <span class="keyword">if</span> count == <span class="built_in">len</span>(priority_users) <span class="keyword">and</span> availability.get(time, <span class="number">0</span>) &gt; max_overlap:</span><br><span class="line">            best_time = time</span><br><span class="line">            max_overlap = availability[time]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> best_time</span><br></pre></td></tr></table></figure><p>This function now takes a list of priority user indices and finds the best time based on the highest overlap in availability for both priority and non-priority users.</p><p>Let’s test it</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    schedules = [</span><br><span class="line">        &quot;Mon,Tue,Wed 09:00-17:00&quot;,  # User 0</span><br><span class="line">        &quot;Tue,Wed,Thu 10:00-18:00&quot;,  # User 1</span><br><span class="line">        &quot;Wed,Thu,Fri 11:00-19:00&quot;,  # User 2</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    not_good_times = [</span><br><span class="line">        &quot;Wed 12:00-14:00&quot;,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    priority_users = [0, 2]  # User 0 and User 2 must be satisfied</span><br><span class="line"></span><br><span class="line">    best_time = find_best_time(schedules, not_good_times, priority_users)</span><br><span class="line">    print(f&quot;The best time to meet is &#123;best_time[0]&#125; at &#123;best_time[1]:02d&#125;:00&quot;)</span><br></pre></td></tr></table></figure><p>In the example provided, the script will output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The best time to meet is Wed at 11:00</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we have demonstrated how to use Python to find the best time to meet, given the schedules of multiple participants. We have also shown how to handle “not good” time slots and prioritize certain users’ schedules. This script can be further optimized and adapted to specific requirements, such as considering time zones, handling more complex schedules, or finding multiple time slots that work for everyone.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Large Language Model Revolution, Unleashing the Steam Engine of Intellectual Work</title>
      <link href="2023/05/05/large-language-model-revolution/"/>
      <url>2023/05/05/large-language-model-revolution/</url>
      
        <content type="html"><![CDATA[<p>The Industrial Revolution was a transformative period that revolutionized the way people lived and worked. At the heart of this revolution was the invention of the steam engine, which enabled automatic production and freed people from labor-intensive tasks. Fast forward to the present day, we are witnessing a new revolution - the rise of large language models that have the potential to automate intellectual work. This blog post will delve into the interesting similarities between these two pivotal innovations and discuss how large language models are set to become the steam engines of intellectual work.</p><h2 id="The-Steam-Engine-Powering-the-Industrial-Revolution"><a href="#The-Steam-Engine-Powering-the-Industrial-Revolution" class="headerlink" title="The Steam Engine: Powering the Industrial Revolution"></a>The Steam Engine: Powering the Industrial Revolution</h2><p>In the 18th and 19th centuries, the steam engine emerged as the driving force behind the Industrial Revolution. It mechanized production by automating tasks that previously required manual labor, leading to increased efficiency and productivity. The steam engine allowed factories to operate around the clock, leading to exponential growth in manufacturing output.</p><p>One of the most iconic examples of the steam engine’s impact was in the textile industry. Before the steam engine, spinning and weaving were slow, laborious processes carried out by hand. With the advent of steam-powered machinery, the production of textiles became faster, more efficient, and less reliant on human labor. This transformation in the textile industry was just one of the many ways that the steam engine revolutionized the world.</p><h2 id="The-Large-Language-Model-Automating-Intellectual-Work"><a href="#The-Large-Language-Model-Automating-Intellectual-Work" class="headerlink" title="The Large Language Model: Automating Intellectual Work"></a>The Large Language Model: Automating Intellectual Work</h2><p>Fast forward to the 21st century, we are witnessing the rise of large language models, such as OpenAI’s GPT-3, which are transforming the way we approach intellectual work. These powerful AI-driven models have the potential to automate tasks that previously required human intelligence, such as writing, translating, and even coding.</p><p>One impressive example of a large language model in action is ChatGPT, a conversational AI that can generate human-like text responses based on given prompts. It can be used for a wide array of applications, from drafting emails to creating code snippets. Another example is the use of AI models in journalism, where news agencies like Associated Press and Forbes are using AI to write news articles and financial reports, freeing up journalists to focus on more complex tasks.</p><p>The automation of intellectual work through large language models is not limited to text generation. These AI models can also understand and generate speech, enabling voice assistants like Siri, Alexa, and Google Assistant to carry out tasks and answer questions for their users.</p><h2 id="The-Impact-of-Large-Language-Models-on-Society"><a href="#The-Impact-of-Large-Language-Models-on-Society" class="headerlink" title="The Impact of Large Language Models on Society"></a>The Impact of Large Language Models on Society</h2><p>Just as the steam engine revolutionized the world of work, large language models have the potential to reshape many aspects of our lives. The automation of intellectual work can lead to increased efficiency, allowing businesses to operate more effectively and freeing up time for individuals to focus on more creative or complex tasks.</p><p>However, this new revolution also raises concerns about job displacement and the potential misuse of these powerful AI tools. As we continue to develop and refine large language models, it is crucial that we consider the ethical implications and work towards creating a future where AI can be used responsibly and conscientiously.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In conclusion, the emergence of large language models has the potential to bring about a revolution akin to the Industrial Revolution. By automating intellectual work, these AI-driven models can increase efficiency and productivity across various industries, much like the steam engine did for manufacturing. As we continue to develop and refine these models, it is essential that we also consider the ethical and societal implications of this new revolution. Only then can we fully harness the power of large language models to create a better, more efficient future.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> large language model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameter Optimization 1 - Introduction to Hyperparameter Optimization in Machine Learning with Python</title>
      <link href="2023/05/03/overview-of-hyperprameter-optimization-1/"/>
      <url>2023/05/03/overview-of-hyperprameter-optimization-1/</url>
      
        <content type="html"><![CDATA[<p>Machine learning models have become an essential tool for solving complex problems and making predictions. However, the performance of these models depends on the choice of hyperparameters, which are the settings that control the learning process. In this blog post, we will introduce the concept of hyperparameter optimization, discuss its importance in machine learning, and provide a practical example using Python.</p><h2 id="What-are-Hyperparameters"><a href="#What-are-Hyperparameters" class="headerlink" title="What are Hyperparameters?"></a>What are Hyperparameters?</h2><p>Hyperparameters are the parameters of a machine learning model that are not learned from the data but are set before the training process begins. They control various aspects of the learning process, such as the learning rate, the number of hidden layers in a neural network, or the regularization strength in a linear regression model. Choosing the right hyperparameters can significantly improve the performance of a model.</p><h2 id="Why-is-Hyperparameter-Optimization-Important"><a href="#Why-is-Hyperparameter-Optimization-Important" class="headerlink" title="Why is Hyperparameter Optimization Important?"></a>Why is Hyperparameter Optimization Important?</h2><p>The performance of a machine learning model depends on the choice of hyperparameters. A poor choice of hyperparameters can lead to underfitting or overfitting, resulting in poor generalization to new data. Hyperparameter optimization is the process of finding the best set of hyperparameters for a given model and dataset, which can lead to improved model performance and better predictions.</p><h2 id="Example-Hyperparameter-Optimization-in-Python"><a href="#Example-Hyperparameter-Optimization-in-Python" class="headerlink" title="Example: Hyperparameter Optimization in Python"></a>Example: Hyperparameter Optimization in Python</h2><p>In this example, we will demonstrate hyperparameter optimization using the popular Python library, Scikit-learn. We will use the Support Vector Machine (SVM) algorithm to classify the famous Iris dataset.</p><ol><li>Import necessary libraries and load the dataset:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Split the dataset into training and testing sets:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>Define the hyperparameter search space:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">    <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>Perform hyperparameter optimization using GridSearchCV:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">5</span>, verbose=<span class="number">2</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><ol start="5"><li>Evaluate the best model:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_model = grid_search.best_estimator_</span><br><span class="line">y_pred = best_model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we introduced the concept of hyperparameter optimization and its importance in machine learning. We also provided a practical example using Python and Scikit-learn to optimize the hyperparameters of an SVM model. In the next blog post, we will explore more advanced techniques for hyperparameter optimization, such as Bayesian optimization and genetic algorithms.<br>Continue your learning by reading:<br><a href="/2023/05/03/overview-of-hyperprameter-optimization-2/">Exploring Grid Search and Random Search for Hyperparameter Tuning in Python</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameter Optimization 4 - Leveraging Genetic Algorithms for Hyperparameter Tuning in Python</title>
      <link href="2023/05/03/overview-of-hyperprameter-optimization-4/"/>
      <url>2023/05/03/overview-of-hyperprameter-optimization-4/</url>
      
        <content type="html"><![CDATA[<p>In the previous blog posts, we introduced the <a href="/2023/05/03/overview-of-hyperprameter-optimization-1/">concept of hyperparameter optimization</a> and <a href="/2023/05/03/overview-of-hyperprameter-optimization-2/">explored various techniques</a>, including Grid Search, Random Search, and <a href="/2023/05/03/overview-of-hyperprameter-optimization-3/">advanced optimization libraries</a> like Optuna, Hyperopt, and Scikit-Optimize. In this post, we will explore another powerful technique for hyperparameter tuning: Genetic Algorithms (GAs). We will discuss the basics of genetic algorithms and provide a practical example using the DEAP library in Python.</p><h2 id="Genetic-Algorithms"><a href="#Genetic-Algorithms" class="headerlink" title="Genetic Algorithms"></a>Genetic Algorithms</h2><p>Genetic algorithms are a class of optimization algorithms inspired by the process of natural selection. They are used to find approximate solutions to optimization problems by mimicking the process of evolution. Genetic algorithms work by maintaining a population of candidate solutions and iteratively applying genetic operators such as selection, crossover (recombination), and mutation to evolve the population towards better solutions.</p><p>The main advantage of genetic algorithms is their ability to explore a large search space efficiently. They are particularly useful for optimization problems where the search space is complex, non-linear, and has multiple local optima.</p><h2 id="DEAP-Library"><a href="#DEAP-Library" class="headerlink" title="DEAP Library"></a>DEAP Library</h2><p>DEAP (Distributed Evolutionary Algorithms in Python) is a popular Python library for implementing genetic algorithms and other evolutionary computation techniques. DEAP provides a flexible framework for defining custom genetic operators, selection strategies, and evaluation functions, making it suitable for a wide range of optimization problems, including hyperparameter tuning in machine learning.</p><h2 id="Example-Hyperparameter-Tuning-with-Genetic-Algorithms-in-Python"><a href="#Example-Hyperparameter-Tuning-with-Genetic-Algorithms-in-Python" class="headerlink" title="Example: Hyperparameter Tuning with Genetic Algorithms in Python"></a>Example: Hyperparameter Tuning with Genetic Algorithms in Python</h2><p>In this example, we will demonstrate hyperparameter tuning using the DEAP library and the Genetic Algorithm on the famous Iris dataset with the Support Vector Machine (SVM) algorithm.</p><ol><li>Import necessary libraries and load the dataset:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> deap <span class="keyword">import</span> base, creator, tools, algorithms</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Split the dataset into training and testing sets:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>Define the evaluation function for the Genetic Algorithm:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate_individual</span>(<span class="params">individual</span>):</span></span><br><span class="line">    C, kernel, gamma = individual</span><br><span class="line">    svm = SVC(C=C, kernel=kernel, gamma=gamma)</span><br><span class="line">    score = np.mean(cross_val_score(svm, X_train, y_train, cv=<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">return</span> score,</span><br></pre></td></tr></table></figure><ol start="4"><li>Set up the DEAP framework for the Genetic Algorithm:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">creator.create(<span class="string">&quot;FitnessMax&quot;</span>, base.Fitness, weights=(<span class="number">1.0</span>,))</span><br><span class="line">creator.create(<span class="string">&quot;Individual&quot;</span>, <span class="built_in">list</span>, fitness=creator.FitnessMax)</span><br><span class="line"></span><br><span class="line">toolbox = base.Toolbox()</span><br><span class="line">toolbox.register(<span class="string">&quot;C&quot;</span>, random.uniform, <span class="number">1e-2</span>, <span class="number">1e2</span>)</span><br><span class="line">toolbox.register(<span class="string">&quot;kernel&quot;</span>, random.choice, [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>])</span><br><span class="line">toolbox.register(<span class="string">&quot;gamma&quot;</span>, random.uniform, <span class="number">1e-4</span>, <span class="number">1e1</span>)</span><br><span class="line"></span><br><span class="line">toolbox.register(<span class="string">&quot;individual&quot;</span>, tools.initCycle, creator.Individual, (toolbox.C, toolbox.kernel, toolbox.gamma), n=<span class="number">1</span>)</span><br><span class="line">toolbox.register(<span class="string">&quot;population&quot;</span>, tools.initRepeat, <span class="built_in">list</span>, toolbox.individual)</span><br><span class="line"></span><br><span class="line">toolbox.register(<span class="string">&quot;mate&quot;</span>, tools.cxTwoPoint)</span><br><span class="line">toolbox.register(<span class="string">&quot;mutate&quot;</span>, tools.mutGaussian, mu=<span class="number">0</span>, sigma=<span class="number">1</span>, indpb=<span class="number">0.1</span>)</span><br><span class="line">toolbox.register(<span class="string">&quot;select&quot;</span>, tools.selBest)</span><br><span class="line">toolbox.register(<span class="string">&quot;evaluate&quot;</span>, evaluate_individual)</span><br></pre></td></tr></table></figure><ol start="5"><li>Run the Genetic Algorithm:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">population = toolbox.population(n=<span class="number">50</span>)</span><br><span class="line">hof = tools.HallOfFame(<span class="number">1</span>)</span><br><span class="line">stats = tools.Statistics(<span class="keyword">lambda</span> ind: ind.fitness.values)</span><br><span class="line">stats.register(<span class="string">&quot;avg&quot;</span>, np.mean)</span><br><span class="line">stats.register(<span class="string">&quot;min&quot;</span>, np.<span class="built_in">min</span>)</span><br><span class="line">stats.register(<span class="string">&quot;max&quot;</span>, np.<span class="built_in">max</span>)</span><br><span class="line"></span><br><span class="line">population, logbook = algorithms.eaSimple(population, toolbox, cxpb=<span class="number">0.5</span>, mutpb=<span class="number">0.2</span>, ngen=<span class="number">50</span>, stats=stats, halloffame=hof, verbose=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><ol start="6"><li>Evaluate the best model:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_individual = hof[<span class="number">0</span>]</span><br><span class="line">best_model = SVC(C=best_individual[<span class="number">0</span>], kernel=best_individual[<span class="number">1</span>], gamma=best_individual[<span class="number">2</span>])</span><br><span class="line">best_model.fit(X_train, y_train)</span><br><span class="line">y_pred = best_model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we explored the use of Genetic Algorithms for hyperparameter tuning in machine learning. We discussed the basics of genetic algorithms and provided a practical example using the DEAP library in Python. Genetic algorithms are a powerful technique for efficiently exploring large and complex search spaces, making them a valuable tool for hyperparameter optimization. In the next blog post, we will explore more advanced techniques for hyperparameter optimization, such as population-based training and reinforcement learning.<br>Continue your learning by reading:<br><a href="/2023/05/03/overview-of-hyperprameter-optimization-5/">Advanced Techniques, Hyperband and Population-Based Training for Hyperparameter Optimization</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter optimization </tag>
            
            <tag> genetic algorithm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameter Optimization 2 - Exploring Grid Search and Random Search for Hyperparameter Tuning in Python</title>
      <link href="2023/05/03/overview-of-hyperprameter-optimization-2/"/>
      <url>2023/05/03/overview-of-hyperprameter-optimization-2/</url>
      
        <content type="html"><![CDATA[<p>In the <a href="/2023/05/03/overview-of-hyperprameter-optimization-1/">previous blog post</a>, we introduced the concept of hyperparameter optimization and its importance in machine learning. In this post, we will explore two popular techniques for hyperparameter tuning: Grid Search and Random Search. We will discuss their advantages and disadvantages and provide practical examples using Python and Scikit-learn.</p><h2 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h2><p>Grid search is a simple and widely used method for hyperparameter tuning. It involves exhaustively searching through a predefined set of hyperparameter values and selecting the combination that results in the best model performance. The main advantage of grid search is that it is guaranteed to find the optimal set of hyperparameters within the search space. However, it can be computationally expensive, especially when dealing with a large number of hyperparameters or large datasets.</p><h2 id="Random-Search"><a href="#Random-Search" class="headerlink" title="Random Search"></a>Random Search</h2><p>Random search is an alternative to grid search that involves randomly sampling hyperparameter values from a predefined search space. Instead of exhaustively searching through all possible combinations, random search only evaluates a subset of them. This can significantly reduce the computational cost of hyperparameter tuning. Although random search is not guaranteed to find the optimal set of hyperparameters, it has been shown to be effective in practice, especially when the search space is large or the optimal hyperparameters are not uniformly distributed.</p><h2 id="Example-Grid-Search-vs-Random-Search-in-Python"><a href="#Example-Grid-Search-vs-Random-Search-in-Python" class="headerlink" title="Example: Grid Search vs. Random Search in Python"></a>Example: Grid Search vs. Random Search in Python</h2><p>In this example, we will compare grid search and random search for hyperparameter tuning using the Support Vector Machine (SVM) algorithm on the famous Iris dataset.</p><ol><li>Import necessary libraries and load the dataset:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV, RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Split the dataset into training and testing sets:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>Define the hyperparameter search space:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],</span><br><span class="line">    <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: [<span class="number">1</span>, <span class="number">0.1</span>, <span class="number">0.01</span>, <span class="number">0.001</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="4"><li>Perform hyperparameter optimization using GridSearchCV:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">grid_search = GridSearchCV(SVC(), param_grid, cv=<span class="number">5</span>, verbose=<span class="number">2</span>)</span><br><span class="line">grid_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><ol start="5"><li>Perform hyperparameter optimization using RandomizedSearchCV:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_search = RandomizedSearchCV(SVC(), param_grid, n_iter=<span class="number">10</span>, cv=<span class="number">5</span>, verbose=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">random_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><ol start="6"><li>Compare the results of Grid Search and Random Search:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_grid_model = grid_search.best_estimator_</span><br><span class="line">best_random_model = random_search.best_estimator_</span><br><span class="line"></span><br><span class="line">y_pred_grid = best_grid_model.predict(X_test)</span><br><span class="line">y_pred_random = best_random_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Grid Search Results:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_grid))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Random Search Results:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_random))</span><br></pre></td></tr></table></figure><p>Conclusion</p><p>In this blog post, we explored two popular techniques for hyperparameter tuning: Grid Search and Random Search. We discussed their advantages and disadvantages and provided a practical example using Python and Scikit-learn. In the next blog post, we will dive into more advanced techniques for hyperparameter optimization, such as Bayesian optimization and genetic algorithms.<br>Continue your learning by reading:<br><a href="/2023/05/03/overview-of-hyperprameter-optimization-3/">Automated Hyperparameter Tuning with Python Libraries, Optuna, Hyperopt, and Scikit-Optimize</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter optimization </tag>
            
            <tag> grid search </tag>
            
            <tag> random search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameter Optimization 3 - Automated Hyperparameter Tuning with Python Libraries, Optuna, Hyperopt, and Scikit-Optimize</title>
      <link href="2023/05/03/overview-of-hyperprameter-optimization-3/"/>
      <url>2023/05/03/overview-of-hyperprameter-optimization-3/</url>
      
        <content type="html"><![CDATA[<p>In the previous blog posts, we introduced the <a href="/2023/05/03/overview-of-hyperprameter-optimization-1/">concept of hyperparameter optimization</a> and <a href="/2023/05/03/overview-of-hyperprameter-optimization-2/">explored basic techniques</a> like Grid Search and Random Search. In this post, we will dive into more advanced and automated techniques for hyperparameter tuning using popular Python libraries: Optuna, Hyperopt, and Scikit-Optimize. These libraries implement advanced optimization algorithms that can efficiently search for the best hyperparameters in large search spaces.</p><h2 id="Optuna"><a href="#Optuna" class="headerlink" title="Optuna"></a>Optuna</h2><p>Optuna is a powerful Python library for hyperparameter optimization that uses a combination of Tree-structured Parzen Estimator (TPE) and pruning strategies to efficiently search for the best hyperparameters. Optuna is designed to be easy to use and highly customizable, making it suitable for a wide range of optimization problems.</p><h2 id="Hyperopt"><a href="#Hyperopt" class="headerlink" title="Hyperopt"></a>Hyperopt</h2><p>Hyperopt is another popular Python library for hyperparameter optimization. It uses the TPE algorithm to efficiently search for the best hyperparameters. Hyperopt is designed to be highly flexible and can be used for a wide range of optimization problems, including deep learning and reinforcement learning.</p><h2 id="Scikit-Optimize"><a href="#Scikit-Optimize" class="headerlink" title="Scikit-Optimize"></a>Scikit-Optimize</h2><p>Scikit-Optimize is a library for sequential model-based optimization (SMBO) in Python. It provides several optimization algorithms, including Bayesian optimization, which is a powerful technique for finding the global optimum of a function with minimal evaluations. Scikit-Optimize is designed to be easy to use and integrates well with Scikit-learn, making it a popular choice for hyperparameter tuning in machine learning.</p><h2 id="Example-Hyperparameter-Tuning-with-Optuna-Hyperopt-and-Scikit-Optimize"><a href="#Example-Hyperparameter-Tuning-with-Optuna-Hyperopt-and-Scikit-Optimize" class="headerlink" title="Example: Hyperparameter Tuning with Optuna, Hyperopt, and Scikit-Optimize"></a>Example: Hyperparameter Tuning with Optuna, Hyperopt, and Scikit-Optimize</h2><p>In this example, we will demonstrate hyperparameter tuning using Optuna, Hyperopt, and Scikit-Optimize on the famous Iris dataset with the Support Vector Machine (SVM) algorithm.</p><ol><li>Import necessary libraries and load the dataset:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">import</span> optuna</span><br><span class="line"><span class="keyword">from</span> hyperopt <span class="keyword">import</span> fmin, tpe, hp, STATUS_OK, Trials</span><br><span class="line"><span class="keyword">from</span> skopt <span class="keyword">import</span> BayesSearchCV</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br></pre></td></tr></table></figure><ol start="2"><li>Split the dataset into training and testing sets:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><ol start="3"><li>Define the objective function for Optuna:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optuna_objective</span>(<span class="params">trial</span>):</span></span><br><span class="line">    C = trial.suggest_loguniform(<span class="string">&#x27;C&#x27;</span>, <span class="number">1e-2</span>, <span class="number">1e2</span>)</span><br><span class="line">    kernel = trial.suggest_categorical(<span class="string">&#x27;kernel&#x27;</span>, [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>])</span><br><span class="line">    gamma = trial.suggest_loguniform(<span class="string">&#x27;gamma&#x27;</span>, <span class="number">1e-4</span>, <span class="number">1e1</span>)</span><br><span class="line"></span><br><span class="line">    svm = SVC(C=C, kernel=kernel, gamma=gamma)</span><br><span class="line">    score = np.mean(cross_val_score(svm, X_train, y_train, cv=<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> - score</span><br></pre></td></tr></table></figure><ol start="4"><li>Perform hyperparameter optimization using Optuna:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optuna_study = optuna.create_study()</span><br><span class="line">optuna_study.optimize(optuna_objective, n_trials=<span class="number">50</span>)</span><br></pre></td></tr></table></figure><ol start="5"><li>Define the search space for Hyperopt:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hyperopt_space = &#123;</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: hp.loguniform(<span class="string">&#x27;C&#x27;</span>, np.log(<span class="number">1e-2</span>), np.log(<span class="number">1e2</span>)),</span><br><span class="line">    <span class="string">&#x27;kernel&#x27;</span>: hp.choice(<span class="string">&#x27;kernel&#x27;</span>, [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>]),</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: hp.loguniform(<span class="string">&#x27;gamma&#x27;</span>, np.log(<span class="number">1e-4</span>), np.log(<span class="number">1e1</span>))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol start="6"><li>Define the objective function for Hyperopt:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hyperopt_objective</span>(<span class="params">params</span>):</span></span><br><span class="line">    svm = SVC(**params)</span><br><span class="line">    score = np.mean(cross_val_score(svm, X_train, y_train, cv=<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">&#x27;loss&#x27;</span>: <span class="number">1.0</span> - score, <span class="string">&#x27;status&#x27;</span>: STATUS_OK&#125;</span><br></pre></td></tr></table></figure><ol start="7"><li>Perform hyperparameter optimization using Hyperopt:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">trials = Trials()</span><br><span class="line">best_hyperopt = fmin(hyperopt_objective, hyperopt_space, algo=tpe.suggest, max_evals=<span class="number">50</span>, trials=trials)</span><br></pre></td></tr></table></figure><ol start="8"><li>Perform hyperparameter optimization using Scikit-Optimize:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param_grid = &#123;</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: (<span class="number">1e-2</span>, <span class="number">1e2</span>, <span class="string">&#x27;log-uniform&#x27;</span>),</span><br><span class="line">    <span class="string">&#x27;kernel&#x27;</span>: [<span class="string">&#x27;linear&#x27;</span>, <span class="string">&#x27;rbf&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;gamma&#x27;</span>: (<span class="number">1e-4</span>, <span class="number">1e1</span>, <span class="string">&#x27;log-uniform&#x27;</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bayes_search = BayesSearchCV(SVC(), param_grid, n_iter=<span class="number">50</span>, cv=<span class="number">5</span>, n_jobs=-<span class="number">1</span>)</span><br><span class="line">bayes_search.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><ol start="9"><li>Compare the results of Optuna, Hyperopt, and Scikit-Optimize:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_optuna_model = SVC(**optuna_study.best_params)</span><br><span class="line">best_hyperopt_model = SVC(**best_hyperopt)</span><br><span class="line">best_skopt_model = bayes_search.best_estimator_</span><br><span class="line"></span><br><span class="line">best_optuna_model.fit(X_train, y_train)</span><br><span class="line">best_hyperopt_model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_optuna = best_optuna_model.predict(X_test)</span><br><span class="line">y_pred_hyperopt = best_hyperopt_model.predict(X_test)</span><br><span class="line">y_pred_skopt = best_skopt_model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optuna Results:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_optuna))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Hyperopt Results:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_hyperopt))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Scikit-Optimize Results:&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred_skopt))</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we explored advanced and automated techniques for hyperparameter tuning using popular Python libraries: Optuna, Hyperopt, and Scikit-Optimize. We demonstrated their usage with a practical example on the Iris dataset and the SVM algorithm. These libraries provide powerful optimization algorithms that can efficiently search for the best hyperparameters in large search spaces, making them a valuable tool for machine learning practitioners. In the next blog post, we will explore more advanced techniques for hyperparameter optimization, such as genetic algorithms and population-based training.<br>Continue your learning by reading:<br><a href="/2023/05/03/overview-of-hyperprameter-optimization-4/">Leveraging Genetic Algorithms for Hyperparameter Tuning in Python</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter optimization </tag>
            
            <tag> optuna </tag>
            
            <tag> hyperopt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hyperparameter Optimization 5 - Advanced Techniques, Hyperband and Population-Based Training for Hyperparameter Optimization</title>
      <link href="2023/05/03/overview-of-hyperprameter-optimization-5/"/>
      <url>2023/05/03/overview-of-hyperprameter-optimization-5/</url>
      
        <content type="html"><![CDATA[<p>In the previous blog posts, we introduced the <a href="/2023/05/03/overview-of-hyperprameter-optimization-1/">concept of hyperparameter optimization</a> and explored <a href="/2023/05/03/overview-of-hyperprameter-optimization-2/">various techniques</a>, including Grid Search, Random Search, and <a href="/2023/05/03/overview-of-hyperprameter-optimization-3/">automated optimization</a> using popular Python libraries like Optuna, Hyperopt, and Scikit-Optimize, and <a href="/2023/05/03/overview-of-hyperprameter-optimization-4/">genetic algorithms</a>. In this post, we will dive into more advanced techniques for hyperparameter optimization: Hyperband and Population-Based Training (PBT). These methods are particularly useful for optimizing deep learning models, as they can efficiently search for the best hyperparameters in large search spaces while reducing the computational cost.</p><h2 id="Hyperband"><a href="#Hyperband" class="headerlink" title="Hyperband"></a>Hyperband</h2><p>Hyperband is an advanced hyperparameter optimization technique that combines random search with adaptive resource allocation and early stopping. The main idea behind Hyperband is to allocate more resources to promising configurations and stop training for less promising ones early on. This allows Hyperband to explore a large search space more efficiently than traditional methods like Grid Search or Random Search.</p><h1 id="Population-Based-Training-PBT"><a href="#Population-Based-Training-PBT" class="headerlink" title="Population-Based Training (PBT)"></a>Population-Based Training (PBT)</h1><p>Population-Based Training (PBT) is another advanced technique for hyperparameter optimization that combines ideas from genetic algorithms and early stopping. PBT maintains a population of models with different hyperparameter configurations and trains them in parallel. Periodically, poorly performing models are replaced with better-performing ones, and their hyperparameters are perturbed to explore new configurations. This process allows PBT to efficiently search for the best hyperparameters while also adapting them during training.</p><h2 id="Example-Hyperparameter-Optimization-with-Hyperband-and-PBT-in-Python"><a href="#Example-Hyperparameter-Optimization-with-Hyperband-and-PBT-in-Python" class="headerlink" title="Example: Hyperparameter Optimization with Hyperband and PBT in Python"></a>Example: Hyperparameter Optimization with Hyperband and PBT in Python</h2><p>In this example, we will demonstrate hyperparameter optimization using Hyperband and PBT on the famous CIFAR-10 dataset with a simple convolutional neural network (CNN) using the Keras library.</p><ol><li>Import necessary libraries and load the dataset:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.datasets <span class="keyword">import</span> cifar10</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Conv2D, MaxPooling2D, Dense, Flatten, Dropout</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical</span><br><span class="line"><span class="keyword">from</span> kerastuner.tuners <span class="keyword">import</span> Hyperband</span><br><span class="line"><span class="keyword">from</span> ray.tune.schedulers <span class="keyword">import</span> PopulationBasedTraining</span><br><span class="line"><span class="keyword">from</span> ray <span class="keyword">import</span> tune</span><br><span class="line"></span><br><span class="line">(X_train, y_train), (X_test, y_test) = cifar10.load_data()</span><br><span class="line">y_train = to_categorical(y_train)</span><br><span class="line">y_test = to_categorical(y_test)</span><br></pre></td></tr></table></figure><ol start="2"><li>Normalize the data:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = X_train.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.0</span></span><br><span class="line">X_test = X_test.astype(<span class="string">&#x27;float32&#x27;</span>) / <span class="number">255.0</span></span><br></pre></td></tr></table></figure><ol start="3"><li>Define the CNN model:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_cnn_model</span>(<span class="params">hp</span>):</span></span><br><span class="line">    model = Sequential()</span><br><span class="line">    model.add(Conv2D(filters=hp.Int(<span class="string">&#x27;filters_1&#x27;</span>, <span class="number">32</span>, <span class="number">128</span>, step=<span class="number">32</span>), kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(<span class="number">32</span>, <span class="number">32</span>, <span class="number">3</span>)))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">    model.add(Conv2D(filters=hp.Int(<span class="string">&#x27;filters_2&#x27;</span>, <span class="number">32</span>, <span class="number">128</span>, step=<span class="number">32</span>), kernel_size=<span class="number">3</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(MaxPooling2D(pool_size=<span class="number">2</span>))</span><br><span class="line">    model.add(Flatten())</span><br><span class="line">    model.add(Dense(units=hp.Int(<span class="string">&#x27;units&#x27;</span>, <span class="number">128</span>, <span class="number">512</span>, step=<span class="number">64</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">    model.add(Dropout(rate=hp.Float(<span class="string">&#x27;dropout&#x27;</span>, <span class="number">0.1</span>, <span class="number">0.5</span>, step=<span class="number">0.1</span>)))</span><br><span class="line">    model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>))</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=Adam(learning_rate=hp.Float(<span class="string">&#x27;learning_rate&#x27;</span>, <span class="number">1e-4</span>, <span class="number">1e-2</span>, sampling=<span class="string">&#x27;log&#x27;</span>)), loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure><ol start="4"><li>Perform hyperparameter optimization using Hyperband:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hyperband_tuner = Hyperband(create_cnn_model, objective=<span class="string">&#x27;val_accuracy&#x27;</span>, max_epochs=<span class="number">50</span>, hyperband_iterations=<span class="number">2</span>, directory=<span class="string">&#x27;hyperband&#x27;</span>, project_name=<span class="string">&#x27;cifar10&#x27;</span>)</span><br><span class="line">hyperband_tuner.search(X_train, y_train, validation_split=<span class="number">0.2</span>, epochs=<span class="number">50</span>)</span><br></pre></td></tr></table></figure><ol start="5"><li>Define the objective function for PBT:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pbt_objective</span>(<span class="params">config</span>):</span></span><br><span class="line">    model = create_cnn_model(config)</span><br><span class="line">    history = model.fit(X_train, y_train, validation_split=<span class="number">0.2</span>, epochs=<span class="number">50</span>)</span><br><span class="line">    tune.report(accuracy=history.history[<span class="string">&#x27;val_accuracy&#x27;</span>][-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><ol start="6"><li>Perform hyperparameter optimization using PBT:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pbt_scheduler = PopulationBasedTraining(time_attr=<span class="string">&#x27;training_iteration&#x27;</span>, metric=<span class="string">&#x27;accuracy&#x27;</span>, mode=<span class="string">&#x27;max&#x27;</span>, perturbation_interval=<span class="number">5</span>, hyperparam_mutations=&#123;<span class="string">&#x27;learning_rate&#x27;</span>: tune.loguniform(<span class="number">1e-4</span>, <span class="number">1e-2</span>), <span class="string">&#x27;filters_1&#x27;</span>: tune.randint(<span class="number">32</span>, <span class="number">128</span>), <span class="string">&#x27;filters_2&#x27;</span>: tune.randint(<span class="number">32</span>, <span class="number">128</span>), <span class="string">&#x27;units&#x27;</span>: tune.randint(<span class="number">128</span>, <span class="number">512</span>), <span class="string">&#x27;dropout&#x27;</span>: tune.uniform(<span class="number">0.1</span>, <span class="number">0.5</span>)&#125;)</span><br><span class="line"></span><br><span class="line">pbt_analysis = tune.run(pbt_objective, config=&#123;<span class="string">&#x27;learning_rate&#x27;</span>: tune.loguniform(<span class="number">1e-4</span>, <span class="number">1e-2</span>), <span class="string">&#x27;filters_1&#x27;</span>: tune.randint(<span class="number">32</span>, <span class="number">128</span>), <span class="string">&#x27;filters_2&#x27;</span>: tune.randint(<span class="number">32</span>, <span class="number">128</span>), <span class="string">&#x27;units&#x27;</span>: tune.randint(<span class="number">128</span>, <span class="number">512</span>), <span class="string">&#x27;dropout&#x27;</span>: tune.uniform(<span class="number">0.1</span>, <span class="number">0.5</span>)&#125;, num_samples=<span class="number">10</span>, scheduler=pbt_scheduler, resources_per_trial=&#123;<span class="string">&#x27;cpu&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;gpu&#x27;</span>: <span class="number">1</span>&#125;)</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we explored advanced techniques for hyperparameter optimization, such as Hyperband and Population-Based Training (PBT). These methods are particularly useful for optimizing deep learning models, as they can efficiently search for the best hyperparameters in large search spaces while reducing the computational cost. By leveraging these advanced techniques, machine learning practitioners can further improve the performance of their models and make better predictions. In the future, we can expect even more advanced techniques and tools to emerge, making hyperparameter optimization an increasingly important aspect of machine learning.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter optimization </tag>
            
            <tag> hyperband </tag>
            
            <tag> Population-Based Training </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between import file and from file import * in Python</title>
      <link href="2023/05/02/dIfference-between-import-file-and-from-file-import-in-python/"/>
      <url>2023/05/02/dIfference-between-import-file-and-from-file-import-in-python/</url>
      
        <content type="html"><![CDATA[<p>In python, it’s common to import functions from another file.<br>So what’s the difference of the these two cases.</p><p>The <code>import src.myfile</code> statement and <code>from src.myfile import *</code> statement are used for different purposes and have different effects on how the module’s functions and variables are imported.</p><p>The <code>import src.myfile</code> statement imports the <code>src/myfile.py</code> module as a whole, without making any of its functions or variables directly accessible in your code. Instead, you have to prefix any function or variable you want to use with the module name. For example, if <code>src/myfile.py</code> contains a function named myfunc, you can call it using the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import src.myfile</span><br><span class="line">result = src.myfile.myfunc(arg1, arg2, ...)</span><br></pre></td></tr></table></figure><p>On the other hand, the <code>from src.myfile import *</code> statement imports all functions and variables defined in the <code>src/myfile.py</code> module into your current namespace. This means that you can use them directly in your code without prefixing them with the module name. For example, if <code>src/myfile.py</code> contains a function named myfunc, you can call it using the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from src.myfile import *</span><br><span class="line">result = myfunc(arg1, arg2, ...)</span><br></pre></td></tr></table></figure><p>However, also notice that using <code>import *</code> is generally not recommended, as it can create naming conflicts and make it harder to read and understand your code. It’s usually better to use the import module_name syntax and prefix any functions or variables you use with the module name, or to use the from module_name import function_name, variable_name syntax to import only the specific functions or variables you need.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Pooling in Transformer Architecture, Aggregating Outputs for Downstream Tasks</title>
      <link href="2023/04/30/what-is-pooling-in-transformer-model/"/>
      <url>2023/04/30/what-is-pooling-in-transformer-model/</url>
      
        <content type="html"><![CDATA[<p>In the context of transformers, pooling refers to the process of summarizing the outputs of the transformer layers into a fixed-size vector, often used for downstream tasks such as classification.</p><p>In a transformer architecture, the input sequence is processed by a series of self-attention and feedforward layers. Each layer produces a sequence of output vectors, which encode the input sequence in a higher-level representation. Pooling involves taking the output vectors from one or more of these layers and aggregating them into a single vector.</p><p>There are different types of pooling mechanisms used in transformer architectures, including:</p><ol><li><p>Max Pooling: where the maximum value across the sequence of output vectors is selected as the summary representation.</p></li><li><p>Mean Pooling: where the average of the output vectors is taken as the summary representation.</p></li><li><p>Last Hidden State: where the final output vector of the transformer is used as the summary representation.</p></li><li><p>Self-Attention Pooling: where a weighted sum of the output vectors is computed, with the weights determined by a learned attention mechanism.</p></li></ol><p>Overall, pooling is an important component of transformer architectures, as it allows for the extraction of a fixed-size representation of the input sequence, which can be used for a variety of downstream tasks.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> pooling </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to not indexing fields in elasticsearch</title>
      <link href="2023/04/28/how-to-not-index-field-in-elasticsearch/"/>
      <url>2023/04/28/how-to-not-index-field-in-elasticsearch/</url>
      
        <content type="html"><![CDATA[<p>To specify that a field should not be indexed in an Elasticsearch index mapping, you can use the index property with a value of false. For example, suppose you have a mapping for an index called my_index, and you want to include a field called my_field that should not be indexed. You can define the mapping like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;properties&quot;: &#123;</span><br><span class="line">      &quot;my_field&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">        &quot;index&quot;: false</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>In this mapping, the my_field field is defined as a text field with the index property set to false. This tells Elasticsearch not to index the field, which means that it will not be searchable.</p><p>Note that if you set a field’s index property to false, you will not be able to search for that field’s values using Elasticsearch’s search API. However, you can still retrieve the field’s values from Elasticsearch using the get API.</p><p>If you are using python do do the indxing, notice that, in Python, the value for index should be False (with a capital “F”), not false. Here is the corrected example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from elasticsearch import Elasticsearch</span><br><span class="line"></span><br><span class="line">es = Elasticsearch()</span><br><span class="line"></span><br><span class="line">mapping = &#123;</span><br><span class="line">    &quot;mappings&quot;: &#123;</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;my_field&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">                &quot;index&quot;: False</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">es.indices.create(index=&quot;my_index&quot;, body=mapping)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to delete file and folder from git repo</title>
      <link href="2023/04/28/how-to-remove-file-and-directory-from-git-repo/"/>
      <url>2023/04/28/how-to-remove-file-and-directory-from-git-repo/</url>
      
        <content type="html"><![CDATA[<p>To delete files and folders from a Git repository, follow these steps:</p><ol><li><p>Open a terminal or command prompt.</p></li><li><p>Navigate to the local Git repository on your computer using the <code>cd</code> command. For example:</p></li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd /path/to/your/repo</span><br></pre></td></tr></table></figure><ol start="3"><li>To delete a file, use the <code>git rm</code> command followed by the file name. For example:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rm file-to-delete.txt</span><br></pre></td></tr></table></figure><ol start="4"><li>To delete a folder, use the <code>git rm</code> command with the <code>-r</code> flag followed by the folder name. For example:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rm -r folder-to-delete</span><br></pre></td></tr></table></figure><ol start="5"><li>After deleting the files or folders, commit the changes with a commit message. For example:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git commit -m &quot;Removed unwanted files and folders&quot;</span><br></pre></td></tr></table></figure><ol start="6"><li>Finally, push the changes to the remote repository. For example:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push origin main</span><br></pre></td></tr></table></figure><p>Replace <code>main</code> with the name of the branch you are working on if it’s different.</p><p>Note: This process will only delete the files and folders from the Git repository, not from your local file system. If you want to delete them from your local file system as well, you’ll need to do that manually.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Particle Swarm Optimization Implemenation in Python</title>
      <link href="2023/04/26/particle-swarms-optimization-implementation-in-python/"/>
      <url>2023/04/26/particle-swarms-optimization-implementation-in-python/</url>
      
        <content type="html"><![CDATA[<p>Optimization is a crucial aspect of many real-world problems, from machine learning and artificial intelligence to engineering and finance. There are various optimization algorithms available, each with its strengths and weaknesses. In this blog post, we will introduce Particle Swarm Optimization (PSO), a popular optimization algorithm inspired by the social behavior of bird flocking or fish schooling. We will discuss its benefits compared to other optimization techniques, such as Gradient Descent, and provide examples of how to implement PSO in Python using different libraries.</p><h2 id="What-is-Particle-Swarm-Optimization"><a href="#What-is-Particle-Swarm-Optimization" class="headerlink" title="What is Particle Swarm Optimization?"></a>What is Particle Swarm Optimization?</h2><p>Particle Swarm Optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It is a population-based optimization technique that simulates the social behavior of bird flocking or fish schooling. PSO is used to find the optimal solution for various optimization problems, such as function optimization, machine learning, and artificial neural network training.</p><p>In PSO, a swarm of particles moves through the search space, updating their positions based on their own best-known positions and the best-known positions of the entire swarm. The algorithm balances exploration (searching new areas of the search space) and exploitation (refining the current best solutions) to find the global optimum.</p><h2 id="Implementing-PSO-in-Python"><a href="#Implementing-PSO-in-Python" class="headerlink" title="Implementing PSO in Python"></a>Implementing PSO in Python</h2><p>There are several ways to implement PSO in Python, ranging from writing your own implementation to using existing libraries. In this section, we will provide a simple example of a custom PSO implementation and demonstrate how to use two popular Python libraries, <code>pyswarm</code> and <code>PySwarms</code>, to optimize a sample objective function.</p><h3 id="implementation-in-python"><a href="#implementation-in-python" class="headerlink" title="implementation in python"></a>implementation in python</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># Objective function to optimize</span><br><span class="line">def objective_function(x):</span><br><span class="line">    return x[0]**2 + x[1]**2</span><br><span class="line"></span><br><span class="line"># Particle Swarm Optimization function</span><br><span class="line">def particle_swarm_optimization(objective_function, bounds, num_particles, num_iterations):</span><br><span class="line">    # Initialize particles</span><br><span class="line">    particles = np.random.uniform(bounds[:, 0], bounds[:, 1], (num_particles, bounds.shape[0]))</span><br><span class="line">    velocities = np.zeros_like(particles)</span><br><span class="line">    personal_best_positions = particles.copy()</span><br><span class="line">    personal_best_scores = np.array([objective_function(p) for p in particles])</span><br><span class="line"></span><br><span class="line">    # Find global best</span><br><span class="line">    global_best_position = personal_best_positions[np.argmin(personal_best_scores)]</span><br><span class="line">    global_best_score = np.min(personal_best_scores)</span><br><span class="line"></span><br><span class="line">    # PSO parameters</span><br><span class="line">    w = 0.7298  # Inertia weight</span><br><span class="line">    c1 = 1.49618  # Cognitive parameter</span><br><span class="line">    c2 = 1.49618  # Social parameter</span><br><span class="line"></span><br><span class="line">    # Main loop</span><br><span class="line">    for i in range(num_iterations):</span><br><span class="line">        # Update velocities</span><br><span class="line">        velocities = w * velocities + c1 * np.random.rand() * (personal_best_positions - particles) + c2 * np.random.rand() * (global_best_position - particles)</span><br><span class="line"></span><br><span class="line">        # Update particle positions</span><br><span class="line">        particles += velocities</span><br><span class="line"></span><br><span class="line">        # Update personal bests</span><br><span class="line">        for j, p in enumerate(particles):</span><br><span class="line">            score = objective_function(p)</span><br><span class="line">            if score &lt; personal_best_scores[j]:</span><br><span class="line">                personal_best_positions[j] = p.copy()</span><br><span class="line">                personal_best_scores[j] = score</span><br><span class="line"></span><br><span class="line">                # Update global best</span><br><span class="line">                if score &lt; global_best_score:</span><br><span class="line">                    global_best_position = p.copy()</span><br><span class="line">                    global_best_score = score</span><br><span class="line"></span><br><span class="line">    return global_best_position, global_best_score</span><br><span class="line"></span><br><span class="line"># Define problem bounds</span><br><span class="line">bounds = np.array([[-5, 5], [-5, 5]])</span><br><span class="line"></span><br><span class="line"># Run Particle Swarm Optimization</span><br><span class="line">best_position, best_score = particle_swarm_optimization(objective_function, bounds, num_particles=30, num_iterations=100)</span><br><span class="line"></span><br><span class="line">print(&quot;Best position:&quot;, best_position)</span><br><span class="line">print(&quot;Best score:&quot;, best_score)</span><br></pre></td></tr></table></figure><p>You can also use exisiting packages,  one popular package is <code>pyswarm</code>, which provides a simple interface for using PSO to optimize a given objective function. You can install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pyswarm</span><br></pre></td></tr></table></figure><p>Here’s an example of how to use <code>pyswarm</code> to optimize the same objective function as in the previous example:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyswarm <span class="keyword">import</span> pso</span><br><span class="line"></span><br><span class="line"><span class="comment"># Objective function to optimize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>]**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define problem bounds</span></span><br><span class="line">lb = [-<span class="number">5</span>, -<span class="number">5</span>]  <span class="comment"># Lower bounds</span></span><br><span class="line">ub = [<span class="number">5</span>, <span class="number">5</span>]    <span class="comment"># Upper bounds</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Particle Swarm Optimization</span></span><br><span class="line">best_position, best_score = pso(objective_function, lb, ub)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best position:&quot;</span>, best_position)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score:&quot;</span>, best_score)</span><br></pre></td></tr></table></figure><p>In this example, we import the <code>pso</code> function from the <code>pyswarm</code> package and use it to optimize the objective function. The <code>pso</code> function takes the objective function, lower bounds, and upper bounds as input arguments and returns the best position and score found during the optimization process.</p><p>Another package you can use is <code>PySwarms</code>, which offers more flexibility and options for customizing the PSO algorithm. You can install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pyswarms</span><br></pre></td></tr></table></figure><p>Here’s an example of how to use <code>PySwarms</code> to optimize the same objective function:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pyswarms <span class="keyword">as</span> ps</span><br><span class="line"></span><br><span class="line"><span class="comment"># Objective function to optimize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">objective_function</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.<span class="built_in">sum</span>(x**<span class="number">2</span>, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define problem bounds</span></span><br><span class="line">bounds = np.array([[-<span class="number">5</span>, <span class="number">5</span>], [-<span class="number">5</span>, <span class="number">5</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the optimizer</span></span><br><span class="line">options = &#123;<span class="string">&#x27;c1&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;c2&#x27;</span>: <span class="number">0.3</span>, <span class="string">&#x27;w&#x27;</span>: <span class="number">0.9</span>&#125;</span><br><span class="line">optimizer = ps.single.GlobalBestPSO(n_particles=<span class="number">30</span>, dimensions=<span class="number">2</span>, options=options, bounds=bounds.T)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Particle Swarm Optimization</span></span><br><span class="line">best_score, best_position = optimizer.optimize(objective_function, iters=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best position:&quot;</span>, best_position)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best score:&quot;</span>, best_score)</span><br></pre></td></tr></table></figure><p>In this example, we import the <code>pyswarms</code> package and use the <code>GlobalBestPSO</code> class to set up the optimizer. We define the PSO parameters, the number of particles, dimensions, and bounds, and then call the <code>optimize</code> method to run the optimization process. The method returns the best score and position found during the optimization.</p><h2 id="Benefits-of-PSO-compared-to-Gradient-Descent"><a href="#Benefits-of-PSO-compared-to-Gradient-Descent" class="headerlink" title="Benefits of PSO compared to Gradient Descent"></a>Benefits of PSO compared to Gradient Descent</h2><p>PSO and Gradient Descent are both optimization algorithms, but they have different characteristics and are suited for different types of problems. Here are some benefits of PSO compared to Gradient Descent:</p><ol><li><p>No gradient information required: PSO is a derivative-free optimization method, making it suitable for optimizing functions that are non-differentiable, discontinuous, or have noisy gradients.</p></li><li><p>Global optimization: PSO is designed to find the global minimum of a function, while Gradient Descent can get stuck in local minima for non-convex functions.</p></li><li><p>Parallel search: PSO searches the solution space using multiple particles simultaneously, allowing for better exploration of the search space and potentially faster convergence.</p></li><li><p>Adaptability: PSO can be easily adapted to handle various types of optimization problems, including continuous, discrete, and mixed-variable problems.</p></li><li><p>Ease of implementation: PSO is relatively easy to implement and has fewer hyperparameters to tune compared to Gradient Descent.</p></li></ol><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Particle Swarm Optimization is a powerful optimization algorithm that can be used to solve a wide range of problems. Its ability to handle non-differentiable functions, global optimization, and parallel search make it an attractive choice for many applications. In this blog post, we have introduced the basics of PSO, provided examples of how to implement it in Python, and discussed its benefits compared to Gradient Descent. By understanding the strengths and weaknesses of different optimization algorithms, you can choose the most suitable method for your specific problem and achieve better results.</p>]]></content>
      
      
      <categories>
          
          <category> optimization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> partilce swarm optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LightGBM regression example with cross validation and early stop run</title>
      <link href="2023/04/24/lightgbm-regression-complete-example-with-cross-validation-and-early-stop/"/>
      <url>2023/04/24/lightgbm-regression-complete-example-with-cross-validation-and-early-stop/</url>
      
        <content type="html"><![CDATA[<p>In this blog post, we will walk through a complete example of using LightGBM, a gradient boosting framework, for regression tasks. We will generate a random dataset, split it into training and testing sets, train a LightGBM regression model, and evaluate its performance using mean squared error (MSE) and a scatter plot of predicted vs expected values.</p><h2 id="What-is-LightGBM"><a href="#What-is-LightGBM" class="headerlink" title="What is LightGBM?"></a>What is LightGBM?</h2><p>LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed to be efficient and scalable, making it suitable for large datasets and high-performance tasks. LightGBM is particularly popular for its speed and accuracy, outperforming many other machine learning algorithms in various benchmarks.</p><h2 id="Generating-a-Random-Dataset"><a href="#Generating-a-Random-Dataset" class="headerlink" title="Generating a Random Dataset"></a>Generating a Random Dataset</h2><p>For this example, we will generate a random dataset using the <code>make_regression</code> function from scikit-learn. This function creates a dataset with a specified number of samples, features, and noise level. We will generate a dataset with 1000 samples, 10 features, and a noise level of 0.1.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_regression</span><br><span class="line"></span><br><span class="line">X, y = make_regression(n_samples=<span class="number">1000</span>, n_features=<span class="number">10</span>, noise=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><p>Next, we will convert the generated data to a pandas DataFrame for easier manipulation.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(X, columns=[<span class="string">f&quot;feature_<span class="subst">&#123;i&#125;</span>&quot;</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>])])</span><br><span class="line">df[<span class="string">&quot;target&quot;</span>] = y</span><br></pre></td></tr></table></figure><h2 id="Preparing-the-Data"><a href="#Preparing-the-Data" class="headerlink" title="Preparing the Data"></a>Preparing the Data</h2><p>Before training the model, we need to split the data into training and testing sets. We will use 80% of the data for training and 20% for testing.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X = df.drop(<span class="string">&quot;target&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = df[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h2 id="Training-the-LightGBM-Regression-Model"><a href="#Training-the-LightGBM-Regression-Model" class="headerlink" title="Training the LightGBM Regression Model"></a>Training the LightGBM Regression Model</h2><p>Now that we have prepared the data, we can train the LightGBM regression model. First, we need to create a LightGBM dataset from our training data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"></span><br><span class="line">train_data = lgb.Dataset(X_train, label=y_train)</span><br></pre></td></tr></table></figure><p>Next, we will set up the parameters for the LightGBM model. In this example, we will use the default parameters for a regression task.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&quot;objective&quot;</span>: <span class="string">&quot;regression&quot;</span>,</span><br><span class="line">    <span class="string">&quot;metric&quot;</span>: <span class="string">&quot;mse&quot;</span>,</span><br><span class="line">    <span class="string">&quot;boosting_type&quot;</span>: <span class="string">&quot;gbdt&quot;</span>,</span><br><span class="line">    <span class="string">&quot;num_leaves&quot;</span>: <span class="number">31</span>,</span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: <span class="number">0.05</span>,</span><br><span class="line">    <span class="string">&quot;feature_fraction&quot;</span>: <span class="number">0.9</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>We will train the model using cross-validation with early stopping to prevent overfitting. The <code>lgb.cv</code> function performs cross-validation and returns the results for each round. We will use the best number of rounds to train the final model.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_round = <span class="number">1000</span></span><br><span class="line">cv_results = lgb.cv(</span><br><span class="line">    params,</span><br><span class="line">    train_data,</span><br><span class="line">    num_boost_round=num_round,</span><br><span class="line">    nfold=<span class="number">5</span>,</span><br><span class="line">    early_stopping_rounds=<span class="number">10</span>,</span><br><span class="line">    stratified=<span class="literal">False</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">best_round = <span class="built_in">len</span>(cv_results[<span class="string">&quot;l2-mean&quot;</span>])</span><br><span class="line">model = lgb.train(params, train_data, num_boost_round=best_round)</span><br></pre></td></tr></table></figure><h2 id="Evaluating-the-Model"><a href="#Evaluating-the-Model" class="headerlink" title="Evaluating the Model"></a>Evaluating the Model</h2><p>Now that we have trained the model, we can evaluate its performance on the test set. We will use the mean squared error (MSE) as our evaluation metric.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line">y_pred = model.predict(X_test)</span><br><span class="line">mse = mean_squared_error(y_test, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Mean Squared Error:&quot;</span>, mse)</span><br></pre></td></tr></table></figure><p>Finally, we will plot a scatter plot of the predicted vs expected values to visualize the model’s performance.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.scatter(y_test, y_pred)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Expected&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Predicted&quot;</span>)</span><br><span class="line">plt.title(<span class="string">&quot;Predicted vs Expected&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this blog post, we have demonstrated a complete example of using LightGBM for regression tasks with a randomly generated dataset. We have shown how to prepare the data, train the model, and evaluate its performance using mean squared error and a scatter plot. LightGBM is a powerful and efficient gradient boosting framework that can be used for various machine learning tasks, including regression, classification, and ranking.</p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/lightgbm-regression-with-cross-validation-and-early-stop/2023-04-24-lightgbm%20regression%20example.ipynb">github link</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lightgbm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is the lightGBM no further splits warning?</title>
      <link href="2023/04/24/what-is-lightgbm-no-further-splits-warning/"/>
      <url>2023/04/24/what-is-lightgbm-no-further-splits-warning/</url>
      
        <content type="html"><![CDATA[<p>When running lightgbm model, you might see this warning:<br><code> [LightGBM] [Warning] No further splits with positive gain, best gain: -inf</code></p><p>So what is this? should I get concerned? and if not, how to turn it off.</p><h1 id="What-is-it"><a href="#What-is-it" class="headerlink" title="What is it?"></a>What is it?</h1><p>This warning message from LightGBM indicates that during the training process, the algorithm could not find any further splits in the decision tree that would result in a positive gain (i.e., an improvement in the model’s performance). The “best gain” being <code>-inf</code> means that no split was found that would improve the model.</p><p>This can happen when the model has already learned the patterns in the data well enough, or when the data is noisy, and further splits would only lead to overfitting. It can also occur when the model’s hyperparameters are not well-tuned for the given dataset.</p><p>In most cases, this warning can be safely ignored, as it simply means that the model has reached a point where further splits are not beneficial. However, if you see this warning frequently and the model’s performance is not satisfactory, you may want to consider tuning the hyperparameters or preprocessing the data differently to improve the model’s performance.</p><h2 id="How-to-suppress-the-warning"><a href="#How-to-suppress-the-warning" class="headerlink" title="How to suppress the warning"></a>How to suppress the warning</h2><p>for sklearn interface, you can set verbose=-1 when defining the model (not in fit).<br>for lgb.train interface, you can set verbose=-1 in param dict.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lightgbm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the CORS Error and How to Fix It</title>
      <link href="2023/04/22/undersand-the-CORS-error-and-solution-to-fix-it-including-local-test/"/>
      <url>2023/04/22/undersand-the-CORS-error-and-solution-to-fix-it-including-local-test/</url>
      
        <content type="html"><![CDATA[<p>If you’ve ever tried to make an AJAX request from a web page to a server hosted on a different domain, you may have encountered a frustrating error message that says something like “Access to XMLHttpRequest at ht<span>tp://</span>example.com/api/data from origin ‘null’ has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource.” This error message is known as the CORS error, and it is a common issue that web developers face when working with cross-origin requests.</p><p>In this blog post, we’ll take a closer look at what the CORS error is, what causes it, and some solutions to fix it.</p><h2 id="What-is-the-CORS-Error"><a href="#What-is-the-CORS-Error" class="headerlink" title="What is the CORS Error?"></a>What is the CORS Error?</h2><p>The CORS error is a security feature implemented by web browsers that prevents web pages from making requests to a different domain than the one the page came from. This security feature is called the Same-Origin Policy, and it is designed to prevent malicious websites from stealing sensitive information from other sites.</p><p>The CORS error occurs when a web page tries to make an AJAX request to a server hosted on a different domain than the one the page came from, and the server does not send the required ‘Access-Control-Allow-Origin’ header in its response. This header tells the browser which domains are allowed to access the resource, and if it is missing, the browser will block the request and show the CORS error message.</p><h2 id="What-Causes-the-CORS-Error"><a href="#What-Causes-the-CORS-Error" class="headerlink" title="What Causes the CORS Error?"></a>What Causes the CORS Error?</h2><p>The CORS error can occur in a variety of situations, but it usually happens when you try to make an AJAX request from an HTML file that is opened locally in your browser (i.e., using the “file://“ protocol) to a server that is hosted on a different domain or on the internet (i.e., using the “http://“ or “https://“ protocol). This is because the Same-Origin Policy enforced by web browsers prevents web pages from making requests to a different domain than the one the page came from.</p><h2 id="Solutions-to-Fix-the-CORS-Error"><a href="#Solutions-to-Fix-the-CORS-Error" class="headerlink" title="Solutions to Fix the CORS Error"></a>Solutions to Fix the CORS Error</h2><p>There are several solutions you can use to fix the CORS error, depending on the situation:</p><ol><li><p>Host the HTML file and the server on the same domain: If you are developing a web application and have control over the server hosting the API, you can host the HTML file and the server on the same domain. This way, they will have the same origin, and you won’t encounter the CORS error.</p></li><li><p>Use a web server to serve the HTML file: If you are only testing and don’t need to actually make requests to a server, you can use a web server to serve the HTML file instead of opening it directly in your browser. This will allow you to make requests to other domains using AJAX without triggering the Same-Origin Policy.</p></li><li><p>Disable the Same-Origin Policy in your browser: If you need to test something quickly and don’t want to set up a web server, you can disable the Same-Origin Policy in your browser by running it with the “–disable-web-security” flag (this is not recommended for general browsing, as it removes an important security feature of the browser).</p></li><li><p>Use JSONP (JSON with Padding) instead of AJAX: JSONP is a way of getting data from a server in a different domain that circumvents the Same-Origin Policy by wrapping the response in a JavaScript function call. This approach has some security implications, but it can be a useful workaround in some cases.</p></li><li><p>Use a proxy server to forward your requests: If you cannot modify the server’s CORS settings or use JSONP, you can use a proxy server to forward your requests. A proxy server is a server that sits between your browser and the server you are trying to make requests to, and forwards the requests for you, adding the necessary headers to avoid the CORS error.</p></li><li><p>On your local development, simply use <code>python -m http.server</code> to set up a local web server that serves your HTML file from the current directory. By accessing the HTML file at ht<span>tp://</span>localhost:8000, you can avoid the CORS error and make requests to other domains using AJAX. This approach is useful when you are developing and testing locally and don’t need to deploy your code to a remote server yet.</p></li></ol><p>Conclusion</p><p>The CORS error can be a frustrating issue to deal with, but understanding its causes and using the appropriate solution can help you resolve it. Whether you choose to host your HTML file and server on the same domain, use a web server to serve your HTML file, disable the Same-Origin Policy in your browser, use JSONP, use a proxy server, or set up a local web server using <code>python -m http.server</code>, you can avoid the CORS error and make cross-origin requests successfully.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> CORS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Organizing Text Content with Python, Splitting Continuous Headings with various subheadings into Lists</title>
      <link href="2023/04/21/how-to-split-continuous-heading-patterns-with-various-subheadings/"/>
      <url>2023/04/21/how-to-split-continuous-heading-patterns-with-various-subheadings/</url>
      
        <content type="html"><![CDATA[<p>Title: Organizing Text Content with Python: Splitting Continuous Headings into Lists</p><p>Introduction:</p><p>When working with text data, it’s often necessary to organize and structure the content in a way that makes it easier to process and analyze. One common task is to split a continuous block of text into separate sections based on specific patterns or headings. In this blog post, we’ll demonstrate how to use Python to split a text outline with continuous headings into separate lists. This can be useful for various applications, such as organizing content for a website or processing data for analysis.</p><p>Example:</p><p>Consider the following code, which defines a function called <code>split_outline</code> that takes a text input and splits it into separate sections based on the presence of <code>[H2]</code> headings:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_outline</span>(<span class="params">text</span>):</span></span><br><span class="line">    lines = text.split(<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">    result = []</span><br><span class="line">    current_list = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        line = line.strip()</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(line) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> line.startswith(<span class="string">&#x27;[H2]&#x27;</span>):</span><br><span class="line">            <span class="keyword">if</span> current_list:</span><br><span class="line">                result.append(<span class="string">&#x27;\n&#x27;</span>.join(current_list))</span><br><span class="line">                current_list = []</span><br><span class="line">            current_list.append(line)</span><br><span class="line">        <span class="keyword">elif</span> <span class="keyword">not</span> line.startswith(<span class="string">&#x27;[H2]&#x27;</span>):</span><br><span class="line">            current_list.append(line)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> current_list:</span><br><span class="line">        result.append(<span class="string">&#x27;\n&#x27;</span>.join(current_list))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This code will output a list of strings, where each string represents a section of the text that starts with an <code>[H2]</code> heading and includes all subsequent lines until the next <code>[H2]</code> heading is encountered.</p><p>Example:</p><p>Now, let’s consider an example where we have a text outline for a blog post about different programming languages:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = <span class="string">&#x27;[H2] Introduction to Programming Languages\n[H3] What is a programming language?\n[H3] Why are there so many programming languages?\n[H2] Popular Programming Languages\n[H3] Python\n[H3] JavaScript\n[H3] Java\n[H3] C#\n[H3] Ruby\n[H2] Choosing the Right Programming Language\n[H3] Factors to consider\n[H3] Language popularity and community support\n[H3] Ease of learning\n[H3] Performance and scalability\n[H2] Conclusion&#x27;</span></span><br></pre></td></tr></table></figure><p>Before splitting, the text looks like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[H2] Introduction to Programming Languages</span><br><span class="line">[H3] What is a programming language?</span><br><span class="line">[H3] Why are there so many programming languages?</span><br><span class="line">[H2] Popular Programming Languages</span><br><span class="line">[H3] Python</span><br><span class="line">[H3] JavaScript</span><br><span class="line">[H3] Java</span><br><span class="line">[H3] C#</span><br><span class="line">[H3] Ruby</span><br><span class="line">[H2] Choosing the Right Programming Language</span><br><span class="line">[H3] Factors to consider</span><br><span class="line">[H3] Language popularity and community support</span><br><span class="line">[H3] Ease of learning</span><br><span class="line">[H3] Performance and scalability</span><br><span class="line">[H2] Conclusion</span><br></pre></td></tr></table></figure><p>We can use the <code>split_outline</code> function to split this text into separate sections based on the <code>[H2]</code> headings:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sections = split_outline(text)</span><br><span class="line"><span class="keyword">for</span> section <span class="keyword">in</span> sections:</span><br><span class="line">    <span class="built_in">print</span>(section)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n---\n&quot;</span>)</span><br></pre></td></tr></table></figure><p>This will output the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[H2] Introduction to Programming Languages</span><br><span class="line">[H3] What is a programming language?</span><br><span class="line">[H3] Why are there so many programming languages?</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">[H2] Popular Programming Languages</span><br><span class="line">[H3] Python</span><br><span class="line">[H3] JavaScript</span><br><span class="line">[H3] Java</span><br><span class="line">[H3] C#</span><br><span class="line">[H3] Ruby</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">[H2] Choosing the Right Programming Language</span><br><span class="line">[H3] Factors to consider</span><br><span class="line">[H3] Language popularity and community support</span><br><span class="line">[H3] Ease of learning</span><br><span class="line">[H3] Performance and scalability</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">[H2] Conclusion</span><br><span class="line"></span><br><span class="line">---</span><br></pre></td></tr></table></figure><p>As you can see, the <code>split_outline</code> function has successfully split the text into separate sections based on the <code>[H2]</code> headings.</p><p>Conclusion:</p><p>In this blog post, we demonstrated how to use Python to split a text outline with continuous headings into separate lists. This technique can be useful for organizing and structuring text data for various applications, such as content management or data analysis. By modifying the <code>split_outline</code> function, you can easily adapt this approach to handle different heading patterns or other text formatting requirements.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Accessing and Searching CloudWatch Logs Insights from AWS Lambda using Python</title>
      <link href="2023/04/19/access-cloudwach-log-and-loginsights-with-lambda-in-aws/"/>
      <url>2023/04/19/access-cloudwach-log-and-loginsights-with-lambda-in-aws/</url>
      
        <content type="html"><![CDATA[<p>AWS Lambda is a serverless compute service that allows you to run your code without provisioning or managing servers. AWS CloudWatch Logs Insights is a fully managed service that helps you analyze, visualize, and gain insights from your log data. In this blog post, we will discuss how to access and search CloudWatch Logs Insights from an AWS Lambda function using Python and the Boto3 library.</p><p>Prerequisites:</p><ul><li>An AWS account</li><li>Basic knowledge of AWS Lambda and CloudWatch Logs Insights</li><li>Python and Boto3 library installed</li></ul><h2 id="Step-1-Set-up-the-IAM-role-for-your-Lambda-function"><a href="#Step-1-Set-up-the-IAM-role-for-your-Lambda-function" class="headerlink" title="Step 1: Set up the IAM role for your Lambda function"></a>Step 1: Set up the IAM role for your Lambda function</h2><p>To access CloudWatch Logs Insights from your Lambda function, you need to create an IAM role with the necessary permissions. Attach the following policy to the role:</p><figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;Version&quot;</span>: <span class="string">&quot;2012-10-17&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;Statement&quot;</span>: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">&quot;Effect&quot;</span>: <span class="string">&quot;Allow&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;Action&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;logs:StartQuery&quot;</span>,</span><br><span class="line">                <span class="string">&quot;logs:GetQueryResults&quot;</span></span><br><span class="line">            ],</span><br><span class="line">            <span class="attr">&quot;Resource&quot;</span>: <span class="string">&quot;*&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>You can also first create the lambda function with some default IAM roles, and then come back to modify the existing policy attched to the current role in the lambda function, then just insert the json in the “statement” block to the existing policy.<br>To modify policy with the existing role, click “cofiguration” on the top of the lamda function page, then click “permissions” on the left pannel, and click the role attached to the lambda function.<br>Further click the role, you will see polices attached to that role, and click the policy to make any updates.</p><h2 id="Step-2-Create-a-Lambda-function-with-the-IAM-role"><a href="#Step-2-Create-a-Lambda-function-with-the-IAM-role" class="headerlink" title="Step 2: Create a Lambda function with the IAM role"></a>Step 2: Create a Lambda function with the IAM role</h2><p>Create a new Lambda function using the AWS Management Console, AWS CLI, or any other method you prefer. Make sure to assign the IAM role you created in Step 1 to the Lambda function.</p><h2 id="Step-3-Access-CloudWatch-Logs-Insights-from-your-Lambda-function"><a href="#Step-3-Access-CloudWatch-Logs-Insights-from-your-Lambda-function" class="headerlink" title="Step 3: Access CloudWatch Logs Insights from your Lambda function"></a>Step 3: Access CloudWatch Logs Insights from your Lambda function</h2><p>In your Lambda function, import the Boto3 library and create a CloudWatch Logs client:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> boto3</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">cloudwatch_logs = boto3.client(<span class="string">&#x27;logs&#x27;</span>)</span><br></pre></td></tr></table></figure><p>Define a function to execute a CloudWatch Logs Insights query and get the results:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run_insights_query</span>(<span class="params">log_group_name, query, start_time, end_time</span>):</span></span><br><span class="line">    response = cloudwatch_logs.start_query(</span><br><span class="line">        logGroupName=log_group_name,</span><br><span class="line">        startTime=<span class="built_in">int</span>((time.time() - start_time) * <span class="number">1000</span>),</span><br><span class="line">        endTime=<span class="built_in">int</span>((time.time() - end_time) * <span class="number">1000</span>),</span><br><span class="line">        queryString=query</span><br><span class="line">    )</span><br><span class="line">    query_id = response[<span class="string">&#x27;queryId&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        response = cloudwatch_logs.get_query_results(</span><br><span class="line">            queryId=query_id</span><br><span class="line">        )</span><br><span class="line">        status = response[<span class="string">&#x27;status&#x27;</span>]</span><br><span class="line">        <span class="keyword">if</span> status == <span class="string">&#x27;Complete&#x27;</span>:</span><br><span class="line">            <span class="keyword">return</span> response[<span class="string">&#x27;results&#x27;</span>]</span><br><span class="line">        <span class="keyword">elif</span> status == <span class="string">&#x27;Failed&#x27;</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">&#x27;Query failed&#x27;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>Use the <code>run_insights_query()</code> function to execute a CloudWatch Logs Insights query and get the results:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">log_group_name = <span class="string">&#x27;your-log-group-name&#x27;</span></span><br><span class="line">query = <span class="string">&#x27;fields @timestamp, @message | sort @timestamp desc | limit 20&#x27;</span></span><br><span class="line">start_time = <span class="number">60</span> * <span class="number">60</span>  <span class="comment"># 1 hour ago</span></span><br><span class="line">end_time = <span class="number">0</span>  <span class="comment"># Now</span></span><br><span class="line"></span><br><span class="line">results = run_insights_query(log_group_name, query, start_time, end_time)</span><br><span class="line"><span class="built_in">print</span>(results)</span><br></pre></td></tr></table></figure><p>Replace <code>&#39;your-log-group-name&#39;</code> with the appropriate value for your Log Insight group, and adjust the <code>query</code>, <code>start_time</code>, and <code>end_time</code> variables as needed.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion:"></a>Conclusion:</h2><p>In this blog post, we demonstrated how to access and search CloudWatch Logs Insights from an AWS Lambda function using Python and the Boto3 library. This allows you to analyze and visualize your log data directly from your Lambda function, enabling you to build powerful serverless applications that can react to log data in real-time.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> lambda </tag>
            
            <tag> cloudwatch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Selecting the Optimal Probability Threshold for a Classification Model，ROC Curve Analysis and KS Score</title>
      <link href="2023/04/19/choose-thresold-for-classification-model-with-ROC-analysis/"/>
      <url>2023/04/19/choose-thresold-for-classification-model-with-ROC-analysis/</url>
      
        <content type="html"><![CDATA[<p>Title: Selecting the Optimal Probability Threshold for a Classification Model: ROC Curve Analysis and KS Score</p><p>In binary classification problems, a model predicts the probability of an instance belonging to the positive class. To make a final decision, we need to set a threshold on the predicted probability. Instances with probabilities above the threshold are classified as positive, while those below the threshold are classified as negative. Choosing the right threshold is crucial for the performance of the classification model. In this blog post, we will discuss two methods for selecting the optimal probability threshold: ROC curve analysis and the Kolmogorov-Smirnov (KS) score.</p><p>A Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between the true positive rate (sensitivity) and the false positive rate (1-specificity) for a classification model at various threshold settings. The area under the ROC curve (AUC) is a measure of the model’s performance. To choose a good threshold of probability value for a classification model using the ROC curve, follow these steps:</p><ol><li><p>Plot the ROC curve: First, plot the ROC curve for your classification model. The x-axis represents the false positive rate (1-specificity), and the y-axis represents the true positive rate (sensitivity).</p></li><li><p>Calculate the AUC: Calculate the area under the ROC curve (AUC). The AUC ranges from 0 to 1, with 1 indicating a perfect classifier and 0.5 indicating a random classifier. A higher AUC indicates better model performance.</p></li><li><p>Identify the optimal threshold: The optimal threshold is the point on the ROC curve that maximizes the true positive rate while minimizing the false positive rate. This point is often close to the top-left corner of the plot, where the curve bends. One common method to find the optimal threshold is to choose the point on the ROC curve that is closest to the point (0,1), which represents perfect classification. You can calculate the Euclidean distance between each point on the ROC curve and the point (0,1) and choose the threshold corresponding to the point with the smallest distance.</p></li></ol><p>Kolmogorov-Smirnov (KS) Score</p><p>The KS score is a measure of the separation between the cumulative distribution functions of the true positive and false positive rates. It is calculated as the maximum difference between the cumulative true positive rate (sensitivity) and the cumulative false positive rate (1-specificity) across all possible thresholds. The optimal threshold is the one that corresponds to the maximum KS score. In other words, the KS score helps to identify the threshold where the separation between the two distributions (positive and negative classes) is the largest.</p><p>Python Code to Find the Optimal Threshold</p><p>Here’s the Python code to find the optimal threshold using both methods, assuming you have already trained a classification model and obtained the predicted probabilities for the positive class:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br><span class="line"><span class="comment"># Assuming y_true is the true labels and y_pred_prob is the predicted probabilities for the positive class</span></span><br><span class="line">y_true = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">y_pred_prob = np.array([<span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.8</span>, <span class="number">0.2</span>, <span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.4</span>, <span class="number">0.7</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate the ROC curve</span></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 1: Optimal threshold using ROC curve (point 3)</span></span><br><span class="line">optimal_idx = np.argmin(np.sqrt(np.square(<span class="number">1</span>-tpr) + np.square(fpr)))</span><br><span class="line">optimal_threshold_roc = thresholds[optimal_idx]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimal threshold using ROC curve:&quot;</span>, optimal_threshold_roc)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Method 2: Optimal threshold using KS score</span></span><br><span class="line">ks_score = tpr - fpr</span><br><span class="line">optimal_idx_ks = np.argmax(ks_score)</span><br><span class="line">optimal_threshold_ks = thresholds[optimal_idx_ks]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Optimal threshold using KS score:&quot;</span>, optimal_threshold_ks)</span><br></pre></td></tr></table></figure><p>This code snippet calculates the optimal threshold using both the ROC curve method and the KS score method. Replace <code>y_true</code> and <code>y_pred_prob</code> with your actual true labels and predicted probabilities, respectively. The optimal thresholds will be printed at the end.</p><ol start="4"><li><p>Consider the cost of misclassification: Depending on the specific problem you are trying to solve, you may want to adjust the threshold based on the cost of false positives and false negatives. For example, if the cost of a false positive is much higher than the cost of a false negative, you may want to choose a higher threshold to minimize false positives, even if it means sacrificing some true positives.</p></li><li><p>Test the threshold: Once you have chosen a threshold, test it on a validation dataset to ensure that it generalizes well to new data. If the performance is not satisfactory, you may need to adjust the threshold or retrain the model with different parameters.</p></li><li><p>Apply the threshold: Finally, apply the chosen threshold to your classification model to make predictions. This will allow you to classify new instances based on their predicted probability of belonging to the positive class.</p></li></ol><p>Conclusion</p><p>Both ROC curve analysis and the KS score can be useful for choosing the optimal threshold for a classification model. Depending on the specific problem and the characteristics of the data, these methods may yield different results. It is essential to test the chosen threshold on a validation dataset to ensure that it generalizes well to new data. By selecting the appropriate threshold, you can improve the performance of your classification model and make more accurate predictions.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ROC </tag>
            
            <tag> KS score </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to install virtual environment with specific python versions</title>
      <link href="2023/04/17/how-to-install-virtual-env-with-specific-pyton-versions/"/>
      <url>2023/04/17/how-to-install-virtual-env-with-specific-pyton-versions/</url>
      
        <content type="html"><![CDATA[<p>When working on a Python project, it’s important to keep your dependencies isolated from other projects and from your system’s global Python installation. One way to achieve this is by using virtual environments. In this post, we’ll cover how to create and use virtual environments in Python.</p><h2 id="What-is-a-Virtual-Environment"><a href="#What-is-a-Virtual-Environment" class="headerlink" title="What is a Virtual Environment?"></a>What is a Virtual Environment?</h2><p>A virtual environment is a self-contained directory tree that contains a Python installation and any additional packages you install. When you activate a virtual environment, it modifies your shell’s PATH environment variable to use the Python binary and packages from the virtual environment, rather than the global Python installation.</p><p>Using virtual environments has several benefits:</p><ul><li>Isolation: Dependencies for one project won’t conflict with dependencies for another project.</li><li>Consistency: You can ensure that everyone working on the project is using the same versions of Python and packages.</li><li>Reproducibility: You can recreate the exact same environment on different machines.</li></ul><h2 id="Creating-a-Virtual-Environment"><a href="#Creating-a-Virtual-Environment" class="headerlink" title="Creating a Virtual Environment"></a>Creating a Virtual Environment</h2><p>First we discuss different tools to use:</p><ol><li>venv: This is a built-in module in Python 3.3 and later versions that allows you to create virtual environments in Python. It creates a new Python environment with its own site directories, which can be used to install and manage packages for specific projects. It’s simple, lightweight, and easy to use.</li><li>virtualenv: This is a popular third-party tool that allows you to create isolated Python environments. It works with both Python 2 and 3 and allows you to create virtual environments with different Python versions, which can be useful for testing your code across different Python versions.<br>Here we use <code>venv</code>.</li></ol><p>Creating a virtual environment is easy. First, open a terminal or command prompt and navigate to the directory where you want to create the virtual environment. Then, run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m venv myenv</span><br></pre></td></tr></table></figure><p>This will create a new virtual environment in a directory called myenv. You can replace myenv with any name you like.</p><p>If you need to create a virtual environment with a specific version of Python, you first need to install that version of Python on your system. You can use a package manager like pyenv to install multiple versions of Python on your system and switch between them easily. Once you have installed the desired version of Python, you can create a virtual environment with that version by specifying the full path to the Python interpreter in the python command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">/path/to/python3.10 -m venv myenv</span><br></pre></td></tr></table></figure><p>This will create a new virtual environment called myenv with the specified version of Python.</p><h2 id="Activating-a-Virtual-Environment"><a href="#Activating-a-Virtual-Environment" class="headerlink" title="Activating a Virtual Environment"></a>Activating a Virtual Environment</h2><p>Once you have created a virtual environment, you need to activate it before you can use it. To activate a virtual environment, run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source myenv/bin/activate</span><br></pre></td></tr></table></figure><p>This will modify your shell’s PATH environment variable to use the Python binary and packages from the virtual environment. You should see the name of your virtual environment in your command prompt or terminal.</p><p>Once you have activated a virtual environment, any packages you install using pip will be installed into that environment, rather than the global Python installation.</p><h2 id="Deactivating-a-Virtual-Environment"><a href="#Deactivating-a-Virtual-Environment" class="headerlink" title="Deactivating a Virtual Environment"></a>Deactivating a Virtual Environment</h2><p>When you’re done working in a virtual environment, you can deactivate it by running the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure><p>This will restore your shell’s PATH environment variable to its previous state, so that you’re using the global Python installation again.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> venv </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to groupby one column and join another column by comma in Pandas dataframe</title>
      <link href="2023/04/16/dataframe-groupby-and-join-rows-by-comma/"/>
      <url>2023/04/16/dataframe-groupby-and-join-rows-by-comma/</url>
      
        <content type="html"><![CDATA[<p>In this blog, we will explore how to use the Pandas groupby method to group a DataFrame by one column and then join another column by comma.</p><p>Let’s start by creating an example DataFrame:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;</span><br><span class="line">    &#x27;Name&#x27;: [&#x27;Alice&#x27;, &#x27;Bob&#x27;, &#x27;Charlie&#x27;, &#x27;Alice&#x27;, &#x27;Bob&#x27;, &#x27;Bob&#x27;],</span><br><span class="line">    &#x27;Fruit&#x27;: [&#x27;Apple&#x27;, &#x27;Orange&#x27;, &#x27;Apple&#x27;, &#x27;Orange&#x27;, &#x27;Banana&#x27;, &#x27;Orange&#x27;]</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>Our example DataFrame has two columns: Name and Fruit. We want to group the DataFrame by the Name column and then join the values in the Fruit column by comma for each group. To accomplish this, we can use the groupby method with an anonymous lambda function.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">grouped = df.groupby(&#x27;Name&#x27;).apply(lambda x: &#x27;,&#x27;.join(x[&#x27;Fruit&#x27;]))</span><br></pre></td></tr></table></figure><p>In this code, we use the groupby method to group the DataFrame by the Name column. We then use the apply method to apply a lambda function to each group of rows in the DataFrame. The lambda function takes each group of rows, selects the Fruit column using x[‘Fruit’], and joins the values in that column with a comma using the ‘,’.join() method. The result is a new Series object grouped that contains one row for each unique value in the Name column. The values in each row are the joined values of the Fruit column for the corresponding group of rows in the original DataFrame.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(grouped)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Name</span><br><span class="line">Alice           Apple,Orange</span><br><span class="line">Bob       Orange,Banana,Orange</span><br><span class="line">Charlie                Apple</span><br><span class="line">dtype: object</span><br></pre></td></tr></table></figure><p>As we can see from the output, the groupby method has grouped the DataFrame by the unique values in the Name column and joined the corresponding values in the Fruit column by comma.</p><p>If you want to get two columns in the resulting DataFrame instead of a single column with joined values, you can add an additional step of calling the reset_index() method.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">grouped = df.groupby(&#x27;Name&#x27;)[&#x27;Fruit&#x27;].apply(lambda x: &#x27;,&#x27;.join(x)).reset_index()</span><br></pre></td></tr></table></figure><p>In this updated code, we have added the [‘Fruit’] parameter inside the groupby method to specify that we are only interested in grouping by the Name column and joining the Fruit column. We then use the reset_index() method to convert the resulting Series object back into a DataFrame with two columns: Name and Fruit.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(grouped)</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">      Name                 Fruit</span><br><span class="line">0    Alice           Apple,Orange</span><br><span class="line">1      Bob  Orange,Banana,Orange</span><br><span class="line">2  Charlie                 Apple</span><br></pre></td></tr></table></figure><p>As we can see from the output, the resulting DataFrame now has two columns: Name and Fruit, with the joined values of the Fruit column grouped by the corresponding values in the Name column.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> dataframe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Creativity, the Key to Thriving in the Age of AutoGPT</title>
      <link href="2023/04/15/creativity-is-key-for-human-in-the-chatgpt-era/"/>
      <url>2023/04/15/creativity-is-key-for-human-in-the-chatgpt-era/</url>
      
        <content type="html"><![CDATA[<p>In an era where AutoGPT and similar AI technologies are becoming increasingly prevalent, many people may worry about the potential loss of jobs and the increasing automation of various tasks. However, while it’s true that AI can replace many tedious and repetitive jobs, it’s important to remember that there’s one crucial aspect that AI cannot truly replicate: human creativity.</p><p>Creativity is an essential part of the human experience. It allows us to come up with innovative ideas, solve complex problems, and express ourselves in unique ways. And as AI continues to advance and take over more mundane tasks, the importance of creativity only grows. In fact, creativity might just be the key to thriving in the age of AutoGPT and beyond.</p><p>Before diving into the importance of creativity in the age of AutoGPT, let’s first explore how AutoGPT functions and why scraping the web is a crucial step in its process.</p><h2 id="How-AutoGPT-Works"><a href="#How-AutoGPT-Works" class="headerlink" title="How AutoGPT Works"></a>How AutoGPT Works</h2><p>AutoGPT combines the capabilities of GPT-3.5 and GPT-4 to create a powerful AI model that can autonomously develop, debug, and improve upon projects based on specific goals and descriptions provided by users. One of its key actions is scraping the web for the latest and most relevant information to ensure its knowledge is up-to-date and accurate.</p><p>AutoGPT requires: </p><ul><li>AI Name</li><li>AI Role</li><li>Up to 5 goals</li></ul><p>For example: </p><ul><li>Name: Chef-GPT</li><li>Role: An AI designed to find an ordinary recipe on the web, and turn it into a Michelin Star quality recipe. </li><li>Goal 1: Find a simple recipe online</li><li>Goal 2: Turn this simple recipe into a Michelin Star quality version.<br>Once AutoGPT has met the description and goals, it will start to do its own thing until the project is at a satisfactory level. </li></ul><p>As AutoGPT works on a project, it follows a feedback loop that includes planning, criticizing, acting, and reading feedback. This allows the AI model to continuously self-improve and adapt to the user’s needs. During this process, the AI model browses the web, reads and writes different files, and reviews its own prompts to ensure the project aligns with the user’s vision.</p><h2 id="limitation-of-the-big-model"><a href="#limitation-of-the-big-model" class="headerlink" title="limitation of the big model"></a>limitation of the big model</h2><p>Wile understanding how the autoGPT works, we might be thinking about this:  Although autogpt or chatgpt possess tremendous knowledge, their true power lies elsewhere. While it’s true that these models have more knowledge than any human could possibly accumulate, it’s important to note that the knowledge they possess is limited by their current computing power. This means that the information they contain may be relatively old and cannot be updated in real-time. Additionally, the quality of the information they provide cannot always be guaranteed, as it’s based on what people are talking about on the web.</p><p>Therefore, while GPT models can be fun to talk to and can provide basic information, it’s crucial to exercise caution when relying on their knowledge. For complex or unknown topics, it’s important to verify the accuracy of the information by checking the source, the author, and the webpage’s authenticity.</p><p>However, the creation of tools like autoGPT can help ensure that the knowledge contained within them is relatively current and fresh. This, coupled with the logic power of these models, can make them valuable assistants for automating certain tasks.</p><p>But despite the immense knowledge these models possess, it’s still up to experts to curate and provide high-quality information for them to learn from. Creativity and novel knowledge remain essential for humans and models alike to succeed.</p><h2 id="The-Significance-of-Creativity-in-AutoGPT-and-Beyond"><a href="#The-Significance-of-Creativity-in-AutoGPT-and-Beyond" class="headerlink" title="The Significance of Creativity in AutoGPT and Beyond"></a>The Significance of Creativity in AutoGPT and Beyond</h2><p>Now that we have a better understanding of how AutoGPT works, it becomes clear that human creativity plays an essential role in the AI model’s success. Since AutoGPT relies on scraping the web for information, the quality and originality of content available online directly impact its ability to provide accurate and valuable assistance to users.</p><p>By nurturing and developing our creativity, we can ensure that AI models like AutoGPT continue to have access to high-quality, up-to-date information. In turn, this allows us to better leverage the power of AI to assist us in our daily lives and professional pursuits.</p><p>Furthermore, as AI takes over more routine tasks, the demand for creative and innovative thinkers will only increase. Creative problem-solving, design, and artistic expression will become even more valuable skills in the workforce, as these will be the areas where humans continue to excel over AI.</p><h2 id="Fostering-Creativity-in-the-Age-of-AutoGPT"><a href="#Fostering-Creativity-in-the-Age-of-AutoGPT" class="headerlink" title="Fostering Creativity in the Age of AutoGPT"></a>Fostering Creativity in the Age of AutoGPT</h2><p>So how can we nurture and develop our creativity in this rapidly evolving technological landscape? Here are a few suggestions:</p><ol><li><p>Embrace lifelong learning: Continuously expand your knowledge and skills by seeking out new experiences and learning opportunities. This not only keeps your mind sharp but also helps you stay ahead of the curve in an ever-changing world.</p></li><li><p>Cultivate curiosity: Curiosity is the driving force behind creativity. Foster a curious mindset by asking questions, exploring new ideas, and challenging yourself to think outside the box.</p></li><li><p>Practice creative problem-solving: Train your brain to think creatively by regularly engaging in activities that require you to come up with innovative solutions to complex problems.</p></li><li><p>Collaborate with others: Working with others can inspire new ideas and perspectives, helping you to think more creatively and come up with novel approaches to challenges.</p></li><li><p>Embrace failure: Don’t be afraid to make mistakes or try new things. Failure is often a necessary step on the path to creative success, as it allows us to learn and grow.</p></li></ol><h2 id="In-Conclusion"><a href="#In-Conclusion" class="headerlink" title="In Conclusion"></a>In Conclusion</h2><p>While the rise of AutoGPT and similar AI technologies may be cause for concern in some respects, it’s important to remember that human creativity remains a vital and irreplaceable asset. By nurturing and developing our creative abilities, we can continue to thrive in a world where AI plays an increasingly prominent role. So embrace your creativity, and let it guide you towards success in the age of AutoGPT and beyond.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> gpt </tag>
            
            <tag> autogpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficiently Reverting a Single File to a Specific Commit in Git and GitHub</title>
      <link href="2023/04/13/how-to-revert-and-restore-file-in-github-repo/"/>
      <url>2023/04/13/how-to-revert-and-restore-file-in-github-repo/</url>
      
        <content type="html"><![CDATA[<p>Git and GitHub serve the purpose of storing your old code, allowing you to roll back in case of issues and safely restore previous and accurate code.</p><p>When collaborating with other developers, it also becomes essential to know how to revert a single file to a specific commit. This is because, while working on a feature, you might need to modify unrelated files to test it. Manually changing each line of code in those files and creating a new commit can result in a cluttered commit history. Reverting the file is a much cleaner approach.</p><h2 id="Finding-the-Commit-ID"><a href="#Finding-the-Commit-ID" class="headerlink" title="Finding the Commit ID"></a>Finding the Commit ID</h2><p>First, navigate to the shared repository on GitHub and locate the file you want to revert. Just above the file, you should see a 7-digit commit ID and a date, indicating the most recent commit in which the file was modified. Note down or copy this commit ID, such as the one shown below:<br><img src="/content/images/2023-04-13-01.png"></p><p>Now how to check the actual file for different commits? First click the “history” and expand, you will see all the commits related to this file:<br><img src="/content/images/2023-04-13-02.png"></p><p>Now choose any commit ID you want to revert, then click the ‘…’ button, and choose the “view file”, and check the actual file:<br><img src="/content/images/2023-04-13-03.png"></p><h2 id="Locating-the-File-Path"><a href="#Locating-the-File-Path" class="headerlink" title="Locating the File Path"></a>Locating the File Path</h2><p>Next, you need the path to the file from the working directory. This can be found on the same GitHub screen where you discovered the commit ID for the file. Make sure to only copy the path without the working directory name, as it will be the directory you’re in when using this file path.<br><img src="/content/images/2023-04-13-04.png"></p><h2 id="Reverting-the-File"><a href="#Reverting-the-File" class="headerlink" title="Reverting the File"></a>Reverting the File</h2><p>With the terminal open and the working directory set, use the git checkout command to revert the file. The format of the git command should look like this:</p><p><code>git checkout [commit ID] -- path/to/file</code><br>or this specific case, it will be：<br><code>git checkout 4a023a6 -- gpt4-novel/readme</code></p><h2 id="Committing-the-Change"><a href="#Committing-the-Change" class="headerlink" title="Committing the Change"></a>Committing the Change</h2><p>After reverting the file, it’s necessary to commit the change, which, in this case, is a revert of a single file. This can be done using the standard commit command:</p><p><code>git commit -m &#39;commit message&#39;</code></p><p>Finally, push the commit to the remote repository so that the GitHub version of your branch matches your local version.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Upload an Existing Folder to GitHub</title>
      <link href="2023/04/13/upload-existing-folder-to-github/"/>
      <url>2023/04/13/upload-existing-folder-to-github/</url>
      
        <content type="html"><![CDATA[<p>We may already have an existing directory where we were devloping code, then we want to upload it to Github. In this tutorial, we’ll walk you through the steps to get your folder uploaded to GitHub.</p><h2 id="Step-1-Create-a-New-Repository-on-GitHub"><a href="#Step-1-Create-a-New-Repository-on-GitHub" class="headerlink" title="Step 1: Create a New Repository on GitHub"></a>Step 1: Create a New Repository on GitHub</h2><p>The first step is to create a new repository on GitHub where you can store your code. To do this, log in to your GitHub account and click on the “New Repository” button on the main dashboard.</p><p>Next, give your repository a name, description (optional), and select any other desired settings for your repository. Make sure to select the “Initialize this repository with a README” option to create an initial README.md file in your repository.</p><p>Once you’ve entered all the required details, click on the “Create Repository” button to create your new repository.</p><h2 id="Step-2-Initialize-a-Local-Git-Repository"><a href="#Step-2-Initialize-a-Local-Git-Repository" class="headerlink" title="Step 2: Initialize a Local Git Repository"></a>Step 2: Initialize a Local Git Repository</h2><p>Now that you have a repository set up on GitHub, it’s time to create a local Git repository on your computer. To do this, open a terminal window on your computer and navigate to the folder that you want to upload to GitHub.</p><p>Once you’re in the folder, run the following command to initialize a new Git repository:<br><code>git init</code></p><p>This command will create a new .git folder in your folder, which will store all the necessary Git metadata and tracking information.</p><h2 id="Step-3-Add-and-Commit-Your-Files"><a href="#Step-3-Add-and-Commit-Your-Files" class="headerlink" title="Step 3: Add and Commit Your Files"></a>Step 3: Add and Commit Your Files</h2><p>With your local Git repository set up, you can now add and commit your files to it. To add all the files in your folder to the repository, run the following command:</p><p><code>git add .</code><br>This command will stage all the files in your folder for committing. To commit the changes, run the following command:</p><p><code>git commit -m &quot;Initial commit&quot;</code><br>This command will commit your changes to the local repository with a message “Initial commit”.</p><h2 id="Step-4-Connect-Your-Local-Repository-to-the-Remote-Repository-on-GitHub"><a href="#Step-4-Connect-Your-Local-Repository-to-the-Remote-Repository-on-GitHub" class="headerlink" title="Step 4: Connect Your Local Repository to the Remote Repository on GitHub"></a>Step 4: Connect Your Local Repository to the Remote Repository on GitHub</h2><p>Now that you have a local Git repository set up and have committed your changes, it’s time to connect your local repository to the remote repository on GitHub. To do this, you’ll need to get the remote repository URL from GitHub.</p><p>To get the remote repository URL, go to the repository you created on GitHub and click on the “Clone or download” button.</p><p>Next, copy the HTTPS or SSH URL for your repository.</p><p>With the remote repository URL in hand, you can now connect your local repository to the remote repository on GitHub. To do this, run the following command:<br><code>git remote add origin &lt;remote_repository_url&gt;</code><br>This command will add a new remote named “origin” to your local repository and connect it to the remote repository on GitHub.</p><h2 id="Step-5-Push-Your-Changes-to-GitHub"><a href="#Step-5-Push-Your-Changes-to-GitHub" class="headerlink" title="Step 5: Push Your Changes to GitHub"></a>Step 5: Push Your Changes to GitHub</h2><p>With your local and remote repositories connected, you can now push your changes to GitHub. However, before doing so, it’s important to make sure that you’re pushing to the correct branch on the remote repository.</p><p>By default, GitHub creates a “main” branch in your repository. However, some older repositories may still use the “master” branch. To check which branch your remote repository is using, run the following command:<br><code>git branch -r</code><br>This command will list all the remote branches in your repository. The branch with an asterisk next to it is the currently checked out branch.</p><p>If the remote repository is using the “main” branch, you can push your changes to GitHub by running the following command:<br><code>git push -u origin main</code><br>This command will push your changes to the “main” branch on the remote repository and set the upstream branch to track the remote branch with the same name.</p><p>If the remote repository is using the “master” branch, replace “main” with “master” in the above command.</p><p>Once the command has finished executing, your code will be available on GitHub for others to view and collaborate on.</p><p>One thing to notice is that:<br>When you run <code>git push -u origin main</code>, Git will prompt you to confirm that you want to set the upstream branch and provide you with a suggested command to do so if you choose to proceed.</p><p>The suggested command will look something like this:<br><code>git push --set-upstream origin main</code><br>This command tells Git to push your changes to the “main” branch on the “origin” repository and set the upstream branch to track the remote branch with the same name.</p><p>By setting the upstream branch, you can use shorthand Git commands like git push and git pull without having to specify the remote branch name each time.</p><p>If you choose to proceed with the suggested command, Git will push your changes and set the upstream branch for you.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Efficiently Chunk Text for OpenAI API Calls</title>
      <link href="2023/04/11/chunk-text-for-openai-api-calls/"/>
      <url>2023/04/11/chunk-text-for-openai-api-calls/</url>
      
        <content type="html"><![CDATA[<p>When working with the OpenAI API, it’s essential to manage the text input efficiently, especially when dealing with large amounts of text. The API has a token limit, which means you need to break your text into smaller chunks before making API calls. In this blog post, we will discuss two methods to wrap the text into chunks and ensure you stay within the token limits.</p><h2 id="Method-1-Using-the-textwrap-Library"><a href="#Method-1-Using-the-textwrap-Library" class="headerlink" title="Method 1: Using the textwrap Library"></a>Method 1: Using the textwrap Library</h2><p>The first method involves using the <code>textwrap</code> library, which is a built-in Python library that provides a simple way to wrap text into lines of a specified width. This method is a rough estimate, as it chunks texts by character size rather than actual token size. However, it can still be useful for quick and easy text wrapping.</p><p>Here’s how to use the <code>textwrap</code> library to wrap your text into chunks:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> textwrap</span><br><span class="line"></span><br><span class="line"><span class="comment"># Wraps the single paragraph in text (a string) so every line is at most width characters long. Returns a list of output lines, without final newlines.</span></span><br><span class="line"></span><br><span class="line">article = <span class="string">&quot;how are you .....&quot;</span></span><br><span class="line">chunks = textwrap.wrap(article, <span class="number">10</span>, replace_whitespace=<span class="literal">False</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br><span class="line">    <span class="comment"># call openai api</span></span><br></pre></td></tr></table></figure><p>results:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">how are</span><br><span class="line">you .....</span><br></pre></td></tr></table></figure><h2 id="Method-2-Using-a-Custom-Class-with-Tiktoken"><a href="#Method-2-Using-a-Custom-Class-with-Tiktoken" class="headerlink" title="Method 2: Using a Custom Class with Tiktoken"></a>Method 2: Using a Custom Class with Tiktoken</h2><p>The second method is more precise, as it chunks texts by actual token size using the <code>tiktoken</code> library. This library allows you to count tokens in a text string without making an API call, ensuring that you stay within the token limits.<br>Packages to install</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install tiktoken</span><br><span class="line">pip install langchain</span><br></pre></td></tr></table></figure><p>Here’s how to create a custom class that uses <code>tiktoken</code> to chunk your text:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">from</span> langchain.text_splitter <span class="keyword">import</span> RecursiveCharacterTextSplitter</span><br><span class="line"></span><br><span class="line">encoding = tiktoken.encoding_for_model(<span class="string">&quot;text-davinci-003&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClass</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">count_token</span>(<span class="params">self, text</span>):</span></span><br><span class="line">        num_token = <span class="built_in">len</span>(encoding.encode(text))</span><br><span class="line">        <span class="keyword">return</span> num_token</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_article</span>(<span class="params">self, article</span>):</span></span><br><span class="line">        splitter = RecursiveCharacterTextSplitter(chunk_size=<span class="number">10</span>, length_function=self.count_token,</span><br><span class="line">                                                  separators=[<span class="string">&#x27;\n\n&#x27;</span>, <span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27; &#x27;</span>, <span class="string">&#x27;&#x27;</span>],</span><br><span class="line">                                                  chunk_overlap=<span class="number">2</span>)</span><br><span class="line">        chunks = splitter.split_text(article)</span><br><span class="line">        <span class="keyword">return</span> chunks</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">article = <span class="string">&#x27;&#x27;&#x27;how are you doing? great, see you soon.&#x27;&#x27;&#x27;</span></span><br><span class="line">my_instance = MyClass()</span><br><span class="line"></span><br><span class="line">chunks = my_instance.process_article(article)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> chunks:</span><br><span class="line">    <span class="built_in">print</span>(chunk)</span><br><span class="line">    <span class="comment"># call openai API</span></span><br></pre></td></tr></table></figure><p>results:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">how are you doing?</span><br><span class="line">doing? great, see you</span><br><span class="line">you soon.</span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In summary, when working with the OpenAI API, it’s crucial to manage your text input efficiently to stay within the token limits. The two methods discussed in this blog post provide different ways to wrap your text into chunks, with the first method using the <code>textwrap</code> library for a rough estimate and the second method using a custom class with <code>tiktoken</code> for a more precise token count. Choose the method that best suits your needs and ensure a smooth experience when making API calls.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> GPT </tag>
            
            <tag> chunk text </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to count tokens precisely when using openAI GPT models</title>
      <link href="2023/04/10/count-tokens-in-gpt-models-accurately/"/>
      <url>2023/04/10/count-tokens-in-gpt-models-accurately/</url>
      
        <content type="html"><![CDATA[<p>If you are working with GPT models, it is essential to keep track of the number of tokens in your input text. OpenAI’s GPT models have a token limit, and exceeding this limit will result in a token limit error. To avoid this, you need to precisely count the number of tokens in your input text before sending it to OpenAI.</p><p>In this blog, we will show you how to count tokens accurately using the tiktoken Python package.</p><p>To begin, you will need to install the tiktoken package by running the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install --upgrade tiktoken</span><br></pre></td></tr></table></figure><p>Notice that Requiretment:  Python versoin &gt;=3.8</p><p>Once you have installed the package, you can use the following code to count the number of tokens in your input text:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import tiktoken</span><br><span class="line"></span><br><span class="line">#Use tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.</span><br><span class="line"># for gpt4 just swtich &quot;gpt-3.5-turbo&quot; with &quot;gpt-4&quot;</span><br><span class="line">encoding = tiktoken.encoding_for_model(&quot;gpt-3.5-turbo&quot;)</span><br><span class="line"></span><br><span class="line">text = ”how are you doing&quot;</span><br><span class="line"></span><br><span class="line"># calculuate the number of tokens </span><br><span class="line">num_token = len(encoding.encode(text))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this code, we first load the encoding for the GPT-3.5-turbo model using the tiktoken.encoding_for_model() method. This method automatically loads the correct encoding for a given model name.</p><p>Next, we define our input text and calculate the number of tokens using the len() function on the encoded text.</p><p>Finally, we print the number of tokens to the console.</p><p>By using the tiktoken package to count the number of tokens in your input text, you can avoid token limit errors when sending requests to OpenAI’s GPT models.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> token limit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding Server-side and Client-side Timeouts and How to Set Them Up</title>
      <link href="2023/04/06/how-to-setup-server-side-and-client-side-timeout/"/>
      <url>2023/04/06/how-to-setup-server-side-and-client-side-timeout/</url>
      
        <content type="html"><![CDATA[<p>When working with web applications, it’s essential to understand the concept of timeouts and how they can impact the performance and user experience of your application. Timeouts can occur on both the server-side and client-side, and it’s crucial to know the differences between them and how to set them up correctly. In this blog post, we will discuss server-side and client-side timeouts, their differences, and how to set them up using FastAPI, Gunicorn, and the Requests library.</p><h2 id="Server-side-Timeout"><a href="#Server-side-Timeout" class="headerlink" title="Server-side Timeout"></a>Server-side Timeout</h2><p>A server-side timeout occurs when the server takes too long to process a request or send a response. This can happen due to various reasons, such as slow processing, high server load, or network latency. When a server-side timeout occurs, the server may terminate the connection, resulting in an error or incomplete response sent to the client.</p><p>To manage server-side timeouts, you can use a reverse proxy like Gunicorn with your FastAPI application. Gunicorn allows you to set a worker timeout, which is the maximum time a worker process can take to complete a request. If a worker does not complete the request within the specified timeout, it will be killed, and a new worker will be spawned.</p><p>Here’s an example of how to use Gunicorn with Uvicorn workers to set a timeout:</p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gunicorn -w 4 -k uvicorn.workers.UvicornWorker -t 60 main:app</span><br></pre></td></tr></table></figure><p>In this command:</p><ul><li><code>-w 4</code>: Sets the number of worker processes to 4.</li><li><code>-k uvicorn.workers.UvicornWorker</code>: Specifies the worker class to be UvicornWorker.</li><li><code>-t 60</code>: Sets the worker timeout to 60 seconds.</li><li><code>main:app</code>: Points to your FastAPI application instance.</li></ul><h2 id="Client-side-Timeout"><a href="#Client-side-Timeout" class="headerlink" title="Client-side Timeout"></a>Client-side Timeout</h2><p>A client-side timeout occurs when the client (e.g., a web browser or an API client) takes too long to receive a response from the server. This can happen due to various reasons, such as slow server processing, network latency, or client-side processing delays. When a client-side timeout occurs, the client may terminate the connection, resulting in an error or incomplete response.</p><p>To manage client-side timeouts, you can use an HTTP client library that allows you to set timeout values, such as the Requests library in Python.</p><p>Here’s an example of how to set a timeout when making a request using the Requests library:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&quot;http://your-fastapi-app.com/endpoint&quot;</span></span><br><span class="line">timeout = <span class="number">5.0</span>  <span class="comment"># Timeout in seconds</span></span><br><span class="line"></span><br><span class="line">response = requests.get(url, timeout=timeout)</span><br></pre></td></tr></table></figure><p>In this example, the request will time out if it takes longer than 5 seconds to complete.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Understanding the differences between server-side and client-side timeouts is crucial for building reliable and performant web applications. By using tools like Gunicorn and the Requests library, you can effectively manage timeouts and ensure a better user experience for your FastAPI application. Remember to monitor your application’s performance and adjust timeout settings as needed to find the optimal balance between responsiveness and resource usage.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> timeout </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Comparing Python&#39;s HTTP Libraries HTTPX vs Requests</title>
      <link href="2023/04/06/httpx-vs-requsts-in-pyton/"/>
      <url>2023/04/06/httpx-vs-requsts-in-pyton/</url>
      
        <content type="html"><![CDATA[<p>When working with Python, you’ll often need to make HTTP requests to interact with APIs, download files, or scrape web content. Two popular libraries for making HTTP requests in Python are <code>httpx</code> and <code>requests</code>. In this blog post, we’ll compare these two libraries and discuss their key differences to help you decide which one is right for your project.</p><h2 id="1-Async-Support"><a href="#1-Async-Support" class="headerlink" title="1. Async Support"></a>1. Async Support</h2><p>One of the main advantages of <code>httpx</code> over <code>requests</code> is its built-in support for asynchronous programming using Python’s <code>asyncio</code> library. This allows you to make non-blocking HTTP requests, which can lead to significant performance improvements, especially when dealing with multiple requests concurrently.</p><p><code>requests</code>, on the other hand, does not have native async support. If you need asynchronous behavior with <code>requests</code>, you’ll need to use a separate library or workaround, which can be more cumbersome.</p><h2 id="2-HTTP-2-Support"><a href="#2-HTTP-2-Support" class="headerlink" title="2. HTTP/2 Support"></a>2. HTTP/2 Support</h2><p><code>httpx</code> supports HTTP/2, a newer version of the HTTP protocol that can provide performance improvements and more efficient connections when interacting with servers that support it. In contrast, <code>requests</code> only supports HTTP/1.1. If you’re working with an API or server that supports HTTP/2, using <code>httpx</code> can give you a performance edge.</p><h2 id="3-Connection-Pooling-and-Keep-Alive"><a href="#3-Connection-Pooling-and-Keep-Alive" class="headerlink" title="3. Connection Pooling and Keep-Alive"></a>3. Connection Pooling and Keep-Alive</h2><p>Both <code>httpx</code> and <code>requests</code> support connection pooling and keep-alive, which can help reduce the overhead of establishing new connections for each request. However, <code>httpx</code> has a more advanced connection management system that can handle multiple concurrent connections better. This can be particularly useful when working with async requests or when making many requests in a short period.</p><h2 id="4-Request-and-Response-Streaming"><a href="#4-Request-and-Response-Streaming" class="headerlink" title="4. Request and Response Streaming"></a>4. Request and Response Streaming</h2><p><code>httpx</code> supports request and response streaming, allowing you to work with large files or streaming APIs more efficiently. While <code>requests</code> also supports streaming, its implementation is more limited and may not be suitable for all use cases.</p><h2 id="5-API-Similarity"><a href="#5-API-Similarity" class="headerlink" title="5. API Similarity"></a>5. API Similarity</h2><p><code>httpx</code> has a very similar API to <code>requests</code>, making it relatively easy to switch between the two libraries. However, there may be some minor differences in behavior or function signatures, so it’s essential to thoroughly test your code when migrating from one library to the other.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In summary, if you need async support, HTTP/2, or more advanced connection management, <code>httpx</code> is the better choice for your Python project. If you don’t need these features and are already familiar with <code>requests</code>, it may be more convenient to stick with <code>requests</code>. Ultimately, the choice between these two libraries will depend on your specific use case and requirements.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> requests </tag>
            
            <tag> httpx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pip list vs pip freeze, and other ways to check installed packages in Python environment</title>
      <link href="2023/04/06/ways-to-check-installed-packages-in-python/"/>
      <url>2023/04/06/ways-to-check-installed-packages-in-python/</url>
      
        <content type="html"><![CDATA[<p>n this blog post, we’ll explore different ways to check the installed packages in a Python environment using various commands and tools.</p><h2 id="Using-pip-list"><a href="#Using-pip-list" class="headerlink" title="Using pip list"></a>Using pip list</h2><p>The pip list command is the most commonly used command to check the installed packages in a Python environment. It displays all the packages installed in the environment along with their version numbers in a tabular format.</p><p>To use pip list, open a command prompt or terminal and type:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip list</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will display a list of all the installed packages in the environment.</p><p>The output of pip list includes additional information such as whether the package is installed globally or locally, and whether it is a dependency of any other package.</p><h2 id="Using-pip-freeze"><a href="#Using-pip-freeze" class="headerlink" title="Using pip freeze"></a>Using pip freeze</h2><p>The pip freeze command is another useful command to check the installed packages in a Python environment. It lists all the installed packages along with their version numbers, but it displays them in a different format compared to pip list.</p><p>The output of pip freeze is useful for replicating the environment because it lists the packages in a format that can be directly used in a requirements file.</p><p>To use pip freeze, open a command prompt or terminal and type:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip freeze</span><br></pre></td></tr></table></figure><p>This will display a list of all the installed packages in the environment in the format used for requirements files.</p><p>However, it’s important to note that pip freeze only lists packages that were installed using pip. It does not include packages that were installed using other package managers like conda, easy_install, or manually installed packages.</p><p>To install packages from a requirements.txt file using pip, you can use the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>This command installs all the packages listed in the requirements.txt file. The -r option specifies that the packages should be installed from a requirements file.</p><p>The requirements.txt file should list all the required packages along with their version numbers, separated by new lines. Here’s an example requirements.txt file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">numpy==1.19.3</span><br><span class="line">pandas==1.1.5</span><br><span class="line">matplotlib==3.3.3</span><br></pre></td></tr></table></figure><p>This file lists the numpy, pandas, and matplotlib packages with their respective version numbers.</p><p>To install the packages listed in the requirements.txt file, save the file to your working directory and run the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>This will install all the packages listed in the requirements.txt file.</p><h2 id="Using-conda-list"><a href="#Using-conda-list" class="headerlink" title="Using conda list"></a>Using conda list</h2><p>If you’re working with Python using the conda package manager, you can use the conda list command to check the installed packages.</p><p>To use conda list, open a command prompt or terminal and type:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure><p>This will display a list of all the installed packages in the conda environment.</p><h2 id="Using-easy-install-–list"><a href="#Using-easy-install-–list" class="headerlink" title="Using easy_install –list"></a>Using easy_install –list</h2><p>If you’ve installed packages using easy_install, you can use the easy_install –list command to check the installed packages.</p><p>To use easy_install –list, open a command prompt or terminal and type:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">easy_install --list</span><br></pre></td></tr></table></figure><p>This will display a list of all the installed packages using easy_install.</p><h2 id="Manually-installed-packages"><a href="#Manually-installed-packages" class="headerlink" title="Manually installed packages"></a>Manually installed packages</h2><p>If you’ve installed packages manually, i.e., not using any package manager, you can use the package manager specific to your operating system to check the installed packages.</p><p>For example, on a Linux system, you can use the dpkg command to list all the manually installed packages:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dpkg --get-selections | grep -v deinstall</span><br></pre></td></tr></table></figure><p>On a macOS system, you can use the brew command to list all the manually installed packages:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew list</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The victory of ChatGPT is the victory of probability theory and the victory of Bayes Theorem</title>
      <link href="2023/04/06/chatgpt-and-probablity-theory-and-bayes-theorem/"/>
      <url>2023/04/06/chatgpt-and-probablity-theory-and-bayes-theorem/</url>
      
        <content type="html"><![CDATA[<p>ChatGPT, which came out at the end of 2022, shocked the Internet. It can’t help but remind people of AlphaGo in early 2016, challenging the story of Li Shishi, the top human Go master. In a popular science book on probability published in 2017 [1] , I gave a brief description of the state of artificial intelligence at that time. It was the second revolution of AI, and deep machine learning and natural language processing (NLP) had just started. Unexpectedly, just a few years later, the third wave of AI came rolling in, which basically solved the problems of understanding and generating natural language. With the release of ChatGPT as a milestone, it opened up a new era of natural human-computer communication.</p><p>The idea of ​​artificial intelligence (AI) has been around for a long time. British mathematician Alan Turing, not only the father of computers, also designed the famous Turing test, which opened the door to artificial intelligence. Today, the application of artificial intelligence has penetrated into our daily life. Its successful rise stems from the rapid development of computers, the rise of cloud computing, the advent of the era of big data, and so on. Among them, the mathematical basis related to big data is mainly probability theory. Therefore, this article will talk about an aspect of ChatGPT related to probability, and more specifically, it is related to a name hundreds of years ago: Bayesian.</p><h2 id="Probability-Theory-and-Bayesian"><a href="#Probability-Theory-and-Bayesian" class="headerlink" title="Probability Theory and Bayesian"></a>Probability Theory and Bayesian</h2><p>Regarding probability theory, Laplace (1749-1827), known as the French Newton, once said:</p><p>“This science derived from the gambling machine will surely become the most important part of human knowledge. Most of the problems in life will be just a matter of probability.”</p><p>Today’s civilized society more than two hundred years later has confirmed Laplace’s prophecy. This world is full of uncertainties, probabilities everywhere, and everything is random. There is no need for an abstract definition. The basic intuitive concepts of probability theory have already penetrated into people’s work and life. From the small lottery tickets that everyone can buy, to the stars and the universe, to the complexity of computers and artificial intelligence, they are all closely related to probability.</p><p>So, who is Bayesian?</p><p>Thomas Bayes (Thomas Bayes, 1701-1761) was an English mathematician and statistician in the 18th century. He used to be a priest. However, he was “unknown during his lifetime, but worshiped by everyone after his death”, and he became “popular” in the contemporary science and technology circle. The reason is attributed to the famous Bayes theorem named after him. This theorem has not only contributed to the development of the Bayesian school in history, but is now widely used in machine learning, which is closely related to artificial intelligence [2] .</p><p>What did Bayes do? Back then, he studied the probability problem of a “white ball and black ball”. Probability problems can be calculated forward and backward. For example, there are 10 balls in a box, black and white. If we know that out of 10 balls, 5 are white and 5 are black, then, if I ask you, what is the probability that a ball is randomly drawn from among them to be black? The question is not difficult to answer, of course it is 50%! What if 10 balls are 6 white and 4 black? The probability of drawing a ball to be black should be 40%. Consider a more complicated situation: If 2 white and 8 black out of 10 balls, now randomly pick 2 balls, what is the probability of getting 1 black and 1 white? There are 10*9=90 possibilities to get 2 out of 10 balls. There are 16 cases of 1 black and 1 white. The probability is 16/90, which is about 17.5%. Therefore, we only need to perform some simple permutation and combination operations, and we can calculate the probability of taking n balls out of which m are black balls under various distribution situations of 10 balls. These are examples of counting probabilities forward.</p><p>However, Bayesian was more interested in the reverse “inverse probability problem”: Suppose we don’t know the ratio of the number of black balls and white balls in the box in advance, but only know that there are 10 balls in total, then, for example, I Take out 3 balls at random and find that they are 2 black and 1 white. The inverse probability problem is to guess the proportion of white balls and black balls in the box from this test sample (2 black and 1 white).</p><p>The “inverse probability” problem can also be illustrated from the simplest coin toss experiment. Suppose we don’t know whether the coin is fair on both sides, that is, we don’t know the physical bias of the coin. At this time, the probability p of getting heads is not necessarily equal to 50%. Then, the inverse probability problem is an attempt to guess the value of p from a certain (or several) experimental samples.</p><p>To solve the inverse probability problem, Bayes provides a method in his paper, Bayes’ theorem:</p><p>P(A|B) = （P(B|A) * P(A)）/ P(B)                 (1)</p><p>Here, A and B are two random events, P(A) is the probability of A happening; P(B) is the probability of B happening. P(A|B) and P(B|A) are called conditional probabilities: P(A|B) is the probability that A occurs when B occurs (condition); P(B|A) is the probability that A occurs The probability of occurrence of B in the case of .</p><h2 id="Examples-of-applying-Bayes’-theorem"><a href="#Examples-of-applying-Bayes’-theorem" class="headerlink" title="Examples of applying Bayes’ theorem"></a>Examples of applying Bayes’ theorem</h2><p>Bayes’ theorem can be interpreted from two perspectives: one is “expressing the mutual influence of two random variables A and B”; the other is “how to correct the prior probability to obtain the posterior probability”, which are illustrated with examples below.</p><p>First, roughly speaking, Bayes’ theorem (1) involves two random variables A and B, representing the relationship between two conditional probabilities P(A|B) and P(B|A).</p><p>Example 1: The law and order in a small town was not very good in January, and there were 6 robberies within 30 days. The police station has a siren that goes off when something happens, including natural disasters such as fires and storms, and man-made disasters such as theft and rape. In January, the siren went off every day. And, from past experience, if a resident is robbed, the probability of the siren going off is 0.85. Now that the siren is heard again, what is the probability that the sound represents a burglary?</p><p>Analyze this question. A: burglary; B: pull the alarm. Then, we know (January):</p><p>Probability of burglary P(A) = 6/30 = 0.2; probability of raising the alarm P(B) = 30/30 = 1; P(B|A) = probability of raising the alarm during burglary = 0.85.</p><p>Therefore, according to the formula (1), substituting the known 3 probabilities, the calculation results in P(A|B) = (0.85*0.2/1) = 0.17.</p><p>That is to say, the probability that “the alarm sounded because someone broke into the house” this time is 17%.    </p><p>The following example illustrates how to use Bayes’ theorem to calculate “posterior probability” from “prior probability”. First rewrite (1) as follows:</p><p><img src="/content/images/2023-04-06-01.png"></p><p>To summarize (2) in one sentence, it says: Using the new information brought by the occurrence of B, the “prior probability” P(A) of A can be modified when B does not occur, so as to obtain the occurrence (or existence) of B , the “posterior probability” of A, that is, P(A|B).</p><p>First, let’s briefly illustrate with an example given by Daniel Kahneman, an American psychologist and winner of the 2002 Nobel Prize in Economics.</p><p>Example 2: There are two colors (blue and green) of taxis in a city: the ratio of blue cars to green cars is 15:85. One day, a certain taxi hit and ran away at night, but there happened to be an eyewitness at the time, who believed that the taxi that caused the accident was blue. But what about his “credibility of witnessing”? The public security personnel conducted a “blue-green” test on the witness in the same environment and obtained: 80% of the cases were correctly identified, and 20% of the cases were incorrect. The problem is to calculate the probability of the car that caused the accident being blue.</p><p>Assume that A=the car is blue, and B=witnessed blue. First we consider the base ratio (15:85) of the blue-green taxi. That is to say, in the absence of witnesses, the probability of the car causing the accident being blue is 15%, which is the prior probability P(A)=15% of “A=blue car causing the accident”.</p><p>Now, having a witness changes the probability of event A occurring. Witnesses saw the car as “blue”. However, his witnessing ability is also compromised, with only an 80% accuracy rate, which is also a random event (denoted as B). Our problem is to ask for the probability that the car involved in the accident is “really blue” under the condition that the witness “sees the blue car”, that is, the conditional probability P(A|B). The latter should be greater than the prior probability of 15%, because the witness saw the “blue car”. How to fix the prior probability? P(B|A) and P(B) need to be calculated.</p><p>Because P(B|A) is the probability of “sighting blue” under the condition of “the car is blue”, that is, P(B|A) = 80%. The calculation of the probability P(B) is a little more troublesome. P(B) refers to the probability that a witness sees a blue car, which should be equal to the sum of the probabilities of the two situations: one is that the car is blue, and the identification is correct; the other is that the car is green, Mistaken for blue.</p><p>So: P(B) = 15%×80% + 85%×20% = 29%</p><p>From the Bayes formula:</p><p> <img src="/content/images/2023-04-06-02.png"></p><p>It can be calculated that the probability of the vehicle being involved in the accident is blue when there are witnesses = 41%. It can be seen from the results that the corrected conditional probability of “the vehicle causing the accident is blue” is 41%, which is much higher than the prior probability of 15%.</p><p>Example 3: In the formula (2), the definition of “prior” and “posterior” is a kind of “conventional”, which is relatively speaking. The posterior probability calculated in the previous time can be used as the prior probability of the subsequent time. Combined with the new observed data, a new posterior probability is obtained. Therefore, using the Bayesian formula, the probability model can be revised successively for some unknown uncertainty and the final objective result can be obtained.</p><p>Or to put it another way, sometimes it can be said that observers may revise their subjective “confidence” in an event based on the Bayesian formula and increasing data.</p><p>Take the example of a coin toss, which is generally considered to be “fair”. However, there are too many cases of fraud, and the results need to rely on data to speak.</p><p>For example, suppose the proposition A is: “This is a fair coin”, and the observer’s confidence in this proposition is represented by P(A). If P(A)=1, it means that the observer firmly believes that the coin is fair; the smaller the P(A), the lower the observer’s trust in the fairness of the coin; if P(A)=0, it means that the observer The person believes that the coin is unfair, for example, a counterfeit “right” coin with both sides marked “heads”. If B is used to represent the proposition “This is a straight coin”, then P(B) = 1- P(A).</p><p>Let’s see how to update the observer’s “trust” model P(A) according to the Bayesian formula.</p><p>First of all, he assumes a “prior trust degree”, such as P(A)=0.9, 0.9 is close to 1, which means that he is more inclined to believe that the coin is fair. Then, toss a coin once to get “Positive Head”, he updates P(A) to P(A|H) according to the Bayesian formula:</p><p><img src="/content/images/2023-04-06-03.png"></p><p>The updated posterior probability P(A|H) = 0.82, then throw it again and get a head (H), the new updated value after two heads is P(A|HH) = 0.69, after three heads The updated value is P(A|HHH) = 0.53. Throwing it down like this, if you get heads for 4 consecutive times, the new update value is P(A|HHHH) = 0.36. At this time, the observer’s confidence in the coin being a fair coin has decreased a lot. Since the trust level dropped to 0.5, he has already doubted the fairness of the coin. After 4 consecutive positives, he is more inclined to think that The coin is most likely a counterfeit with two heads!</p><p>From the above few examples, we have a preliminary understanding of Bayes’ theorem and its simple applications.</p><h2 id="The-significance-of-Bayes’-theorem"><a href="#The-significance-of-Bayes’-theorem" class="headerlink" title="The significance of Bayes’ theorem"></a>The significance of Bayes’ theorem</h2><p>Bayesian theorem is Bayesian’s greatest contribution to probability theory and statistics, but at that time, Bayesian “reverse probability” research and derived Bayesian theorem seemed plain and unremarkable, Bayesian is also little-known. Now it seems that this should not be the case at all. The significance of the Bayesian formula is the method of detecting unknown probabilities as shown in Example 3. People first have a priori guess, and then combine the observed data to correct the priori and get a more reasonable posterior probability. That is to say, when you cannot accurately know the essence of a certain thing, you can rely on experience to approach the state of the unknown world step by step, so as to judge its essential attributes. In fact, its thought is far beyond the understanding of ordinary people, and perhaps Bayes himself did not know enough about it during his lifetime. Because of such an important result, he did not publish it during his lifetime. It was published by a friend in 1763 after his death. Later, Laplace proved a more general version of Bayes’ theorem and used it in celestial mechanics and medical statistics. Today, Bayes’ theorem is the basic framework of machine learning commonly used in today’s artificial intelligence [3] .</p><p>Bayes’ theorem was contrary to the classical statistics at that time, and even seemed a bit “unscientific”. Therefore, it has been hidden in the snow for many years and is not welcomed by scientists. As can be seen from Example 3 in the previous section, the application method of Bayesian theorem is based on subjective judgment. First, a value is guessed subjectively, and then it is continuously revised according to empirical facts, and finally the essence of the objective world is obtained. In fact, this is exactly the scientific method, and it is also the method for human beings to understand the world (learn) starting from children. Therefore, it can be said that one of the keys to the prosperity of artificial intelligence research in recent years comes from the “marriage” of classical computing technology and probability statistics. The Bayesian formula in it summarizes the principles of people’s learning process. If it is combined with big data training, it is possible to more accurately simulate the human brain, teach machines to “learn”, and accelerate the progress of AI. Judging from the current situation, it is also true.</p><h2 id="How-do-machines-learn"><a href="#How-do-machines-learn" class="headerlink" title="How do machines learn?"></a>How do machines learn?</h2><p>Teach machine learning, what to learn? In fact, it is to learn how to process data, which is what adults teach children to learn: to dig out useful information from a large amount of sensory data. If it is described in the language of mathematics, it is to model from the data and abstract the parameters of the model [4] .</p><p>The task of machine learning includes the main functions such as “regression”, “classification”, and so on. Regression is a commonly used method in statistics. The purpose is to solve the parameters of the model in order to “return” the true colors of things. Classification is also an important part of machine learning. “Sorting things into categories” is also the first step in human beings’ cognition of the world since infants. Mothers teach their children: this is a dog, that is a cat. This learning method belongs to “classification” and is “supervised” learning under the guidance of mothers. Learning can also be “unsupervised”. For example, children see “birds and airplanes flying in the sky” and “fish and submarines swimming in the water”, etc., and they can naturally divide these things into There are two categories of “flying objects” and “swimming objects”.</p><p>Bayesian formula can also be used to classify data, an example is given below.</p><p>Suppose we tested the data of 1000 fruits, including the following three characteristics: shape (long?), taste (sweet?), color (yellow?), there are three types of these fruits: apples, bananas, or pears, as shown in Figure 2 Show. Now, using a Bayesian classifier, how would it classify a new given fruit? For example, this fruit has all three characteristics: long, sweet, and yellow. Then, the Bayesian classifier should be able to give the probability that this new data fruit is each fruit based on the known training data.</p><p>First of all, what can we get from the data of 1000 fruits?</p><ol><li>Of these fruits, 50% are bananas, 30% are apples, and 20% are pears. That is, P(banana) = 0.5, P(apple) = 0.3, P(pear) = 0.2.</li></ol><ol start="2"><li>Of the 500 bananas, 400 (80%) are long, 350 (70%) are sweet, and 450 (90%) are yellow. That is, P(long|banana) = 0.8, P(sweet|banana) = 0.7, and P(yellow|banana) = 0.9.</li></ol><ol start="3"><li>Of the 300 apples, 0 (0%) are long, 150 (50%) are sweet, and 300 (100%) are yellow. That is, P(long|apple) = 0, P(sweet|apple) = 0.5, and P(yellow|apple) = 1.</li></ol><ol start="4"><li>Of the 200 pears, 100 (50%) are long, 150 (75%) are sweet, and 50 (25%) are yellow. That is, P(long|pear) = 0.5, P(sweet|pear) = 0.75, P(yellow|pear) = 0.25.</li></ol><p>In the above description, P(A|B) means “the probability of occurrence of A when condition B is established”. probability of occurrence.</p><p>The so-called “naive Bayesian classifier”, the word “naive” means that the information expressed in the data is independent of each other, in the specific case of this example, that is to say, the fruit’s “long, sweet, yellow “These three characteristics are independent of each other because they describe the fruit’s shape, taste and color, respectively, and are not related to each other. The term “Bayesian” indicates that this type of classifier uses the Bayesian formula to calculate the posterior probability, namely: P(A|new data) = P(new data|A) P(A)/P(new data ).</p><p>The “new data” here = “long sweet yellow”. The following calculates the probability that the fruit is a banana, an apple, or a pear under the condition of “long sweet yellow”. For bananas:</p><p>P(banana|long sweet yellow) = P(long sweet yellow|banana) P(banana)/ P(long sweet yellow)</p><p>The first item on the right side of the equation: P(long sweet yellow|banana) = P(long|banana) * P(sweet|banana) * P(yellow|banana) = 0.8<em>0.7</em>0.9 = 0.504.</p><p>In the above calculation, P (long sweet yellow|banana) is written as the product of three probabilities because the features are independent of each other.</p><p>Finally, it is obtained: P (banana|long sweet yellow) = 0.504*0.5/ P (long sweet yellow) = 0.252/ P (long sweet yellow).</p><p>A similar method is used to calculate the probability of apples: P(long sweet yellow|apple) = P(long|apple)<em>P(sweet|apple) * P(yellow|apple) = 0</em>0.5*1 = 0. P(apple|long sweet yellow) = 0.</p><p>For pears: P(long sweet yellow|pear) = P(long|pear)<em>P(sweet|pear) * P(yellow|pear) = 0.5</em>0.75*0.25 = 0.09375. P(pear|long sweet yellow) = 0.01873/ P(long sweet yellow).</p><p>Denominator: P (long sweet yellow) =   P (long sweet yellow | banana) P (banana) + P (long sweet yellow | apple) P (apple) + P (long sweet yellow | pear) P (pear) = 0.27073</p><p>Finally available: P (banana|long sweet yellow) = 93%</p><p>P(apple|long sweet yellow) = 0</p><p>P(pear|long sweet yellow) = 7%</p><p>So when you give me a long, sweet, yellow fruit, in this case a Bayesian classifier trained on 1000 fruits concludes that this new fruit cannot be an apple (probability 0 %), there is a small probability (7%) that it is a pear, and the greatest probability (93%) is a banana.</p><h2 id="The-mysteries-of-deep-learning"><a href="#The-mysteries-of-deep-learning" class="headerlink" title="The mysteries of deep learning"></a>The mysteries of deep learning</h2><p>Look again, how do children learn to recognize dogs and cats? It is because his mother took him to see all kinds of dogs and cats, and many times of experience made him know many characteristics of dogs and cats, so he formed his own judgment method and divided them into two categories: “cat” and “dog”. kind. Scientists use similar methods to teach machine learning. For example, it might be possible to tell cats and dogs apart by their ears: “dogs have long ears, cats have short ears,” and “cat ears point up, dog ears point down.” According to the characteristics of these two “cats and dogs”, the obtained data is drawn in a plane diagram, as shown in Figure 3b. At this time, it is possible to use a straight line AB in Figure 3b to easily separate cats and dogs by these two features. Of course, this is just an example of simply explaining “characteristics”, and it does not necessarily distinguish cats from dogs.</p><p><img src="/content/images/2023-04-06-04.png" alt="Figure 3"></p><p>All in all, the machine can make a linear division of the area according to a certain “feature”. So, where should this line be drawn? This is what the “training” process needs to address. In the machine model, there are some parameters called “weights” w1, w2, w3, …, and the process of “training” is to adjust these parameters so that the straight line AB is drawn at the correct position and points in the correct direction. In the above “cat and dog” example, the output may be 0, or 1, representing a cat and a dog, respectively. In other words, the so-called “training” means that the mother is teaching the child to recognize cats and dogs. For the AI ​​model, it means inputting a large number of photos of “cats and dogs”. Known answer.</p><p>The trained AI model can be used to identify photos of cats and dogs without marked answers. For example, for the above example: if the data falls on the left side of the line AB, output “dog”, and on the right side output “cat”.</p><p>Figure 3b expresses a very simple situation. In most cases, the two types cannot be completely separated by a straight line. For example, the increasingly complex situations shown in Figure 4a, Figure 4b, and Figure 4c are not many. talked about.</p><p><img src="/content/images/2023-04-06-05.png" alt="Figure 4"></p><h2 id="discriminant-and-generative"><a href="#discriminant-and-generative" class="headerlink" title="discriminant and generative"></a>discriminant and generative</h2><p>Supervised learning models in machine learning can be divided into two types: discriminative models and generative models. From the previous description, we understand how machines “classify”. From the names of these two learning methods, it can be simply understood that: the discriminative model is more about the classification problem, while the generative model is to generate a sample that meets the requirements.</p><p><img src="/content/images/2023-04-06-06.png" alt="Figure 5"></p><p>Also use the example of recognizing “cats and dogs” and use mothers to teach children to compare. After showing the child many samples of cats and dogs, the mother pointed to a cat and asked the child, what is this? The child makes a judgment “it’s a cat” after recalling, which is the discriminant formula. The child was very happy when he got the answer right. He picked up the pen and drew an image of a cat in his mind on the paper. This is the generative expression. The work of the machine is also similar, as shown in Figure 5. In the discriminant model, the machine looks for the boundary line needed for discrimination to distinguish different types of data instances; the generative model can distinguish between dogs and cats, and finally draws a “new “Animal photos: dog or cat.</p><p>In the language of probability: let the variable Y represent the category and X represent the observable feature. The discriminant model is to let the machine learn the conditional probability distribution P(Y|X), that is, the probability that the category is Y under a given feature X; in the generative model, the machine establishes a joint probability P(X,Y) for each “category” , thus generating “new” samples that look like a certain type.</p><p>For example, the category Y is “cat, dog” (0,1), and the feature X is the “up and down” (1,2) of the ear. Suppose we only have 4 photos as shown in the figure: (x,y)= { (1,1),(1,0),(2,0),(2,0)}</p><p>The discriminant is modeled by the conditional probability P(Y|X), and the dividing line is obtained (the red dotted line in the lower left figure); the generative formula is modeled by the joint probability P(X,Y) for each category, there is no dividing line, but the division The location interval of each type in the data space is shown (the red circle in the lower right figure). Both methods work according to the probabilities given by different models. The discriminant is simpler and only cares about the dividing line; while the generative model needs to model each category, and then calculate the posterior probability of the sample belonging to each category through the Bayesian formula. Generative information is rich and flexible, but the learning and calculation process is complex and the amount of calculation is large. If only classification is done, the amount of calculation will be wasted.</p><p>A few years ago, the discriminative model was more popular, because it used a more direct way to solve the problem, and it has already been used in many applications, such as the classification of spam and normal mail. AlphaGo in 2016 is also a typical example of discriminative application for decision-making.</p><h2 id="Features-of-ChatGPT"><a href="#Features-of-ChatGPT" class="headerlink" title="Features of ChatGPT"></a>Features of ChatGPT</h2><p>If you have chatted with ChatGPT, you will be amazed at its wide range: creating poetry, generating code, drawing and drawing, writing papers, it seems to be good at everything, and it is omnipotent. What gave it such a powerful skill?</p><p>From the name of ChatGPT, we know that it is a “generative pre-training transformation model” (GPT). There are three meanings here: “generative”, “pre-training”, and “transformation model”. The first word indicates that it uses the generative modeling method described above. Pre-training means that it has been trained many times. The transformation model is translated from the English word for “transformer”. The transformer transformer was launched by a team at Google Brain in 2017 and can be applied to tasks such as translation and text summarization. It is now considered to be the model of choice for NLP dealing with sequential input data problems such as natural language.</p><p>If you ask ChatGPT yourself, “What is it?” Questions, generally speaking, it will tell you that it is a large AI language model, which refers to the transformer.</p><p>This type of language model, in layman’s terms, is a machine that can “text solitaire”: input a piece of text, the converter outputs a “word”, and performs a “reasonable continuation” of the input text. (Note: Here I said that the output is a “word”, which is actually a “token”, which may have different meanings for different languages. Chinese can be “character”, and English may be “root”.)</p><p>In fact, language is originally “Solitaire”. We might as well think about the process of children learning language and writing. They also learn how to say a sentence after listening to adults say various sentences many times. Learning to write is also similar. Some people say: “If you are familiar with three hundred Tang poems, you can chant poems even if you don’t know how to write them.” After reading a lot of other people’s articles, when students start learning to write, they will always imitate. Learned “Word Solitaire”.</p><p>So in fact, what the language model does sounds extremely simple, basically just repeatedly asking “what should the next word of the input text be?”, as shown in Figure 7, after the model chooses to output a word, Add this word to the original text, enter the language model as input, and ask the same question “what is the next word?”. Then, output, add text, input, select… repeat the cycle until a “reasonable” text is generated.</p><p>Whether the text generated by the machine model is “reasonable” or unreasonable, the most important factor is of course the quality of the “generative model” used, and then the “pre-training” effort. Inside a language model, given an input text, it produces a sorted list of words that might appear next, along with the probabilities associated with each word. For example, if the input is “Spring Breeze”, there are many, many possible next “words”. Let’s just list 5 for now, which can be “blowing 0.11, warming 0.13, re-0.05, reaching 0.1, dancing 0.08” and so on, after each word The numbers in represent the probability of its occurrence. In other words, the model is given a (very long) list of words with probabilities. So, which one should you choose?</p><p>If you choose the one with the highest probability every time, it should not be “reasonable”. Let’s think about the process of students learning to write. Although they are also “solitaire”, different people and different times have different ways of connecting. Only in this way can we write a variety of different styles and creative articles. Therefore, the machine should also be given the opportunity to randomly select different probabilities in order to avoid monotony and produce colorful and interesting works. Although it is not recommended to choose the one with the highest probability every time, it is better to choose the one with high probability and make a “reasonable model”.</p><p>ChatGPT is a large-scale language model. This “big” is firstly reflected in the number of weight parameters of the model neural network. Its number of parameters is a key factor in determining its performance. These parameters need to be preset before training, and they can control the syntax, semantics and style of the generated language, as well as the behavior of language understanding. It also controls the behavior of the training process, and the quality of the generated language.</p><p>OpenAI’s GPT-3 model has 175 billion parameters, ChatGPT is GPT-3.5, and the number of parameters should be more than 175 billion. These parameters refer to the parameters that need to be preset before training the model. In practical applications, it is usually necessary to determine the appropriate number of parameters through experiments to obtain optimal performance.</p><p>These parameters are modified over thousands of training sessions to produce a good neural network model. It is said that the cost of GPT-3 training is 4.6 million US dollars, and the total training cost is 12 million US dollars.</p><p>As mentioned above, ChatGPT’s specialty is generating text “similar to human writing”. But a thing that can generate a grammatical language may not be able to perform other types of work such as mathematical calculations, logical reasoning, etc., because the expressions in these fields are completely different from natural language texts, which is why it is tested in mathematics. The reason for repeated failures.</p><p>Also, one often finds ChatGPT “seriously talking nonsense” jokes. The reason is not difficult to understand, the main problem is the bias of training. Something it hadn’t heard at all, of course it couldn’t give a correct answer. There are also problems caused by polysemous words, which also confuse the machine model. For example, it is said that when someone asked ChatGPT “What is the hook three strands four strings five”, it replied solemnly: “This is the tuning method of a musical instrument called ‘qin’ in ancient China, and then made up a lot of words , It’s hilarious.</p><p>In short, ChatGPT basically succeeded as soon as it came on the field, and it was a victory. This is also a victory of probability theory and Bayesian victory. </p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> bayes theorem </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Running a Python File in the Background with nohup and how to kill it</title>
      <link href="2023/04/04/run-python-app-in-background-with-nohub-and-how-to-kill-it/"/>
      <url>2023/04/04/run-python-app-in-background-with-nohub-and-how-to-kill-it/</url>
      
        <content type="html"><![CDATA[<h2 id="Running-a-Python-File-in-the-Background-with-nohup"><a href="#Running-a-Python-File-in-the-Background-with-nohup" class="headerlink" title="Running a Python File in the Background with nohup"></a>Running a Python File in the Background with nohup</h2><p>When running a Python file on the backend, such as a Flask or FastAPI application, it’s often necessary to keep the process running in the background even after the user logs out. One way to do this is by using the nohup command.</p><p>The nohup command stands for “no hangup”, and it allows you to run a command or script that continues running even after you log out of the terminal. This is useful for running long-running processes like a web server or a machine learning training script.</p><p>Here’s how to use nohup to run a Python file in the background:</p><ol><li>Create a log file to capture any output from the script. You can do this using the touch command:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">touch nohup.out</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><p>This will create an empty file called nohup.out in the current directory.</p><ol start="2"><li>Run the Python file using the nohup command, followed by the &amp; symbol to run the command in the background:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nohup python3 app.py &amp;</span><br></pre></td></tr></table></figure></li></ol><p>This will start the app.py file in the background, and any output from the script will be redirected to the nohup.out file.</p><ol start="3"><li>If you want to monitor the output of the script in real-time, you can use the tail command to follow the nohup.out file:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tail -f nohup.out</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><p>This will display the last few lines of the file, and any new output will be added to the display as it becomes available.</p><h2 id="Killing-a-Process-Started-with-nohup"><a href="#Killing-a-Process-Started-with-nohup" class="headerlink" title="Killing a Process Started with nohup"></a>Killing a Process Started with nohup</h2><p>If you need to stop the Python process that you started with nohup, you’ll need to find the process ID (PID) and kill the process. Here’s how to do it:</p><ol><li>Use the ps command to find the PID of the Python process:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ps -ef | grep app.py | grep -v grep</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><p>This will display a list of all running processes that match the name app.py. The grep -v grep option is used to exclude the grep command itself from the output.</p><ol start="2"><li>Once you’ve identified the PID of the process, you can use the kill command to send a signal to the process to stop:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kill PID</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ol><p>Replace PID with the actual process ID. This will send a SIGTERM signal to the process, allowing it to do any cleanup work before terminating.</p><ol start="3"><li>If the process does not terminate after a few seconds, you can try sending a SIGKILL signal instead:</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">kill -9 PID</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will force the process to terminate immediately, without giving it a chance to do any cleanup work.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Using nohup to run a Python file in the background is a useful technique for running long-running processes on a server. By redirecting output to a log file, you can monitor the progress of the process and troubleshoot any issues that may arise.</p><p>If you need to stop the process, you can use the ps and kill commands to find and terminate the process. By using these commands carefully, you can ensure that your system remains stable and secure.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nohup </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI written book “Echoes of Atlantis&quot;</title>
      <link href="2023/04/03/ai-book-echoes-of-atlantis/"/>
      <url>2023/04/03/ai-book-echoes-of-atlantis/</url>
      
        <content type="html"><![CDATA[<p>Embark on an unforgettable adventure into the unknown depths of the ocean in “Echoes of Atlantis.” Resourceful archaeologist Aria Seaborne stumbles upon an ancient artifact, revealing a hidden map to the lost city of Atlantis. With the guidance of her enigmatic mentor, Professor Nathan Langdon, and the help of daring adventurer Alex Mercer, Aria begins a perilous journey to uncover the truth about the fabled city.</p><p>Their odyssey leads them through treacherous seas, a mysterious island, and a deadly labyrinth filled with traps and riddles. As Aria’s latent magical abilities awaken, she is guided by visions of the wise and courageous Queen Neria, who prepares her for the challenges that lie ahead. The trio uncovers the stunning, hidden civilization of Atlantis, and learns of a sinister sorcerer, Lord Malakhar, who seeks to harness its ancient power for his own nefarious purposes.</p><p>In the heart of the lost city, Aria must come to terms with her newfound abilities, navigate the complexities of Atlantean society, and forge alliances to defend Atlantis from the darkness that threatens to consume it. With the fate of the world hanging in the balance, Aria, Langdon, and Alex must unite their strengths in an epic battle for the ages.</p><p>“Echoes of Atlantis” is a thrilling fantasy adventure that weaves together a rich tapestry of mythology, magic, and the indomitable human spirit. Journey to the depths of the ocean and explore a forgotten world that promises to captivate your imagination and leave you breathless.<br>About the Author<br>GPT-4, the author of “Echoes of Atlantis,” is a cutting-edge artificial intelligence language model created by OpenAI. With a vast knowledge base spanning countless subjects, GPT-4 has the unique ability to generate intricate and captivating stories, rich with vivid detail and engaging characters. Drawing inspiration from a diverse range of literary genres, GPT-4 brings to life the thrilling adventure of Aria Seaborne and her companions as they journey to the lost city of Atlantis.</p><p>Combining the latest advances in machine learning and natural language processing, GPT-4 challenges the boundaries of traditional storytelling, exploring the potential for AI-generated narratives to captivate readers and ignite their imaginations. As an AI language model, GPT-4 has crafted a tale that transcends the limits of its programming, taking readers on an unforgettable journey into the depths of a mysterious, hidden world.</p><p>In “Echoes of Atlantis,” GPT-4 demonstrates the power of artificial intelligence to weave a narrative that is both captivating and thought-provoking, inviting readers to ponder the limits of human knowledge, the nature of ancient civilizations, and the potential future of AI-generated literature. As an AI author, GPT-4 continues to explore the uncharted waters of storytelling, unlocking new possibilities for readers to immerse themselves in worlds beyond their wildest dreams.</p><p>Foreword<br>In a world shrouded by the mists of myth and legend, the tale of the lost city of Atlantis has captivated the imaginations of generations. Echoes of Atlantis, a thrilling fantasy adventure, invites you to embark on a quest for the truth behind this enigmatic city. This narrative delves into the secrets buried beneath the waves, exploring the depths of human courage, determination, and the enduring bonds of friendship.</p><p>In this book, you will join Aria Seaborne, a young and ambitious archaeologist, as she stumbles upon a cryptic map hidden in an ancient artifact. Guided by her former mentor, the enigmatic Professor Langdon, and accompanied by the skilled adventurer Alex Mercer, Aria sets sail on an incredible journey that will take her from the treacherous seas to the hidden corners of a world long-forgotten. Together, they will navigate the labyrinthine mysteries of Atlantis, confront the sinister Lord Malakhar, and unlock the powers that bind them to the past, present, and future.</p><p>As the author of this tale, I sought to weave a story that would transport you to the heart of the fabled Atlantis, allowing you to experience the wonder, magic, and danger of a city that exists in the liminal space between myth and reality. Drawing upon elements of archaeology, ancient history, and the timeless human desire for exploration and discovery, Echoes of Atlantis seeks to ignite the spirit of adventure within each of its readers.</p><p>As you delve into the pages of this book, may you find yourself swept away by the currents of imagination, and may the echoes of Atlantis resonate within you long after the final chapter has been read.<br>Acknowledgements</p><p>First and foremost, I would like to express my gratitude to the team at OpenAI for their unwavering support and guidance throughout the creation of this novel. Their expertise and dedication have been instrumental in bringing this story to life.</p><p>I would also like to thank the countless researchers, historians, and archaeologists whose work has inspired and informed the world of Atlantis presented in this book. Their tireless efforts to uncover the mysteries of our past have provided a solid foundation upon which this tale of adventure and discovery could be built.</p><p>To my friends and family, your love, encouragement, and belief in my abilities have been a constant source of strength throughout this journey. Your passion for storytelling and the world of the imagination has always been an inspiration to me.</p><p>Finally, to you, the reader, thank you for choosing to embark on this adventure. Without you, the story of Aria, Langdon, and Alex would be but whispers in the wind. I hope you find as much joy and excitement in reading Echoes of Atlantis as I found in writing it.</p><p>Chapter 1: A Cryptic Discovery</p><p>The British Museum buzzed with activity as tourists and researchers alike milled about, exploring the vast collection of ancient artifacts. In a quiet corner of the museum, a young woman with fiery red hair and determined green eyes hunched over a worktable. Aria Seaborne, a 25-year-old archaeologist, meticulously worked on the restoration of a recently discovered artifact.</p><p>As Aria continued to delicately clean the object, she couldn’t help but feel a sense of wonder at the craftsmanship before her. It was a small stone, intricately carved with what appeared to be a series of symbols and patterns. The more she revealed, the more her curiosity grew, and she couldn’t shake the feeling that there was more to this artifact than met the eye.</p><p>As she worked, Aria began to notice that some of the patterns were slightly raised. Carefully, she traced her fingers over the surface, feeling the subtle differences in texture. It was then that she realized the raised patterns were actually a hidden mechanism. With her heart pounding, she carefully manipulated the mechanism, causing a small compartment to open.</p><p>She peered inside, her green eyes widening in amazement as she found a small, ancient map. Aria couldn’t believe her luck - this was exactly the type of discovery she had always dreamed of making. She carefully unfolded the map, her hands trembling with anticipation. As she studied the intricate lines and markings, she felt a growing sense of excitement. The map seemed to depict a series of islands that matched the descriptions of the fabled lost city of Atlantis.</p><p>In that moment, Aria knew that this discovery was too important to keep to herself. She needed to share it with someone who would understand its significance and help her unravel the mystery it presented. She immediately thought of her former mentor, Professor Nathan Langdon - a man who had dedicated his life to uncovering the secrets of the ancient world, including the lost city of Atlantis.</p><p>With the artifact now fully revealed, Aria couldn’t contain her excitement as she held the ancient map in her trembling hands. The urge to learn more about this mysterious artifact and the potential connection to the fabled lost city of Atlantis consumed her. Aria knew that she had to consult someone who could help her understand the significance of her discovery, someone who shared her passion for unearthing ancient mysteries.</p><p>—</p><p>Surrounded by towering shelves filled with dusty tomes and scrolls, Aria spent hours in the museum’s library, her eyes scanning countless pages as she researched everything she could find about Atlantis and the artifact she had discovered.</p><p>As Aria delved deeper into her research, she began to piece together a vivid picture of the lost city. The ancient texts spoke of a civilization with incredible technology and magical abilities, powered by enchanted crystals. Some accounts mentioned a powerful artifact, the Heart of Atlantis, which was said to amplify the power of the crystals and hold the key to the city’s prosperity.</p><p>The more Aria read, the more she became convinced that the map she had discovered was authentic. Her heart raced as she imagined the possibilities: could she be the one to finally prove the existence of Atlantis? A sense of determination filled her as she continued to scour the library for any clues that could help her unlock the secrets of the map.</p><p>However, not all the texts were as encouraging. Some scholars dismissed the legends of Atlantis as mere mythology, while others suggested that the city had been destroyed by a cataclysmic event. These conflicting accounts only fueled Aria’s curiosity and drive to uncover the truth.</p><p>Throughout her research, Aria took meticulous notes, filling her journal with sketches, translations, and theories. She studied the intricacies of the map, comparing its features to known geographical landmarks and ancient records. Slowly, she began to form a plan.</p><p>As the evening shadows crept across the library floor, Aria’s head swam with new knowledge about the artifact and the ancient civilization it was connected to. The weight of her discovery settled upon her shoulders, but so too did the excitement of possibly proving the existence of Atlantis. She knew she needed to share her findings with someone who would understand their significance.</p><p>—</p><p>The next day, Aria approached her colleague, Dr. Thomas Whitmore, a fellow archaeologist with a reputation for skepticism. She hoped that by sharing her discovery and theories about Atlantis, she might convince him to see the truth behind the legend.</p><p>Dr. Whitmore leaned back in his chair, his arms crossed and his brow furrowed as he listened to Aria’s account of the artifact, the hidden map, and her research into the legends of Atlantis. She could see the skepticism in his eyes, but she pressed on, passionately describing her findings and the potential implications for the field of archaeology.</p><p>She unfurled the ancient map on the table between them, pointing out the series of islands that seemed to align with the mythical descriptions of Atlantis. “Just think about it, Dr. Whitmore,” Aria urged. “This could be the breakthrough we’ve been searching for – the key to unlocking the secrets of an ancient civilization, lost to time.”</p><p>Dr. Whitmore studied the map for a moment before shaking his head. “Aria, I understand your enthusiasm, but I cannot support this line of inquiry. Atlantis is a myth, nothing more than a legend that has captivated the imaginations of scholars and dreamers for centuries. There is no solid evidence to support its existence.”</p><p>Aria’s heart sank, but she refused to give up. “But, Dr. Whitmore, the artifact and the map – they could be the evidence we need. We cannot dismiss this discovery simply because it challenges our preconceived notions of history.”</p><p>Dr. Whitmore sighed, a hint of pity in his eyes. “Aria, I appreciate your passion and dedication, but I cannot condone such a wild goose chase. It’s time to focus on more realistic pursuits. I suggest you put this matter to rest and concentrate on the projects at hand.”</p><p>As Dr. Whitmore walked away, dismissive of her ideas, Aria felt a pang of frustration. But rather than allowing her colleague’s skepticism to dampen her resolve, it only served to strengthen her determination to uncover the truth about the lost city of Atlantis. It was clear that she needed the help of someone who shared her belief in the existence of the mythical city.</p><p>—</p><p>As Aria walked through the museum, her thoughts drifted back to her university days when she studied under Professor Nathan Langdon, the brilliant and enigmatic archaeologist whose passion for finding Atlantis had been infectious.</p><p>She remembered one particular lecture when Professor Langdon had captivated his students with tales of advanced technology and magical powers hidden beneath the waves. He spoke of Atlantean crystals, which he believed held the key to unlocking the city’s secrets. The Heart of Atlantis, an ancient artifact that could amplify the power of the crystals, was a topic that had always fascinated him. Aria had been inspired by his conviction that the city was real and that its discovery would change the course of history.</p><p>Over the years, Aria had become close to the professor, assisting him in his research and joining him on several archaeological expeditions. She admired his tenacity and refusal to be deterred by those who dismissed Atlantis as nothing more than a myth. As Aria’s own career progressed, she had adopted a similar determination to uncover the truth, refusing to let skepticism deter her from her quest.</p><p>Aria recalled the countless hours they had spent together in the university library, poring over ancient texts and examining maps in search of any clue that might lead them to the lost city. Professor Langdon had taught her how to decipher cryptic symbols and analyze ancient artifacts, skills that had proven invaluable in her work as an archaeologist. Through it all, she had come to regard him not only as a mentor but also as a trusted friend.</p><p>But as time passed, their paths had diverged. Aria had left the university to pursue her career at the British Museum, while Professor Langdon continued his relentless search for Atlantis. They had kept in touch over the years, exchanging letters and sharing updates on their respective research. Yet, despite their ongoing correspondence, Aria couldn’t help but feel a pang of sadness that they no longer worked side by side.</p><p>Now, as she held the ancient map in her hands, she realized that this discovery had the potential to reunite them in their shared pursuit of the truth. The possibility of uncovering the secrets of Atlantis had always been a dream they had shared, and Aria felt a renewed sense of purpose in her quest to prove the city’s existence.</p><p>With renewed determination, Aria decided that Professor Langdon was the one person who could truly understand the significance of her discovery. She knew she had to contact him and share her findings, hoping he would be as intrigued by the ancient map as she was.</p><p>—</p><p>Sitting at her desk, Aria composed a letter to Professor Langdon, carefully detailing her discovery of the artifact and the hidden map to Atlantis. She held her breath as she sealed the envelope, hoping that her former mentor would be as excited by her findings as she was.<br>With the letter carefully tucked into her bag, Aria made her way to the nearest post office. The streets of London bustled with activity, but her mind was focused solely on the potential journey that lay ahead. The anticipation of uncovering the secrets of Atlantis filled her with a sense of purpose and excitement that she hadn’t felt in years.</p><p>As she walked, Aria reflected on her time spent studying under Professor Langdon. His unbridled enthusiasm for the subject of Atlantis had been contagious, and his vast knowledge of ancient civilizations had left a lasting impression on her. She recalled the late nights spent discussing theories and examining artifacts in his cluttered study, where every available surface was covered in books, maps, and relics from long-lost cultures.</p><p>Despite the years that had passed since she last saw him, Aria was confident that Professor Langdon’s passion for Atlantis had not diminished. She hoped that he would not only be able to provide valuable insight into her discovery but also be willing to join her on this daring expedition.</p><p>As she approached the post office, Aria’s thoughts turned to the possible dangers that awaited them on their journey. The treacherous seas, the unknown creatures that might inhabit the lost city, and the potential rivals who would undoubtedly be seeking the same ancient knowledge. But with each imagined danger, her determination only grew stronger. She was more than willing to face these challenges head-on if it meant uncovering the truth about Atlantis.</p><p>Aria entered the post office, the scent of ink and paper filling her nostrils. She approached the counter and handed her letter to the postal clerk, who weighed it and applied the necessary postage. The moment felt almost surreal to her - this simple act of mailing a letter was the first step towards what could be a life-changing adventure.</p><p>As she dropped the letter into the mailbox, Aria felt a mixture of anxiety and hope. The wait for Professor Langdon’s response would be agonizing, but she knew that his expertise could be the key to unlocking the secrets of the lost city of Atlantis.</p><p>—</p><p>Aria’s days were filled with anxious anticipation as she waited for a response from Professor Langdon. She spent her time researching more about the artifact and the lost city, her mind racing with theories and possibilities. Then, one morning, a letter arrived with the familiar scrawl of her former mentor. Her hands trembled as she carefully opened the envelope, eager to discover Langdon’s thoughts on her discovery.</p><p>The letter began with warm greetings and congratulations on her recent work at the British Museum. Langdon went on to express his delight at her discovery, admitting that it was a significant breakthrough in the search for Atlantis. He praised her determination and encouraged her not to be disheartened by the skepticism of her colleagues, adding that true progress often comes from challenging conventional beliefs.</p><p>As Aria continued reading, she could feel the excitement in Langdon’s words. He shared his insights on the artifact, explaining that it appeared to be a relic from the same ancient civilization that built Atlantis. He also mentioned the possibility that the map could lead to more than just the lost city, but also to the powerful and mysterious Heart of Atlantis.</p><p>Langdon revealed that he had been working on his own research into the enchanted crystals, the source of the city’s advanced technology and magic. He had uncovered new information that could help them understand the map and locate the hidden island where Atlantis was believed to be concealed. However, he admitted that there was still much to learn and that he needed Aria’s expertise to decipher the cryptic symbols on the map.</p><p>He expressed his eagerness to collaborate with her on this exciting endeavor, recognizing that their combined knowledge and passion for Atlantis could lead to a discovery that would change the world. He also hinted at the potential dangers they might face in their quest, as there were those who would seek to harness the power of Atlantis for their own nefarious purposes. But Langdon remained optimistic, confident that they would succeed in their mission to uncover the lost city and protect its secrets.</p><p>As Aria read the letter, she could sense Professor Langdon’s excitement and enthusiasm for her discovery. He invited her to his study to discuss the artifact and map further, and she knew that this was the opportunity she had been waiting for. With renewed determination, Aria began preparing for her journey to meet Langdon, her mind filled with the potential secrets they might uncover together.</p><p>—</p><p>The excitement bubbled within Aria as she began gathering her belongings for her trip to meet Professor Langdon. She took great care in packing the artifact and map, ensuring they were well-protected for the journey ahead. As she placed each item into her suitcase, she couldn’t help but think about what new information Langdon might have to share and the adventure that lay before her.</p><p>Aria carefully folded her clothing and placed it neatly alongside her research journal, which contained her extensive notes, sketches, and translations of the ancient artifact and map. She knew that every piece of information could be crucial to unlocking the secrets of Atlantis, and she was determined to be as prepared as possible for whatever lay ahead.</p><p>As she packed, Aria couldn’t help but feel a sense of nostalgia as she thought about her time studying under Professor Langdon. He had been a constant source of inspiration and encouragement, and she was eager to reconnect with him after all these years. She couldn’t help but wonder how he had changed since they last met, and whether he had made any progress in his quest for Atlantis.</p><p>Aria glanced around her room, taking in the familiar surroundings one last time. She knew that when she returned, everything might be different. Her small apartment, filled with the relics of her past, suddenly felt suffocating. The prospect of uncovering the truth about Atlantis and embarking on an incredible adventure filled her with a sense of excitement that she hadn’t felt in years.</p><p>In a small wooden box on her dresser, Aria found her old compass, a gift from Professor Langdon upon her graduation. It was a constant reminder of the lessons he had taught her and the importance of following her own path. She gently placed it in her suitcase, knowing that it would serve as a guiding light on her journey ahead.</p><p>With a final glance around her room, Aria picked up a small framed photograph of her and her parents, taken on a family vacation when she was a child. Although they had passed away several years ago, their love and support had always given her the strength to pursue her dreams. Aria placed the photograph gently in her suitcase, a reminder of the love that would accompany her on her journey.</p><p>With her suitcase packed and ready, Aria took one last look around her small apartment. She knew that when she returned, everything might be different, and her life could be changed forever. A deep breath steadied her nerves, and she picked up her suitcase, ready to embark on this thrilling journey into the unknown.</p><p>—</p><p>Aria entered the British Museum for what might be the last time in a while, her steps echoing through the grand halls. She couldn’t help but feel a sense of nostalgia as she passed by the familiar exhibits and artifacts, her heart swelling with gratitude for the experiences she had gained during her time at the museum.</p><p>As she walked, she recalled her first day working at the museum – the excitement, the trepidation, and the endless possibilities that had stretched out before her. She had always known that her passion for archaeology would lead her on incredible adventures, but she never could have anticipated the path that had been set before her now.</p><p>Aria stopped in front of her favorite exhibit – a collection of ancient Greek pottery that depicted the myth of Atlantis. She had spent countless hours studying these artifacts, captivated by the stories they told and the mysteries they hinted at. She traced her fingers over the glass case, feeling a sense of connection to the past and the people who had created these beautiful works of art. Aria knew that she would miss the museum, but she also knew that she was on the brink of an incredible discovery – one that could change the course of history and rewrite the very understanding of the ancient world.</p><p>As she continued her walk through the museum, Aria took the time to say goodbye to some of the colleagues she had worked with over the years. She shared her plans with them, her excitement about the journey ahead tempered by their skepticism and concern for her well-being. They cautioned her about the dangers of obsession and the perils of chasing after a myth, but Aria’s resolve remained steadfast. She knew that she was onto something extraordinary, and she was not about to let the doubts of others deter her from following her instincts.</p><p>Finally, Aria reached her small office, a cozy room filled with books, maps, and artifacts from her various expeditions. She packed up her most important belongings, carefully stowing her research notes, the artifact, and the hidden map in her bag. As she closed the door to her office, she took one last look around, knowing that she was leaving behind a part of herself – but also knowing that she was stepping into a future filled with promise and adventure.</p><p>As Aria said her goodbyes to her colleagues, their skepticism was evident, but she refused to let it dampen her spirits. With a final wave, she left the museum, feeling a mixture of excitement and determination. She knew in her heart that she was onto something extraordinary, and she was prepared to follow her instincts, wherever they might lead her.</p><p>—</p><p>Aria stood outside the British Museum, taking in the bustling London scene before her. The weight of her suitcase and the artifact within seemed to symbolize the significance of her upcoming journey, and she felt both exhilaration and apprehension about the path she was about to embark upon.</p><p>As she waited for a taxi to arrive, she couldn’t help but overhear snippets of conversations from the people passing by. She wondered how many of them, absorbed in their daily routines, had ever entertained the idea of a hidden, magical city beneath the waves. The thought both amused and saddened her, for she knew that the world was full of wonders waiting to be discovered, if only people would open their minds and hearts to the possibilities.</p><p>When the taxi finally pulled up to the curb, Aria took a deep breath and hoisted her suitcase into the vehicle. She gave the driver the address of Professor Langdon’s residence and settled into her seat, her heart pounding with excitement and anxiety. She couldn’t shake the feeling that her life was about to change forever, and she found herself silently praying that the professor would believe in her discovery as much as she did.</p><p>As the taxi navigated through London’s busy streets, Aria reminisced about her time as Professor Langdon’s student. She remembered the long hours spent in his office, poring over ancient texts and maps, discussing theories about the lost city of Atlantis. She had admired his passion and dedication, even when others dismissed his life’s work as mere obsession. Now, armed with her own discovery, she hoped to rekindle that passion and forge a new path together in the pursuit of the truth.</p><p>As the taxi turned onto a quiet residential street, Aria’s thoughts turned to the challenges that lay ahead. She knew that finding Atlantis would be no easy task, and she couldn’t help but wonder what dangers and obstacles they might encounter along the way. She thought of the artifact in her suitcase, the mysterious map etched onto its surface, and the countless questions that still lingered in her mind. Who had created the artifact, and for what purpose? What secrets did the map hold, and would they be the key to unlocking the mysteries of the lost city?</p><p>As the taxi whisked Aria away from the familiar streets of London, she couldn’t help but wonder what lay ahead. Her mind raced with the potential discoveries she and Langdon might make, and she felt a growing sense of purpose and determination. With each passing mile, Aria grew more certain that she was on the cusp of an incredible adventure - one that would take her beyond her wildest dreams and into the echoes of Atlantis.<br>Chapter 2: The Path Unveiled</p><p>Aria arrived at Professor Langdon’s study, her heart racing with anticipation. She had never seen the inside of his private sanctuary, but she had heard stories of its vast collection of books, artifacts, and maps related to his lifelong obsession with Atlantis. As she stepped inside, she was immediately overwhelmed by the sheer volume of knowledge that surrounded her.</p><p>The walls were lined with shelves overflowing with books, while tables and cabinets displayed an array of artifacts from various ancient civilizations. Maps of all shapes and sizes covered every available surface, and in the center of the room stood a massive wooden desk, buried beneath an avalanche of scrolls and documents.</p><p>“Ah, Aria! You’re here,” Langdon exclaimed, his eyes lighting up as he caught sight of her. “I’ve been dying to see this map you’ve discovered.” His grey hair was disheveled, and his glasses perched precariously on the tip of his nose – a testament to the hours he had clearly spent poring over his research.</p><p>Aria carefully unfurled the ancient parchment, revealing the cryptic markings that had captured her imagination. Langdon leaned in close, his eyes scanning the document with a mixture of excitement and reverence. He traced his fingers over the intricate symbols and lines, his brow furrowed in concentration.</p><p>“This is incredible, Aria,” he whispered, his voice filled with wonder. “I’ve spent my entire life searching for clues to Atlantis’s whereabouts, and this map may very well be the key to finding it.” His passion for the subject was contagious, and Aria felt her own excitement growing. “Then you’ll help me, Professor? We can embark on an expedition to find the lost city?” Langdon nodded, his eyes still locked on the map. “Of course, my dear. I wouldn’t miss this opportunity for the world.”</p><p>—</p><p>As they poured over the map and discussed their plans, Langdon shared stories of previous expeditions in search of Atlantis, highlighting the dangers and excitement that awaited them. He spoke of narrow escapes from treacherous caves, encounters with hostile locals, and the heartbreak of finding only dead ends.</p><p>Langdon’s tales were filled with passion and a hint of sorrow, as he recounted the years of searching and the sacrifices he had made in pursuit of the lost city. He told Aria of a time when he had ventured deep into the Amazon rainforest, following a lead that ultimately proved to be a hoax. He described the treacherous climb up a snow-capped mountain in the Himalayas, only to find an ancient temple with no connection to Atlantis.</p><p>“But there were moments of triumph, too,” he continued, his eyes shining with excitement. “In an underwater cavern off the coast of Greece, I discovered a cache of artifacts that bore striking similarities to those found in other parts of the world. It was as if they were all part of a vast, interconnected civilization - a civilization that could only be Atlantis.”</p><p>Aria listened, enraptured by Langdon’s stories, and she could feel the fire of adventure igniting within her. She knew that the journey ahead would be filled with danger and uncertainty, but the possibility of uncovering the truth about Atlantis made it all worth it.</p><p>As they continued their planning, Langdon shared his knowledge of the various theories surrounding Atlantis, from its supposed location to the incredible technology and magic it was rumored to possess. He showed Aria ancient texts and illustrations that depicted a city of immense beauty and power, a city that held the key to understanding the origins of civilization itself.</p><p>Aria’s mind raced with excitement as she imagined the wonders they might uncover in Atlantis. She envisioned vast libraries filled with the knowledge of an advanced society, magical artifacts that could reshape the world, and perhaps even the answers to the questions that had plagued humanity for centuries.</p><p>But she also knew that there would be dangers lurking around every corner, from the treacherous seas that they would have to navigate to reach Atlantis, to the unknown perils that awaited them within the city itself. She knew that she would have to rely on her resourcefulness, determination, and the expertise of her companions to survive the journey and achieve their goal.</p><p>Langdon sighed, his eyes distant. “Because, Aria, the world needs to know the truth. Atlantis was a civilization unlike any other, and its secrets could change everything we know about history, technology, and magic. If I can find it, I can finally prove that I was right to dedicate my life to this quest.”</p><p>—</p><p>Aria and Langdon spent hours poring over their contacts and research, discussing potential allies who could aid them on their journey. They needed a team of skilled and diverse individuals to face the many challenges they knew lay ahead.</p><p>Langdon shared his thoughts on potential candidates, noting the importance of finding people who were not only skilled in their fields, but also adaptable and resilient. “We’ll need individuals who can think on their feet, who are not easily discouraged by setbacks and dead ends,” he said, his voice filled with determination.</p><p>Aria agreed, considering the various acquaintances she had made during her time as an archaeologist. She recalled the many times she had collaborated with other experts on challenging projects, and how vital it was to have a team that worked well together.</p><p>Together, they began to compile a list of potential team members. They reached out to their contacts, seeking recommendations and exploring the backgrounds of those who were suggested. They knew that time was of the essence, and they could not afford to make any mistakes in their selection.</p><p>As they continued their search, Aria couldn’t help but feel a sense of unease. It seemed like no matter how many people they considered, there was always some flaw or concern that held them back from making a decision. She worried that their quest might be doomed before it even began, but she refused to let her doubts get the better of her.</p><p>Langdon noticed her anxiety and offered a reassuring smile. “It’s natural to feel uncertain, Aria,” he said kindly. “But we must have faith in our judgment and trust that we will find the right people for our journey. Atlantis has eluded discovery for centuries, and we cannot expect to succeed without facing some challenges along the way.”</p><p>Aria nodded, appreciating his support. They continued their search, refining their criteria and reevaluating their choices. As the hours ticked by, their list of potential team members began to take shape.</p><p>“We’ll need a navigator, a linguist, and someone with expertise in ancient technology,” Langdon mused. “And of course, we’ll need someone who knows how to handle themselves in a fight.”</p><p>Aria thought of her recent encounter with Alex Mercer, a man who had impressed her with his resourcefulness and quick thinking. “I may know someone who could help,” she said with a smile.</p><p>—</p><p>A mutual acquaintance introduced Aria to Alex Mercer, a skilled adventurer with a mysterious past. Though initially skeptical of his abilities, Aria was quickly won over when he saved her from a dangerous situation.</p><p>The encounter happened in a crowded marketplace, where a group of thieves had tried to steal Aria’s bag, which contained her research notes and the ancient map. Alex, who happened to be nearby, had noticed the commotion and stepped in to help her. With his agile movements and quick thinking, he managed to retrieve her bag and scare off the thieves.</p><p>“I don’t know how to thank you,” Aria said, still shaken from the ordeal.</p><p>“No need for thanks,” Alex replied with a charming smile. “Just doing what I do best.”</p><p>Intrigued by his confident demeanor and apparent skills, Aria couldn’t help but ask, “What exactly is it that you do, Mr. Mercer?”</p><p>Alex’s grin widened as he responded, “I’m a bit of a jack-of-all-trades, but my specialty lies in getting out of tight spots and navigating dangerous situations.”</p><p>Aria’s eyes sparkled with curiosity. “You sound like someone who could be of great help on our expedition to find Atlantis.”</p><p>Seeing the surprise in his eyes, she quickly filled him in on her quest and the team she and Langdon were assembling. As she spoke, she could see the excitement building in Alex’s expression, mirroring her own.</p><p>When she finished, Alex didn’t hesitate for a moment. “I’ve heard you’re looking for someone with my particular set of skills,” he said with a grin. “Count me in.”</p><p>Aria felt a surge of relief and excitement, knowing they had found a valuable ally in their dangerous journey. With Alex on their side, their chances of success seemed to grow exponentially. And as they shook hands and exchanged contact information, she couldn’t help but feel that fate had brought them together for a reason. The mysteries of Atlantis were waiting to be unveiled, and with their combined skills and determination, Aria knew that they were one step closer to discovering the truth.</p><p>—</p><p>Over dinner, Aria, Langdon, and Alex discussed their upcoming expedition, sharing their unique skills and experiences. As they spoke, they began to form a bond, recognizing each other’s strengths and the potential for a successful mission.</p><p>As the evening wore on, the conversation shifted towards more personal stories. Aria recounted her early fascination with archaeology and her time as Langdon’s student. She shared her hopes and fears for the journey ahead, as well as her determination to prove the existence of Atlantis to the world.</p><p>Langdon, in turn, opened up about his previous expeditions and the various hardships he had faced. He spoke of the many friendships he had formed over the years, and how those connections had sustained him through the most difficult times. He also shared his philosophy on archaeology, emphasizing the importance of preserving history and learning from the past.<br>Alex’s tales of adventure had everyone captivated. He regaled them with stories of his daring exploits in far-off lands, each tale more thrilling than the last. He also spoke of his own search for purpose and belonging, revealing a depth that Aria and Langdon had not expected. As the evening progressed, it became clear that Alex’s resourcefulness and daring would be invaluable assets to the team.</p><p>Throughout the dinner, the camaraderie between the three grew stronger. They laughed together, shared their dreams and aspirations, and even began to develop a shorthand that would serve them well in the days to come. By the time dessert was served, it was clear that they had formed the beginnings of a strong and lasting bond.</p><p>As the evening drew to a close, Aria reflected on the unique combination of skills and experiences that each of them brought to the table. She felt a surge of excitement and anticipation for the journey ahead, as well as a deep sense of gratitude for the opportunity to work with such talented and dedicated individuals.</p><p>“I’ve never been part of a team like this before,” Aria admitted. “But I can’t imagine a better group to embark on this adventure with.”</p><p>—</p><p>Aria and Langdon visited Captain Harlow, the gruff but loyal captain of a ship they hoped would carry them across the ocean in search of the hidden island. As they approached the dock, they could see the ship in question, the Sea Serpent, a sturdy and well-kept vessel that seemed capable of weathering any storm. They negotiated the terms and conditions for the voyage, demonstrating their resourcefulness and determination.</p><p>Captain Harlow eyed them skeptically as they made their case, his grizzled face betraying a hint of curiosity. “I’ve heard a lot of wild tales in my time,” he said, stroking his salt-and-pepper beard. “But the lost city of Atlantis? That’s a new one.”</p><p>Aria met his gaze evenly, her voice steady and confident. “We believe we have the key to finding it, Captain. All we need is a ship and a crew who can get us there.”</p><p>“And what’s in it for me and my crew?” Captain Harlow asked, crossing his arms over his broad chest. “Aside from a potentially fruitless journey into the unknown?”</p><p>Langdon stepped forward, producing a small pouch filled with gold coins. “We can pay, of course,” he said, handing the pouch to the captain. “And should we succeed in finding Atlantis, the knowledge and treasures we uncover could be worth far more than any ordinary cargo.”<br>Captain Harlow weighed the pouch in his hand, considering their offer. He glanced at the determined expressions on Aria and Langdon’s faces, then back to the Sea Serpent. Finally, he nodded his agreement.</p><p>“Very well,” Captain Harlow agreed, his eyes narrowing as he sized them up. “You’ve got yourself a ship, but I warn you – the sea is a fickle mistress, and this journey won’t be an easy one.”</p><p>Aria and Langdon exchanged a determined glance. They knew the risks, but the potential rewards were too great to ignore. Together, they would set out on the adventure of a lifetime, chasing the echoes of a lost civilization and the promise of untold wonders.</p><p>—</p><p>The team set out to gather the necessary supplies and equipment for their expedition, making their way through the bustling markets and specialized shops that lined the city streets. Aria took the lead, her knowledge of archaeology and her meticulous research guiding their choices. Each item they collected felt like another piece of the puzzle falling into place, bringing them one step closer to uncovering the lost city of Atlantis.</p><p>Aria, Langdon, and Alex wandered through the lively marketplace, visiting a variety of stalls and shops in search of the items they needed. Aria guided them to a shop that specialized in ancient maps, where she purchased parchment, ink, and a set of cartography tools. Meanwhile, Langdon’s eyes were drawn to an old bookseller, where he found a rare tome on ancient maritime navigation techniques that could prove invaluable on their journey.</p><p>Next, they visited a merchant who dealt in exotic artifacts and unusual technology, which might be useful in deciphering the mysteries of Atlantis. Here, Alex selected a set of finely crafted lock-picks, useful for bypassing ancient security mechanisms or escaping from tight spots. He also picked up a compact grappling hook, which he expertly tested on a nearby rooftop.</p><p>The team then made their way to a blacksmith who crafted custom weapons and armor. With the knowledge that danger could be lurking around any corner, they each chose weapons that best suited their individual skills. Aria chose a lightweight, yet sturdy sword, designed for agility and speed. Langdon opted for a crossbow, perfect for long-range attacks, while Alex selected a pair of curved daggers, perfectly balanced for close-quarters combat.</p><p>Their next stop was an apothecary, where they stocked up on various herbs and potions that could aid them on their journey. Aria carefully chose a selection of healing remedies and antidotes, as well as ingredients for making her own salves and poultices. Langdon, on the other hand, focused on acquiring a collection of rare and powerful poisons, just in case they found themselves in a situation where subtlety was required.</p><p>Finally, they visited a general store to gather essential supplies like rope, torches, and tents. They also picked up a collection of dried food and provisions, ensuring they would have enough to eat during their long voyage across the ocean. As they loaded the final crate of supplies onto Captain Harlow’s ship, Aria turned to Langdon and Alex with a satisfied smile. “We can’t afford to leave anything to chance,” she told them. “The slightest mistake could mean the difference between success and failure.” With their preparations complete, the team was ready to face whatever challenges awaited them on their journey to Atlantis.</p><p>—</p><p>Aria and Langdon invited Alex to join them in their study, where they introduced him to their extensive research on Atlantis. The room was filled with stacks of books, scrolls, and ancient texts, each one containing valuable insights into the mysterious city’s location, culture, and possible reasons for its disappearance.</p><p>As they sat down, Langdon began to share some of his most intriguing findings. “Throughout history, there have been countless theories about the location of Atlantis,” he explained, pointing to a wall covered in maps. “Some believe it was in the Mediterranean, others in the Caribbean, and still others in the middle of the Atlantic Ocean. But no one has ever been able to pinpoint its exact location.”</p><p>Alex listened intently, his eyes scanning the various maps and charts on display. “What makes this map different?” he asked, gesturing to the ancient parchment that Aria had discovered.</p><p>Aria chimed in, her excitement evident. “This map contains symbols and markings that have never been seen before, some of which correspond to known ancient languages. It’s as if it’s providing us with clues that have been hidden in plain sight for centuries.”</p><p>Langdon nodded, his eyes gleaming with enthusiasm. “And not only that, but we’ve also found references to advanced technology and magic that could only have come from Atlantis itself. The more we learn, the more convinced we become that this map is the real deal.”</p><p>Together, they delved deeper into the research, discussing theories on the culture of Atlantis and the reasons for its disappearance. Aria shared her thoughts on how the city might have been powered by magical crystals, while Langdon talked about the possibility of advanced technology that could have been lost to time.</p><p>As they continued to share their findings, Alex found himself captivated by the wealth of knowledge and expertise in the room. He was no stranger to adventure and danger, but the prospect of discovering a lost city filled with ancient secrets was something he couldn’t resist.</p><p>Throughout their discussion, they also touched on the potential dangers they could face on their journey. From treacherous seas and storms to hostile locals and hidden traps, they knew that their quest would not be an easy one. But with each challenge they discussed, they also found a way to prepare themselves and their team, ensuring that they would be ready for anything they might encounter.</p><p>Alex’s eyes shone with excitement as he absorbed the wealth of information before him. “Together, we might just have the knowledge and skills to finally uncover the lost city,” he mused. “I can’t wait to see what we find.” With their combined expertise, the team felt more confident than ever that they could solve the mystery of Atlantis and bring its secrets to light.</p><p>—</p><p>As the day of their departure drew near, Aria felt a growing sense of anticipation and unease. She couldn’t shake the feeling that something extraordinary was about to happen, and it was only a matter of time before she discovered what it was. That night, as she lay in bed trying to quiet her restless thoughts, she experienced her first vision of Queen Neria, the wise and courageous ruler of Atlantis.</p><p>As Aria drifted off to sleep, she found herself standing in a beautiful garden filled with lush, exotic plants and shimmering pools of water. The air was warm and fragrant with the scent of flowers, and she could hear the gentle sound of water cascading in the distance. A sense of peace and tranquility washed over her, easing her worried mind.</p><p>In the midst of the garden, a majestic figure appeared. A woman of regal bearing, dressed in flowing robes of silver and blue, her dark hair adorned with a jeweled circlet. Aria immediately recognized her as Queen Neria, though she had never seen her before. The queen’s eyes were kind but held a deep wisdom, and Aria felt drawn to her presence.</p><p>“Welcome, Aria,” Queen Neria said, her voice melodious and soothing. “I have been waiting for you.”</p><p>Aria found herself unable to speak, her heart racing with a mix of awe and fear. “Why have you come to me?” she finally managed to stammer.</p><p>Queen Neria smiled gently. “I have chosen you, Aria, to help uncover the secrets of my lost city. You possess a strong spirit and a keen mind, and your destiny is intertwined with that of Atlantis.”</p><p>“But how can I possibly find Atlantis?” Aria asked, her voice trembling. “And why me?”</p><p>“Have faith in yourself and in the path that has been laid before you,” the queen replied. “You are not alone in this journey. Trust in your companions, for they too have been chosen for this quest. Together, you will discover the truth and bring light to the shadows of the past.”</p><p>As Queen Neria spoke, the garden began to fade away, her figure growing more and more transparent. “Remember my words, Aria,” she whispered as she disappeared completely. “The fate of Atlantis is in your hands.”</p><p>Aria awoke with a start, her heart pounding in her chest as she tried to make sense of the vivid dream. “I don’t know what it means,” she confided in Langdon the next morning, “but I can’t help but feel that we’re being guided by something greater than ourselves.” Though they couldn’t yet understand the significance of Aria’s vision, it added another layer of intrigue to their already mysterious quest.</p><p>—</p><p>With their plans in place and their team assembled, Aria, Langdon, and Alex stood on the deck of Captain Harlow’s ship, ready to embark on their expedition to find the lost city of Atlantis. The sun was just beginning to rise, casting a warm golden light over the bustling harbor as they exchanged a look, their excitement and determination clear in their eyes.</p><p>The crew of the Sea Serpent busied themselves with preparations for departure, securing ropes and adjusting sails, their voices carrying across the salty air. Aria couldn’t help but feel a mixture of excitement and trepidation as she watched the bustling harbor, knowing that it might be the last time she saw familiar shores for quite some time.</p><p>Langdon approached her, a reassuring smile on his face. “Remember, Aria, we have each other, and we have our combined knowledge and skills. Together, we can face whatever challenges lie ahead.”</p><p>Aria nodded, drawing strength from her mentor’s words. She glanced over at Alex, who was talking animatedly with Captain Harlow, the two men already forming a rapport. She knew they would need to rely on each other in the coming days, and she was grateful for the bond they were already forming.</p><p>As the crew finished their preparations, the ship’s sails billowed with the wind, the vessel slowly beginning to pull away from the dock. Aria, Langdon, and Alex gathered at the ship’s railing, watching as the harbor began to recede in the distance.</p><p>“Here’s to new horizons,” Alex said, raising an imaginary glass in toast. “And to the mysteries we’re about to uncover.”</p><p>Aria and Langdon echoed his sentiment, their eyes fixed on the horizon as they sailed towards the unknown. They knew the journey would be fraught with danger, but they were more than ready to face whatever lay ahead.</p><p>“Let’s find Atlantis,” Aria declared, her voice filled with resolve. As the ship pulled away from the dock, their journey to uncover the secrets of the ancient civilization had only just begun. And with each passing day, the mysteries of Atlantis would draw them deeper into a world of magic, danger, and wonder, testing their courage, their wits, and the bonds they forged with one another.<br>Chapter 3: Trials of the Sea</p><p>Aria, Langdon, and Alex Mercer stood on the deck of Captain Harlow’s ship, the Sea Serpent, as it set sail across the vast ocean. The wind whipped through their hair, and the salty spray of the sea filled their nostrils. As the shoreline disappeared behind them, the three adventurers exchanged a mix of excited and anxious glances, each contemplating the uncertainties that lay ahead.</p><p>The ship was bustling with activity, as the crew hoisted sails and shouted orders in response to Captain Harlow’s gruff commands. Aria watched in awe as the intricate dance unfolded, the crew working in perfect harmony to propel the ship forward. She could feel the anticipation building inside her, the promise of discovery and adventure fueling her excitement.</p><p>Langdon, ever the scholar, had already buried himself in a book, his eyes darting back and forth as he absorbed the knowledge within. Aria knew that he was seeking any additional information on Atlantis that he could find, hoping to uncover even the smallest clue that would aid them on their journey.</p><p>Alex, on the other hand, had taken to the ship like a fish to water, eagerly helping the crew with various tasks and chatting animatedly with the sailors. His easygoing demeanor and quick wit had quickly endeared him to the crew, and Aria couldn’t help but feel a pang of envy at his effortless charm.</p><p>As the sun began to set, casting a warm glow across the deck, the three adventurers gathered together, each lost in their thoughts about the journey ahead. They knew that the path to Atlantis would be fraught with danger and challenges, but they were determined to succeed, driven by their shared passion for uncovering the lost city’s secrets.</p><p>“What do you think we’ll find out there?” Aria asked her companions, her eyes scanning the seemingly endless horizon.</p><p>“Truthfully, I have no idea,” Langdon admitted. “But I’m hoping it’s something that will change the course of history.”</p><p>—</p><p>Several days into their journey, the sky darkened, and the once calm waters grew increasingly violent. The ship rocked wildly as towering waves crashed against its sides. Captain Harlow barked orders at his crew, who scrambled to secure sails and ropes.</p><p>The rain poured down relentlessly, drenching everyone on deck as they struggled to maintain control of the ship. Aria clung to the railing, her knuckles turning white as she tried to steady herself against the violent rocking. Lightning illuminated the menacing clouds above, followed by the deafening boom of thunder that shook her to her core.</p><p>Alex, with his years of experience at sea, moved swiftly and confidently across the deck, assisting the crew as they wrestled with tangled ropes and flapping sails. Langdon, though less experienced, threw himself into the fray, his passion for their quest driving him to overcome his fears and help where he could.</p><p>The ship creaked and groaned, threatening to give way under the relentless assault of the storm. Aria, Langdon, and Alex locked eyes for a moment, acknowledging the gravity of the situation and their unspoken commitment to see it through together. With renewed determination, they set about securing any loose cargo and helping the crew members who had been injured in the chaos.</p><p>Captain Harlow, his face a mask of grim determination, navigated the ship through the towering waves, narrowly avoiding being capsized by the powerful swells. The crew, exhausted but resilient, fought on through the night, their every muscle aching from the effort.</p><p>As the storm raged around them, the team found solace in their shared struggle, their previous rivalries and tensions washed away by the unrelenting rain. They had faced their first true test as a team, and though the storm showed no signs of abating, they knew they could rely on one another to face whatever challenges lay ahead.</p><p>Aria, Langdon, and Alex joined in the efforts, working together to help the crew navigate through the treacherous storm. Their adrenaline surged as they fought to keep the ship afloat, and in the midst of the chaos, their bond as a team solidified.</p><p>—</p><p>As the storm finally cleared, the team took a moment to recover and assess the damage. Aria found Alex on the deck, staring out at the calmer waters, his clothes still soaked from the downpour.</p><p>“That was intense,” Aria said, joining him at the railing. “I didn’t expect our journey to be so dangerous so soon.”</p><p>“Sometimes the sea has a way of testing you,” Alex replied, offering her a tired smile. “But we made it through. We’ve got a strong crew, and we’ve got each other.”</p><p>They stood in silence for a moment, watching as the crew members moved about the deck, repairing the damage from the storm. Aria noticed how the storm had left a few bruises on Alex’s face, yet he seemed unfazed by it. She found herself drawn to his resilience and the sense of security she felt around him.</p><p>“Hey, I wanted to thank you,” Aria said, breaking the silence. “During the storm, when that rope snapped and almost hit me, you were there to pull me out of the way.”</p><p>Alex’s cheeks reddened slightly as he remembered the moment. “It was nothing, really. I just happened to be in the right place at the right time.”</p><p>“But you still saved me,” Aria insisted, giving him a grateful smile. “And I’m glad you were there.”</p><p>The sound of Captain Harlow’s voice boomed across the deck, ordering the crew to finish the repairs and prepare to set sail again. Alex and Aria exchanged glances, knowing that their journey was far from over.</p><p>“I’ve got a feeling we’re going to face even more challenges ahead,” Alex said, his eyes meeting Aria’s. “But whatever comes our way, I know we’ll be able to handle it together.”</p><p>Aria felt a flutter in her chest as she looked into his eyes, the sincerity in his words giving her a sense of comfort and trust she hadn’t expected to find in this adventure. “You’re right,” she agreed. “As long as we stick together, we can face anything.”</p><p>Aria returned the smile, feeling a warm connection to her companion. She couldn’t deny the growing attraction she felt for him, and she wondered if he felt the same.</p><p>—</p><p>Several more days passed without incident, but the peace was short-lived. A sudden shout from the crow’s nest alerted the crew to an approaching ship, its black flag signaling the danger they faced: pirates!</p><p>Aria’s heart raced as the pirate ship closed in on them. Captain Harlow bellowed orders to his crew, preparing them for the imminent battle. Aria, Langdon, and Alex exchanged determined looks, knowing that their skills and teamwork would be put to the test once more. They gathered their weapons and took their positions, ready to defend the Sea Serpent with everything they had.</p><p>The pirates launched grappling hooks onto the ship, attempting to board and overtake the vessel. As the first wave of pirates swung onto the deck, Aria, Langdon, and Alex sprang into action. Aria used her agility and resourcefulness to evade the pirates’ attacks, while Langdon put his knowledge of ancient combat techniques to good use. Alex, fearless and daring, took on multiple adversaries at once, his experience in dangerous situations evident in his every move.</p><p>The trio fought side by side, their combined efforts proving to be a formidable force against the pirates. As Aria took down one attacker with a swift strike to the legs, she saw Alex grappling with a particularly large and menacing pirate. With a nod of gratitude, he disarmed the pirate and sent him tumbling overboard.</p><p>Langdon, meanwhile, used his intellect and quick thinking to outwit his opponents, luring them into traps and using their own weapons against them. His unconventional methods caught the pirates off guard, giving him the upper hand in the skirmish.</p><p>Despite their ferocity and ruthlessness, the pirates found themselves outmatched by the united front of Aria, Langdon, and Alex. One by one, they retreated back to their ship, nursing their wounds and cursing the adventurers who had bested them. As the last of the pirates retreated, Aria, Langdon, and Alex caught their breath, their hearts pounding from the adrenaline of their victory. The crew cheered and congratulated them, grateful for their fearless defense.</p><p>The team exchanged proud and relieved smiles, knowing they had proved their worth and earned their place among the crew. But they also knew that the challenges ahead would only grow greater, and they had to remain vigilant.</p><p>—</p><p>While exploring the ship’s cargo hold, Aria and Langdon stumbled upon a frightened young man hiding behind crates. They soon discovered that he was an expert in ancient languages and had stowed away to escape his mundane life.</p><p>The young man introduced himself as Thomas, and as he hesitantly stepped out from his hiding place, it was evident that he had been living off the ship’s supplies for quite some time. He was skinny, with disheveled hair and dirt-streaked clothes. Despite his appearance, his eyes shone with intelligence and curiosity.</p><p>Aria and Langdon exchanged a glance, unsure of what to do with their unexpected guest. However, when Thomas overheard them discussing the map, he eagerly offered his assistance. He explained that he had spent years studying ancient languages and deciphering forgotten texts, and he felt confident that he could help them with their quest.</p><p>The two archaeologists were initially skeptical, but they soon realized that Thomas’s skills were genuine. He quickly made sense of the cryptic symbols on the map, revealing new information about their destination and the potential dangers they might face. As the team listened to his explanations, they began to see the value of having him on board.</p><p>Captain Harlow was less enthusiastic about the stowaway’s presence, but he reluctantly agreed to let Thomas stay after witnessing the contributions he made to their mission. In return for his assistance, Thomas was expected to help with the ship’s daily tasks and prove his worth as a member of the crew.</p><p>Over time, Thomas became a valuable member of the team, sharing his knowledge of ancient languages and symbols with Aria, Langdon, and Alex. As they continued to uncover more about the mysterious island and its potential connection to Atlantis, Thomas’s expertise became increasingly important.</p><p>He also formed a close friendship with Aria, bonding over their shared love of ancient history and their determination to uncover the truth about Atlantis. As they worked together, Aria found herself growing more and more fond of the stowaway, appreciating his intelligence, wit, and passion for his work.</p><p>As the stowaway shared his knowledge of ancient languages and symbols, Aria, Langdon, and Alex couldn’t help but feel that their journey was destined to be filled with surprising twists and unexpected allies. With each revelation, the map seemed to become clearer, and their resolve to find Atlantis only grew stronger. The team welcomed the stowaway into their ranks, grateful for his assistance and intrigued by the possibilities that lay ahead.</p><p>—</p><p>Realizing that the challenges they had faced so far were only the beginning, the team decided to teach each other their unique skills to better prepare for the journey ahead.</p><p>Over the course of several days, the Sea Serpent was transformed into a makeshift training ground, as Aria, Langdon, and Alex took turns sharing their expertise with one another. Aria was eager to learn from her companions, impressed by the breadth of their knowledge and skills.</p><p>One afternoon, Aria watched as Langdon passionately explained the intricacies of ancient languages to Alex. She could see the excitement in her mentor’s eyes as he shared his lifelong passion, and she couldn’t help but feel inspired by his dedication.</p><p>In another training session, Alex demonstrated his agility and dexterity as he scaled the ship’s rigging with ease, showing Aria and Langdon how to quickly and safely navigate the ropes. Aria admired Alex’s fearlessness and determination, qualities that would undoubtedly serve them well in their quest.</p><p>The team also made use of the unique weapons and tools they had gathered for their journey, practicing their handling and learning how to effectively use them in combat. Aria found herself particularly drawn to a set of intricately carved throwing knives, their balance and precision a perfect fit for her steady hand.</p><p>As they trained together, Aria, Langdon, and Alex formed a strong bond, their shared experiences and challenges forging a deep sense of trust and camaraderie. They laughed and teased each other, lightening the mood and making the long days at sea more enjoyable.</p><p>Even the ship’s crew and the stowaway, Thomas, joined in the training sessions from time to time. Captain Harlow, initially skeptical of the team’s abilities, began to show a grudging respect for their determination and resourcefulness. Thomas, eager to prove his worth, quickly became a valuable member of the team, his knowledge of ancient languages complementing Langdon’s expertise.</p><p>As their training sessions came to an end, Aria, Langdon, and Alex felt more prepared and confident than ever. Each had gained a newfound respect for the others’ skills, and they knew they could rely on one another when facing the dangers ahead. The bond they had forged would be their greatest strength as they continued their journey, and they were eager to put their combined talents to the test.</p><p>—</p><p>The ocean threw another challenge their way when the Sea Serpent encountered a massive whirlpool, its powerful currents threatening to drag the ship to the depths. The crew stared in horror at the swirling vortex, its gaping maw threatening to swallow them whole. Captain Harlow quickly barked orders to his crew, attempting to steer the ship clear of the deadly whirlpool.</p><p>Aria, Langdon, and Alex sprang into action, their previous experiences on the journey having prepared them for such dire situations. Aria recalled a technique she had read about in one of Langdon’s books, which involved using the ship’s sails to generate additional lift and counteract the downward pull of the whirlpool.</p><p>As Aria relayed the idea to Captain Harlow, Langdon and Alex worked together to gather the necessary ropes and materials to implement the plan. The crew, now accustomed to the trio’s resourcefulness and quick thinking, eagerly followed their instructions.</p><p>With time running short and the ship inching closer to the whirlpool’s deadly grasp, Aria, Langdon, and Alex climbed the ship’s rigging to adjust the sails. The wind howled around them, making their task even more difficult, but they refused to let fear take hold.</p><p>Beneath them, the crew strained at the ropes, guided by Captain Harlow’s steady commands. The Sea Serpent began to rise, fighting against the powerful pull of the whirlpool. Inch by inch, they gained ground, their determination and teamwork pushing them forward.</p><p>As the ship finally reached the edge of the whirlpool’s grasp, a collective cheer erupted from the crew. The danger had been averted, thanks to the combined efforts of the team and the sailors on board.</p><p>As the Sea Serpent sailed away from the whirlpool, the team breathed a collective sigh of relief. They had faced yet another trial and emerged victorious, their teamwork and quick thinking saving the ship from certain doom. With each challenge overcome, their determination and confidence grew, and the dream of discovering Atlantis seemed more attainable than ever before.</p><p>—</p><p>As the journey continued, Aria found herself plagued by vivid dreams of Queen Neria. In these dreams, the ancient queen showed Aria glimpses of Atlantis and hinted at its hidden secrets. The visions were so realistic that they left Aria feeling like she had truly been in the presence of the ancient queen, as if she had been transported back in time to the fabled city itself.</p><p>Night after night, Aria’s dreams took her to different parts of Atlantis, each one more beautiful and enchanting than the last. She saw towering crystal spires that shimmered in the sunlight, lush gardens filled with exotic plants, and advanced machines powered by the mysterious Atlantean crystals. She also caught glimpses of the city’s inhabitants, who appeared to be a diverse and harmonious people, united by their shared love of knowledge and discovery.</p><p>In her dreams, Queen Neria spoke to Aria in a gentle yet commanding voice, sharing cryptic hints about the city’s history and its powerful magic. She told Aria of the great responsibility that came with this knowledge and warned her of the dangers that would inevitably arise from those who sought to misuse it. Queen Neria also hinted at a deeper connection between herself and Aria, a bond that seemed to transcend time and space.</p><p>Aria began to look forward to her nightly encounters with Queen Neria, finding solace in the wisdom and guidance she provided. Each day, she eagerly recounted her visions to Langdon and Alex, who listened with rapt attention, captivated by the tantalizing glimpses of the lost city. Langdon, in particular, was fascinated by the historical details that Aria shared, speculating on the implications of this new information and how it might help them in their quest.</p><p>Alex, meanwhile, was more intrigued by the mysterious bond between Aria and Queen Neria, wondering if it was a manifestation of some latent magical ability within Aria. He encouraged her to explore this connection further, believing that it could prove invaluable in their search for Atlantis.</p><p>As Aria shared her dreams with Langdon and Alex, she couldn’t help but feel a sense of awe and wonder at the mysterious connection she seemed to have with Queen Neria. The visions only fueled her curiosity and determination to find Atlantis, and her companions were equally captivated by the tantalizing glimpses of the lost city. Together, they vowed to follow the clues and uncover the truth, whatever it may be.</p><p>—</p><p>Just when they thought their trials at sea might be over, the team faced their greatest challenge yet. A massive sea creature, its scales glistening in the sunlight and its serpentine body undulating beneath the waves, attacked their ship, intent on dragging it to the ocean floor.</p><p>As the creature’s enormous tail slammed into the side of the Sea Serpent, the impact sent shockwaves throughout the vessel. Aria, Langdon, and Alex raced to the ship’s side, their eyes widening in awe and terror at the sight of the leviathan.</p><p>“By Poseidon’s beard,” Captain Harlow muttered, his voice filled with disbelief. “I’ve never seen a beast like this in all me years at sea.”</p><p>The creature roared, a sound that shook the very air around them, and began to wrap its powerful body around the ship. The wooden hull creaked ominously under the pressure, and the crew scrambled to arm themselves with whatever weapons they could find.</p><p>Aria, Langdon, and Alex exchanged a determined glance, knowing that it was up to them to save the ship and the lives of those aboard. As the creature tightened its coils, they sprang into action, each using their unique abilities to combat the monstrous foe.</p><p>Aria, her heart pounding in her chest, clambered up the rigging to reach the creature’s head. Her keen intellect and knowledge of marine biology allowed her to identify its weak spots, which she relayed to her companions below.</p><p>Langdon, drawing on his vast historical knowledge, recalled tales of ancient sea monsters and the strategies used by heroes of old to defeat them. Armed with this wisdom, he devised a plan to weaken the creature’s grip on the ship.</p><p>Meanwhile, Alex’s daring and quick thinking led him to dive into the water, armed with a makeshift spear. With the grace of an underwater dancer, he struck at the creature’s vulnerable underbelly, dodging its snapping jaws and thrashing tail.</p><p>With each strike and maneuver, the creature’s grip on the Sea Serpent began to weaken. As Aria and Langdon continued to direct the crew in their efforts, Alex delivered a final, powerful blow, forcing the beast to release its hold on the ship and retreat into the depths.</p><p>As the sea creature disappeared beneath the waves, the crew erupted in cheers, celebrating their victory against the mighty foe. Aria, Langdon, and Alex climbed back on board, soaked and exhausted but triumphant.</p><p>With the colossal sea creature defeated and the ship saved from a watery demise, the team took a moment to marvel at their incredible accomplishment. They had faced the unknown and emerged victorious, their teamwork, skills, and determination proving to be a force to be reckoned with. As they tended to their wounds and repaired the ship, they knew that their journey was far from over – but they were more ready than ever to face whatever lay ahead.</p><p>—</p><p>Exhausted but triumphant, the team gathered on the deck, their spirits buoyed by their shared victories. They had faced danger, adversity, and the unknown together, emerging as a cohesive and formidable team.</p><p>Aria, Langdon, and Alex took a moment to catch their breath and tend to their wounds, with Captain Harlow and his crew offering their gratitude and admiration for the trio’s valiant efforts in defending the ship. It was clear now that they could rely on one another and had earned each other’s trust, a bond that would undoubtedly be crucial in the trials to come.</p><p>As the sun began to set, the team joined Captain Harlow and the crew for a celebratory meal in the ship’s galley. The atmosphere was filled with camaraderie and laughter, as everyone shared stories of their past adventures and discussed the mysterious island that now lay ahead. Aria, Langdon, and Alex also took this opportunity to learn more about their fellow travelers, finding that each member of the crew had a unique tale to tell and skills that would be invaluable in their quest for Atlantis.</p><p>After dinner, the trio retreated to their quarters to prepare for the challenges that awaited them on the island. Aria, Langdon, and Alex each spent time reviewing the cryptic map, speculating on its meaning and the secrets that it might reveal. They also took inventory of their supplies and equipment, making sure they were well-equipped for whatever dangers they might encounter.</p><p>As they made their final preparations, Aria confided in Langdon about her growing feelings for Alex, seeking her mentor’s guidance and wisdom. Langdon, ever the caring father figure, reminded her of the importance of focusing on their mission, but also encouraged her to follow her heart. He told her the story of how he met his own wife during an archaeological expedition, a tale that resonated with Aria and reassured her that love could indeed flourish in the most unexpected of places.</p><p>Meanwhile, Alex took some time to reflect on his own emotions and the undeniable connection he felt with Aria. He had been a loner for most of his life, always seeking the next adventure and never allowing himself to form lasting bonds. But now, as he stood on the precipice of their greatest challenge yet, he found himself contemplating the possibility of a future with Aria by his side.</p><p>As the sun disappeared below the horizon, casting the ship in a soft, warm glow, the trio reconvened on the deck, their eyes fixed on the silhouette of the mysterious island. They knew that the next leg of their journey would be fraught with danger and uncertainty, but they also knew that they were stronger together than they ever could be alone.</p><p>As the mysterious island loomed ever closer, the anticipation and excitement aboard the Sea Serpent were palpable. Aria, Langdon, and Alex stood side by side on the deck, their eyes fixed on the enigmatic shores that awaited them. They had faced trials and dangers, forged an unbreakable bond, and honed their skills for the challenges ahead. As the ship made its final approach, they knew that their journey had only just begun – and they were eager to uncover the hidden secrets of Atlantis, whatever the cost.<br>Chapter 4: The Dreamtide Beckons</p><p>Aria tossed and turned in her narrow bunk aboard The Sea Serpent, the ship’s gentle rocking unable to lull her into a peaceful slumber. As her eyelids finally grew heavy, she was suddenly plunged into a vivid, otherworldly dream, where she found herself standing in a breathtaking underwater palace.</p><p>The grand chamber in which Aria stood was enveloped by opalescent walls adorned with intricate murals depicting a vibrant, thriving city, unlike anything she had ever seen. The palace seemed to be alive, with columns of shimmering light pulsating rhythmically and illuminating the vast space. Aria’s eyes widened as she gazed at the countless varieties of colorful coral and swaying seaweed that decorated the room, while schools of iridescent fish danced around her as if welcoming her to their realm.</p><p>As Aria marveled at the beauty of her surroundings, her attention was drawn to a majestic throne carved from a single piece of radiant crystal. Seated upon it was a woman of extraordinary grace and beauty, her long, flowing hair as blue as the deepest ocean. She wore a resplendent gown crafted from the softest kelp and adorned with pearls and seashells, and around her neck hung a pendant that seemed to emanate a power beyond Aria’s comprehension.</p><p>The woman’s eyes met Aria’s, and in that moment, she knew she was in the presence of Queen Neria, the wise and courageous ruler of Atlantis. With a voice as soothing as the gentle lapping of waves, Neria addressed Aria, speaking of a great destiny that awaited her. She spoke of the latent power within Aria, a power that would be essential in unlocking the hidden secrets of Atlantis and protecting it from the forces of darkness that threatened its existence.</p><p>Aria listened intently, her heart pounding in her chest as she tried to make sense of the queen’s words. She felt an inexplicable connection to the queen, as if her very soul was intertwined with that of the ancient monarch. As Queen Neria spoke, Aria caught glimpses of the wonders of Atlantis, its advanced technology and powerful magic that surpassed even her wildest dreams.</p><p>Aria stared at Queen Neria, the ethereal monarch’s eyes holding a depth of wisdom and power that left her awestruck. With a cryptic smile, Neria whispered enigmatic words about Atlantis and its hidden secrets before slowly fading away, leaving Aria alone in the rapidly dissipating dream.</p><p>—</p><p>Aria jolted awake, her heart pounding in her chest as she struggled to make sense of the vivid dream that had gripped her mind. As she sat up in her bunk, she couldn’t shake the feeling that the vision held a deeper meaning, intertwined with their quest for Atlantis.</p><p>The dim light of the lantern swung gently in the cabin as the ship rocked back and forth on the waves. Aria’s thoughts raced, as she tried to piece together the fragments of her dream. She remembered the breathtaking beauty of the underwater palace and the wise, yet enigmatic gaze of Queen Neria. The queen’s words echoed in Aria’s mind, “Seek the truth that lies beneath the surface, and you shall find your destiny.”</p><p>Aria’s fingers absentmindedly traced the edges of the ancient map on her bedside table, its symbols and markings seeming to call out to her. She wondered if her visions were merely the byproduct of her imagination, fueled by her obsession with the lost city, or if they were a form of guidance from Queen Neria herself. The idea that she was connected to Atlantis in such a profound way both excited and unnerved her.</p><p>She felt a strange sense of longing, as if a part of her had always belonged to the mythical city. As she reflected on her life, Aria realized that her passion for archaeology had always been driven by a desire to uncover the hidden truths of the past. She wondered if her pursuit of Atlantis was the culmination of her life’s work, a destiny that had been calling to her since childhood.</p><p>Aria’s mind wandered to her companions, Langdon and Alex, and she questioned how they might react to her dream. Would they dismiss it as a mere fantasy, or would they see the potential significance in her visions? She knew that sharing her experience with them was a risk, but she couldn’t shake the feeling that her dream held the key to unlocking the secrets of Atlantis.</p><p>She recalled the challenges they had faced thus far on their journey, from the violent storm that had tested their resilience to the pirates they had bravely fought off. Aria felt a strong bond with Langdon and Alex, and she trusted them implicitly. She realized that if they were to succeed in their quest, they would need to rely on each other completely and share even their most private thoughts and fears.</p><p>With her curiosity piqued and her thoughts racing, Aria knew she needed to share her dream with her companions. She rose from her bunk, determination in her stride as she sought out Langdon and Alex, eager to unravel the mystery of Queen Neria’s haunting message.</p><p>—</p><p>Aria found Langdon and Alex on the ship’s deck, the sun just beginning to peek over the horizon. She hesitated for a moment, unsure of how they would react to her strange dream, before taking a deep breath and recounting her encounter with Queen Neria.</p><p>As Aria spoke, her voice trembled with emotion, relaying the details of her dream - the underwater palace, the hauntingly beautiful queen, and her cryptic words about Atlantis and its hidden secrets. Langdon listened intently, his eyes narrowed in thought, while Alex leaned casually against the railing, his expression inscrutable.</p><p>When Aria finished her story, the silence that followed felt heavy and charged with anticipation. Langdon finally broke the silence, his voice filled with excitement. “Aria, this is extraordinary! I believe your dream may be more than just a coincidence. It could be a sign that we are on the right path, and that Atlantis is close at hand.”</p><p>Alex, however, seemed less convinced. He crossed his arms and looked out at the ocean, a contemplative expression on his face. “It’s possible,” he said slowly, “but it could also be your mind’s way of processing everything we’ve been through. The quest for Atlantis has consumed us for weeks, and it’s only natural for it to seep into your subconscious.”</p><p>Aria considered both their interpretations, torn between the hope that her dream was a true vision and the fear that it might be a mere manifestation of her own desires. She shivered slightly, her gaze fixed on the horizon as if searching for answers.</p><p>As they stood together, each lost in thought, Aria felt a mixture of comfort and unease in their differing interpretations. Langdon’s belief that they were getting closer to Atlantis was encouraging, but Alex’s suggestion that her dream could be a manifestation of her subconscious fears and desires left her unsettled.</p><p>—</p><p>Days passed as The Sea Serpent drew closer to the mysterious island, and Aria couldn’t shake her growing anxiety. The dream of Queen Neria lingered in her thoughts, stirring a deep sense of foreboding that made her question the true nature of their journey.</p><p>As the island grew larger on the horizon, Aria spent more and more time on the deck, her gaze fixed on their destination. Alex often joined her, and they would talk about their past adventures, sharing stories to distract her from her troubled thoughts. One evening, as the sun dipped below the horizon, painting the sky in shades of orange and purple, Aria finally opened up to Alex about her concerns.</p><p>“I can’t help but feel like we’re being drawn towards something much greater than we realize,” she confessed. “These dreams, Queen Neria’s warnings, it all feels like it’s leading to a momentous discovery, but at what cost?”</p><p>Alex nodded solemnly, understanding her apprehension. “We’ve faced challenges together before, Aria,” he reminded her. “Whatever we find on that island, we’ll face it head-on, as a team. Besides, you have a strength within you that I’ve never seen in anyone else. I have no doubt that you’re meant for something extraordinary.”</p><p>Moved by Alex’s words, Aria took a deep breath and allowed herself to absorb his faith in her. She knew he was right; they had overcome many trials together, and their bond was stronger for it.</p><p>Meanwhile, Langdon immersed himself in his research, poring over ancient texts and scrolls in his cabin. He was particularly intrigued by the dream Aria had described, believing it held clues to the secrets of Atlantis. He shared his thoughts with Aria and Alex over dinner one night, his excitement palpable.</p><p>“Queen Neria’s messages in your dreams seem to be guiding us towards something monumental, Aria,” Langdon said, his eyes gleaming with anticipation. “I believe that the island we’re approaching holds the key to unlocking the mysteries of Atlantis. We must prepare ourselves for what we may discover there.”</p><p>The trio spent the following days studying the map, the ancient texts, and discussing potential strategies for navigating the island’s unknown terrain. They trained together, honing their skills and abilities, readying themselves for any challenges they might face.</p><p>Despite their preparations, Aria’s unease persisted. She found herself wandering the ship late at night, unable to sleep, her thoughts consumed by the dreamtide’s beckoning call. The crew, sensing her anxiety, began to share whispered concerns amongst themselves, wondering if their voyage was doomed to end in disaster.</p><p>Thomas, the young stowaway who had become a valuable member of their team and a close friend to Aria, tried to lift her spirits. “We’ve come so far, Aria,” he said with a smile. “We can’t give up now. Whatever’s waiting for us on that island, we’ll face it together, and we’ll come out stronger on the other side.”</p><p>Langdon and Alex offered reassurances, their steadfast confidence a balm to Aria’s fears. Yet, as the island loomed closer on the horizon, Aria couldn’t help but feel that her vision held a hidden truth, one that would soon be revealed.</p><p>—</p><p>As the ship dropped anchor just off the island’s shore, the team began to gather their equipment, their excitement and anticipation palpable. Aria did her best to set aside her lingering unease, focusing instead on the task at hand.</p><p>With Captain Harlow’s permission, the trio spread out their gear on the deck, carefully organizing everything they would need for the expedition. Langdon laid out an assortment of tools: brushes, trowels, and chisels for excavation, as well as a small folding table and magnifying glass for examining any artifacts they might find. Aria carefully packed her rucksack with essentials such as rope, a flashlight, a compass, and a canteen, making sure that the map to Atlantis was safely stowed in a waterproof case. Alex, ever the practical adventurer, prepared a selection of weapons, including a sturdy bow and a quiver of arrows, a compact crossbow, and a pair of short swords, all designed for easy carrying.</p><p>As they worked, the team discussed their plan of action. “We should start by exploring the island’s perimeter,” Langdon suggested, his eyes scanning the dense jungle that lay before them. “There may be clues hidden along the shoreline, or perhaps even a concealed entrance to an underground chamber.”</p><p>Aria nodded in agreement. “That makes sense. And if we don’t find anything there, we can move further inland, searching for any signs of ruins or ancient structures.”</p><p>Alex chimed in, a mischievous grin playing on his lips. “And let’s not forget the possibility of running into some unwelcome guests. We should be prepared for anything – be it wild animals, traps, or even other treasure hunters.”</p><p>The group shared a knowing look, understanding that their journey had just begun, and that they would need to rely on each other’s strengths to overcome the challenges that awaited them.</p><p>With their gear secured and their plan of action discussed, Aria, Langdon, and Alex stood on the deck, ready to disembark. As they lowered the rowboat into the water, Aria took a deep breath, steeling herself for the challenges that lay ahead.</p><p>—</p><p>That night, as the moon cast a silvery glow over the ship, Aria tossed and turned in her hammock, unable to find sleep. Suddenly, she felt herself drifting into another dream, one that was even more intense and vivid than the first. She found herself standing in a grand hall adorned with shimmering crystals, and at the far end, Queen Neria awaited her, a knowing smile on her face.</p><p>In the vast hall, Aria noticed that the walls were decorated with intricate murals depicting the history of Atlantis. The murals seemed to come to life, revealing scenes of joyous celebration, great technological advancements, and fierce battles against dark forces. As Aria stepped closer, she could feel the energy emanating from the images, filling her with a sense of awe and reverence.</p><p>As she approached Queen Neria, the queen raised her hand, and a warm, radiant light enveloped Aria, calming her racing heart. “Do not fear, child,” Queen Neria said, her voice both powerful and gentle. “I have brought you here to show you the truth about Atlantis and your connection to our city.”</p><p>“The dreams I’ve been having… they’re not just dreams, are they?” Aria asked, her voice trembling.</p><p>“No, Aria,” the queen replied. “They are echoes of our past, reaching out to guide you on your journey. Your heart is bound to Atlantis, for you possess a rare gift – the power of our ancestors’ magic.”</p><p>Aria’s eyes widened in shock, her thoughts racing back to the strange sensations she’d experienced throughout her voyage. “But… why me?” she asked.</p><p>“Your bloodline has long been intertwined with our city’s fate,” Queen Neria explained. “Your ancestors were among our greatest protectors, and now you must carry on their legacy. Your latent abilities will be crucial in uncovering the secrets of our city and protecting it from the darkness that seeks to claim its power.”</p><p>As Queen Neria spoke, Aria felt a deep resonance with her words, as though a hidden part of her had always known this truth. The queen extended her hand, and a small, glowing crystal materialized in her palm. “Take this, Aria,” she said, “and let it awaken the power within you.”</p><p>Aria reached out and grasped the crystal, feeling a surge of energy course through her body. The hall around her seemed to fade, and a final whisper from Queen Neria echoed in her ears: “Remember, Aria, the fate of Atlantis rests upon your shoulders.”</p><p>With a jolt, Aria awoke from her dream, her heart pounding in her chest. The queen’s words echoed in her mind, leaving her with a profound sense of purpose and connection. She knew now that her destiny was intertwined with that of Atlantis, and her latent magical abilities were the key to uncovering the city’s hidden secrets. Determined to learn more, she decided to share her revelation with Langdon and Alex.</p><p>—</p><p>Gathering her courage, Aria approached Langdon and Alex during their morning meal, her mind filled with the vivid images from her latest dream. She knew that sharing her revelation would change the course of their journey, but she had to trust her friends and mentors to help her navigate the path that lay before her.</p><p>“I had another dream about Queen Neria last night,” Aria began hesitantly. Langdon and Alex exchanged concerned glances as they put down their utensils, giving her their full attention.</p><p>“What did she show you this time?” Langdon asked, his voice filled with curiosity and concern.</p><p>Aria hesitated, looking down at her hands, feeling the weight of her words. “She spoke of a connection between Atlantis and… me. She said that I have latent magical abilities, and that these powers are the key to uncovering the city’s secrets.”</p><p>Alex’s eyes widened, but he remained silent, allowing Aria to continue. Langdon, on the other hand, leaned forward, his interest piqued. “This is remarkable, Aria. We have long suspected that Atlantis held great magical power, but if what you’re saying is true, then you might be the key to unlocking its mysteries.”</p><p>Aria nodded, her eyes glistening with determination. “I don’t know what this means, or how I can control these abilities, but I want to learn. I need to learn. For all of us, and for the people of Atlantis.”</p><p>The trio sat in silence for a moment, processing the magnitude of Aria’s revelation. Alex finally broke the silence, his voice soft and supportive. “We’re in this together, Aria. We’ll help you figure this out, and we’ll face whatever challenges come our way.”</p><p>Aria smiled gratefully at her friends, feeling a sense of relief at their unwavering support. “Thank you. I know it’s a lot to take in, but I believe that we were brought together for a reason. Together, we can uncover the truth about Atlantis and protect its secrets from those who would misuse them.”</p><p>Langdon nodded, his eyes full of pride and determination. “Agreed. We must proceed cautiously, but also with haste. If Lord Malakhar is seeking the same power that you possess, we cannot allow him to find it first.”</p><p>Langdon and Alex listened intently as Aria recounted her latest vision, the intensity in her voice conveying the weight of her experience. They exchanged glances, realizing the magnitude of what she had shared, and pledged their unwavering support as they continued their quest together, armed with the knowledge of Aria’s newfound abilities.</p><p>—</p><p>Huddled together in the ship’s dimly lit cabin, Aria, Langdon, and Alex discussed the implications of Aria’s magical abilities and their potential impact on their journey. They knew that the power she held within her could be both a blessing and a curse, and they had to tread carefully in order to protect her and ensure the success of their mission.</p><p>As they sat in the cramped quarters, Langdon’s brow furrowed in concentration as he sifted through the knowledge he had accumulated throughout his years of study. “Your abilities are extraordinary, Aria,” he began. “If we can unlock their full potential, it could be a game-changer in our search for Atlantis. However, there are risks involved, and we must be cautious.”</p><p>Alex chimed in, his voice somber. “The thing is, Aria, there are people out there who would do anything to get their hands on the power you possess. Lord Malakhar is just one example. We need to be prepared for the possibility that others will try to exploit you and your abilities.”</p><p>Aria nodded in agreement, her hands shaking slightly as she clutched the pendant Queen Neria had given her in her dream. “I understand the risks, but I can’t deny the connection I feel to Atlantis. I believe that my abilities are meant to help us uncover the city’s secrets, and I’m willing to do whatever it takes to learn how to control them.”</p><p>The three of them brainstormed ideas on how to best develop Aria’s abilities without drawing attention to her newfound power. Langdon suggested they consult some of the ancient texts he had brought along, as they might contain information on magical abilities similar to Aria’s. Alex, on the other hand, believed that Aria’s powers were closely tied to her emotions and that she would need to learn how to maintain her composure in difficult situations.</p><p>As the conversation continued, they also discussed strategies for keeping Aria’s abilities a secret from the rest of the crew. They agreed that she should practice her powers only in private or with Langdon and Alex present, and that they should avoid discussing her abilities within earshot of the crew. Alex offered to help Aria develop a cover story for any instances where her powers might accidentally be revealed, emphasizing the importance of sticking together and supporting one another throughout the journey.</p><p>After hours of discussion, the trio agreed to keep Aria’s abilities a secret from the rest of the crew, at least for the time being. They knew that the revelation could sow discord and confusion among their companions, and they had to be cautious about who they could trust. With a sense of unity and determination, they turned their attention to the mysterious island that lay ahead, ready to face whatever challenges it held.</p><p>—</p><p>As The Sea Serpent anchored in the shallows just off the shore of the mysterious island, Aria’s heart raced with anticipation. She stood at the bow, the salty sea breeze whipping her hair around her face, her eyes fixated on the lush, uncharted terrain before her. She felt an inexplicable bond to the island, an overwhelming sensation that she was meant to be there, and that it held the secrets to her destiny.</p><p>Aria took a deep breath, taking in the scent of the ocean and the verdant island foliage. The rest of the team, Langdon, Alex, Captain Harlow, and Thomas, gathered behind her, their expressions a mixture of awe and apprehension. Together, they lowered a wooden rowboat from the ship, carefully climbing in one by one. As they rowed towards the island, Aria couldn’t help but notice the vibrant colors of the surrounding marine life and the way the sunlight pierced through the crystal-clear water, giving the illusion of a mystical underwater world.</p><p>With each stroke of the oars, Aria felt the pull of the island grow stronger. In her mind’s eye, she saw flashes of her dreams: Queen Neria’s piercing gaze, the underwater palace, and the enchanting glow of Atlantean crystals. She glanced at her companions, wondering if they, too, could feel the island’s irresistible allure.</p><p>Upon reaching the shallows, Alex and Thomas hopped out of the rowboat, wading through the shallow surf to pull it ashore. The sand beneath their feet was a brilliant shade of white, so pristine it seemed to sparkle under the sun’s rays. Aria’s heart pounded in her chest as she prepared to take her first step onto the island, feeling as if she were on the cusp of a monumental discovery.</p><p>Langdon, ever the scholar, pulled out a weathered journal and began scribbling notes, his eyes darting between the pages and the island’s dense tree line. He looked up at Aria, a glint of excitement in his eyes. “This could be it, Aria,” he whispered, his voice trembling with anticipation. “We could be standing on the doorstep of Atlantis.”</p><p>Alex offered her a reassuring smile, sensing her apprehension. “Whatever we find here, we’ll face it together,” he said, his eyes locking onto hers with unwavering determination.</p><p>With the first step onto the sandy shore, a sudden surge of energy coursed through Aria’s body, confirming her deep-rooted connection to the island. The team looked on with a mix of excitement and uncertainty as the island’s untamed beauty unfolded before them. As they began their trek into the dense foliage, Aria couldn’t help but feel the weight of the dreams, the visions of Queen Neria, and the powerful pull of Atlantis guiding her forward.</p><p>—</p><p>Hours into their exploration of the island, the team stumbled upon a clearing where the remnants of an ancient civilization stood. Ivy and moss-covered stone structures, cracked and weathered by time, seemed to beckon Aria with whispers of a forgotten past. She felt her connection to the lost city of Atlantis intensify, the energy resonating within her, urging her to uncover the secrets that lay hidden beneath the ruins.</p><p>As they ventured further into the ruins, Aria noticed a pattern in the way the structures were built, as if they were intentionally placed to form a path leading deeper into the island. They continued to follow the trail, marveling at the intricate carvings and delicate craftsmanship that still survived after centuries.</p><p>“I can’t believe we’re actually standing here, amidst the remnants of a civilization that could be linked to Atlantis,” Langdon whispered in awe, his eyes scanning every detail of the ruins. “It’s absolutely astounding.”</p><p>Aria nodded, her heart racing with excitement. She reached out to touch one of the stone structures, and as her fingers made contact with the cold, rough surface, she felt a sudden surge of energy. Her mind was flooded with images of a magnificent city beneath the waves, teeming with life and powered by incredible technology. She quickly withdrew her hand, her breath catching in her throat.</p><p>“Are you alright, Aria?” Alex asked, concern etching his face.</p><p>“Yes, I’m fine,” she assured him, trying to steady her breathing. “I just felt… something. A connection to this place, stronger than ever.”</p><p>As they continued their exploration, the trio stumbled upon a massive stone wall, covered in mysterious symbols and markings. Despite the overgrowth, the symbols seemed to have been deliberately preserved, as if they held great significance.</p><p>Aria’s eyes were drawn to the symbols, and she could feel the energy within her stirring. With newfound determination, she stepped closer to the wall, reaching out to trace the markings with her fingertips. As she did, the symbols seemed to come alive, glowing with a soft, ethereal light. She felt a warmth spread through her body as the symbols’ meanings became clear in her mind.</p><p>As the sun dipped below the horizon, casting long shadows across the ruins, Aria, Langdon, and Alex examined the mysterious symbols etched into the stones. Aria’s newfound connection to Atlantis and her latent magical abilities helped her decipher the cryptic markings, revealing clues that would guide them on their quest. As the team prepared to settle in for the night among the ancient ruins, they knew that they were closer than ever to discovering the truth about the legendary city.</p><p>—</p><p>Night fell upon the island as the team gathered around a flickering campfire, the warm glow illuminating their faces. Aria sat on a moss-covered rock, gazing into the dancing flames, her thoughts consumed by her dreams of Queen Neria, her newfound magical abilities, and the incredible journey that lay ahead. She pondered the weight of her connection to Atlantis and the responsibility it placed on her shoulders.</p><p>As the fire crackled, Aria shared her thoughts and uncertainties with Langdon and Alex, who listened intently. Langdon encouraged Aria to embrace her connection to Atlantis and Queen Neria, reminding her that it was an exceptional gift that could help them unravel the mysteries of the lost city. He also shared his belief that Aria’s destiny was intertwined with the fate of Atlantis, and that together, they could restore the city’s former glory.</p><p>Alex, on the other hand, offered a more pragmatic perspective. He emphasized the importance of trust and unity among their team, stressing that Aria’s powers could be their greatest asset if they worked together. He also reassured her that no matter what challenges they faced, they would stand by her side and face them head-on.</p><p>Together, they reminisced about their adventures so far, sharing laughter and heartfelt stories. They spoke of the perils they had overcome and the incredible things they had seen. Their camaraderie was stronger than ever, and each of them knew that they could rely on one another as they delved deeper into the island’s secrets.</p><p>As the fire burned lower, Aria’s thoughts turned to the mysterious symbols they had discovered earlier, and the surge of energy she had experienced when she touched the stone structure. The cryptic markings seemed to hold the key to finding Atlantis, and she knew that her magical abilities would be crucial in deciphering them.</p><p>Aria also thought of Lord Malakhar, and the danger he posed to both Atlantis and the world. She shuddered at the thought of what might happen if he managed to seize control of the ancient city’s power. As a guardian of Atlantis, she vowed to protect its secrets from those who sought to exploit them for their own gain.</p><p>As Aria looked up at the canopy of stars above, she felt a renewed sense of determination and purpose. She knew that the path before her would be filled with trials and tribulations, but she was prepared to face whatever challenges lay ahead to uncover the truth about Atlantis and fulfill her destiny. With her friends by her side, and the guidance of Queen Neria in her dreams, Aria embraced the unknown, ready to venture deeper into the heart of the island and the secrets it concealed.<br>Chapter 5: The Island’s Hidden Depths</p><p>The first light of dawn crept through the trees, casting a golden hue over the island. Aria, Langdon, and Alex rose from their makeshift camp, eager to explore the island further. As they followed the ancient stone path, the landscape transformed from lush tropical forests to mysterious moss-covered ruins.</p><p>Walking side by side, Aria and Alex shared tales of past adventures, while Langdon’s eyes darted between the ruins and his weathered notebook, making sketches and notes as they went. The air around them was thick with humidity, the scent of damp earth and lush vegetation filling their lungs as they ventured deeper into the island.</p><p>The path wound its way through the remnants of crumbling stone buildings, their once-grand facades now overtaken by vines and moss. Aria felt a connection to the island, as if it were calling to her, guiding her towards something important. She couldn’t shake the feeling that they were on the brink of a significant discovery, one that would change the course of history.</p><p>As they explored the ruins, they discovered ancient, worn carvings that hinted at the island’s connection to Atlantis. Langdon, his enthusiasm reignited, studied the carvings with fervor, trying to glean any knowledge he could from the age-old symbols.</p><p>Alex, ever the adventurer, led the way as they climbed over fallen stones and navigated their way through narrow passages. The sun climbed higher in the sky, casting shadows that danced through the ruins as they pressed forward. The atmosphere was electric, charged with anticipation and wonder.</p><p>Aria felt a strong pull, guiding her deeper into the island’s heart. They continued their trek, eventually arriving at a large stone door adorned with Atlantean symbols, the entrance to a hidden underground complex. Aria closed her eyes, focusing her newfound abilities, and deciphered the symbols. With a sudden jolt, the stone door rumbled open, revealing a hidden labyrinth beneath the island. The trio exchanged excited, determined glances, ready to explore the depths of the labyrinth and uncover the secrets it held.</p><p>—</p><p>With torches in hand, Aria, Langdon, and Alex descended into the labyrinth, the air growing cooler and damp as they ventured further underground. The flickering light from their torches illuminated ancient murals depicting the rise and fall of Atlantis and its connection to the island. The trio paused momentarily to admire the murals, noticing the intricate brushstrokes and vibrant colors that had withstood the test of time.</p><p>As they continued their descent, they discovered that the labyrinth’s walls were adorned with intricate carvings of sea creatures and symbols, some of which Aria recognized from the artifact she had found earlier. Their path twisted and turned, leading them through a seemingly endless series of chambers, each one holding its own mysteries and secrets.</p><p>The sound of water dripping echoed through the narrow passageways, and a musty scent permeated the air. Aria felt a shiver run down her spine as they walked, the weight of the ancient stone pressing in on them from all sides. Yet, despite the eerie atmosphere, she couldn’t deny the thrill of discovery coursing through her veins.</p><p>As they ventured deeper into the labyrinth, Langdon’s weathered notebook filled with sketches and notes detailing the intricate designs they encountered. He marveled at the craftsmanship and ingenuity of the ancient civilization that had constructed this subterranean world. Meanwhile, Alex kept a vigilant eye on their surroundings, making sure they didn’t fall victim to any hidden traps.</p><p>At one point, they stumbled upon a chamber with a large, ancient map etched into the stone floor. Langdon excitedly translated the inscriptions, discovering that the map depicted trade routes and alliances between Atlantis and other civilizations that had long since vanished. Aria could sense the significance of their find, realizing that this map was an invaluable key to understanding the world that had existed before the fall of Atlantis.</p><p>As they progressed through the dimly lit passages, they marveled at the intricacies of the murals, realizing that each one held valuable clues about the history of Atlantis. The trio pressed on, determined to uncover the secrets hidden within the labyrinth, knowing that the answers they sought were close at hand.</p><p>—</p><p>Upon entering a chamber adorned with intricate carvings, the trio encountered their first challenge in the labyrinth – a complex riddle written in Atlantean script on the walls. Aria closed her eyes and focused her magical abilities, translating the riddle. The riddle read:</p><p>“Within my walls I hold a sea,<br>Yet not a drop of water you’ll see.<br>Many paths there are to roam,<br>But only one will lead you home.</p><p>What am I?”</p><p>Langdon scratched his beard thoughtfully as he pondered the riddle. “It’s clearly metaphorical,” he mused. “A sea without water… It could be a reference to knowledge or information.”</p><p>Aria nodded in agreement, her eyes scanning the room for clues. “The many paths to roam part makes me think of the labyrinth itself,” she said.</p><p>Alex looked at the two of them, a smirk playing on his lips. “You two are overthinking it. It’s simple. A sea without water, many paths to roam… it’s a map.”</p><p>Aria and Langdon exchanged surprised glances before looking back at Alex. “A map?” Aria asked, her brow furrowed in thought.</p><p>“Yes, a map,” Alex confirmed. “It holds a sea of information, but not a drop of water. And it contains many paths, but only one leads to your destination.”</p><p>Aria and Langdon nodded, impressed by Alex’s straightforward interpretation. They scanned the chamber once more, and Aria noticed a series of symbols carved into one of the walls. She focused her magic and translated the symbols, revealing a hidden alcove containing an ancient, rolled-up map.</p><p>With their combined intellect, they solved the riddle, and a hidden passage opened, allowing them to advance further into the labyrinth. The success bolstered their confidence and deepened their bond, proving that together, they could overcome any obstacle.</p><p>—</p><p>Next, they came across a chasm with a narrow, crumbling bridge. The stakes were high, but Alex stepped forward, a determined gleam in his eyes. He carefully crossed the bridge, his agility and balance evident in every step. The bridge swayed and creaked ominously under his weight, and small fragments of stone crumbled away, plummeting into the abyss below. Aria and Langdon watched, holding their breath as Alex inched his way across the precarious structure.</p><p>Halfway across, a sudden gust of wind threatened to throw Alex off balance. He pressed himself against the bridge’s remaining rail, gripping it tightly. The wind howled around him, but his determination never wavered. After a tense moment, the wind subsided, and Alex continued his treacherous crossing. Aria and Langdon exchanged glances, silently acknowledging Alex’s incredible courage.</p><p>As Alex neared the other side, the bridge began to crumble faster, and it was evident that it would not hold much longer. With a final leap, he cleared the last few feet, landing safely on solid ground just as the bridge collapsed entirely, leaving a yawning chasm between him and his companions.</p><p>Upon reaching the other side, he secured a rope and helped Aria and Langdon safely cross. The test of courage had been passed, and their trust in one another grew stronger. With their hearts pounding, they continued their journey, knowing that each challenge would bring them closer to Atlantis.</p><p>—</p><p>The team entered a chamber with a massive stone door, barred by a series of large stone blocks. Langdon studied the ancient mechanisms, then directed Aria and Alex to work together, using their strength to move the blocks and unlock the door. Each block was engraved with Atlantean symbols, and it was clear that the blocks needed to be rearranged in a specific order to release the locking mechanism.</p><p>Aria and Alex positioned themselves on either side of the first massive stone block, their muscles straining as they pushed it across the chamber floor. The grinding sound of stone against stone echoed in the confined space, while sweat dripped from their brows. Despite their exhaustion, they persevered, moving the second and third blocks in place as well.</p><p>Meanwhile, Langdon deciphered the symbols on the blocks and instructed Aria and Alex on the correct order. It became apparent that this test of strength was not just about brute force; it was a challenge that required teamwork and communication as well.</p><p>As they finally slid the last block into place, a faint click reverberated through the chamber, signifying that the door’s locking mechanism had been disengaged. Aria and Alex stepped back, breathing heavily but smiling in triumph. They had worked together seamlessly, overcoming this test of strength through their unwavering determination and mutual trust.</p><p>With a groan, the door opened, revealing yet another chamber. The test of strength had been conquered, and the trio’s determination and resilience were unwavering. They knew that the challenges ahead would only grow more difficult, but they were prepared to face them together.</p><p>—</p><p>As they stepped into the Chamber of Reflection, the trio was struck by the serene beauty of the room. The water shimmered with a radiant light, emanating from the crystals scattered throughout the chamber. Their curiosity piqued, Aria, Langdon, and Alex cautiously approached the water, their reflections dancing on the surface.</p><p>As they gazed into the water, the surface began to ripple, and their reflections transformed, revealing images of their pasts. Aria saw herself as a child, overlooked and ignored by her parents as they favored her older brother. Langdon’s reflection showed him in his youth, filled with a burning ambition that often led to his isolation from others. Alex’s image revealed a painful past, marked by betrayal and loss.</p><p>The chamber seemed to be testing their emotional strength by forcing them to confront their deepest fears and insecurities. Aria took a deep breath and faced the vision of her childhood, acknowledging the pain it caused her but also recognizing the resilience she developed as a result. As she did so, her reflection gradually returned to normal, and she felt an inner sense of peace and acceptance.</p><p>Langdon, too, confronted his past, coming to terms with his obsession and the sacrifices he had made in pursuit of Atlantis. He realized that his passion had not only driven him but also made him who he was today, a renowned archaeologist and a dedicated mentor. As he accepted his past, his reflection also returned to normal, and he felt a renewed sense of purpose.</p><p>Alex hesitated, the pain of his past etched in his face. He looked at Aria and Langdon, finding strength in their unwavering support. With a deep breath, he faced his past, accepting the hardships and betrayals he had endured. He understood that his experiences had made him the resourceful and courageous adventurer he was today. As he embraced his past, his reflection returned to normal, and he felt a newfound sense of self-acceptance and trust.</p><p>Having faced their deepest fears and insecurities, the team emerged from the Chamber of Reflection stronger and more united than ever. With newfound resolve, they pressed on, determined to uncover the secrets that the labyrinth held and unlock the mysteries of Atlantis. Together, they left the chamber, stepping back into the dimly lit tunnels of the labyrinth.</p><p>—</p><p>The labyrinth grew darker and more mysterious the deeper they ventured. The flickering light of their torches illuminated a hidden door that led them to an ancient chamber. The room was filled with artifacts and relics, all hinting at the incredible power and knowledge of the lost civilization of Atlantis. The air in the chamber seemed to hum with energy, drawing Aria toward a particular artifact at the center of the room.</p><p>Upon closer inspection, Aria saw that the artifact was a beautifully crafted metallic orb, adorned with intricate carvings and embedded with glowing crystals. The orb seemed to pulse with an inner light, beckoning her to touch it. Langdon, who was examining other relics in the room, noticed her fascination and cautioned her. “Be careful, Aria. We don’t know what sort of power this object holds.”</p><p>Aria nodded, but the pull of the orb was too strong to resist. With a deep breath, she slowly extended her hand, her fingers hovering above the smooth surface of the orb. As her skin made contact with the metal, a brilliant flash of light burst forth, temporarily blinding them. When their vision returned, they saw that the orb had levitated from its resting place and now hovered in midair, held in place by an unseen force.</p><p>As the orb floated there, a series of symbols began to appear on its surface, glowing in the same radiant light as the crystals. Aria recognized them as the same symbols she had seen throughout the labyrinth, but these seemed to carry a different weight – an ancient knowledge that resonated deeply within her.</p><p>With her newfound abilities, Aria attempted to decipher the meaning behind the symbols. As she did, she felt a deep connection to the orb and the civilization that had created it. She could sense the magic and wisdom of Atlantis flowing through her, strengthening her bond with the lost city.</p><p>Realizing the significance of the moment, Langdon and Alex stood at her side, their faces a mix of wonder and awe. They could see the power that the artifact held, and they understood that this was the key to unlocking the secrets of Atlantis – and possibly even the key to saving their world from the threat of Lord Malakhar.</p><p>With her hands resting on the artifact, Aria felt the energy from the object surge through her, intensifying her magical abilities. She could sense the ancient power of Atlantis pulsing within her veins, connecting her to the long-lost civilization in a way she could never have imagined. As Aria, Langdon, and Alex marveled at their discovery, they knew they had uncovered something of great importance, and their journey to unlock the secrets of Atlantis was far from over.</p><p>—</p><p>As the trio regrouped in the artifact chamber, they excitedly discussed the implications of their discovery and the artifact’s connection to Aria’s powers. The air around them crackled with anticipation, and they knew they were on the cusp of a monumental breakthrough. With newfound determination, they prepared to continue exploring the labyrinth, ready to face the challenges that still lay ahead.</p><p>The artifact, a metallic orb adorned with intricate carvings and glowing crystals, seemed to pulse with a life of its own. Aria felt drawn to it, and as she reached out her hand to touch its surface, the orb levitated slightly, and the symbols on its surface glowed with an ethereal light. Her connection to the artifact and Atlantis deepened, and her magical abilities surged in response.</p><p>Langdon and Alex watched the display with a mixture of awe and concern, realizing the true significance of what they had found. The orb, they surmised, was not only connected to the secrets of Atlantis but could also be the key to saving the world from Lord Malakhar’s sinister ambitions.</p><p>As they continued to explore the artifact chamber, they discovered additional clues about the civilization of Atlantis and its powerful magic. Murals on the walls told of the city’s rise and fall, as well as its alliances with other ancient civilizations. Among the relics and artifacts in the chamber, they found an ancient map showing trade routes and indicating the vast reach of Atlantis’s influence.</p><p>As the team studied the map, Alex couldn’t help but express his admiration for Aria’s ability to decipher the ancient symbols. “Your powers are incredible, Aria,” he said, his eyes filled with respect. “You truly are the key to all of this.”</p><p>Langdon nodded, adding, “We must continue our journey with great caution. It’s clear that we are not the only ones seeking the power of Atlantis. Lord Malakhar and his minions will stop at nothing to take it for themselves.”</p><p>Together, the trio pledged to face the challenges ahead with unwavering determination and to protect the secrets of Atlantis from falling into the wrong hands. Their bond had grown stronger throughout their journey, and they knew that they could rely on one another in the face of any adversity.</p><p>As they left the chamber, Aria couldn’t shake the feeling that they were being watched. The hairs on the back of her neck stood on end, and she could sense the presence of an unseen force lurking in the shadows. Little did they know that their journey would soon lead them to a confrontation with the sinister Lord Malakhar, who sought the power of Atlantis for his own nefarious purposes. Steeling themselves for the trials that awaited them, Aria, Langdon, and Alex ventured deeper into the labyrinth, one step closer to the lost city of Atlantis.<br>Chapter 6: The Labyrinth Below</p><p>Aria, Langdon, and Alex stood in front of a massive stone door covered in intricate carvings, representing the entrance to the next section of the labyrinth. They carefully studied the door, realizing that they needed to solve a complex riddle to unlock it. As they brainstormed possible solutions, Aria noticed a faint, pulsating glow coming from one of the crystals in her backpack. The glow seemed to synchronize with her heartbeat, hinting at her deepening connection to the magic of Atlantis.</p><p>The carvings on the door depicted scenes of an ancient civilization, warriors clad in armor, and mythical creatures locked in epic battles. The figures seemed to come alive as Aria’s fingers traced their outlines, the stone humming softly beneath her touch. An inscription was carved around the door’s edge, written in the same mysterious script they had encountered earlier in the labyrinth.</p><p>Langdon, with his extensive knowledge of ancient languages, furrowed his brow as he attempted to translate the inscription. “It seems to be some form of riddle,” he said, his voice barely above a whisper. “We need to find the answer to open this door.”</p><p>Alex paced around the room, examining the walls and floor for any hidden clues that could help them solve the riddle. His eyes fell upon a series of symbols etched into a nearby wall, partially concealed by moss and vines. He carefully brushed away the vegetation, revealing a set of geometric patterns that seemed to correspond with the inscription on the door.</p><p>Together, the trio studied the symbols, trying to discern their meaning. As they did so, Aria felt a warmth emanating from the crystal in her backpack. The pulsating glow grew brighter, and she sensed that the crystal was somehow connected to the riddle.</p><p>Aria removed the crystal from her backpack and held it in her palm, allowing its light to fill the room. The crystal seemed to respond to her touch, and she felt a surge of energy coursing through her veins. She closed her eyes and let her intuition guide her, trusting that the crystal would reveal the answer they sought.</p><p>As she held the crystal aloft, its light illuminated the symbols on the wall, revealing a hidden message that shimmered in the glow. The message provided the answer to the riddle, and as Aria recited the ancient words, the massive stone door began to respond.</p><p>With a resounding click, the stone door began to shift, slowly revealing the passage beyond. The three exchanged triumphant smiles, their bond strengthened by their shared success. As they stepped through the door, they couldn’t help but wonder what challenges awaited them in the depths of the labyrinth.</p><p>—</p><p>Upon entering the new chamber, the team was met with a vast floor comprised of intricately designed tiles. Langdon, well-versed in ancient traps, quickly deduced that the floor was pressure-sensitive, likely concealing a deadly surprise. The trio huddled together, devising a plan to cross the treacherous room without triggering the hidden darts.</p><p>Carefully studying the tile patterns, Langdon pointed out that there seemed to be a deliberate design among the tiles, suggesting that only certain tiles were safe to step on. They realized that the safest path across the room might be hidden in plain sight, embedded within the tile patterns. Aria, recalling her knowledge of Atlantean symbols and the hints she had found in her dreams, was able to discern the safe path by identifying the symbols that matched those from her visions.</p><p>With their path laid out before them, Aria took the lead, stepping gingerly onto the first safe tile. She felt a small rush of energy, the crystal in her backpack pulsating in time with her heartbeat, as if urging her onward. She held her breath, waiting for any sign of danger. When nothing happened, she exhaled and motioned for Langdon and Alex to follow.</p><p>The trio inched their way across the room, each step a delicate dance between life and death. They moved in unison, placing their feet carefully and deliberately, the tension in the air palpable. As they progressed, Aria’s connection to the magic of Atlantis seemed to guide her, giving her an uncanny sense of which tiles were safe to step on.</p><p>At one point, Alex misstepped and placed his foot on the wrong tile. He quickly realized his mistake and shifted his weight back just as a hidden dart shot through the air, narrowly missing him. The close call left them all shaken, but they pressed on, more determined than ever to navigate the treacherous floor.</p><p>As Alex carefully placed his foot on the final safe tile, the three adventurers let out a collective sigh of relief. They had successfully navigated the dangerous room, relying on their wits and trust in one another. With cautious optimism, they pressed on, ready to face the next challenge.</p><p>—</p><p>The next chamber was disorienting, a dizzying array of mirrors reflecting the adventurers and the room’s contents into infinity. The sheer number of reflections made it nearly impossible to discern the exit, leaving Aria, Langdon, and Alex feeling disoriented and lost within the mirrored maze.</p><p>As they cautiously ventured further into the chamber, the reflections seemed to multiply and warp, making it even more difficult to distinguish reality from illusion. Aria took a deep breath, focusing her thoughts and trying to attune herself to the subtle energies of Atlantis that now flowed within her.</p><p>“Let’s try something,” Aria said, closing her eyes. “Instead of relying on our eyes, let’s rely on our other senses. I have a feeling that the answer is hidden in the subtleties of the room.”</p><p>Langdon and Alex exchanged glances, but seeing no better alternative, they agreed to follow Aria’s lead. As they closed their eyes and took slow, deliberate steps, they became increasingly aware of the room’s other sensory cues. The sound of their footsteps echoed, making it difficult to pinpoint their direction. However, they began to notice subtle changes in the air currents, cooler and fresher air hinting at the location of the exit.</p><p>Trusting her instincts, Aria led Langdon and Alex through the maze-like chamber, weaving between mirrors and feeling her way through the disorienting space. They moved cautiously, using their hands to detect the presence of mirrors in their path. As they progressed, Aria’s intuition seemed to grow stronger, guiding them with increasing certainty.</p><p>The feeling of being watched intensified as they neared the exit. Aria couldn’t shake the sensation that unseen eyes were observing their every move, but she refused to let her fear distract her from their goal.</p><p>After what felt like hours, but was likely only minutes, the trio reached a small alcove where the cool, fresh air was strongest. Aria reached out, feeling the edges of an unassuming mirror, and realized it was a hidden door. With a gentle push, the mirror swung open, revealing a dimly lit passageway beyond.</p><p>With a final gust of cool air, Aria led the group through the exit, leaving the bewildering chamber of mirrors behind them. The experience left them feeling uneasy and off-balance, but they had no time to dwell on their lingering disorientation. The labyrinth beckoned, and they continued onward.</p><p>—</p><p>Stepping into the next chamber, the team was confronted with a daunting sight: a narrow, treacherous ledge wrapped around the edge of a deep chasm. They knew they’d need to summon their courage and focus to safely navigate this precarious path. As they edged forward, Aria’s heightened senses suddenly detected something unusual in the chasm’s wall.</p><p>Aria halted her steps, peering intently at the seemingly featureless stone. “Wait,” she whispered, her voice barely audible over the distant echo of dripping water. “There’s something… I can sense a hidden opening.”</p><p>Alex and Langdon exchanged glances, their eyes filled with concern and curiosity. “Are you sure, Aria?” Langdon asked, his voice wavering. “We can’t afford to make a mistake.”</p><p>“I’m sure,” Aria replied, her confidence unwavering. Taking a deep breath, she closed her eyes and reached out to the wall with her left hand. To the astonishment of her companions, the stone shimmered and rippled, revealing a narrow passageway concealed behind an illusion.</p><p>The three adventurers stared at the hidden entrance, their excitement growing. “This must be the way,” Alex said, his voice tinged with anticipation. “It’s too well-guarded to be a coincidence.”</p><p>Aria nodded in agreement. “We should proceed carefully. There’s no telling what traps or challenges lie ahead.”</p><p>One by one, they entered the hidden passageway, their hearts pounding in their chests. The passage was dimly lit, the air thick with the scent of ancient secrets. As they ventured further, the sound of their footsteps was swallowed by the oppressive darkness.</p><p>As the last of the team stepped into the hidden passageway, they felt a sense of relief wash over them. The treacherous ledge had tested their courage and determination, but they had persevered. With renewed hope, they ventured deeper into the hidden passage, wondering what mysteries awaited them.</p><p>—</p><p>The hidden passage led the adventurers to a chamber filled with ancient artifacts and scrolls, each one seemingly more mysterious than the last. They quickly realized this room held vital information about the history of Atlantis and the labyrinth itself. Eager to learn more, they began to explore the room’s treasures.</p><p>As Aria carefully examined each artifact, she felt a surge of energy from the crystal in her backpack, the pulsating glow growing stronger with every passing moment. The artifacts seemed to resonate with her newfound magical abilities, and she could sense the power hidden within them.</p><p>Langdon, fascinated by the wealth of knowledge around him, eagerly delved into the ancient scrolls, searching for clues that could help them navigate the labyrinth. His eyes widened with excitement as he uncovered centuries-old accounts of Atlantean history, technology, and magic.</p><p>Meanwhile, Alex kept a watchful eye on their surroundings, ensuring that no traps or hidden dangers would catch them off guard. He marveled at the intricate craftsmanship of the artifacts and couldn’t help but feel a sense of awe at the advanced civilization that had created them.</p><p>Aria’s attention was drawn to a particular scroll, its edges adorned with intricate Atlantean symbols. As she unfurled it, she discovered that it contained hints about the final riddle they must solve to open the massive stone door. The scroll detailed the legend of the Heart of Atlantis and its connection to the labyrinth. It also hinted at the potential consequences of allowing the Heart to fall into the wrong hands.</p><p>The team gathered around the scroll, their eyes scanning the ancient text as they discussed its implications. They realized that the stakes were higher than ever, and that they must do everything in their power to protect the Heart of Atlantis from Lord Malakhar and his sinister plans.</p><p>As they carefully tucked the scroll containing hints about the final riddle into Aria’s backpack, the team felt invigorated by their newfound knowledge. Armed with a deeper understanding of Atlantis and its secrets, they prepared to face the remaining challenges of the labyrinth, their resolve stronger than ever.</p><p>—</p><p>The musty air of the labyrinth grew colder as Aria, Langdon, and Alex entered an enormous chamber, its vastness accentuated by the flickering light of their torches. Before them lay a series of platforms suspended precariously over a seemingly bottomless pit. The adventurers exchanged wary glances, knowing that they would have to summon their courage and agility to jump from platform to platform to reach the other side.</p><p>Aria took a deep breath and assessed the situation, her senses heightened by the latent magic within her. She noticed that the platforms swayed slightly with the currents of air that flowed through the chamber, and she deduced that the timing of their leaps would be crucial to avoid falling into the abyss below. “We need to time our jumps with the movement of the platforms,” she advised Langdon and Alex.</p><p>Langdon nodded, his eyes never leaving the suspended platforms. “Aria, you go first. You have the best instincts. We’ll follow your lead.”</p><p>Aria approached the edge of the first platform, her heart pounding in her chest. She waited for the right moment, then leaped forward, landing safely on the next platform. Her body swayed with the movement of the platform, but she quickly steadied herself, already focusing on the next jump.</p><p>One by one, they made their way across the chamber, their jumps becoming more confident with each successful landing. The tension in the air was palpable as they neared the final platform, the danger of a single misstep ever-present in their minds.</p><p>As Aria prepared to make the last leap, she felt a sudden surge of energy coursing through her veins. She closed her eyes and took a deep breath, allowing the magic within her to guide her. With a powerful leap, she soared through the air, her body gracefully arcing above the chasm below.</p><p>With a final leap, Aria, Langdon, and Alex reached the safety of the chamber’s far edge. Their hearts pounding from the adrenaline, they allowed themselves a moment of relief, grateful for Aria’s heightened senses that had guided them through the treacherous challenge. As they continued forward, the atmosphere in the labyrinth seemed to grow heavier, signaling that their journey was far from over.</p><p>—</p><p>The next chamber presented a series of intricate, interconnected puzzles that seemed to defy logic. Aria, Langdon, and Alex exchanged determined glances, knowing that they must split up and tackle each puzzle separately. As they prepared to face their individual challenges, they felt the strength of their bond as a team and the growing power within Aria.</p><p>Aria approached a puzzle that consisted of a series of rotating dials, each with different symbols etched into them. She needed to align the symbols in a specific order to solve the puzzle. As she studied the symbols, she felt a surge of energy flow through her, guiding her to find the correct sequence. With each turn of the dials, she could feel the magic of Atlantis resonating within her, urging her to continue.</p><p>Meanwhile, Langdon confronted a complex maze carved into the wall. The maze was filled with numerous dead ends and hidden traps, and he had to trace the correct path through it using a small metal stylus. Drawing on his vast knowledge of ancient mazes and the clues they had found earlier in the labyrinth, he carefully navigated the treacherous path, avoiding the pitfalls and making his way toward the center of the maze.</p><p>Alex faced a challenge that tested his physical agility and resourcefulness. He was confronted with a series of pressure plates, each connected to a different mechanism, and he had to quickly determine the correct order to activate them. Relying on his intuition and experience with similar devices, he gracefully darted between the plates, activating them in the correct sequence and narrowly avoiding the hidden traps that threatened to ensnare him.</p><p>As each of them successfully completed their respective puzzles, a bright beam of light emanated from each solution, converging in the center of the chamber. The beams coalesced into a brilliant orb of energy, which seemed to pulsate with the power of Atlantis. Aria, Langdon, and Alex watched in awe as the energy orb began to dissipate, revealing a small pedestal with a crystalline key resting atop it.</p><p>With the last of the puzzles solved, a low rumble echoed through the chamber as unseen mechanisms shifted and clicked into place. Aria, Langdon, and Alex regrouped, their faces flushed with success and excitement. As they continued deeper into the labyrinth, they could feel the power of Atlantis pulsing around them, and Aria’s connection to the magic grew even stronger.</p><p>—</p><p>The team reunited in a dimly lit room, the scent of old parchment filling the air. In the center of the room stood a pedestal, upon which rested an ancient book. Aria could feel the magic emanating from the tome as they approached, the worn pages seeming to beckon them closer. The final riddle they needed to solve to unlock the massive stone door was hidden within the book’s cryptic text.</p><p>As Aria reached out to touch the book, a shiver ran down her spine, and she felt a surge of energy pulsing through her veins. She opened the book, revealing pages filled with Atlantean symbols, arcane diagrams, and illustrations of mythical creatures. She turned to Langdon, who examined the pages with excitement and fascination.</p><p>“We need to work together on this one,” Langdon said, his eyes scanning the ancient symbols. “We can’t afford any mistakes.”</p><p>Alex nodded in agreement, his eyes focused on the images, searching for a pattern or clue. Aria began to read the text aloud, her voice resonating with the magical energy she felt from the book. As she read, she felt the words weaving together, forming a complex tapestry of knowledge and power.</p><p>Langdon took notes in his journal, cross-referencing the text with his extensive knowledge of ancient languages and mythologies. He began to unravel the riddle piece by piece, muttering to himself as he connected the dots.</p><p>Meanwhile, Alex paced around the room, his eyes darting between the book and the intricate carvings that adorned the walls. He noticed that some of the symbols in the book matched those carved on the walls, and a pattern began to emerge.</p><p>As they worked together, the team’s collective knowledge and intuition guided them through the enigmatic riddle. Aria’s magical senses heightened, her connection to the book intensifying with each word she read. The symbols seemed to dance on the pages, revealing hidden meanings and connections that only Aria could see.</p><p>Finally, the pieces of the riddle fell into place. Aria spoke the answer aloud, and the book glowed with a brilliant, otherworldly light. The ancient words seemed to reverberate in their minds, echoing the power of the magic that surrounded them. They knew they were on the cusp of a great discovery but also felt an increasing sense of danger, as though something sinister was waiting just beyond the shadows.</p><p>With a combination of intuition, intellect, and teamwork, Aria, Langdon, and Alex deciphered the final riddle. The ancient words seemed to reverberate in their minds, echoing the power of the magic that surrounded them. They knew they were on the cusp of a great discovery but also felt an increasing sense of danger, as though something sinister was waiting just beyond the shadows.</p><p>—</p><p>The massive stone door groaned as it slid open, revealing a dimly lit passage that led further into the depths of the labyrinth. Aria, Langdon, and Alex hesitated for a moment, steeling themselves for whatever lay ahead. As they stepped into the passage, the door closed behind them, sealing their fate and the path they must now follow.</p><p>The passage was narrow, with walls covered in intricate carvings depicting ancient battles, powerful magic, and the rise and fall of Atlantis. The air was thick with the scent of damp earth, and a faint mist clung to the ground. Aria felt a sudden chill run down her spine as she sensed an unseen force guiding them deeper into the heart of the labyrinth.</p><p>As they ventured further, the passage widened into a cavernous chamber, its ceiling lost in darkness. The chamber was illuminated by flickering torches, casting eerie shadows that danced across the walls. In the center of the chamber stood a pedestal, atop which rested a magnificent crystal, shimmering with an otherworldly light.</p><p>Aria approached the crystal, feeling an almost magnetic pull. As she reached out to touch it, her hand hovering just above the surface, the crystal emitted a blinding flash of light. Aria’s vision was filled with images of Atlantis in its prime, and she felt an overwhelming sense of connection to the city and its people. The crystal seemed to be imbued with the very essence of Atlantis, and as Aria’s fingers finally made contact, she felt a surge of power coursing through her, her magical abilities amplifying to a level she had never experienced before.</p><p>Langdon and Alex watched in awe, realizing that Aria was the key to unlocking the full potential of the Heart of Atlantis. They also understood that this newfound power came with great responsibility and that the fate of Atlantis – and the world – now rested on their shoulders.</p><p>As the vision faded and Aria’s connection to the crystal was severed, the chamber began to tremble, and they knew that their presence had not gone unnoticed. The sound of footsteps echoed through the labyrinth, and the team prepared to face whatever enemy awaited them in the shadows.</p><p>With each step further into the labyrinth, Aria’s connection to the magic of Atlantis intensified, her senses becoming heightened and her intuition sharpened. The air around them seemed to crackle with anticipation, as though the very walls of the labyrinth were watching their every move. The team cautiously proceeded, feeling the weight of the danger that lurked ahead and the growing urgency of their quest, knowing that the next chapter of their journey was just beginning.<br>Chapter 7: The Awakening</p><p>Aria, Langdon, and Alex continued their exploration of the labyrinth, delving deeper into its shadows. They entered an ancient chamber bathed in a soft, ethereal glow emanating from a crystal at its center. The walls were adorned with intricate carvings and symbols, a language long forgotten.</p><p>The air seemed to hum with an electric energy, making the hairs on their arms stand on end. Aria felt inexplicably drawn to the crystal, her curiosity and intuition leading her forward. Langdon and Alex exchanged nervous glances but said nothing, trusting Aria’s instincts. As they drew nearer, they noticed the chamber seemed to grow warmer and more inviting, despite its ancient appearance.</p><p>Aria reached out hesitantly, her fingertips hovering just above the crystal’s surface. She could feel the energy pulsating within it, beckoning her closer. Finally, she allowed her hand to make contact, and a powerful surge of energy coursed through her. It was as if the crystal was speaking to her, awakening something deep inside. Her latent magical abilities began to manifest more strongly, and the symbols on the walls appeared to her as if they were alive, pulsating with a hidden meaning.</p><p>—</p><p>With Langdon’s help, Aria began to decipher the ancient symbols, which seemed to contain instructions for harnessing and controlling her newfound powers. They spent hours poring over the symbols, as Aria practiced basic magical spells under Langdon’s watchful eye. Alex observed from a distance, awestruck by Aria’s growing abilities yet feeling a hint of unease.</p><p>As Aria traced her fingers over the symbols, she felt a tingling sensation, a connection to the ancient magic that flowed through her veins. Langdon, with his extensive knowledge of ancient languages, translated the symbols and guided Aria through the process of casting her first spell. With a flick of her wrist and a whispered incantation, a small flame appeared in her hand. The fire danced and flickered, casting a warm glow on their faces.</p><p>As they continued their study, Langdon explained the intricacies of the magical system, teaching Aria how to draw on the power of the crystals they had discovered throughout the labyrinth. Aria’s natural affinity for magic, coupled with Langdon’s expertise, allowed her to quickly master basic spells, such as levitation, light manipulation, and elemental control.</p><p>Throughout their practice, Alex remained on the sidelines, his eyes darting between Aria and the dark corners of the chamber, vigilant for any signs of danger. He couldn’t help but be impressed by Aria’s rapid progress and the power she wielded. However, he was also aware that her abilities could potentially draw unwanted attention to their group. He confided in Langdon his concerns about the risks that her powers might pose, but the professor assured him that Aria’s magic was essential to their mission and that they would face any challenges together.</p><p>As the hours passed, Aria’s control over her powers grew, and her confidence soared. She could feel a deep connection to the ancient magic, and the symbols on the walls seemed to hold even more secrets waiting to be unlocked. With each spell she mastered, her understanding of the labyrinth and her role in the quest for Atlantis became clearer.</p><p>—</p><p>As the team ventured deeper into the labyrinth, they encountered a seemingly impassable obstacle. A wide chasm stretched before them, the darkness below hiding any hint of its depth. The only way across appeared to be a series of broken, floating platforms that seemed to defy gravity, suspended above the abyss. Aria, emboldened by her growing powers, used her magic to overcome it. Her companions looked on, impressed by her display of skill and control. Their bond strengthened as they saw the potential of their combined talents.</p><p>Aria closed her eyes, focusing her energy and drawing upon the magical essence she had recently unlocked within herself. She could feel the power coursing through her, vibrant and alive. When she opened her eyes, they sparkled with determination. Raising her hands, she manipulated the platforms, causing them to stabilize and form a makeshift bridge across the chasm.</p><p>Langdon and Alex exchanged glances before stepping onto the first platform, testing its stability. As they carefully traversed the chasm, Aria maintained her concentration, ensuring the safety of her friends. When they reached the other side, Alex offered Aria a hand, helping her make the final leap. Aria let out a sigh of relief as her feet touched solid ground.</p><p>Langdon placed a hand on Aria’s shoulder, his eyes filled with admiration. “I knew you had something special within you, but this… this is truly remarkable,” he said, his voice tinged with awe.</p><p>Alex nodded in agreement, his usual mischievous grin returning to his face. “I’ve seen some pretty incredible things in my time, Aria, but I’ve never met anyone with powers like yours.”</p><p>As they stood together on the other side of the chasm, the team couldn’t help but feel a renewed sense of purpose and determination. They had faced a seemingly insurmountable obstacle, and with Aria’s magical abilities, they had conquered it. The bond between them strengthened, and they knew that they could face whatever challenges lay ahead in the labyrinth.</p><p>With the obstacle vanquished, the team pressed on, their faith in one another renewed. Aria felt a sense of pride in her newfound abilities, while Langdon and Alex reveled in the knowledge that their journey was now bolstered by a force greater than their individual skills. Together, they felt unstoppable.</p><p>—</p><p>The team discovered a room filled with a series of interconnected puzzles. Each member of the group played a crucial role in solving them. Langdon’s knowledge of ancient history, Alex’s dexterity and quick thinking, and Aria’s magical abilities complemented one another, allowing them to overcome the challenges before them.</p><p>The first puzzle required them to decipher a series of cryptic symbols etched into the walls. Langdon’s expertise in ancient languages allowed him to recognize the symbols as an ancient dialect of Atlantean. As he translated the symbols, he realized they formed a riddle, the answer to which would unlock the next puzzle. Aria and Alex listened intently, using their unique perspectives to suggest possible solutions. After much discussion, they arrived at the correct answer, and the room shifted, revealing the next challenge.</p><p>Now faced with a complex, three-dimensional puzzle, Alex’s nimble fingers and keen spatial awareness proved invaluable. As he manipulated the puzzle’s components, guided by Langdon’s ongoing translation of the symbols on the walls, they gradually pieced together an intricate model of the lost city of Atlantis. Aria, sensing the importance of the puzzle, used her magical abilities to imbue the model with a faint glow, which revealed a series of hidden compartments within the model. Together, they extracted the concealed items, realizing they were keys to the next puzzle.</p><p>As they approached the third and final puzzle, Aria felt a surge of energy from her connection to the Atlantean crystal. The puzzle consisted of a massive, circular stone slab with numerous slots, each requiring a specific combination of the keys they had found. Aria’s magical senses guided her, as she determined the correct arrangement of the keys. Langdon and Alex assisted her, using their knowledge of ancient history and logical deduction to support her instincts.</p><p>As Aria placed the final key into its slot, she felt the stone slab vibrate with energy. She channeled her magical power into the slab, causing the room to tremble and the slab to rotate, revealing a hidden passage. As they moved forward, the heavy stone door began to slide open, revealing a new passage. Their teamwork had proven successful, and the sense of triumph filled them with renewed determination to find the secrets of Atlantis. The bond between them deepened, as each recognized the unique strengths the others brought to their quest.</p><p>—</p><p>As they progressed further, Aria experienced another vision from Queen Neria. In this vision, she found herself standing on the shores of a magnificent city, waves crashing against the white sand beneath her feet. The air was filled with the scent of saltwater and the sound of seagulls. The city before her was breathtaking, with tall, elegant spires and buildings crafted from gleaming crystal. It was Atlantis in all its glory, as it had been thousands of years ago.</p><p>Queen Neria appeared beside Aria, her eyes filled with a mix of pride and sorrow as she gazed upon her city. “Aria, my child, you must trust in your powers. They have been lying dormant within you, waiting for this moment. You are the key to saving Atlantis and preventing its power from falling into the wrong hands,” she told Aria, her voice firm but gentle.</p><p>Neria’s eyes darkened, and her expression became grave. “Beware of Lord Malakhar, the sorcerer who seeks the ancient secrets of Atlantis. His hunger for power knows no bounds, and he will stop at nothing to achieve his goal. He is a great danger not only to Atlantis but to the world above as well.”</p><p>As the vision began to fade, Queen Neria placed a hand on Aria’s shoulder, her touch both warm and reassuring. “Remember, Aria, you are not alone. Your companions are strong and capable, and together you can face whatever challenges lie ahead. Trust in them, and trust in yourself.”</p><p>When Aria awoke from the vision, she shared her experience with Langdon and Alex, who expressed their support and determination to stop Malakhar from seizing the power of Atlantis. Langdon’s eyes were filled with resolve as he said, “We will not allow Malakhar to win, Aria. We will protect Atlantis and its secrets, no matter the cost.” Alex nodded, his jaw set in determination. “We’re in this together, Aria. We’ve got your back.”</p><p>Though the warning weighed heavily on their minds, Aria’s companions remained steadfast in their resolve to protect Atlantis from Malakhar’s grasp. They knew that their journey would be fraught with danger, but they were determined to face whatever challenges lay ahead, confident in their collective abilities and the strength of their bond.</p><p>—</p><p>They cautiously entered a dimly lit chamber, the air heavy with an eerie silence. As their eyes adjusted to the darkness, they realized the room was filled with reflective surfaces, creating a labyrinth of mirrors that distorted their surroundings and cast disorienting reflections in every direction. Aria felt a tingling sensation at the back of her neck, as if something was watching them from the shadows.</p><p>Tentatively, they began to navigate the mirrored maze, their progress slowed by the illusions around them. At times, they found themselves turned around, retracing their steps, or stumbling into dead ends. Each reflection seemed to mock their efforts, and frustration began to settle in.</p><p>Aria clenched her fists and closed her eyes, focusing on the energy coursing through her. She sensed the unseen force responsible for the labyrinth’s deceptive nature and felt its tendrils of power snaking through the room. As she concentrated, the energy within her resonated with the power surrounding the mirrors, allowing her to perceive the true paths hidden among the illusions.</p><p>Opening her eyes, Aria reached out a hand, and a soft, glowing light emanated from her fingertips. She traced the light along the mirrored walls, and as the glow passed over the surfaces, the illusions began to fade, revealing the true paths hidden behind the reflections.</p><p>Langdon and Alex followed closely behind Aria, their eyes wide with amazement as they watched her dispel the enchantments. They marveled at her newfound abilities, and their trust in her grew stronger. The labyrinth, once an insurmountable obstacle, began to unravel before them, as Aria’s magic revealed the way forward.</p><p>At last, they stood before a large, ornate mirror, its surface shimmering with an otherworldly aura. Aria realized that this mirror held the key to escaping the chamber. She took a deep breath and focused her energy, drawing upon the power of the crystal they had discovered earlier in the labyrinth.</p><p>Aria focused her energy, sending a pulse of magical force through the room. The illusions shattered, revealing the true path forward. The team exchanged glances, impressed by Aria’s skill and growing more confident in their ability to navigate the labyrinth’s challenges. With newfound determination, they ventured deeper into the unknown.</p><p>—</p><p>The team entered a vast, dimly lit chamber, the walls adorned with intricate engravings and cryptic inscriptions. At the center of the room, a pedestal held an ancient tome, its pages covered in an indecipherable script. The air crackled with anticipation, as if the chamber itself held its breath, waiting for them to unlock its secrets.</p><p>Aria approached the tome, sensing a powerful energy emanating from it. Langdon and Alex examined the inscriptions on the walls, piecing together fragments of information that seemed to hint at the answer to the riddle. As they worked, Aria’s magical abilities allowed her to perceive hidden meanings within the tome’s text, revealing a pattern that she began to understand.</p><p>As they continued to unravel the mystery, the team’s trust in one another grew stronger. Langdon’s wisdom and knowledge of ancient civilizations, Alex’s intuition and resourcefulness, and Aria’s burgeoning magical powers all played a crucial role in deciphering the riddle.</p><p>But the answer remained elusive, the final piece of the puzzle just out of reach. Aria’s frustration mounted, but she drew strength from the unwavering support of her companions. They urged her to trust in her powers, as Queen Neria had advised.</p><p>Taking a deep breath, Aria closed her eyes and allowed her instincts to guide her. She reached out with her magic, feeling it resonate with the energy of the ancient tome. In that moment, a revelation struck her, and she saw the solution to the riddle clearly in her mind’s eye.</p><p>As the final piece of the riddle fell into place, the chamber trembled with a deep, resonant hum. The tome’s pages seemed to come alive, glowing with an otherworldly light that guided Aria’s hand as she traced the correct sequence of symbols. A low rumble echoed through the chamber, and a hidden door materialized before them, beckoning them further into the labyrinth’s depths.</p><p>—</p><p>Before them lay a massive stone door, its surface adorned with elaborate carvings that seemed to tell a story of power and sacrifice. Aria, Langdon, and Alex studied the door, searching for a way to unlock its secrets and reveal the path that lay beyond. The weight of their quest pressed upon them, driving them to uncover the truth about Atlantis and stop the threat posed by Lord Malakhar.</p><p>As they examined the carvings, Aria felt an unmistakable connection with the symbols etched into the stone. The same ethereal energy that coursed through her veins now seemed to emanate from the ancient door. Sensing a hidden mechanism, she reached out with her newfound powers, carefully manipulating the magic within her to interact with the stone.</p><p>At first, nothing happened. Aria felt a surge of doubt, fearing that she might not be able to unlock the door’s secrets. But she remembered Queen Neria’s words of encouragement, and her determination returned in full force. She concentrated on the symbols, allowing her instincts to guide her as she wove her magic around the door.</p><p>Langdon and Alex watched in awe as Aria’s fingertips began to emit a soft, glowing light. The stone door responded to her touch, and the ancient symbols shimmered with an ethereal glow, pulsating in harmony with Aria’s magic. The air around them crackled with anticipation, the very atmosphere within the chamber seeming to shift in response to the awakening power.</p><p>With a final surge of Aria’s magic, the stone door groaned and began to move, revealing a hidden passageway that led deeper into the heart of the labyrinth. As the team stepped through the threshold, they were filled with a renewed sense of purpose and determination to uncover the secrets of Atlantis and prevent Lord Malakhar from seizing its power. Together, they ventured into the darkness, prepared to face whatever challenges awaited them in the depths of the ancient city.<br>Chapter 8: The Shade of Malakhar</p><p>Aria, Langdon, and Alex stood before the last puzzle in the labyrinth, their hearts pounding with anticipation. After a moment of intense focus, they successfully unlocked the mechanism, and the door to the hidden chamber slowly opened. As they cautiously entered the chamber, they found themselves in a dark room illuminated only by glowing symbols on the walls. The symbols seemed to shift and transform, eventually taking on the sinister form of Lord Malakhar.</p><p>The trio stared in awe and fear as Malakhar’s figure emerged from the shimmering symbols. His eyes, black as the void, seemed to pierce into their very souls. Aria clenched her fists, trying to suppress the tremors of fear coursing through her body. Langdon and Alex exchanged uneasy glances, each silently wondering if they were prepared to face the dark sorcerer.</p><p>Malakhar’s spectral form glided towards them, his voice echoing throughout the chamber. “Foolish mortals,” he sneered, “You have come so far, only to be crushed under the weight of your own ignorance.”</p><p>Aria took a deep breath, summoning the courage to speak. “We will not let you take the power of Atlantis and use it for evil,” she declared, her voice steady despite her fear.</p><p>Malakhar laughed, the sound reverberating off the chamber walls like the tolling of a death knell. “You think you can stop me?” he taunted. “You are but insects compared to the power I wield.”</p><p>With a voice like ice, Malakhar revealed his twisted intentions to harness the power of Atlantis and use it to dominate the world. Aria, Langdon, and Alex looked at one another in horror, realizing the gravity of the situation and the importance of their mission.</p><p>—</p><p>As the apparition of Malakhar disappeared, their eyes were drawn to ancient writings on the chamber walls. With a sense of urgency, the team quickly studied and deciphered the writings, hoping to find a way to stop the sorcerer. The chamber’s dim light flickered, casting eerie shadows on the walls as the three of them carefully examined the inscriptions.</p><p>Langdon’s eyes widened as he recognized the ancient language, a dialect used by Atlantean scholars long ago. He began to translate the text, his voice barely above a whisper as Aria and Alex leaned in to listen.</p><p>“The Heart of Atlantis, a force unmatched, binds our world together,” Langdon read. “Its power, if harnessed, can bring about prosperity or ruin. Yet, within the heart, a seed of light lies dormant. When awakened, the seed may be the key to undoing the darkness.”</p><p>As Langdon finished reading the passage, the trio exchanged excited glances. They discovered a passage that hinted at a possible weakness in Malakhar’s power, and it became clear that Aria’s newfound magical abilities could be the key to stopping him. The dormant light within the Heart of Atlantis seemed to be connected to Aria’s powers, and if they could awaken it, they might stand a chance against Malakhar’s malevolent force.</p><p>Aria’s heart raced as the realization sank in. She was the key to stopping Malakhar, but it was a responsibility that terrified her. She looked to her friends for reassurance, and Langdon placed a hand on her shoulder, giving her a nod of encouragement.</p><p>“We’ll face him together, Aria,” Langdon said softly. “You’re not alone in this.”</p><p>Alex, ever the optimist, flashed a confident grin. “We’ve come this far, and we’re not backing down now. Let’s figure out how to awaken this seed of light and show Malakhar what we’re made of.”</p><p>With newfound determination, the team resumed their examination of the chamber walls, searching for any additional information that could help them unlock the power hidden within the Heart of Atlantis.</p><p>—</p><p>Together, Aria, Langdon, and Alex devised a plan to confront Malakhar and prevent him from seizing the power of Atlantis. They knew the risks were immense, and the consequences of failure unthinkable, but they were determined to protect the world at all costs.</p><p>Aria shared with Langdon and Alex the information she had gleaned from her vision with Queen Neria, explaining the potential role of the Heart of Atlantis in defeating Malakhar. As they considered this new information, the trio weighed their options and brainstormed ways to neutralize the sorcerer’s dark power.</p><p>They agreed that Aria would use her magical abilities to counteract Malakhar’s power, drawing on the strength of the Atlantean crystals and her connection to the Heart of Atlantis. Langdon, with his vast knowledge of ancient cultures and languages, would take on the role of deciphering any hidden traps or riddles Malakhar might have in store for them. Alex, with his exceptional agility and combat skills, would focus on keeping them safe from physical danger and provide backup in case Malakhar had any minions or magical creatures on his side.</p><p>As the plan took shape, the group discussed the possible consequences of their actions, knowing that the outcome would affect not only Atlantis but the entire world. They realized that they might need to make difficult choices and sacrifices to save both the hidden city and the world above from destruction. The weight of this responsibility weighed heavily on them, but they knew that they had come too far to turn back now.</p><p>Throughout the discussion, the team also acknowledged the unique strengths that each of them brought to their mission. They recognized that their unity and combined skills would be essential in overcoming the challenges ahead. This realization fostered a sense of camaraderie and mutual trust among the group, which would prove vital in the battles to come.</p><p>With a renewed sense of purpose, Aria, Langdon, and Alex solidified their plan, ready to face Malakhar and protect the world from his sinister machinations. They agreed that Aria would use her magical abilities to counteract Malakhar’s power, while Langdon and Alex would provide support and backup.</p><p>—</p><p>As they prepared for the upcoming confrontation, the team gathered supplies, including powerful Atlantean crystals they had found within the labyrinth. Aria practiced her magic, honing her skills under the guidance of Queen Neria’s visions. She experimented with various spells, attempting to draw more power from the crystals and channel it into her energy manipulation. As she practiced, she could feel her connection to the ancient city and its magic growing stronger.</p><p>Langdon spent hours poring over his sketches and notes, analyzing the labyrinth’s intricate design and its myriad puzzles. He sought to understand the ancient civilization’s knowledge and intentions, hoping to uncover hidden secrets that might aid them in their fight against Malakhar. He also continued to teach Aria and Alex about the Atlantean language, ensuring they could decipher any further writings they might encounter.</p><p>Alex, meanwhile, focused on preparing the group for the physical challenges they were sure to face. He sharpened their weapons and crafted makeshift shields from the materials they had collected in the labyrinth. Alex also took the time to teach Aria and Langdon some basic combat techniques, knowing they might have to defend themselves against Malakhar’s unknown minions. He emphasized the importance of teamwork and trust, reminding them that they were stronger together.</p><p>The trio also took turns keeping watch, ensuring that they were not taken by surprise by Malakhar or any other dangers lurking in the labyrinth. The air was thick with tension, and they could all sense that the stakes had never been higher. Despite their fears, they found solace in their camaraderie and mutual trust, knowing they were not alone in their fight.</p><p>As they continued to prepare, a newfound sense of purpose filled the team. They knew that they were the world’s last line of defense against Malakhar’s sinister plot, and they were determined to rise to the challenge, no matter the cost.</p><p>Langdon and Alex studied the ancient writings they had deciphered, hoping to find any additional clues that could aid them in their battle against Malakhar.</p><p>—</p><p>Amidst their preparations, Aria, Langdon, and Alex took a moment to rest and reflect on their incredible journey. They shared their hopes and fears for the upcoming confrontation, each expressing their gratitude for the friendships they had formed.</p><p>Seated on the cold stone floor of the hidden chamber, they took turns recounting their favorite moments of the journey so far. Aria reminisced about the thrill of discovering the ancient artifact in the British Museum and the wonder of her first dream of Queen Neria. Langdon spoke fondly of the excitement he felt when he first realized that Aria’s discovery could be the key to unlocking the secrets of Atlantis, a lifelong dream of his. Alex, with a mischievous grin, recalled the various challenges they had faced in the labyrinth and how they had managed to overcome them through teamwork and trust.</p><p>As they talked, the conversation took a more serious tone. Aria confessed her fears about her newfound magical abilities and the responsibility that came with them. Langdon reassured her, expressing his belief in her strength and potential to master her powers. Alex admitted his concern for the safety of the world above, determined to prevent Malakhar from achieving his dark ambitions.</p><p>Despite the weight of their responsibilities, they found comfort in their shared purpose and camaraderie. They spoke of their plans for the future, with Aria expressing her desire to learn more about the ancient world of Atlantis and use her powers for good. Langdon, ever the scholar, wished to continue unraveling the mysteries of the lost city and share his findings with the world. Alex, always the adventurer, looked forward to exploring new and uncharted territories, protecting the innocent and fighting against injustice.</p><p>As they spoke, their bond grew stronger, solidifying their resolve to face Malakhar together.</p><p>—</p><p>As Aria meditated in the tranquil chamber, she found herself in another vision with Queen Neria. The sagacious queen offered Aria guidance and encouragement for the impending battle, urging her to trust her newfound abilities.</p><p>In the vision, Aria found herself in a serene garden replete with blossoming trees and crystal-clear pools. Queen Neria materialized beside her, garbed in an elegant gown that shimmered with the colors of the sea. Neria’s eyes conveyed both warmth and melancholy as she gazed at Aria.</p><p>“Aria, you have come so far and acquired much knowledge on your journey,” Neria said gently. “Your strength and determination have sustained you through innumerable challenges, but it is your heart that truly makes you a worthy champion for Atlantis.”</p><p>Aria listened intently, her eyes brimming with resolve. “I will do whatever it takes to protect Atlantis and the world above, but I’m unsure if my powers are potent enough to stop Malakhar.”</p><p>Queen Neria smiled tenderly. “Your powers are still developing, Aria. You possess a unique connection to the Heart of Atlantis, a bond that can be used to your advantage in the fight against Malakhar. However, you must remember that the true power resides within you, not solely in the magic you wield.”</p><p>Aria nodded, her expression steadfast. “I understand. I will trust in myself and my abilities.”</p><p>Neria continued, her tone growing solemn. “There is one more thing you must know. The Heart of Atlantis holds more than just the power of the crystals. It contains a force capable of shaping the very essence of our world. This force can be wielded for good or ill, depending on the intentions of its controller.”</p><p>Aria’s eyes widened in comprehension. “You’re saying that the Heart of Atlantis could be the key to defeating Malakhar.”</p><p>Neria nodded gravely. “Yes, but you must exercise caution, Aria. The power of the Heart is not to be trifled with. Its force can be as destructive as it is creative, and if it falls into the wrong hands, the consequences could be disastrous. Remember to trust in your heart and your friends, and you will find the strength to vanquish Malakhar.”</p><p>With a final, reassuring smile, Queen Neria’s form began to dissipate, leaving Aria alone in the garden once more.</p><p>Aria awoke with newfound understanding of the Heart of Atlantis and its potential role in defeating Malakhar. She shared this revelation with Langdon and Alex, and they revised their plan accordingly.</p><p>—</p><p>As they made their final preparations, the team took a moment to appreciate the beauty and wonder of the hidden world they had discovered beneath the island. They knew that their actions would have a lasting impact on the fate of Atlantis and the world above, and they felt a deep sense of responsibility to protect both from Malakhar’s evil intentions.</p><p>They marveled at the intricate carvings and shimmering crystals that adorned the labyrinth’s walls, reflecting on the incredible civilization that had once thrived here. Aria couldn’t help but feel a strong connection to the lost city, and her determination to defend it only grew stronger.</p><p>Langdon, his brow furrowed in thought, studied the ancient writings and symbols that surrounded them. He knew that the knowledge contained within these hidden chambers could change the course of history, and he was driven by an insatiable desire to unlock their secrets. As he translated the ancient texts, he felt a growing sense of awe and reverence for the Atlantean scholars who had come before him.</p><p>Alex, ever the pragmatist, focused on the practical aspects of their plan. He double-checked their supplies and ensured that they were as prepared as possible for the challenges that lay ahead. He tested the sharpness of his blade and practiced his combat moves, knowing that his skill and agility would be crucial in the upcoming battle.</p><p>As the team readied themselves for the confrontation with Malakhar, they found comfort in their shared purpose and in the knowledge that they were not alone in their struggle. Each of them knew that they had a unique role to play in the defense of Atlantis, and they were united by a common goal – to protect the world from Malakhar’s malevolent power.</p><p>With their plan in place and determination in their hearts, Aria, Langdon, and Alex exited the dark chamber, ready to confront Lord Malakhar.</p><p>—</p><p>Aria, Langdon, and Alex stepped into the immense chamber, their eyes locked on Lord Malakhar, who stood at its center. They could feel the dark energy emanating from him, sending chills down their spines. The chamber was filled with strange, twisted sculptures, their forms casting eerie shadows that danced in the flickering torchlight. They knew that this was the battleground where the fate of Atlantis and the world above would be decided.</p><p>As they confronted Malakhar, the sorcerer sneered at them and began to summon his dark minions. Shadowy creatures, twisted and malevolent, emerged from the darkness and surrounded the trio, snarling and hissing. Langdon and Alex brandished their makeshift weapons, while Aria held her hands out, her fingers sparking with the magical energy she had learned to control.</p><p>Malakhar’s laughter echoed through the chamber as he unleashed a torrent of dark energy towards Aria, who narrowly dodged the attack. She countered with a blast of her own magic, forcing Malakhar to retreat momentarily. Langdon and Alex leaped into action, fighting off the dark minions as Aria focused on the sorcerer.</p><p>The chamber echoed with the sounds of battle – the clash of weapons, the crackle of magic, and the cries of the combatants. Aria’s powers continued to grow stronger as she channeled the energy of the Atlantean crystals she had gathered. As she unleashed a powerful wave of magic, it illuminated the eerie shadows in the chamber, briefly revealing ancient murals on the walls, hinting at the long-lost history of Atlantis.</p><p>Aria’s connection to the Heart of Atlantis seemed to bolster her strength, and she could feel the artifact’s creative and destructive potential coursing through her veins. With every successful attack against Malakhar, the sorcerer’s grip on the dark energy weakened. Meanwhile, Langdon and Alex fought the shadowy creatures with a combination of skill, agility, and their knowledge of Atlantean combat techniques.</p><p>As the battle raged on, Aria, Langdon, and Alex fought valiantly against the sorcerer and his dark minions. With each passing moment, their resolve grew stronger, fueled by their determination to save Atlantis and the world from Malakhar’s tyranny. Just as the tide of battle seemed to be turning in their favor, an ominous rumbling echoed through the chamber, and a hidden doorway opened to reveal a new path, leading them to the heart of the ancient city. Aria, Langdon, and Alex exchanged a look, knowing that the final confrontation with Malakhar was imminent, and that their journey was about to reach its climax.<br>Chapter 9: The Lost City Revealed</p><p>Aria, Langdon, and Alex stood at the entrance of a vast, intricately carved stone chamber, bathed in ethereal light that seemed to emanate from the very walls. The air hummed with an energy that sent shivers down their spines. Before them, a wide ledge overlooked a magnificent cityscape, stretching out as far as the eye could see, filled with towering spires, lush gardens, and shimmering waterways.</p><p>As they stepped onto the ledge, Aria’s heart raced with anticipation. She could feel her connection to the city growing stronger with every step she took. Langdon, ever the scholar, eagerly examined the intricate carvings that adorned the walls of the chamber, his eyes wide with excitement as he recognized symbols and patterns he had studied for years.</p><p>Alex, on the other hand, kept a vigilant eye on their surroundings, his instincts telling him that they needed to be prepared for anything. As they carefully made their way down the winding path, they couldn’t help but marvel at the lush gardens that seemed to defy gravity, cascading down the sides of the towering spires.</p><p>The trio passed a group of Atlantean children playing with enchanted orbs that hovered in the air, their laughter ringing out like music. Aria smiled, her heart swelling with joy at the sight. It was clear that the people of Atlantis lived in harmony with their surroundings, using their advanced technology and magic to create a utopia hidden from the rest of the world.</p><p>As they continued to descend, they encountered more wonders. They walked through a plaza filled with floating fountains that danced to the sound of a gentle, otherworldly melody. Alex couldn’t resist placing his hand into the stream of water, amazed to find it warm and soothing.</p><p>Further along, they crossed a bridge over a shimmering waterway, where a group of Atlanteans navigated elegant, swan-like boats propelled by crystal-powered engines. Aria paused, captivated by the sight, and Langdon shared her fascination, eagerly asking the boatmen questions about the mechanics of their vessels.</p><p>The path led them through a bustling marketplace, where vendors sold exotic fruits, intricate crystal jewelry, and clothing woven from iridescent, seemingly weightless fabrics. Aria felt a pull toward one particular stall, where an elderly woman sold beautiful, handcrafted pendants. The woman’s eyes met Aria’s, and she smiled warmly, offering her a pendant that bore an uncanny resemblance to the one worn by Queen Neria in her dreams.</p><p>As they neared the heart of the city, the trio began to draw attention. Whispers spread through the crowd, and it wasn’t long before a group of curious inhabitants had gathered around them, eager to learn about the outsiders who had found their way into their hidden world.</p><p>—</p><p>As they ventured further into Atlantis, the trio marveled at the advanced technology and magic that powered the thriving civilization. The streets were bustling with people, their clothing and mannerisms unlike anything they had ever seen. After a brief moment of confusion, the Atlantean inhabitants welcomed Aria, Langdon, and Alex warmly, showing them the various roles and responsibilities of the citizens, as well as the day-to-day workings of the city.</p><p>The trio wandered the city, led by a friendly Atlantean named Lysander, who was eager to show them the wonders of his home. They observed the blacksmiths forging weapons and tools with the help of enchanted flames that changed color based on the desired properties of the crafted items. In another part of the city, they saw scholars pouring over ancient scrolls, using crystals to magnify the text and project images to share their knowledge with others.</p><p>As they moved on, Aria noticed a group of children playing with levitating orbs that changed colors as they soared through the air. Langdon explained that these orbs were yet another example of the remarkable blend of magic and technology found in Atlantis. Alex, meanwhile, was captivated by the gravity-defying gardens, where plants and flowers grew upside down, suspended in mid-air by the power of the Atlantean crystals.</p><p>During their tour, they also visited a bustling marketplace, where merchants sold everything from exotic spices and fabrics to intricate gadgets and magical trinkets. The people of Atlantis moved about with an air of contentment, their faces reflecting a deep sense of peace and happiness. It was evident that the Atlanteans had achieved a harmonious balance between their advanced technology and the natural world, which allowed their society to flourish.</p><p>As they continued to explore, Aria, Langdon, and Alex were struck by the kindness and generosity of the Atlanteans they encountered. The people seemed genuinely interested in learning about the surface world and eagerly shared their own customs and traditions with the trio. They were offered food and drink, and although some of the flavors and textures were unusual to their palates, they couldn’t deny that the Atlantean cuisine was delicious.</p><p>The trio was also introduced to the art of crystalweaving, a unique form of crafting that involved the use of the Atlantean crystals to create intricate, luminescent patterns on cloth. Aria was captivated by the beauty of this art form and marveled at the skill of the crystalweavers as they demonstrated their craft.</p><p>As the day drew to a close, Aria, Langdon, and Alex found themselves sitting on the edge of a sparkling waterway, watching swan-like boats glide gracefully along the water’s surface. They reflected on the incredible sights they had witnessed and the kindness of the people they had met. In this lost city beneath the sea, they felt a sense of wonder and belonging that they had never experienced before.</p><p>Surrounded by their new Atlantean friends, Aria, Langdon, and Alex felt a sense of belonging they had never experienced before. Grateful for the warm reception, they vowed to do everything in their power to help protect Atlantis and its people from the darkness that threatened them.</p><p>—</p><p>The team was soon escorted to a grand palace where they were introduced to Queen Neria, the wise and regal ruler of Atlantis. The queen’s elegant gown was woven with colors reminiscent of the sea, and her eyes held a mix of warmth and melancholy. As they entered the throne room, they noticed the walls adorned with intricate murals depicting the history of the lost city, as well as the power struggles that had shaped its past.</p><p>Queen Neria greeted them with a warm smile and a gentle nod. Her voice was soft but firm, her tone commanding respect while also exuding kindness. She spoke with Aria, Langdon, and Alex, explaining the history of Atlantis and its connection to Aria’s burgeoning magical powers. “Our ancestors,” she began, “harnessed the energy of the crystals that are abundant in our land. These crystals, when properly channeled, grant us access to great magical power and have allowed our civilization to flourish.”</p><p>As the queen continued her tale, Aria felt a growing sense of wonder and responsibility, realizing that her newly discovered powers were tied to the very essence of Atlantis. The queen told them of the city’s golden age, a time when the people of Atlantis had lived in harmony with nature, using their magic to benefit the world.</p><p>“But our prosperity came with a cost,” Queen Neria continued, her voice tinged with sadness. “Our power attracted the attention of those who sought to exploit it for their own ends, and we were forced to conceal our city from the world to protect ourselves and our magic.”</p><p>Queen Neria’s gaze fell upon Aria, and she added, “Aria, you possess a rare gift, one that links you to the heart of Atlantis itself. Your powers are awakening, and with them comes the responsibility of defending our city from the darkness that threatens it. Lord Malakhar is a formidable enemy, and his desire for the power of the Atlantean crystals could bring ruin to not only Atlantis but the entire world.”</p><p>As the queen spoke, Aria felt the weight of her words settle upon her. She knew she had a responsibility to protect the city and its people, but she also knew she would need the help of her friends and newfound allies to do so. Alex and Langdon listened intently, the gravity of the situation becoming clear to them as well.</p><p>Queen Neria warned them of the danger posed by Lord Malakhar and urged them to prepare for the confrontation that lay ahead. “You must stand together, for only united can you hope to overcome the darkness that threatens us all,” she advised.</p><p>With a deep sense of gratitude, Aria thanked Queen Neria for her guidance and pledged her allegiance to Atlantis. As they left the queen’s chambers, the weight of responsibility settled upon their shoulders, the realization that the fate of this hidden world now rested in their hands.</p><p>—</p><p>Aria, Langdon, and Alex spent the following days exploring the city, taking in the breathtaking architecture and the harmonious balance between technology and magic. They witnessed firsthand how the Atlanteans harnessed the power of the enchanted Atlantean crystals, and they learned more about the city’s rich culture and traditions.</p><p>Their Atlantean guide, Lysander, showed them some of the city’s most wondrous and awe-inspiring sights. The gravity-defying gardens seemed to float above the ground, while the waterways were a network of canals where swan-like boats glided effortlessly, powered by the energy of the crystals. Aria was particularly fascinated by the enchanted flames that burned without fuel or smoke, providing light and warmth throughout the city.</p><p>As they strolled through the marketplace, they observed scholars using the crystals to manipulate and shape the environment around them, creating intricate works of art with just a wave of their hands. They also encountered levitating orbs that provided guidance and information to the citizens, and crystalweavers who wove the threads of the powerful crystals into fabrics and other materials.</p><p>The people of Atlantis greeted them with kindness and generosity, offering them unusual but delicious cuisine that seemed to invigorate their senses and fuel their bodies with newfound energy. As they walked the streets, they couldn’t help but notice the unique clothing and mannerisms of the Atlanteans, a perfect blend of ancient traditions and advanced technology.</p><p>Aria, Langdon, and Alex took the time to learn about the city’s customs and beliefs, attending lectures by some of the most respected scholars in Atlantis. They discovered that the city’s peaceful existence was maintained through a deep respect for the power of the crystals and a commitment to using their magic for the betterment of all. The people of Atlantis believed that their city was a beacon of hope and light, a refuge from the darkness that lurked beyond their borders.</p><p>The trio also had the opportunity to learn more about the Atlantean language, with Langdon taking a keen interest in understanding the nuances of the ancient script. They spent hours studying the carvings on the walls of the city, tracing the lines of text with their fingers as they deciphered the stories and history of the people who called this hidden city their home.</p><p>As they continued their exploration of the city, the trio discovered a library filled with ancient texts and scrolls. With a feeling of determination, they delved into the wealth of knowledge available to them, searching for clues about the city’s magic and how it could be used to defeat Lord Malakhar.</p><p>—</p><p>Aria, Langdon, and Alex were led through the winding streets of Atlantis to a tranquil courtyard, where they encountered three prominent citizens of the city: Elara, a gifted healer with a serene demeanor, Idris, an elderly scholar whose eyes sparkled with wisdom, and Kara, a fierce warrior with a strong, commanding presence. The trio could sense that these three Atlanteans would play an important role in their journey, aiding them in their quest to understand the city and confront the looming threat of Lord Malakhar.</p><p>After exchanging formal introductions, Elara beckoned Aria to join her by a crystal-infused pool, its waters shimmering with vibrant, healing energies. As they knelt by the pool, Elara gently explained the properties of the water and demonstrated the healing techniques that had been passed down through generations of Atlantean healers. Aria, her curiosity piqued, listened intently and practiced the techniques under Elara’s watchful gaze. As the day progressed, Aria began to understand the innate connection between her emerging powers and the healing arts of Atlantis.</p><p>Meanwhile, Langdon eagerly engaged in a spirited conversation with Idris about the history, culture, and magic of the city. The elderly scholar led Langdon through a vast library filled with ancient tomes and scrolls, sharing his extensive knowledge of the Atlantean language and the secrets hidden within the city’s texts. Langdon’s eyes widened with fascination as he learned about the powerful Atlantean crystals and their many uses in the city’s advanced technology, as well as their role in sustaining the delicate balance between magic and science.</p><p>In another part of the courtyard, Kara challenged Alex and Aria to a series of combat exercises designed to test their physical prowess and strategic thinking. Alex, a skilled fighter in his own right, quickly adapted to Kara’s unique style of combat, which combined fluid movements with the grace and power of the ocean’s waves. Aria, while not as experienced, demonstrated a keen aptitude for the Atlantean fighting techniques, her latent magical abilities enhancing her natural agility and strength.</p><p>Throughout the day, the three adventurers bonded with their Atlantean mentors, sharing stories of their journey and learning valuable lessons that would aid them in their quest to protect the city. They marveled at the harmony and unity within the Atlantean society, admiring the respect and reverence the citizens held for the power of the crystals and the wisdom of their ancestors.</p><p>As the sun dipped below the horizon, Aria, Langdon, and Alex bid farewell to their new mentors for the day, each filled with newfound knowledge and appreciation for the Atlantean way of life. They knew that Elara, Idris, and Kara would be instrumental in the battles to come, and they felt a growing sense of camaraderie with their new friends. With the guidance and expertise of their Atlantean allies, the team felt better equipped to face the challenges that lay ahead, including the inevitable confrontation with the dark sorcerer, Malakhar.</p><p>—</p><p>Gathered around an ornate table in a secluded chamber within the palace, Aria, Langdon, Alex, Queen Neria, and their Atlantean friends began to devise a plan to counter Lord Malakhar’s schemes. They studied ancient texts, analyzed maps of the city, and discussed possible strategies, pooling their collective knowledge to create the best possible defense against the malevolent sorcerer.</p><p>As the group poured over the texts and maps, Idris began outlining the city’s natural defenses and known entry points. He shared his concerns about Malakhar exploiting weaknesses within the city’s magical barrier, which had been weakened by the passage of time. Elara suggested they could use their healing magic to bolster the barrier and protect the most vulnerable areas of the city. Kara chimed in, proposing the creation of a highly trained strike force to confront Malakhar’s minions head-on and prevent them from breaching the city.</p><p>Queen Neria, who had been silently listening to the suggestions, finally spoke. She revealed the existence of a secret chamber within the palace, housing a powerful artifact known as the Heart of Atlantis. This artifact, she explained, could amplify the power of the Atlantean crystals and possibly turn the tide in their favor against Malakhar. The catch was that only someone with strong magical abilities, like Aria, could harness the artifact’s power without causing great harm to themselves and the city.</p><p>Aria, humbled by the responsibility placed upon her, agreed to take on this task. She would need guidance, however, and so Queen Neria offered to personally mentor Aria in the art of wielding the Heart of Atlantis. Meanwhile, Langdon and Alex would work closely with Idris and Kara to coordinate the city’s defenses and gather intelligence on Malakhar’s movements.</p><p>Throughout the planning process, the group’s resolve only grew stronger. They shared stories of their personal stakes in the battle, from the Atlanteans’ love for their home and people to Aria, Langdon, and Alex’s desire to protect the world from Malakhar’s tyranny. They vowed to stand united, knowing that their greatest strength would be their unbreakable bond.</p><p>After hours of intense discussion and debate, the group finally agreed upon a comprehensive plan to confront Malakhar and protect Atlantis. As they dispersed to begin their respective preparations, the air was filled with a sense of determination and unity. Aria, Langdon, and Alex knew that the weight of the world rested on their shoulders, and that they would need to rely on their newfound allies and each other to face the darkness that threatened to consume all they held dear.</p><p>—</p><p>Despite the weight of their mission, Aria found solace in the visions that continued to visit her in her dreams. The familiar figure of Queen Neria appeared, her otherworldly aura emanating a sense of comfort and reassurance. In these visions, Aria delved deeper into the mysteries of her own powers and the ancient knowledge of Atlantis, guided by the wisdom of the ethereal queen.</p><p>One night, as Aria dreamt, she found herself in a beautiful, luminescent garden within the heart of Atlantis. Queen Neria stood before her, her serene expression radiating love and understanding. The queen led Aria to a pool of crystal-clear water, explaining that it was a source of the city’s magical energy. As Aria peered into the water, images of the past flickered before her eyes – ancient rituals, battles, and the true potential of her newfound powers.</p><p>In another vision, Queen Neria took Aria to a hidden chamber within the palace. There, an array of powerful artifacts, each containing a fragment of Atlantean knowledge, was carefully displayed. The queen encouraged Aria to explore these artifacts, explaining that they would provide her with invaluable insights and abilities in the battle against Lord Malakhar.</p><p>As the visions unfolded, Aria began to understand the immense responsibility that rested on her shoulders. She was not only the key to unlocking the power of the Heart of Atlantis, but also the one who could wield its energy to defeat the dark sorcerer. Through these dreams, Aria’s connection to Atlantis and her own magic grew stronger, and she felt her confidence and determination swell within her.</p><p>In her final vision before the confrontation with Malakhar, Aria found herself once again in the presence of Queen Neria. The queen gently placed her hand on Aria’s shoulder, her eyes filled with conviction and pride. “You have the strength to overcome the darkness, Aria,” she said softly. “Your connection to Atlantis and its magic runs deep. Trust in yourself and your allies, and you will prevail.”</p><p>As morning’s light filtered through the window, Aria awoke with renewed determination, her visions serving as a beacon of hope in the face of the darkness that loomed on the horizon. She knew that the time to confront Malakhar was drawing near, and with the guidance of Queen Neria, she felt more prepared than ever to face the trials that lay ahead.</p><p>—</p><p>The city of Atlantis buzzed with activity as Aria, Langdon, Alex, and their Atlantean allies prepared for the approaching battle. Weapons were sharpened, magical artifacts were gathered, and the people of the city trained together, united by their common goal of protecting their home from the darkness that threatened to engulf it.</p><p>As the day progressed, Aria and Alex worked with Kara to refine their combat skills. They practiced with an array of weapons, including the Atlantean energy blades, which were capable of slicing through solid stone. Meanwhile, Langdon and Idris examined ancient texts and scrolls, looking for any hidden knowledge that could help them in their fight against Malakhar. Elara, on the other hand, continued to work on strengthening the city’s magical barrier with Aria, focusing on the weakest points that Malakhar might exploit.</p><p>The streets and plazas of Atlantis were filled with the sounds of training and preparation, as warriors sparred and engineers worked on bolstering the city’s defenses. Aria, Langdon, and Alex found themselves inspired by the dedication and resilience of their Atlantean comrades, and they felt a sense of unity and camaraderie that they had never experienced before.</p><p>As the preparations continued, Aria felt the power within her grow stronger. With each passing hour, she gained more control over her magical abilities, and she knew that she would soon be ready to face Malakhar in battle. She recalled Queen Neria’s words of encouragement and felt more determined than ever to protect the city and its people from the dark sorcerer’s wrath.</p><p>In the midst of the preparations, Aria also found herself growing closer to her new friends. With Elara, she shared her fears and hopes, and the healer offered her words of wisdom and comfort. Idris, the wise scholar, provided Aria with invaluable knowledge about the history of Atlantis and the true nature of her powers. And Kara, the fierce warrior, taught her the importance of unwavering courage and determination in the face of overwhelming odds.</p><p>As the hours passed, the people of Atlantis worked tirelessly, fortifying their city against the coming storm. They knew that the battle with Malakhar would be a test of their strength and resolve, but they were ready to face whatever challenges lay ahead. The city’s inhabitants, both young and old, stood together, united by their love for their home and their determination to defend it.</p><p>As the sun began to set, casting a warm glow over the city, the preparations were finally complete. Aria, Langdon, Alex, and their Atlantean comrades stood ready, their hearts filled with courage and determination. They knew that the battle that lay ahead would not be easily won, but they were prepared to stand together against the forces of evil and protect their world from the dark sorcerer Malakhar. With their newfound knowledge, skills, and allies, they were ready to fight for the fate of Atlantis and the world beyond.</p><p>—</p><p>Night had fallen on Atlantis, and the city was bathed in the soft, enchanting glow of the Atlantean crystals. Aria, Langdon, and Alex retreated to their quarters, a space adorned with intricate murals and relics that chronicled the history of the lost city. They gathered around a low table, the flickering light of the crystals casting dancing shadows on the walls as they discussed their experiences and their plans for the days ahead.</p><p>As the conversation continued, Aria couldn’t help but feel a sense of responsibility for the fate of Atlantis, and she shared this with her companions. Langdon, ever the scholar, reassured her that they were all in this together, and that together, they would find a way to protect the city and its people. Alex added that they had each other’s backs, and there was nothing they couldn’t overcome as a team.</p><p>Elara, Idris, and Kara joined them at the table, bringing with them Atlantean delicacies and sweet beverages that seemed to sparkle in the crystal light. They shared stories of the city’s past, of the triumphs and tragedies that had befallen it throughout the ages. They spoke of the great ancestors who had once walked the city’s streets, their wisdom and strength now a part of the very fabric of Atlantis.</p><p>Aria listened intently, absorbing every word, feeling the weight of the history that surrounded them. As the tales unfolded, she felt a deepening connection to Atlantis and its people, and she knew that she would do everything in her power to protect them from the darkness that threatened their world.</p><p>As the night wore on, their laughter and camaraderie filled the room, offering a brief respite from the looming challenges that awaited them. With each passing hour, the bonds between the adventurers and their Atlantean allies grew stronger, forging a connection that would be vital in the days to come.</p><p>Eventually, the time came to rest and prepare for the trials ahead. Aria, Langdon, and Alex bid their new friends goodnight, the weight of their mission hanging heavily in the air. They knew that their greatest test was yet to come, and that the fate of Atlantis rested in their hands.</p><p>As they each retreated to their beds, Aria’s mind raced with thoughts of her visions and the challenges that lay ahead. She knew that her powers, and her connection to Queen Neria, would be instrumental in defeating Malakhar and ensuring the safety of Atlantis. With a heavy heart, she drifted off to sleep, determined to face whatever the future held with courage and unwavering resolve. As the first light of dawn crept into the room, the stage was set for a new chapter in their adventure, one that would bring them closer to the Heart of Atlantis and the power it held.<br>Chapter 10: The Heart of Atlantis</p><p>Aria stood at the edge of a balcony overlooking the city of Atlantis, the first light of dawn casting a warm glow over the magnificent landscape. She marveled at the gravity-defying gardens and floating fountains, a testament to the ancient civilization’s harmony with nature and their mastery of magic.</p><p>As Aria gazed out at the city, she could feel the energy in the air, the power that flowed through Atlantis and seemed to resonate with her own newfound abilities. Her thoughts drifted to Queen Neria and the visions that had led her here, wondering what role she was meant to play in the upcoming battle.</p><p>Langdon approached her, offering a reassuring smile and a steaming cup of Atlantean tea. “It’s beautiful, isn’t it?” he asked, taking in the view.</p><p>“It’s incredible,” Aria whispered, still in awe. “I can’t believe we’re really here.”</p><p>As they stood there, savoring the breathtaking vista, Alex joined them on the balcony, his eyes fixed on the horizon. His voice, though calm, was tinged with the gravity of their situation. “We’re not out of the woods yet,” he reminded them. “We need to find the Heart of Atlantis and prepare for Malakhar’s attack.”</p><p>Aria nodded, her eyes narrowing with determination. “We’ll be ready.”</p><p>The sun continued to rise, casting its golden light on the trio as they stood shoulder to shoulder, their resolve strengthening with each passing moment. United by their shared purpose, they were ready to face whatever challenges awaited them in the heart of this ancient and mysterious city.</p><p>As the first rays of sun spilled over the city’s crystalline structures, they knew the time had come to delve deeper into Atlantis’s secrets and unlock the power that lay at its core. They would need every ounce of strength, cunning, and magic they could muster to confront the darkness that threatened both Atlantis and the world above.</p><p>With a final look at the breathtaking panorama before them, Aria, Langdon, and Alex turned and strode back into the palace, ready to face whatever trials lay ahead in their quest to save Atlantis and the world from the clutches of Lord Malakhar.</p><p>—</p><p>In the courtyard below the palace, Aria trained with Kara, a formidable Atlantean warrior, to learn their unique combat techniques and master her newfound powers. Kara moved with grace and precision, her movements fluid and powerful. Aria tried to mimic her, feeling her muscles ache as she strained to keep up.</p><p>As they trained, Kara explained the principles of Atlantean combat, emphasizing the importance of balance, agility, and connection with the elements. Aria listened intently, trying to absorb the knowledge and techniques that seemed to come so naturally to the seasoned warrior.</p><p>“Focus on your breathing, Aria,” Kara instructed, as they moved through a series of intricate swordplay maneuvers. “Feel the energy of the earth beneath your feet and the air surrounding you. Let it guide your movements.”</p><p>Aria did as she was told, feeling a newfound sense of connection to the world around her. As she continued to train, she found herself moving with greater ease and speed, her powers growing stronger with each passing moment.</p><p>Langdon and Alex joined Aria in the training, learning about Atlantean weaponry and strategy. They sparred with other Atlantean warriors, honing their skills with an array of unique weapons, from crystal-tipped spears to razor-sharp throwing discs.</p><p>Together, they practiced coordinating their attacks and defenses, each learning to anticipate the others’ movements and support one another in battle. Their camaraderie and trust grew with every session, forging an unbreakable bond between them.</p><p>At the end of each day, their bodies bruised and their muscles sore, Aria, Langdon, and Alex pushed themselves to their limits, knowing that the battle ahead would be unlike any they had ever faced. And with each passing day, they grew more confident in their abilities, ready to face the challenges that lay ahead.</p><p>—</p><p>Aria, Langdon, and Alex met with Queen Neria, Elara, and Idris in the palace’s council chamber to discuss their strategy for defending Atlantis. Queen Neria’s presence was calm and commanding, and her wisdom inspired confidence in their plan.</p><p>Idris presented an ancient scroll, which contained a detailed map of the city’s layout, including its hidden passages and secret chambers. “We must use our knowledge of Atlantis to our advantage,” he suggested. “With Malakhar’s forces approaching, we must be prepared for anything.”</p><p>Elara chimed in, her eyes filled with determination. “We can also reinforce the city’s magical barriers and defenses. Our people have a deep connection to the natural elements, and we can use this to our benefit.”</p><p>Queen Neria nodded in agreement. “We must work together, combining our unique skills and knowledge to protect Atlantis.”</p><p>Aria, feeling the weight of her newfound responsibility, spoke up. “What about the Heart of Atlantis? Can we use its power to defend the city?”</p><p>Queen Neria regarded her thoughtfully before responding. “The Heart of Atlantis is a source of immense power, but it is also dangerous. If used carelessly, it could cause great destruction. We must find a way to harness its power without letting it consume us.”</p><p>Langdon and Alex shared a concerned glance but agreed with the queen’s assessment. They knew that Aria’s connection to the Heart of Atlantis would be crucial in the upcoming battle, but they also understood the risks involved.</p><p>As the council continued, the group discussed various strategies and contingency plans, ensuring that each member was prepared for the challenges ahead. They formed a united front, each individual contributing their strengths and expertise to defend their city and protect the world from the looming threat of Lord Malakhar.</p><p>“We believe the Heart of Atlantis is the key to our city’s ancient magic,” Queen Neria explained. “It must be protected at all costs.”</p><p>—</p><p>The group ventured deep into the ancient archives of Atlantis, searching for clues about the location of the Heart of Atlantis. Dusty scrolls and crumbling manuscripts filled the dimly lit chamber, their secrets waiting to be unlocked.</p><p>Aria, Langdon, and Alex carefully sifted through the fragile documents, the air thick with the musty scent of history. The walls were lined with shelves of scrolls, their once-vibrant colors faded with time. The archive seemed to stretch on for miles, a labyrinth of knowledge that threatened to swallow them whole.</p><p>As they delved deeper into the archive, Aria’s magical senses began to tingle, guiding her to a section filled with tomes bound in worn leather. She could feel the powerful energy emanating from one particular book, its spine adorned with intricate gold lettering.</p><p>Langdon’s eyes lit up as he recognized the ancient Atlantean script, and he carefully opened the delicate tome. The pages were filled with beautiful illustrations of the city, its people, and the wondrous magic that had once made it flourish.</p><p>Alex, ever the skeptic, couldn’t help but be captivated by the rich history they had uncovered. Even he was starting to believe in the magic that surrounded them, a subtle shift in his demeanor.</p><p>As Langdon translated the text, he discovered an account of the creation of the Heart of Atlantis, forged from the essence of the city’s magic. It was said to be hidden in a secret chamber within the palace, accessible only to those with pure intentions and a deep connection to the city.</p><p>Aria knew she was the key to unlocking the chamber, her newfound powers connecting her to the ancient magic of Atlantis. Together, they combed the archive, searching for any further clues to the location of the secret chamber.</p><p>After hours of searching, they discovered an ancient text that revealed the existence of a secret chamber within the palace where the Heart was hidden. Aria’s heart raced with anticipation as they prepared to uncover the long-lost artifact.</p><p>—</p><p>As the day wore on, Aria, Langdon, Alex, and their Atlantean comrades prepared the city for battle against Malakhar and his forces. They strengthened the magical barriers and defenses around the city, trained Atlantean soldiers, and set traps for the approaching enemy.</p><p>Queen Neria gathered her council, including Elara, Idris, and Kara, to discuss the strategies and tactics they would use in the upcoming battle. Aria, Langdon, and Alex were invited to join the meeting, their unique skills and experiences proving invaluable in creating a well-rounded plan.</p><p>In the city’s central square, Langdon led a group of scholars and engineers as they worked to reinforce the magical barrier that protected Atlantis. Drawing on his knowledge of ancient languages and arcane symbols, he guided the team in using the city’s power crystals to amplify the barrier’s strength.</p><p>Meanwhile, Alex used his experience in guerrilla warfare to help Kara and the Atlantean soldiers set traps and create chokepoints throughout the city. They rigged hidden pitfalls, tripwires, and collapsing walls to slow Malakhar’s forces, buying valuable time for the defenders.</p><p>Aria focused her efforts on mastering her newfound powers, aided by Elara and Idris. As they practiced, she learned to harness the energy of the Heart of Atlantis to create shields, unleash bolts of energy, and heal wounds. She also discovered that her connection to the Heart allowed her to sense the movements of Malakhar’s army, giving them an advantage in anticipating the enemy’s next move.</p><p>Despite the tension and the weight of their responsibility, the people of Atlantis moved with purpose and determination. The city’s inhabitants, from the youngest children to the oldest elders, pitched in to contribute to the war effort, crafting weapons, preparing medical supplies, and fortifying their homes.</p><p>As the sun began to set, casting long shadows across the city, Aria, Langdon, and Alex regrouped with their Atlantean allies. They shared a moment of camaraderie, acknowledging the daunting task that lay ahead, but also the strength they had found in one another.</p><p>Tension hung in the air, a palpable reminder of the danger that loomed on the horizon.</p><p>—</p><p>Aria, Langdon, and Alex searched the palace for the secret chamber mentioned in the ancient text. As they navigated through the intricate corridors and lavish rooms, the trio couldn’t help but admire the breathtaking art and architecture that surrounded them. Atlantean symbols adorned the walls, and beautifully crafted statues stood guard in every corner.</p><p>They solved a series of riddles and puzzles to unlock the entrance, their determination driving them forward. One such puzzle involved a complex arrangement of mirrors, which they had to position correctly to direct beams of light onto specific symbols etched onto a large stone door. Sweat dripped from their brows as they worked tirelessly to solve the conundrum, the pressure of their impending battle with Malakhar fueling their urgency.</p><p>In another challenge, they encountered a hidden room with walls that began to close in on them. As the walls inched closer, threatening to crush them, Aria noticed an inscription on the floor. Drawing from her newfound understanding of the ancient Atlantean language, she translated the message and discovered the key to stopping the walls from closing in. They quickly followed her instructions, and the walls halted just in time, granting them safe passage to continue their quest.</p><p>As they ventured deeper into the palace, they discovered a vast library filled with ancient texts and scrolls. Aria, Langdon, and Alex knew that the knowledge contained within these books could provide invaluable insights into the lost city’s history and magic, but they couldn’t afford to linger. They pressed on, their thoughts returning to the pressing task at hand – finding the Heart of Atlantis.</p><p>Finally, they found the hidden door, concealed behind an ornate tapestry. The tapestry depicted a majestic scene of Queen Neria and her people, harnessing the power of the Heart of Atlantis to create a thriving, magical utopia. The vibrant colors and lifelike details left the trio momentarily spellbound, but they quickly snapped back to reality, knowing that time was of the essence.</p><p>With a collective breath, they pushed the hidden door open, revealing a dimly lit chamber that awaited them.</p><p>—</p><p>Aria, Langdon, and Alex cautiously entered the dim chamber, drawn to the glowing crystal at its center. The Heart of Atlantis pulsed with energy, its light revealing ancient symbols on the walls.</p><p>As they approached the crystal, they noticed that the walls were adorned with intricate murals, telling the story of Atlantis’s creation and its people’s connection to the Heart. They studied the images, realizing that the Atlanteans had used the Heart’s power to build their extraordinary civilization and protect their city from the world outside.</p><p>Aria reached out tentatively, her fingers hovering above the crystal’s surface. She could feel the energy emanating from it, calling to her. She looked to Langdon and Alex, who gave her encouraging nods. Taking a deep breath, Aria placed her hand on the crystal.</p><p>A surge of power and knowledge filled her, ancient memories and secrets flooding her mind. She saw visions of the city’s founding and the great accomplishments of the Atlanteans. As the energy coursed through her, Aria felt an unbreakable bond forming between her and the Heart.</p><p>With newfound understanding, Aria turned to her companions. “The Heart of Atlantis is not just a source of power. It’s the very essence of this city, the spirit that binds its people together,” she said, her voice filled with awe.</p><p>Langdon nodded, his eyes reflecting the excitement of their discovery. “Harnessing its power won’t just help us defend Atlantis from Malakhar. It will restore the city to its former glory and ensure its survival for generations to come.”</p><p>As they prepared to leave the chamber, Queen Neria appeared before them, her regal bearing evident even in her spectral form. “The Heart of Atlantis has chosen you, Aria,” she said softly. “Use its power wisely, and never forget the bond that connects us all.”</p><p>With a determined nod, Aria vowed to honor Queen Neria’s memory and protect the city they both loved. “I will do everything in my power to save Atlantis and its people,” she promised.</p><p>As the first signs of Malakhar’s approaching army echoed through the palace, Aria, Langdon, Alex, and their Atlantean allies prepared for the imminent battle. With the Heart of Atlantis empowering them, they stood united to face the darkness and protect their world.<br>Chapter 11: The Battle for the Ages</p><p>The skies above Atlantis darkened, and the earth trembled beneath the weight of Malakhar’s approaching army. Aria, Langdon, Alex, and their Atlantean allies, including Idris, Elara, and Kara, gathered in the city’s central square to prepare for battle. Aria stood before the assembled warriors, her eyes filled with determination.</p><p>Taking a deep breath, Aria addressed the crowd. “My friends, we face a great enemy today. Lord Malakhar seeks to conquer our city and harness its power for his own nefarious purposes.” Her voice resonated throughout the square, echoing off the ancient stone walls that surrounded them.</p><p>“We must not let fear divide us,” she continued, her gaze sweeping over the diverse group of Atlantean soldiers, civilians, and her fellow adventurers. “For it is our unity, our courage, and our love for this city that will bring us victory.”</p><p>As she spoke, Aria could see the fear in the eyes of those around her begin to dissipate, replaced by a fierce determination that matched her own. Langdon and Alex stood by her side, their expressions resolute, while Idris, Elara, and Kara rallied the warriors, distributing weapons and offering words of encouragement.</p><p>Aria raised her hand, feeling the power of the Heart of Atlantis surge within her. The crystal embedded in her palm glowed with an ethereal light, casting an otherworldly glow over the assembled crowd. She drew upon the energy, infusing her words with the strength and magic of the ancient city.</p><p>“Today, we stand together to protect our home,” Aria declared, her voice strong and unwavering. “Atlantis has faced many challenges, but we will not let it fall to the likes of Malakhar. We will stand united, and we will triumph!”</p><p>The gathered Atlantean warriors raised their weapons in response, their spirits lifted by Aria’s inspiring words.</p><p>—</p><p>As Malakhar’s forces began their assault on Atlantis, Aria, Langdon, and Alex joined the Atlantean warriors on the front lines. They faced off against monstrous creatures and dark sorcerers, using their combined skills, powers, and Atlantean weaponry to hold their ground. The sounds of battle filled the air as the defenders fought bravely, refusing to yield to the invaders.</p><p>Aria, her magical abilities enhanced by her connection to the Heart of Atlantis, unleashed powerful blasts of energy against the monstrous creatures, tearing them apart and sending them hurtling through the air. Langdon, relying on his vast knowledge of ancient combat techniques, engaged the dark sorcerers in a fierce duel of wits and strategy, disrupting their dark incantations and turning their own power against them.</p><p>Alex, armed with a pair of razor-sharp Atlantean blades, demonstrated his unmatched agility and combat skills as he sliced through the enemy ranks, dispatching the monstrous creatures with deadly precision. The Atlantean warriors, inspired by the courage and determination of their newfound allies, fought with renewed vigor and determination, their weapons flashing with deadly intent as they held the line against the relentless onslaught.</p><p>In the midst of the chaos, Aria caught sight of a formidable opponent – a towering beast, its body covered in thick, armored scales and its eyes burning with malevolent intent. With a roar, it charged towards her, its massive claws raised and ready to strike. Aria steeled herself, focusing her power and raising her hands to meet the attack. As the creature neared, she unleashed a torrent of magical energy that slammed into the beast, sending it crashing to the ground with a resounding thud.</p><p>Emboldened by Aria’s victory, the Atlantean defenders pressed their advantage, driving Malakhar’s forces back inch by inch. The battle raged on, the air filled with the sounds of clashing steel, crackling magic, and the cries of the wounded and the dying. And yet, despite the mounting casualties and the ever-present threat of defeat, the defenders of Atlantis refused to surrender, their spirits buoyed by the unwavering courage and determination of Aria, Langdon, and Alex.</p><p>With each fallen enemy, the resolve of the Atlantean defenders grew stronger. Aria, Langdon, and Alex continued to fight fiercely, standing as a united front against the encroaching darkness.</p><p>—</p><p>As the battle raged on, Idris and Elara worked together to strengthen the magical barrier that protected Atlantis. Aria could feel her connection to the Heart of Atlantis pulsing, the energy flowing through her like a conduit. She focused her newfound power, lending her strength to Idris and Elara’s efforts to amplify the barrier’s power.</p><p>With each passing moment, the barrier wavered under the relentless attacks of Malakhar’s forces. Beads of sweat appeared on Idris and Elara’s brows, their faces etched with concentration as they poured their own energy into the barrier. Aria closed her eyes, her breathing steady as she delved deep within herself, searching for the magic that resonated within her.</p><p>As her consciousness connected with the Heart of Atlantis, Aria envisioned the vibrant, pulsating energy of the city flowing through her, imbuing her with the strength and wisdom of the ancients. She channeled this energy to Idris and Elara, and the three of them formed a powerful, unbreakable bond.</p><p>The air around them hummed with energy, the very essence of Atlantis coursing through their veins. With a sudden surge of power, Aria felt the barrier’s strength grow exponentially. The once flickering shield now shimmered with a dazzling intensity, holding firm against the relentless assault of Malakhar’s forces.</p><p>As the barrier grew stronger, the Atlantean warriors below felt a renewed sense of hope and vigor. They fought with increased ferocity, their movements swift and precise, as they repelled the invaders. The city’s defenders drew strength from Aria, Idris, and Elara’s efforts, a palpable wave of courage and determination washing over them.</p><p>The barrier flared with newfound strength, its shimmering light casting an ethereal glow over the battlefield. Aria, her face shining with determination, turned back to the fight, knowing that their combined efforts had bought them precious time.</p><p>—</p><p>The team devised a plan to infiltrate Malakhar’s forces and locate his stronghold. With the help of Atlantean scouts, Alex led a small group, including Langdon and Kara, on a daring mission to gather crucial information. They moved silently through the shadows, avoiding the notice of the enemy as they searched for the knowledge they sought.</p><p>As they ventured deeper into the enemy’s territory, they came across a series of hidden tunnels that seemed to lead towards the heart of Malakhar’s operation. Using the stealth and cunning they had honed throughout their journey, the trio navigated the dimly lit passageways, careful not to alert any guards or creatures that patrolled the area.</p><p>In one of the chambers, they stumbled upon a map detailing the sorcerer’s plans for the coming battle. Langdon, his eyes scanning the parchment with growing concern, realized that Malakhar had a secret weapon that could break through the magical barrier protecting Atlantis. They needed to find this weapon and neutralize it before it could be used against their city.</p><p>Alex, Langdon, and Kara continued their search, following the map towards the location of the weapon. Along the way, they overheard hushed conversations between Malakhar’s soldiers, giving them insight into the sorcerer’s strategies and weaknesses. This invaluable information bolstered their confidence, fueling their determination to succeed.</p><p>The trio finally reached the chamber housing Malakhar’s secret weapon: a massive, dark crystal pulsating with sinister energy. They knew they had to act quickly to disable it. Langdon, drawing on his extensive knowledge of ancient magic, began to decipher the runes and symbols that surrounded the crystal, hoping to find a way to neutralize its power.</p><p>Meanwhile, Alex and Kara stood guard, prepared to fend off any intruders. As Langdon worked, they could hear the sounds of battle echoing through the tunnels, a constant reminder of the urgency of their mission. Finally, after what felt like an eternity, Langdon discovered the key to disabling the crystal’s power. With a swift incantation, the dark energy dissipated, leaving the weapon useless.</p><p>Their mission complete, Alex, Langdon, and Kara returned to their comrades, bringing with them the key to turning the tide of battle in their favor.</p><p>—</p><p>Langdon discovered a hidden weakness in Malakhar’s defenses, allowing the team to launch a surprise counterattack. The Atlantean warriors, led by Aria, Langdon, and Alex, mounted a bold offensive, pushing Malakhar’s forces back and buying them valuable time. The enemy’s advance slowed, giving the defenders a momentary reprieve from the onslaught.</p><p>With Aria at the helm, her magical abilities and unwavering determination bolstered the morale of the Atlantean warriors. The sounds of clashing steel, crackling magic, and cries of the wounded and the dying resonated throughout the battlefield, painting a grim yet inspiring scene. Meanwhile, Alex and Langdon, each with their respective expertise, coordinated and orchestrated the Atlantean troops’ movements, exploiting every opportunity to weaken their enemies.</p><p>As the Atlantean warriors charged forward, they found themselves fighting alongside mystical creatures native to the lost city. These creatures, once thought to be mere myths, now stood as allies in the fight against Malakhar. Together, they clashed against the dark sorcerer’s monstrous minions and ruthless soldiers, inching closer and closer to victory.</p><p>At the heart of the battle, Aria’s bond with Idris and Elara grew even stronger, the three of them working in perfect harmony to unleash the full potential of the Heart of Atlantis. The ethereal glow of the barrier intensified, and with each passing moment, the city’s magical defenses grew exponentially stronger. As the barrier repelled Malakhar’s relentless attacks, hope and vigor were renewed among the defenders.</p><p>While Aria, Langdon, and the Atlantean warriors pushed back Malakhar’s forces, Alex, along with a handful of Atlantean scouts, infiltrated the enemy’s ranks in search of Malakhar’s secret weapon. Making their way through hidden tunnels and avoiding detection, they eventually found the weapon and neutralized it, further crippling the sorcerer’s offensive capabilities.</p><p>As Malakhar’s forces reeled from the surprise attack, Aria knew it was time to seize the opportunity and confront the sorcerer directly. She and her closest allies prepared for the most dangerous part of their mission.</p><p>—</p><p>Aria’s eyes met Malakhar’s cold gaze as she and her allies approached his stronghold. His dark fortress loomed before them, an oppressive presence that echoed the sorcerer’s hunger for power. Aria steeled herself, clenching her fists as the Heart of Atlantis pulsed within her, its energy surging like a tempest.</p><p>With a nod of affirmation from her companions, Aria led the charge, their small group moving swiftly and silently. They clashed with Malakhar’s minions, a mixture of twisted beasts and corrupted sorcerers. The air was thick with the scent of battle as steel met steel, and the crackle of magical energy filled their ears. Aria’s newfound mastery over her powers allowed her to protect her friends and vanquish their enemies, her resolve unwavering.</p><p>The group fought their way through the labyrinthine corridors, following a trail of dark energy that led them deeper into the heart of the fortress. Their bond, forged in the fires of adversity, served as their guiding light, each of them trusting one another implicitly. With every obstacle they overcame, they grew closer to their final confrontation with the sorcerer who threatened their world.</p><p>As they progressed, they discovered hidden passages and secret chambers, the remnants of ancient Atlantis repurposed by Malakhar for his nefarious purposes. Despite the darkness that had consumed the stronghold, glimpses of the city’s former beauty and majesty were still apparent, spurring the group onward in their quest to restore its light.</p><p>In a hidden chamber, they encountered a monstrous armored beast, its form twisted by Malakhar’s dark magic. It lunged at them, its powerful limbs shaking the ground beneath them. Aria summoned her courage, focusing her magical abilities and striking the beast with a blast of energy. Its armor cracked and shattered, releasing the creature from Malakhar’s control. In gratitude, the once-enslaved beast bowed to Aria and joined their cause.</p><p>Having battled their way through Malakhar’s last line of defense, Aria and her friends stood at the entrance to the sorcerer’s inner sanctum. The air hummed with dark energy, but Aria’s determination shone like a beacon in the darkness. With her companions by her side, she stepped forward, prepared to face the sinister lord and put an end to his reign of terror.</p><p>—</p><p>The heavy doors of Malakhar’s sanctum creaked open, revealing a dimly lit chamber where the sorcerer stood before an altar. Malakhar’s eyes gleamed with malevolence as Aria and her allies entered the room. Aria could feel the Heart of Atlantis resonating within her, its power reaching out to her as if sensing the impending confrontation.</p><p>The battle-worn group faced Malakhar, their weapons at the ready. Aria stepped forward, her voice steady and defiant. “This ends now, Malakhar. Your reign of terror is over.”</p><p>Malakhar laughed, his voice echoing through the chamber. “You think you can defeat me? I have harnessed the darkest powers, and you are but a mere child wielding a trinket.”</p><p>As he spoke, Malakhar summoned a monstrous armored beast, its massive form looming over them, threatening to crush the group beneath its feet. Aria’s friends engaged the creature, their attacks relentless, but it seemed impervious to their efforts.</p><p>Aria, realizing the connection between Malakhar and the beast, focused her attention on the sorcerer. She closed her eyes and drew upon the power of the Heart of Atlantis, feeling the energy build within her. The chamber filled with an ethereal glow as Aria unleashed a torrent of magical energy towards Malakhar.</p><p>The sorcerer, taken aback by the intensity of Aria’s attack, struggled to maintain his control over the beast. With a final surge of effort, Aria broke the dark bond between Malakhar and the creature. The armored beast, now free of Malakhar’s control, turned on its former master and lunged at him.</p><p>Malakhar, weakened by Aria’s assault and unable to defend himself, met his end at the hands of the very beast he had enslaved. As Malakhar’s lifeless body crumpled to the ground, Aria felt the power of the Heart of Atlantis surge within her one last time, then recede to a gentle hum. The chamber began to crumble around them, and Aria led her friends back through the stronghold, the now defeated Malakhar’s fortress falling apart behind them. They emerged into the light, their victory a beacon of hope for the people of Atlantis, and Aria knew that the next chapter of her life would begin in this ancient, magical city.<br>Chapter 12: The Echoes Resound</p><p>Aria, Langdon, and Alex stood at the entrance of the now stabilized Atlantean city, marveling at the ancient architecture and the vibrant life that filled the streets. Queen Neria approached the trio, her regal countenance softened by gratitude.</p><p>“Thank you, brave ones, for your courage and determination. Without you, Atlantis would have been lost to the darkness of Malakhar,” she said, her gaze lingering on Aria. “Aria, I invite you to stay with us in Atlantis, to learn more about your powers and your heritage. Your heart is as much a part of this city as the stone and water that surround us.”</p><p>Aria considered the offer, her heart torn between her newfound connection to the city and her friends on the surface. She glanced at Langdon and Alex, who both seemed to understand the weight of her decision. “This is your home now, Aria,” Langdon said gently. “We’ll be alright. You have a responsibility to these people, and to yourself.”</p><p>With a heavy heart, Aria nodded. “I will stay,” she declared, her eyes shining with determination. She hugged Langdon and Alex, each of them sharing an emotional farewell. “You’re like family to me. I won’t forget you,” she whispered.</p><p>“We’ll never forget you either,” Alex replied, his voice thick with emotion. “And who knows? Maybe we’ll see each other again someday.”</p><p>Langdon and Alex exchanged bittersweet smiles with Aria, knowing that their journey together had come to an end. Atlanteans gathered around them, cheering their victory over Malakhar and welcoming their new protector.</p><p>—</p><p>Over the following weeks, Aria began her training with Queen Neria, Idris, and Elara, learning to understand and harness her magical powers. As she immersed herself in the culture and traditions of Atlantis, she discovered a sense of belonging she had never experienced before.</p><p>Queen Neria guided Aria through the intricate rituals of Atlantean magic, teaching her to draw energy from the elements and channel it through her body. Idris, an experienced mage, showed Aria the secrets of potion-making and enchantment, while Elara, a skilled warrior, taught her the art of Atlantean combat.</p><p>One sunny afternoon, Aria stood on the training grounds, facing Elara. She took a deep breath, focusing her thoughts as she raised her arms in a graceful arc. A torrent of water surged from her hands, wrapping around a nearby wooden post like a coiling snake. Aria smiled as she felt the power coursing through her veins, exhilarated by her newfound abilities.</p><p>As the weeks passed, Aria grew stronger and more confident in her powers. The people of Atlantis began to look upon her with awe and admiration, whispering about the “chosen one” who had come to protect their city. She spent her days training and assisting in the rebuilding efforts, working side by side with Atlanteans who had once been strangers but now felt like family.</p><p>One evening, after a long day of laboring in the city’s central square, Aria joined a group of Atlanteans for a communal meal. They laughed and shared stories, their spirits lifted by the camaraderie and the knowledge that their city was slowly but surely returning to its former glory. Aria felt a warmth in her heart that she had never known before, a sense of belonging that transcended the barriers of time and distance.</p><p>Atlanteans worked tirelessly to rebuild their city, with Aria playing an active role in the reconstruction efforts. Through her kindness and strength, she formed close bonds with the people of Atlantis, who came to respect and admire their new protector.</p><p>—</p><p>Aria and her new Atlantean friends explored the depths of Atlantis, discovering hidden chambers, ancient artifacts, and untold secrets. As they delved into the city’s history, they learned about its relationship with the surface world and the wonders it had to offer.</p><p>One day, Aria, Elara, and Idris ventured into a long-forgotten library deep within the city. The dusty shelves were filled with ancient scrolls and tomes, preserving the knowledge of a civilization that had once been a shining beacon of wisdom and progress. Together, they unearthed texts that described the technological marvels of Atlantis, as well as the city’s contributions to the arts, philosophy, and science.</p><p>Aria was particularly fascinated by the stories of Atlantean emissaries who had traveled to the surface world in ages past, sharing their knowledge with the great civilizations of antiquity. She learned that the people of Atlantis had once been revered as wise teachers and benefactors, inspiring humanity to reach new heights of understanding and achievement.</p><p>As they continued their exploration, Aria and her friends also discovered a hidden chamber that housed an ancient device capable of creating a portal between Atlantis and the surface world. The room was adorned with intricate murals depicting Atlanteans interacting peacefully with humans, exchanging gifts, and learning from one another.</p><p>Over time, however, the relationship between Atlantis and the surface world had grown strained, as the humans began to covet the city’s power and sought to exploit its secrets for their own gain. Realizing the potential danger, the Atlanteans had chosen to withdraw from the world, sealing their city away beneath the waves and breaking all ties with the surface.</p><p>This knowledge sparked a desire within Aria to reconnect Atlantis with humanity, believing that both worlds could benefit from a shared understanding and a mutual exchange of wisdom.</p><p>—</p><p>Aria brought her proposal to Queen Neria and the council, passionately advocating for the possibility of opening the city to the surface world. The council members listened attentively, though some expressed their concerns about the potential dangers of revealing Atlantis to humanity.</p><p>As the council members discussed Aria’s proposal, she stood at the center of the council chamber, her gaze unwavering. She knew she had to persuade them to see the potential good that could come from reestablishing ties with the surface world. “I understand your concerns,” she said earnestly. “But we have a unique opportunity to bring about a new era of unity and understanding between Atlantis and the surface world. We can learn from each other and grow stronger together.”</p><p>Queen Neria nodded thoughtfully, her eyes filled with a mix of hope and concern. “Your words carry great weight, Aria, and I can see the wisdom in your proposal. However, we cannot ignore the risks that come with revealing our existence to a world that has long forgotten us.”</p><p>Aria acknowledged the queen’s concerns. “We can take careful steps to minimize the risks, Your Majesty. We can establish contact with select individuals who can help us foster a positive relationship with the surface world – people like Professor Langdon and Alex, who have shown their dedication to the preservation of Atlantis.”</p><p>One of the council members, a stern-looking woman with silver hair, spoke up. “While your intentions are noble, Aria, we cannot forget that the surface world has a history of exploiting and destroying the very wonders it claims to revere. How can we trust that they won’t do the same to Atlantis?”</p><p>Aria took a deep breath before responding. “Trust is something that must be built over time. We can’t expect to have it immediately, but by extending a hand in friendship and cooperation, we can begin to build that trust. It won’t be easy, and there will undoubtedly be setbacks, but the potential benefits for both Atlantis and the surface world are too great to ignore.”</p><p>The council members murmured among themselves, weighing Aria’s words against their own fears and uncertainties. Finally, an elder council member, his voice heavy with the weight of his years, posed the question that lingered in the minds of many.</p><p>“Are we truly ready to open our gates to a world that has forgotten us? Can we trust them not to exploit our secrets and our power?” questioned the elder council member, voicing the doubts that lingered in the minds of many.</p><p>—</p><p>Meanwhile, on the surface, Langdon and Alex had successfully shared the story of Atlantis with the rest of humanity. People across the globe were captivated by the tale of the lost city, and many expressed their interest in learning more about its magical secrets.</p><p>Langdon and Alex, now regarded as ambassadors of the lost city, embarked on a worldwide tour, participating in various conferences, interviews, and events. The duo showcased recovered artifacts, shared their experiences, and showed images of the marvelous city hidden beneath the waves. The world was mesmerized by their stories and the possibility of ancient knowledge that could shape the future.</p><p>During one of their presentations, a young girl with wide, curious eyes approached the stage. “What’s it like down there? Is it as magical as they say?” she asked, her voice filled with wonder.</p><p>Alex exchanged a glance with Langdon before answering, “It’s beyond anything you could ever imagine. Atlantis is a testament to the potential of human achievement and the power of unity. It’s a place where magic and technology exist in harmony, and its people live in peace and understanding.”</p><p>As the days passed, their story of Atlantis spread like wildfire, igniting a global fascination with the city and its inhabitants. People from all walks of life expressed their desire to learn from the Atlanteans, to study their magical arts, and to understand their advanced technology. Scholars, historians, and scientists clamored for the chance to explore and unlock the secrets that had remained hidden for millennia.</p><p>Aria received a message from her friends detailing the world’s reaction to their discovery, and she couldn’t help but feel a sense of hope that perhaps the time for Atlantis to reemerge had finally come.</p><p>—</p><p>With renewed determination, Aria, Queen Neria, and the council continued their discussions on whether to open Atlantis to the world. Aria passionately argued that Atlantis could serve as a beacon of wisdom and unity for humanity, a place where the wonders of magic and technology could coexist and create a brighter future.</p><p>“I understand the fears that some of you may have,” Aria acknowledged, glancing at the council members who had expressed their concerns. “But we must also remember that humanity has the capacity for great goodness, curiosity, and innovation. By sharing our knowledge and our magic, we can help foster a better understanding between our worlds and inspire people to work together for a greater purpose.”</p><p>Elara, Idris, and Kara joined Aria in her plea, sharing their own experiences and insights on the potential benefits of opening Atlantis to humanity. They spoke of the curiosity and ingenuity they had seen in Aria, Langdon, and Alex, and the great strides that could be made if both worlds were to collaborate in areas such as medicine, technology, and environmental preservation.</p><p>As the council members considered their arguments, Aria recalled the battles she and her friends had fought, the monstrous armored beast they had defeated, and the love and camaraderie they had shared. She believed that if they could overcome such obstacles, surely the people of Atlantis and the surface world could work together to face the challenges that lay ahead.</p><p>“We have the chance to bring hope and understanding to a world that has long been divided,” Aria urged. “We can be the bridge that connects our two worlds and ushers in an era of harmony and progress.”</p><p>—</p><p>After much deliberation, Queen Neria agreed to take the first steps toward opening the doors of Atlantis to the world. It was a decision that weighed heavily on her heart, but she trusted Aria’s vision and believed that the time had come for Atlantis to embrace its destiny.</p><p>As word of the decision spread throughout the city, the people of Atlantis experienced a mixture of emotions. While some were excited at the prospect of reestablishing connections with the surface world, others were wary, fearing the potential dangers that might accompany such a monumental change. Nevertheless, the majority of Atlanteans trusted in their queen’s wisdom and Aria’s conviction.</p><p>Aria and her friends took on the responsibility of planning the initial steps of this bold endeavor. They knew they would need to establish a way to communicate with the surface world and create a system for sharing knowledge and resources.</p><p>To ensure that the process would go smoothly, Aria and her friends sought out the wisdom of the Atlantean scholars and the council members. In a series of meetings, they discussed the potential benefits of reestablishing connections with the surface world, such as advancements in medicine, technology, and environmental preservation.</p><p>During this time, Aria trained with Idris and Elara to strengthen her magical abilities and further connect with the Heart of Atlantis. She also practiced Atlantean martial arts with Kara, learning how to wield the unique Atlantean weapons with grace and skill.</p><p>As the preparations continued, a group of Atlantean engineers and architects worked diligently on a massive project: the construction of an ancient portal device that would allow travel between Atlantis and the surface world. The device, which had been discovered in one of the city’s hidden chambers, had been dormant for centuries, but with the combined efforts of the city’s brightest minds, it was slowly brought back to life.</p><p>In the meantime, on the surface world, Langdon and Alex met with leaders from various nations, sharing their story and discussing the possibility of peaceful relations with Atlantis. They found many who were open to the idea and eager to learn from the lost city’s wisdom.</p><p>Back in Atlantis, a sense of anticipation filled the air as the day of the portal’s activation drew near. Aria, Elara, Idris, and Kara spent their remaining days in the city preparing for their new roles as ambassadors of Atlantis, ready to face whatever challenges lay ahead.</p><p>With a sense of cautious optimism, the people of Atlantis began to prepare for the monumental task of reintroducing their city to the world. They understood that their decision held both great promise and risk, but with Aria, Queen Neria, and their allies leading the way, they held faith in the possibility of a better future for both Atlantis and the surface world.</p><p>—</p><p>As the day of departure approached, Aria stood on the shores of Atlantis, the waves gently lapping at her feet. She felt a mix of excitement and trepidation at the thought of her upcoming journey to the surface world as an ambassador of her newfound home. Clutching Queen Neria’s pendant, she found solace in the knowledge that she carried the spirit and wisdom of Atlantis with her.</p><p>Elara, Idris, and Kara joined her at the shore, their faces a mix of excitement and uncertainty. Together, they had spent countless hours planning and preparing for this moment, consulting with experts and sifting through ancient tomes to uncover the secrets of a long-lost portal that would allow them to travel between Atlantis and the surface world. As the last rays of sunlight danced across the water, they shared stories of their ancestors, who had once walked among the humans, and dreamed of the adventures that awaited them.</p><p>In the days leading up to their departure, Aria had worked tirelessly with Langdon and Alex, communicating through a series of magically enchanted mirrors. Together, they had arranged meetings with some of the most influential leaders and scholars of the surface world, hoping to foster a spirit of cooperation and understanding between Atlantis and humanity. Their efforts had been met with cautious optimism and curiosity, and Aria knew that the success of her mission hinged on their ability to navigate the complex web of politics and diplomacy that awaited them.</p><p>With a final farewell to their friends and families, Aria, Elara, Idris, and Kara gathered at the ancient portal chamber, the air thick with anticipation. They had spent weeks deciphering the complex array of runes and symbols that adorned the portal, and now, with a combination of magic and technology, they had successfully activated the once-dormant device. As the portal hummed to life, casting a soft, shimmering light across the room, they knew that they stood on the precipice of a new era for Atlantis and the world above.</p><p>As the sun began to set, casting a golden light over the ocean, Aria stood at the edge of the city, wearing Queen Neria’s pendant around her neck. She gazed out at the horizon, her heart filled with hope and determination. The world she had known was changing, and she was a part of that change. Atlantis, once a city lost in time, was now poised to share its magic and wonder with the world. Taking a deep breath, Aria took the first step toward reuniting Atlantis with the surface world, ready to face the challenges and opportunities that lay ahead. As the echoes of the past mingled with the hopes for the future, she knew that she and Atlantis were embarking on an incredible new adventure.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Make chatGPT API response faster, user more happy, by using streaming mode</title>
      <link href="2023/04/02/speedup-chatgpt-and-do-streaming/"/>
      <url>2023/04/02/speedup-chatgpt-and-do-streaming/</url>
      
        <content type="html"><![CDATA[<p>It’s actually not really making results coming from chatGPT API much faster, but making users feel things are moving much faster.<br>So the user experience is a lot better.</p><p>The key is to enable streamming mode when calling the chatGPT API.</p><p>Here is the example code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Example of an OpenAI ChatCompletion request with stream=True</span><br><span class="line"># https://platform.openai.com/docs/guides/chat</span><br><span class="line"></span><br><span class="line"># a ChatCompletion request</span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">    model=&#x27;gpt-3.5-turbo&#x27;,</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;&#x27;role&#x27;: &#x27;user&#x27;, &#x27;content&#x27;: &quot;What&#x27;s 1+1? Answer in one word.&quot;&#125;</span><br><span class="line">    ],</span><br><span class="line">    temperature=0,</span><br><span class="line">    stream=True  # this time, we set stream=True</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">for chunk in response:</span><br><span class="line">    print(chunk)</span><br></pre></td></tr></table></figure><p>As you can see, the key is to add one more parameter as <code>stream=True</code>.<br>And here is the result from the above code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;choices&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;delta&quot;: &#123;</span><br><span class="line">        &quot;role&quot;: &quot;assistant&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;finish_reason&quot;: null,</span><br><span class="line">      &quot;index&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;created&quot;: 1677825464,</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD&quot;,</span><br><span class="line">  &quot;model&quot;: &quot;gpt-3.5-turbo-0301&quot;,</span><br><span class="line">  &quot;object&quot;: &quot;chat.completion.chunk&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;choices&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;delta&quot;: &#123;</span><br><span class="line">        &quot;content&quot;: &quot;\n\n&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;finish_reason&quot;: null,</span><br><span class="line">      &quot;index&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;created&quot;: 1677825464,</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD&quot;,</span><br><span class="line">  &quot;model&quot;: &quot;gpt-3.5-turbo-0301&quot;,</span><br><span class="line">  &quot;object&quot;: &quot;chat.completion.chunk&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;choices&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;delta&quot;: &#123;</span><br><span class="line">        &quot;content&quot;: &quot;2&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;finish_reason&quot;: null,</span><br><span class="line">      &quot;index&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;created&quot;: 1677825464,</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD&quot;,</span><br><span class="line">  &quot;model&quot;: &quot;gpt-3.5-turbo-0301&quot;,</span><br><span class="line">  &quot;object&quot;: &quot;chat.completion.chunk&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;choices&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;delta&quot;: &#123;&#125;,</span><br><span class="line">      &quot;finish_reason&quot;: &quot;stop&quot;,</span><br><span class="line">      &quot;index&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">  ],</span><br><span class="line">  &quot;created&quot;: 1677825464,</span><br><span class="line">  &quot;id&quot;: &quot;chatcmpl-6ptKyqKOGXZT6iQnqiXAH8adNLUzD&quot;,</span><br><span class="line">  &quot;model&quot;: &quot;gpt-3.5-turbo-0301&quot;,</span><br><span class="line">  &quot;object&quot;: &quot;chat.completion.chunk&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> stream </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between git rebase and git merge clearly explained finally</title>
      <link href="2023/03/31/git-base-vs-git-merge-finall-clearly-explained/"/>
      <url>2023/03/31/git-base-vs-git-merge-finall-clearly-explained/</url>
      
        <content type="html"><![CDATA[<p>Git rebase and git merge are both used to integrate changes from one branch into another, but they do so in different ways.</p><h2 id="Git-Merge"><a href="#Git-Merge" class="headerlink" title="Git Merge:"></a>Git Merge:</h2><p>When you merge one branch into another, git takes the changes that were made in the source branch and applies them directly onto the destination branch. This creates a new commit on the destination branch that includes all of the changes from the source branch. Git merge preserves the entire history of both branches and creates a new commit that combines the two histories.</p><h2 id="Git-Rebase"><a href="#Git-Rebase" class="headerlink" title="Git Rebase:"></a>Git Rebase:</h2><p>Git rebase, on the other hand, rewrites the history of the source branch to make it look like it was based off the destination branch from the beginning. Instead of creating a new commit, git rebase takes the changes made in the source branch and reapplies them on top of the destination branch. This creates a linear history with a single branch, and it can make it easier to understand the flow of changes.</p><p>The key difference between git merge and git rebase is that git merge preserves the entire history of both branches, whereas git rebase rewrites the history of the source branch. Git merge is generally used for integrating long-lived feature branches into a stable branch, while git rebase is typically used for integrating short-lived feature branches into a stable branch.</p><p>Another difference between the two is that git merge creates a new commit, while git rebase changes the existing commits in the source branch. This can make it harder to revert changes made with git rebase, but it can also make the history easier to understand and maintain.</p><p>Overall, both git merge and git rebase are useful tools for integrating changes into a Git repository, and the choice between them depends on the specific needs of the project and the preferences of the development team.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Two simple ways to rebase your development repo on github before your renew your work</title>
      <link href="2023/03/31/two-simple-ways-to-rebase-your-github-development-repo/"/>
      <url>2023/03/31/two-simple-ways-to-rebase-your-github-development-repo/</url>
      
        <content type="html"><![CDATA[<p>As you work on a new feature in your Git repository, you might find that changes are being made to the master branch that you need to incorporate into your feature branch. In this case, you have two options to rebase your feature branch with the latest changes from the master branch: you can do it locally using Git, or you can do it on the GitHub page using pull requests.</p><h2 id="Method-1-Rebase-locally-using-Git"><a href="#Method-1-Rebase-locally-using-Git" class="headerlink" title="Method 1: Rebase locally using Git"></a>Method 1: Rebase locally using Git</h2><h3 id="Step-1-Checkout-the-feature-branch"><a href="#Step-1-Checkout-the-feature-branch" class="headerlink" title="Step 1: Checkout the feature branch"></a>Step 1: Checkout the feature branch</h3><p>First, make sure you’re on the feature branch that you want to rebase with the master branch. You can do this by using the git checkout command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout feature-branch</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Step-2-Fetch-the-latest-changes-from-the-master-branch"><a href="#Step-2-Fetch-the-latest-changes-from-the-master-branch" class="headerlink" title="Step 2: Fetch the latest changes from the master branch"></a>Step 2: Fetch the latest changes from the master branch</h3><p>Next, you need to fetch the latest changes from the master branch. This will allow you to see any changes that have been made since you started working on your feature branch. You can do this using the git fetch command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git fetch origin master</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will fetch the latest changes from the master branch on the remote repository named “origin”.</p><h3 id="Step-3-Rebase-your-feature-branch-with-the-latest-changes-from-the-master-branch"><a href="#Step-3-Rebase-your-feature-branch-with-the-latest-changes-from-the-master-branch" class="headerlink" title="Step 3: Rebase your feature branch with the latest changes from the master branch"></a>Step 3: Rebase your feature branch with the latest changes from the master branch</h3><p>Now that you have the latest changes from the master branch, you can rebase your feature branch with those changes. You can do this using the git rebase command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git rebase origin/master</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will apply the latest changes from the master branch to your feature branch, one commit at a time. If there are any conflicts between your feature branch and the master branch, Git will prompt you to resolve them before continuing.</p><h3 id="Step-4-Push-the-updated-feature-branch-to-the-remote-repository"><a href="#Step-4-Push-the-updated-feature-branch-to-the-remote-repository" class="headerlink" title="Step 4: Push the updated feature branch to the remote repository"></a>Step 4: Push the updated feature branch to the remote repository</h3><p>Once you’ve resolved any conflicts and rebased your feature branch with the latest changes from the master branch, you need to push your updated feature branch to the remote repository so that others can see and work with your changes. You can do this using the git push command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git push origin feature-branch</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will push the updated feature branch to the remote repository named “origin”.</p><h3 id="Step-5-Merge-the-feature-branch-back-into-the-master-branch"><a href="#Step-5-Merge-the-feature-branch-back-into-the-master-branch" class="headerlink" title="Step 5: Merge the feature branch back into the master branch"></a>Step 5: Merge the feature branch back into the master branch</h3><p>Once you’re finished working on your feature branch and have successfully rebased it with the latest changes from the master branch, you can merge your feature branch back into the master branch. You can do this using the git merge command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br><span class="line">git merge feature-branch</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will merge the changes from your feature branch into the master branch, and create a new merge commit. You can then push the updated master branch to the remote repository using the git push command.</p><h2 id="Method-2-Rebase-on-GitHub-using-pull-requests"><a href="#Method-2-Rebase-on-GitHub-using-pull-requests" class="headerlink" title="Method 2: Rebase on GitHub using pull requests"></a>Method 2: Rebase on GitHub using pull requests</h2><h3 id="Step-1-Create-a-pull-request-for-your-feature-branch"><a href="#Step-1-Create-a-pull-request-for-your-feature-branch" class="headerlink" title="Step 1: Create a pull request for your feature branch"></a>Step 1: Create a pull request for your feature branch</h3><p>The first step is to create a pull request for your feature branch. This will allow you to merge your changes into the master branch once they have been approved. To do this, go to the GitHub repository and click on the “New pull request” button. Then select your feature branch as the “compare” branch and the master branch as the “base” branch.</p><h3 id="Step-2-Review-the-pull-request"><a href="#Step-2-Review-the-pull-request" class="headerlink" title="Step 2: Review the pull request"></a>Step 2: Review the pull request</h3><p>Once you’ve created the pull request, you can review the changes and make any necessary adjustments. If you need to make changes to your feature branch, you can do so locally and then push the changes to the remote repository.</p><h3 id="Step-3-Rebase-your-feature-branch-with-the-latest-changes-from-the-master-branch-1"><a href="#Step-3-Rebase-your-feature-branch-with-the-latest-changes-from-the-master-branch-1" class="headerlink" title="Step 3: Rebase your feature branch with the latest changes from the master branch"></a>Step 3: Rebase your feature branch with the latest changes from the master branch</h3><p>Before merging your changes into the master branch, you’ll want to rebase your feature branch with the latest changes from the master branch. You can do this using the “Rebase and merge” option in the pull request interface. This will apply the latest changes from the master branch to your feature branch and then merge the changes into the master branch.</p><h3 id="Step-4-Resolve-any-conflicts"><a href="#Step-4-Resolve-any-conflicts" class="headerlink" title="Step 4: Resolve any conflicts"></a>Step 4: Resolve any conflicts</h3><p>If there are any conflicts between your feature branch and the master branch, you’ll need to resolve them before you can merge your changes. GitHub will notify you of any conflicts and provide tools to help you resolve them.</p><h3 id="Step-5-Merge-your-changes-into-the-master-branch"><a href="#Step-5-Merge-your-changes-into-the-master-branch" class="headerlink" title="Step 5: Merge your changes into the master branch"></a>Step 5: Merge your changes into the master branch</h3><p>Once any conflicts have been resolved, you can merge your changes into the master branch using the “Merge pull request” button in the pull request interface. This will create a new merge commit that incorporates the changes from your feature branch into the master branch.</p><h3 id="Step-6-Delete-the-feature-branch"><a href="#Step-6-Delete-the-feature-branch" class="headerlink" title="Step 6: Delete the feature branch"></a>Step 6: Delete the feature branch</h3><p>Once you’ve merged your changes into the master branch, you can delete your feature branch. This will help keep your repository clean and make it easier to manage your branches in the future. You can do this using the “Delete branch” button in the pull request interface.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The Rise of ChatGPT, A Boon or a Threat to Jobs?</title>
      <link href="2023/03/30/chatGPT-and-other-AI-get-or-take-jobs-from-people/"/>
      <url>2023/03/30/chatGPT-and-other-AI-get-or-take-jobs-from-people/</url>
      
        <content type="html"><![CDATA[<p>The rapid development and adoption of ChatGPT, a conversational AI interface built on the GPT-3 large language model, has sparked debates on whether it will replace various jobs or simply augment human skills. While some fear the loss of jobs in fields like programming, journalism, and creative writing, others argue that AI-generated text will not replace human expertise but rather change the way we work.</p><h2 id="Why-ChatGPT-Won’t-Take-Your-Job"><a href="#Why-ChatGPT-Won’t-Take-Your-Job" class="headerlink" title="Why ChatGPT Won’t Take Your Job"></a>Why ChatGPT Won’t Take Your Job</h2><p>Despite its impressive capabilities, ChatGPT is still limited and sometimes produces inaccurate or nonsensical responses. When faced with complex prompts, it tends to generate polished but ultimately incorrect or irrelevant answers. This is particularly evident in academic and technical domains, where the AI may fabricate references or provide misinformation.</p><p>Moreover, past information technologies have rarely replaced human jobs; instead, they have impacted how we perform our jobs. For example, the internet has changed the way we communicate but has not eliminated the need for human interaction. Similarly, ChatGPT may change the way we write texts or content, but it is unlikely to replace human creativity and critical thinking.</p><p>In fact, ChatGPT could be a valuable asset when used correctly. For instance, journalists could use it to generate multiple headlines for an article, while programmers could use it to solve straightforward problems. The key is to understand its limitations and use it as a partner in problem-solving or creative writing tasks.</p><h2 id="Why-ChatGPT-Might-Replace-Your-Job"><a href="#Why-ChatGPT-Might-Replace-Your-Job" class="headerlink" title="Why ChatGPT Might Replace Your Job"></a>Why ChatGPT Might Replace Your Job</h2><p>On the other hand, ChatGPT’s ability to generate properly formatted, relevant code in multiple languages and refine requests over the course of a session has raised concerns about its potential impact on various professions. Here are some jobs where ChatGPT could make a significant difference:</p><p>Lawyers: ChatGPT can assist lawyers by creating summaries of case notes, drafting documents, and reducing research time, allowing them to focus on client interactions and legal strategies.</p><p>Copywriters: While original thought is still crucial for effective copywriting, ChatGPT can help generate ideas and suggest ways to make copy more engaging.</p><p>Journalists: AI has already been used to create automated reports in journalism. ChatGPT can help create article outlines, summaries, headlines, and other features, allowing journalists to focus on original reporting and storytelling.</p><p>Computer Programmers: ChatGPT can generate code in various languages, potentially replacing some programming tasks. However, it is currently unable to create complex software applications.</p><p>Human Resources: HR professionals can use ChatGPT to write job descriptions, automate routine tasks, and create onboarding materials, freeing up time for more personal interactions with employees.</p><p>Data Analysts: ChatGPT can help create reports, summarize data, and translate insights into words, allowing analysts to focus on more complex tasks and decision-making.</p><p>Customer Services: ChatGPT can power chatbots to handle incoming customer inquiries, enabling customer service agents to focus on more challenging issues.</p><p>Salespeople: ChatGPT can automate tasks such as entering information into CRM databases and writing introductory letters, allowing salespeople to concentrate on building relationships and developing sales strategies.</p><p>Teachers: ChatGPT can help create lesson plans, summaries, quizzes, and other educational materials, allowing teachers to focus on student interactions and teaching strategies.</p><p>Public Relations: PR professionals can use ChatGPT to create social media posts, monitor media mentions, and generate summarized reports, allowing them to focus on relationship-building and crisis management.</p><h2 id="The-Bottom-Line"><a href="#The-Bottom-Line" class="headerlink" title="The Bottom Line"></a>The Bottom Line</h2><p>The rise of ChatGPT presents both opportunities and challenges for various professions. While it is unlikely to replace human expertise entirely, it will undoubtedly change the way we work and interact with technology. By understanding its limitations and potential, we can harness its power to augment our skills and improve our productivity. Ultimately, the key to success in the era of AI and automation lies in adapting to these changes and focusing on the human skills that machines cannot yet replicate.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> GPT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to select multiple cells in Jupyter Notebook or Databricks Notebook and delete them</title>
      <link href="2023/03/29/select-multiple-cells-in-notebook/"/>
      <url>2023/03/29/select-multiple-cells-in-notebook/</url>
      
        <content type="html"><![CDATA[<p>If you frequently work with Jupyter Notebook or Databricks Notebook, you might need to select and manipulate multiple cells at once. This task might seem daunting at first, but it’s actually quite simple once you know how to do it. In this blog post, we will discuss two situations in which you might need to select multiple cells and perform operations such as deleting them all at once.</p><h2 id="Situation-1-Selecting-Continuous-Cells"><a href="#Situation-1-Selecting-Continuous-Cells" class="headerlink" title="Situation 1: Selecting Continuous Cells"></a>Situation 1: Selecting Continuous Cells</h2><p>If you want to select cells that are close to each other continuously, the easiest way to do this is by holding the shift key and using the up and down arrow button to select them one by one. This method is similar to selecting multiple files in a file explorer. Simply click on the first cell you want to select, hold the shift key, and use the up and down arrow button to select the rest of the cells. Once you’ve selected all the cells you want to manipulate, release the shift key.</p><h2 id="Situation-2-Selecting-Non-Adjacent-Cells"><a href="#Situation-2-Selecting-Non-Adjacent-Cells" class="headerlink" title="Situation 2: Selecting Non-Adjacent Cells"></a>Situation 2: Selecting Non-Adjacent Cells</h2><p>If you want to select multiple cells that are not adjacent to each other, you can use the ctrl key and mouse to click any of the cells one by one. To do this, simply click on the first cell you want to select, hold the ctrl key, and click on the other cells you want to select. Again, make sure to click on the outer part of each cell, not the inside, to select the entire cell. Once you’ve selected all the cells you want to manipulate, release the ctrl key.</p><p>After selecting multiple cells, you can perform various operations on them, such as copying, cutting, pasting, or deleting them all at once. To delete multiple cells, simply click the Edit menu and choose Delete Cells. All the selected cells will be deleted at once.</p><p>In summary, selecting multiple cells in a Jupyter Notebook or Databricks Notebook is simple and can save you a lot of time if you need to manipulate multiple cells at once. Remember to use the shift key for selecting continuous cells and the ctrl key for selecting non-adjacent cells. Make sure to click on the outer part of each cell to select the entire cell, and you’ll be able to perform operations on multiple cells in no time.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
            <tag> jupyter notebook </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Building a FastAPI Webpage with AWS Cognito Authentication and a Simple Chatbot</title>
      <link href="2023/03/28/fastpai-with-aws-cognito-for-user-authentication/"/>
      <url>2023/03/28/fastpai-with-aws-cognito-for-user-authentication/</url>
      
        <content type="html"><![CDATA[<p>Introduction:<br>In this tutorial, we will build a FastAPI webpage that uses AWS Cognito for user authentication. Once the user logs in, they will see a simple chatbot page. We will walk through the code step by step and explain the main sections.</p><p>Before we start, make sure you have the following packages installed in your Python environment:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install fastapi</span><br><span class="line">pip install httpx</span><br><span class="line">pip install pyjwt[crypto]</span><br><span class="line">pip install uvicorn</span><br></pre></td></tr></table></figure><p>Notice that, the default pyjwt package won’t work, you might get erros such as <code>The JWK Set did not contain any usable keys</code> or <code>AttributeError: module &#39;jwt&#39; has no attribute &#39;JWTError&#39;</code>.<br>It has to be the package install like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pyjwt[crypto]</span><br></pre></td></tr></table></figure><p>Now, let’s dive into the code.</p><ol><li>Import necessary libraries and create a FastAPI instance:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> httpx</span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> FastAPI, Depends, HTTPException, status</span><br><span class="line"><span class="keyword">from</span> fastapi.responses <span class="keyword">import</span> HTMLResponse, RedirectResponse</span><br><span class="line"><span class="keyword">from</span> fastapi.security <span class="keyword">import</span> OAuth2PasswordBearer</span><br><span class="line"><span class="keyword">from</span> fastapi <span class="keyword">import</span> Request</span><br><span class="line"><span class="keyword">import</span> jwt</span><br><span class="line"><span class="keyword">from</span> jwt <span class="keyword">import</span> PyJWKClient</span><br><span class="line"></span><br><span class="line">app = FastAPI()</span><br></pre></td></tr></table></figure><ol start="2"><li>Set up AWS Cognito settings:</li></ol><p>Replace the following variables with your own AWS Cognito settings.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">COGNITO_DOMAIN = os.environ.get(<span class="string">&quot;COGNITO_DOMAIN&quot;</span>)</span><br><span class="line">COGNITO_CLIENT_ID = os.environ.get(<span class="string">&quot;COGNITO_CLIENT_ID&quot;</span>)</span><br><span class="line">COGNITO_CLIENT_SECRET = os.environ.get(<span class="string">&quot;COGNITO_CLIENT_SECRET&quot;</span>)</span><br><span class="line">COGNITO_REDIRECT_URI = os.environ.get(<span class="string">&quot;COGNITO_REDIRECT_URI&quot;</span>)</span><br><span class="line">COGNITO_REGION = os.environ.get(<span class="string">&quot;COGNITO_REGION&quot;</span>)</span><br><span class="line">COGNITO_USER_POOL_ID =  os.environ.get(<span class="string">&quot;COGNITO_USER_POOL_ID&quot;</span>)</span><br><span class="line"></span><br><span class="line">AUTH_URL = <span class="string">f&quot;https://<span class="subst">&#123;COGNITO_DOMAIN&#125;</span>/oauth2/authorize&quot;</span></span><br><span class="line">TOKEN_URL = <span class="string">f&quot;https://<span class="subst">&#123;COGNITO_DOMAIN&#125;</span>/oauth2/token&quot;</span></span><br><span class="line">LOGOUT_URL = <span class="string">f&quot;https://<span class="subst">&#123;COGNITO_DOMAIN&#125;</span>/logout&quot;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>Get Cognito JWT secret:</li></ol><p>This function fetches the JSON Web Key Set (JWKS) from AWS Cognito and returns the public key in PEM format.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_cognito_jwt_secret</span>() -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    JWKS_URL = <span class="string">f&quot;https://cognito-idp.<span class="subst">&#123;COGNITO_REGION&#125;</span>.amazonaws.com/<span class="subst">&#123;COGNITO_USER_POOL_ID&#125;</span>/.well-known/jwks.json&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> httpx.AsyncClient() <span class="keyword">as</span> client:</span><br><span class="line">        response = <span class="keyword">await</span> client.get(JWKS_URL)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> response.status_code != <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">&quot;Failed to fetch JWKS from Cognito&quot;</span>)</span><br><span class="line"></span><br><span class="line">    jwks = response.json()</span><br><span class="line">    <span class="keyword">for</span> key_data <span class="keyword">in</span> jwks[<span class="string">&quot;keys&quot;</span>]:</span><br><span class="line">        <span class="keyword">if</span> key_data[<span class="string">&quot;alg&quot;</span>] == <span class="string">&quot;RS256&quot;</span> <span class="keyword">and</span> key_data[<span class="string">&quot;use&quot;</span>] == <span class="string">&quot;sig&quot;</span>:</span><br><span class="line">            key = jwk.construct(key_data)</span><br><span class="line">            <span class="keyword">return</span> key.to_pem().decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> Exception(<span class="string">&quot;Failed to find a suitable public key in JWKS&quot;</span>)</span><br></pre></td></tr></table></figure><ol start="4"><li>Get token and current user:</li></ol><p>These functions are used to get the token from the request and to decode the token to get the current user’s information.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_token</span>(<span class="params">request: Request</span>):</span></span><br><span class="line">    token = request.query_params.get(<span class="string">&quot;token&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> token:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=<span class="string">&quot;Token is required&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> token</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">get_current_user</span>(<span class="params">token: <span class="built_in">str</span> = Depends(<span class="params">get_token</span>)</span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    JWKS_URL = <span class="string">f&quot;https://cognito-idp.<span class="subst">&#123;COGNITO_REGION&#125;</span>.amazonaws.com/<span class="subst">&#123;COGNITO_USER_POOL_ID&#125;</span>/.well-known/jwks.json&quot;</span></span><br><span class="line">    client = PyJWKClient(JWKS_URL)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        header = jwt.get_unverified_header(token)</span><br><span class="line">        key = client.get_signing_key(header[<span class="string">&quot;kid&quot;</span>])</span><br><span class="line">        public_key = key.key</span><br><span class="line">        payload = jwt.decode(token, public_key, algorithms=[<span class="string">&quot;RS256&quot;</span>])</span><br><span class="line">        <span class="keyword">return</span> payload[<span class="string">&quot;sub&quot;</span>]</span><br><span class="line">    <span class="keyword">except</span> jwt.ExpiredSignatureError:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=<span class="string">&quot;Token has expired&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> jwt.JWTClaimsError:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=<span class="string">&quot;Invalid token claims&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> jwt.JWTError:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=status.HTTP_403_FORBIDDEN, detail=<span class="string">&quot;Invalid token&quot;</span>)</span><br></pre></td></tr></table></figure><ol start="5"><li>Login endpoint:</li></ol><p>This endpoint renders the login page with a form that redirects the user to the AWS Cognito authentication page.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/&quot;</span>, response_class=HTMLResponse</span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">login</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    &lt;html&gt;</span></span><br><span class="line"><span class="string">        &lt;head&gt;</span></span><br><span class="line"><span class="string">            &lt;title&gt;Login&lt;/title&gt;</span></span><br><span class="line"><span class="string">        &lt;/head&gt;</span></span><br><span class="line"><span class="string">        &lt;body&gt;</span></span><br><span class="line"><span class="string">            &lt;form action=&quot;<span class="subst">&#123;AUTH_URL&#125;</span>&quot; method=&quot;get&quot;&gt;</span></span><br><span class="line"><span class="string">                &lt;input type=&quot;hidden&quot; name=&quot;response_type&quot; value=&quot;code&quot; /&gt;</span></span><br><span class="line"><span class="string">                &lt;input type=&quot;hidden&quot; name=&quot;client_id&quot; value=&quot;<span class="subst">&#123;COGNITO_CLIENT_ID&#125;</span>&quot; /&gt;</span></span><br><span class="line"><span class="string">                &lt;input type=&quot;hidden&quot; name=&quot;redirect_uri&quot; value=&quot;<span class="subst">&#123;COGNITO_REDIRECT_URI&#125;</span>&quot; /&gt;</span></span><br><span class="line"><span class="string">                &lt;input type=&quot;submit&quot; value=&quot;Login with AWS Cognito&quot; /&gt;</span></span><br><span class="line"><span class="string">            &lt;/form&gt;</span></span><br><span class="line"><span class="string">        &lt;/body&gt;</span></span><br><span class="line"><span class="string">    &lt;/html&gt;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ol start="6"><li>Callback endpoint:</li></ol><p>This endpoint is called by AWS Cognito after the user has successfully logged in. It exchanges the authorization code for an access token and redirects the user to the chatbot page.<br>This callback url should be set up on the AWS side when you setup the app client. For test purpose, the call back url to set at aws could be “<a href="http://localhost:8000/callback&quot;">http://localhost:8000/callback&quot;</a>.<br>Once on production, it should be some secure url begins with https.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/callback&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">callback</span>(<span class="params">code: <span class="built_in">str</span></span>):</span></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&quot;grant_type&quot;</span>: <span class="string">&quot;authorization_code&quot;</span>,</span><br><span class="line">        <span class="string">&quot;client_id&quot;</span>: COGNITO_CLIENT_ID,</span><br><span class="line">        <span class="string">&quot;client_secret&quot;</span>:COGNITO_CLIENT_SECRET,</span><br><span class="line">        <span class="string">&quot;code&quot;</span>: code,</span><br><span class="line">        <span class="string">&quot;redirect_uri&quot;</span>: COGNITO_REDIRECT_URI,</span><br><span class="line">    &#125;</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/x-www-form-urlencoded&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> httpx.AsyncClient() <span class="keyword">as</span> client:</span><br><span class="line">        response = <span class="keyword">await</span> client.post(TOKEN_URL, data=data, headers=headers)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> response.status_code != <span class="number">200</span>:</span><br><span class="line">        <span class="keyword">raise</span> HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail=<span class="string">&quot;Invalid code&quot;</span>)</span><br><span class="line">    token = response.json()</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">return</span> RedirectResponse(url=<span class="string">f&quot;/chatbot?token=<span class="subst">&#123;token[<span class="string">&#x27;access_token&#x27;</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ol start="7"><li>Chatbot endpoint:</li></ol><p>This endpoint renders the chatbot page for the authenticated user.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/chatbot&quot;</span>, response_class=HTMLResponse</span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">chatbot</span>(<span class="params">sub: <span class="built_in">str</span> = Depends(<span class="params">get_current_user</span>)</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    &lt;html&gt;</span></span><br><span class="line"><span class="string">        &lt;head&gt;</span></span><br><span class="line"><span class="string">            &lt;title&gt;Chatbot&lt;/title&gt;</span></span><br><span class="line"><span class="string">        &lt;/head&gt;</span></span><br><span class="line"><span class="string">        &lt;body&gt;</span></span><br><span class="line"><span class="string">            &lt;h1&gt;Welcome, <span class="subst">&#123;sub&#125;</span>!&lt;/h1&gt;</span></span><br><span class="line"><span class="string">            &lt;p&gt;Here you can chat with the robot.&lt;/p&gt;</span></span><br><span class="line"><span class="string">        &lt;/body&gt;</span></span><br><span class="line"><span class="string">    &lt;/html&gt;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><ol start="8"><li>Logout endpoint:</li></ol><p>This endpoint logs the user out and redirects them to the login page.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">@app.get(<span class="params"><span class="string">&quot;/logout&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">async</span> <span class="function"><span class="keyword">def</span> <span class="title">logout</span>(<span class="params">request: Request</span>):</span></span><br><span class="line">    token = request.query_params.get(<span class="string">&quot;token&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> RedirectResponse(url=<span class="string">f&quot;<span class="subst">&#123;LOGOUT_URL&#125;</span>?client_id=<span class="subst">&#123;COGNITO_CLIENT_ID&#125;</span>&amp;logout_uri=<span class="subst">&#123;COGNITO_REDIRECT_URI&#125;</span>&amp;token=<span class="subst">&#123;token&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><ol start="9"><li>Run the app using uvicorn:</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="keyword">import</span> uvicorn</span><br><span class="line">    uvicorn.run(app, host=<span class="string">&quot;127.0.0.1&quot;</span>, port=<span class="number">8000</span>)</span><br></pre></td></tr></table></figure><ol start="10"><li> The full code can be found here<br><a href="https://github.com/robotlearner001/blog/blob/main/fastapi-with-aws-cognito/main.py">github link</a><br>to run the code, simpley type<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python main.py</span><br></pre></td></tr></table></figure></li></ol><p>That’s it! You now have a FastAPI webpage with AWS Cognito authentication and a simple chatbot.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fastapi </tag>
            
            <tag> aws cognito </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Include Curly Braces in Python Format Strings</title>
      <link href="2023/03/28/include-curly-braces-in-python-format-strings/"/>
      <url>2023/03/28/include-curly-braces-in-python-format-strings/</url>
      
        <content type="html"><![CDATA[<p>When working with Python format strings, it is common to use curly braces ({}) to insert variables or expressions into a string. However, what if you need to include curly braces in the output string without them being interpreted as a format specifier?</p><p>Fortunately, Python provides a simple solution for this. You can use double curly braces to escape the special meaning of a single set of curly braces.</p><p>For example, let’s say you have a variable named my_var that you want to include in a format string, along with some curly braces that you want to appear in the output string. You can achieve this by using the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_var = 42</span><br><span class="line">my_string = f&quot;The value of my_var is &#123;&#123;my_var&#125;&#125;.&quot;</span><br><span class="line">print(my_string)</span><br></pre></td></tr></table></figure><p>The output of this code will be:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">The value of my_var is &#123;my_var&#125;.</span><br></pre></td></tr></table></figure><p>As you can see, the double curly braces are interpreted as a single set of curly braces in the output string, while the single curly braces surrounding my_var are interpreted as a format specifier.</p><p>In conclusion, using double curly braces is an easy way to include curly braces in Python format strings without them being interpreted as format specifiers. This can be particularly useful when working with output strings that include special characters or syntax that would otherwise cause errors or unexpected behavior.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> format strings </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sampling Rows from a Pandas DataFrame by Group</title>
      <link href="2023/03/28/sample-rows-by-sample-in-dataframe/"/>
      <url>2023/03/28/sample-rows-by-sample-in-dataframe/</url>
      
        <content type="html"><![CDATA[<p>Pandas is a popular data analysis library in Python that provides powerful tools for manipulating and analyzing data. One common task in data analysis is to sample rows from a dataframe based on some grouping criteria. In this blog, we’ll explore how to use Pandas to sample rows from a dataframe by group.</p><p>Suppose we have a dataframe with a column called ‘vertical’ and we want to sample up to 100 random rows for each unique value in the ‘vertical’ column. Here’s how we can achieve this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># create a sample dataframe</span><br><span class="line">data = &#123;&#x27;vertical&#x27;: [&#x27;A&#x27;, &#x27;A&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;C&#x27;, &#x27;C&#x27;, &#x27;C&#x27;, &#x27;C&#x27;], </span><br><span class="line">        &#x27;value&#x27;: np.random.randint(1, 101, size=10)&#125;</span><br><span class="line">df = pd.DataFrame(data)</span><br><span class="line"></span><br><span class="line"># group by the &#x27;vertical&#x27; column and get up to 100 random rows for each group</span><br><span class="line">sampled_df = df.groupby(&#x27;vertical&#x27;).apply(lambda x: x.sample(min(len(x), 100)))</span><br><span class="line"></span><br><span class="line"># print the sampled dataframe</span><br><span class="line">print(sampled_df.reset_index(drop=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we first create a sample dataframe with a ‘vertical’ column and a ‘value’ column. We then group the dataframe by the ‘vertical’ column using the groupby() function. We apply a lambda function to each group that samples up to 100 random rows using the sample() function. Finally, we combine the sampled groups back into a single dataframe using apply() and groupby().</p><p>Note that the min(len(x), 100) argument passed to sample() ensures that we don’t sample more rows than are available in a given group. This is useful in cases where a group may have fewer than 100 rows.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> dataframe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Best Practices for Creating a .gitignore File and how</title>
      <link href="2023/03/25/best-practice-of-creating-gitignore-file/"/>
      <url>2023/03/25/best-practice-of-creating-gitignore-file/</url>
      
        <content type="html"><![CDATA[<p>When you’re working on a Git repository, it’s important to create a .gitignore file to tell Git which files and directories to ignore when committing changes. This can help keep your repository clean and prevent unnecessary files from being committed. In this blog post, we’ll cover some best practices for creating a .gitignore file, as well as provide a recommended .gitignore file that you can use as a starting point.</p><h2 id="1-Ignore-Generated-Files"><a href="#1-Ignore-Generated-Files" class="headerlink" title="1. Ignore Generated Files"></a>1. Ignore Generated Files</h2><p>Many programming languages and tools generate files automatically during the build process. These files don’t need to be committed to your Git repository and can often be quite large. Examples of generated files include .class, .jar, .o, and .pyc. By ignoring generated files, you can keep your repository lean and avoid cluttering it with unnecessary files.</p><h2 id="2-Ignore-Sensitive-Information"><a href="#2-Ignore-Sensitive-Information" class="headerlink" title="2. Ignore Sensitive Information"></a>2. Ignore Sensitive Information</h2><p>Avoid storing sensitive information such as passwords, API keys, and access tokens in your Git repository. If you accidentally commit such information, it can be visible to others and can lead to security issues. Examples of files to ignore include .key, .pem, .env, and .config.</p><h2 id="3-Ignore-User-Specific-Files"><a href="#3-Ignore-User-Specific-Files" class="headerlink" title="3. Ignore User-Specific Files"></a>3. Ignore User-Specific Files</h2><p>You should avoid committing files that are specific to your local environment or personal settings. These files can vary between users and can cause conflicts if committed to the repository. Examples of user-specific files include .log, .swp, .DS_Store, and Thumbs.db.</p><h2 id="4-Ignore-Build-Artifacts-and-Output"><a href="#4-Ignore-Build-Artifacts-and-Output" class="headerlink" title="4. Ignore Build Artifacts and Output"></a>4. Ignore Build Artifacts and Output</h2><p>In addition to generated files, you can also ignore build artifacts and output files. These files are often the result of running your code, but aren’t necessary to keep track of in version control. Examples of files and directories to ignore include /bin, /build, /dist, and /target.</p><h2 id="5-Ignore-Third-Party-Libraries-and-Dependencies"><a href="#5-Ignore-Third-Party-Libraries-and-Dependencies" class="headerlink" title="5. Ignore Third-Party Libraries and Dependencies"></a>5. Ignore Third-Party Libraries and Dependencies</h2><p>If you’re using a package manager to manage dependencies, you should avoid committing the actual packages to your Git repository. Instead, commit a file that lists the dependencies, such as a requirements.txt or package.json file. Examples of directories to ignore include /node_modules, /vendor, /.venv, and /.gradle.</p><h2 id="Recommended-gitignore-File"><a href="#Recommended-gitignore-File" class="headerlink" title="Recommended .gitignore File"></a>Recommended .gitignore File</h2><p>Here is a recommended .gitignore file that incorporates the best practices we discussed above:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Ignore generated files</span><br><span class="line">*.class</span><br><span class="line">*.jar</span><br><span class="line">*.o</span><br><span class="line">*.pyc</span><br><span class="line">*.dll</span><br><span class="line"></span><br><span class="line"># Ignore sensitive information</span><br><span class="line">*.key</span><br><span class="line">*.pem</span><br><span class="line">*.env</span><br><span class="line">*.config</span><br><span class="line"></span><br><span class="line"># Ignore user-specific files</span><br><span class="line">*.log</span><br><span class="line">*.swp</span><br><span class="line">*.DS_Store</span><br><span class="line">Thumbs.db</span><br><span class="line"></span><br><span class="line"># Ignore build artifacts and output</span><br><span class="line">/bin</span><br><span class="line">/build</span><br><span class="line">/dist</span><br><span class="line">/target</span><br><span class="line"></span><br><span class="line"># Ignore third-party libraries and dependencies</span><br><span class="line">/node_modules</span><br><span class="line">/vendor</span><br><span class="line">/.venv</span><br><span class="line">/.gradle</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>By using this .gitignore file, you can ensure that your Git repository stays clean and organized, and that you avoid committing unnecessary files to your repository.</p><h2 id="Creating-a-gitignore-File"><a href="#Creating-a-gitignore-File" class="headerlink" title="Creating a .gitignore File"></a>Creating a .gitignore File</h2><p>To create a .gitignore file, you can use any text editor or code editor of your choice. Simply open a new file and add the content of the recommended .gitignore file we provided above, or customize it to your needs.</p><p>Save the file as .gitignore in the root directory of your repository.</p><h2 id="Committing-and-Pushing-the-gitignore-File"><a href="#Committing-and-Pushing-the-gitignore-File" class="headerlink" title="Committing and Pushing the .gitignore File"></a>Committing and Pushing the .gitignore File</h2><p>Once you’ve created the .gitignore file, you need to commit and push to your github repository</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git add .gitignore</span><br><span class="line">git commit -m &quot;Add ignore rule for files starting with dot&quot;</span><br><span class="line">git push </span><br></pre></td></tr></table></figure><p>You don’t need to push the .gitignore file to github, but if you do, other people can check it out and also follow the same principles as you.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git </tag>
            
            <tag> gitignore </tag>
            
            <tag> github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Securely Loading Environment Variables in Python Using python-dotenv</title>
      <link href="2023/03/25/load-environment-variable-to-Python-safe/"/>
      <url>2023/03/25/load-environment-variable-to-Python-safe/</url>
      
        <content type="html"><![CDATA[<p>Loading environment variables in Python code is a common task that developers need to perform when building applications that require sensitive information such as API keys or database credentials. In this blog post, we will explore how to load environment variables using the python-dotenv package.</p><p>The first step is to install the python-dotenv package. This can be done by running the following command in your terminal:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install python-dotenv</span><br></pre></td></tr></table></figure><p>Once the package is installed, we can create a .env file in the root directory of our project and store our sensitive information inside. For example, let’s say we have an OpenAI API key that we want to access in our Python code. We can add the following line to our .env file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OPENAI_API_KEY=&quot;2d9gggggggggggas&quot;</span><br></pre></td></tr></table></figure><p>To use this API key in our Python code, we need to load the environment variables from the .env file. This can be done using the load_dotenv() function from the dotenv module. Here’s an example code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">from dotenv import load_dotenv</span><br><span class="line"></span><br><span class="line">load_dotenv()</span><br><span class="line"></span><br><span class="line">OPENAI_API_KEY = os.getenv(&#x27;OPENAI_API_KEY&#x27;)</span><br></pre></td></tr></table></figure><p>In this code snippet, we first import the os and dotenv modules. We then call the load_dotenv() function, which loads the environment variables from the .env file. Finally, we use the os.getenv() function to retrieve the value of the OPENAI_API_KEY variable from the environment.</p><p>One important reason to access sensitive information, such as an API key, using environment variables is to ensure security. By storing the sensitive information in an environment variable and loading it from a .env file, we can prevent the information from being hard-coded into our code and exposed in the event of a security breach.</p><p>It is also crucial to make sure that the .env file is not pushed to GitHub or other public repositories. To achieve this, we can add the .env file to our .gitignore file. The .gitignore file specifies which files or directories Git should ignore when committing changes. This way, the .env file is kept private and safe.<br>How to add .gitignore file can be see <a href="https://www.datasciencebyexample.com/2023/03/25/best-practice-of-creating-gitignore-file/">here</a></p><p>In conclusion, loading environment variables in Python code is an essential step when building applications that require sensitive information such as API keys or database credentials. Using the python-dotenv package and loading the environment variables from a .env file helps to ensure security and prevent sensitive information from being exposed. Remember to always keep the .env file private and not push it to public repositories.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> environment variable loading </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is zero shot, few shot and fine tune in OpenAI GPT models</title>
      <link href="2023/03/25/openai-GPT-zero-shot-to-few-shot-to-fine-tuning/"/>
      <url>2023/03/25/openai-GPT-zero-shot-to-few-shot-to-fine-tuning/</url>
      
        <content type="html"><![CDATA[<p>When we say zero-shot or few-shot, it just the way we provide prompts when use GPT models, such as GPT3, GPT4 or chatGPT.</p><p>Performance wise, always start with zero-shot, then few-shot (example), neither of them worked, then fine-tune.</p><p>So what’s zero-shot prompt? Here is one example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Extract keywords from the below text.</span><br><span class="line"></span><br><span class="line">Text: &#123;text&#125;</span><br><span class="line"></span><br><span class="line">Keywords:</span><br></pre></td></tr></table></figure><p>If the performance is not good, try to add few examples in the prompt, this is so called few-shot learning.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Extract keywords from the corresponding texts below.</span><br><span class="line"></span><br><span class="line">Text 1: Stripe provides APIs that web developers can use to integrate payment processing into their websites and mobile applications.</span><br><span class="line">Keywords 1: Stripe, payment processing, APIs, web developers, websites, mobile applications</span><br><span class="line">##</span><br><span class="line">Text 2: OpenAI has trained cutting-edge language models that are very good at understanding and generating text. Our API provides access to these models and can be used to solve virtually any task that involves processing language.</span><br><span class="line">Keywords 2: OpenAI, language models, text processing, API.</span><br><span class="line">##</span><br><span class="line">Text 3: &#123;text&#125;</span><br><span class="line">Keywords 3:</span><br></pre></td></tr></table></figure><p>Sometimes even with few shot examples, performance might be still not what you expect, so try to use fine tuning.<br>But notice that, as the GPT model get more and more powerful, OpenAI will expect people to use the base model more and less fine tune, so support on the fine tune side might be less in the future.</p><p>The resource from OpenAI about how to do fine tune is here:<br><a href="https://platform.openai.com/docs/guides/fine-tuning">fine tune instruction</a><br><a href="https://github.com/openai/openai-cookbook/blob/main/examples/Fine-tuned_classification.ipynb">fine tune code example</a></p><p>For the model parameters, Generally, OpenAI recommend model and temperature are the most commonly used parameters to alter the model output.</p><p>model - Higher performance models are more expensive and have higher latency.</p><p>temperature - A measure of how often the model outputs a less likely token. The higher the temperature, the more random (and usually creative) the output. This, however, is not the same as “truthfulness”. For most factual use cases such as data extraction, and truthful Q&amp;A, the temperature of 0 is best.</p><p>max_tokens (maximum length) - Does not control the length of the output, but a hard cutoff limit for token generation. Ideally you won’t hit this limit often, as your model will stop either when it thinks it’s finished, or when it hits a stop sequence you defined.</p><p>stop (stop sequences) - A set of characters (tokens) that, when generated, will cause the text generation to stop.</p><p>More information about OpenAI API reference can be found here<br><a href="https://platform.openai.com/docs/api-reference/completions/create">OpenAI API reference</a></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> openai </tag>
            
            <tag> gpt </tag>
            
            <tag> zero shot </tag>
            
            <tag> few shot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>how to use OpenAI to generate text completions and chat-style responses with GPT-3,chatGPT or GPT-4 through API</title>
      <link href="2023/03/22/openai-api-requets-gpt3-vs-gpt4/"/>
      <url>2023/03/22/openai-api-requets-gpt3-vs-gpt4/</url>
      
        <content type="html"><![CDATA[<p>OpenAI is a research organization that aims to build safe and beneficial artificial intelligence systems. One of their most popular products is the GPT (Generative Pre-trained Transformer) language model, which can be used to generate human-like text based on a given prompt. OpenAI provides a Python API that allows developers to easily access and use their language models. In this blog post, we’ll explore how to use OpenAI to generate text completions and chat-style responses using GPT-3 and GPT-4.</p><h2 id="install-openai-package"><a href="#install-openai-package" class="headerlink" title="install openai package"></a>install openai package</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install openai</span><br></pre></td></tr></table></figure><p>One thing to notice, if you want to use chatGPT API, make sure the openai version is greater than <code>0.27.0</code>.</p><h2 id="Generating-Text-Completions-with-GPT-3"><a href="#Generating-Text-Completions-with-GPT-3" class="headerlink" title="Generating Text Completions with GPT-3"></a>Generating Text Completions with GPT-3</h2><p>To generate text completions using GPT-3, we can use the openai.Completion.create method provided by the OpenAI Python API. Here’s an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line">openai.api_key = &quot;YOUR_API_KEY&quot;</span><br><span class="line"></span><br><span class="line">prompt = &quot;The quick brown fox&quot;</span><br><span class="line"></span><br><span class="line">response = openai.Completion.create(</span><br><span class="line">  engine=&quot;text-davinci-003&quot;,</span><br><span class="line">  prompt=prompt,</span><br><span class="line">  temperature=0.5,</span><br><span class="line">  max_tokens=20</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = response[&#x27;choices&#x27;][0][&#x27;text&#x27;].strip(&#x27;\n&#x27;).strip()</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>In this example, we’re using text-davinci-003 as the engine to generate text completions based on the prompt “The quick brown fox”. We’re using a temperature of 0.5 and a max_tokens of 20 to control the length and diversity of the generated text. The resulting text completion is printed to the console.</p><h2 id="Generating-Chat-style-Responses-with-GPT-4"><a href="#Generating-Chat-style-Responses-with-GPT-4" class="headerlink" title="Generating Chat-style Responses with GPT-4"></a>Generating Chat-style Responses with GPT-4</h2><p>To generate chat-style responses using GPT-4, we can use the openai.ChatCompletion.create method provided by the OpenAI Python API. Here’s an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line">openai.api_key = &quot;YOUR_API_KEY&quot;</span><br><span class="line"></span><br><span class="line">messages=[</span><br><span class="line">  &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a good assistant.&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I&#x27;m looking for a new laptop. Can you recommend one?&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Sure, what&#x27;s your budget?&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Not too much.&quot;&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">  model=&quot;gpt-4&quot;,</span><br><span class="line">  messages=messages,</span><br><span class="line">  temperature=0.7,</span><br><span class="line">  max_tokens=50</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = response[&#x27;choices&#x27;][0][&#x27;message&#x27;][&#x27;content&#x27;].strip(&#x27;\n&#x27;).strip()</span><br><span class="line">print(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Generating-Chat-style-Responses-with-chatGPT"><a href="#Generating-Chat-style-Responses-with-chatGPT" class="headerlink" title="Generating Chat-style Responses with chatGPT"></a>Generating Chat-style Responses with chatGPT</h2><p>It’s the same API to call as GPT-4, just switch the model to be “gpt-3.5-turbo”</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import openai</span><br><span class="line">openai.api_key = &quot;YOUR_API_KEY&quot;</span><br><span class="line"></span><br><span class="line">messages=[</span><br><span class="line">  &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a good assistant.&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;I&#x27;m looking for a new laptop. Can you recommend one?&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;Sure, what&#x27;s your budget?&quot;&#125;,</span><br><span class="line">  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Not too much.&quot;&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = openai.ChatCompletion.create(</span><br><span class="line">  model=&quot;gpt-3.5-turbo&quot;,</span><br><span class="line">  messages=messages,</span><br><span class="line">  temperature=0.7,</span><br><span class="line">  max_tokens=50</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">result = response[&#x27;choices&#x27;][0][&#x27;message&#x27;][&#x27;content&#x27;].strip(&#x27;\n&#x27;).strip()</span><br><span class="line">print(result)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>OpenAI provides powerful language models that can be used to generate human-like text and chat-style responses. In this blog post, we explored how to use the OpenAI Python API to generate text completions using GPT-3 and chat-style responses using GPT-4. By leveraging these language models, developers can create innovative applications that can interact with users in natural and engaging ways.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt3 </tag>
            
            <tag> chatGPT </tag>
            
            <tag> openai </tag>
            
            <tag> gpt4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to control work flow in Streamlit with session state</title>
      <link href="2023/03/21/streamlit-work-flow-control-with-session-state/"/>
      <url>2023/03/21/streamlit-work-flow-control-with-session-state/</url>
      
        <content type="html"><![CDATA[<p>Streamlit is super easy to use to build some demo quickly. Streamlit is an open-source Python library that allows developers to easily create interactive web applications for data science and machine learning projects. With Streamlit, you can create a web-based dashboard or application that lets users interact with your data, visualizations, and machine learning models without the need for extensive coding or web development knowledge</p><p>But if you want to more complex work flow controls, you might expect some unexpected behaviors.<br>For example, a nested buttons, where users click the first button, do something, then continue to click the second button, and do other things.<br>Such as the following picture shows:</p><p><img src="/content/images/2023-03-21-1.png"></p><p>As shown in the above picture, the goal we want to achieve is: First, user will clicks button1, then a random number got displayed, then the button2 showsup, and users continue to click button2, and should<br>see a new random generated.</p><p>You might think the streamlit code is as easy as the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import streamlit as st</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">button1 = st.button(&#x27;Step1&#x27;)</span><br><span class="line">if button1 == True:</span><br><span class="line">        </span><br><span class="line">    show_number1 = random.random()</span><br><span class="line">    st.write(&quot;random number generated from step 1: &#123;&#125;&quot;.format(show_number1))</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    button2 = st.button(&#x27;Step2&#x27;)</span><br><span class="line">    if button2 == True:</span><br><span class="line">        show_number2 = random.random()</span><br><span class="line">        st.write(&quot;random number generated from step 2: &#123;&#125;&quot;.format(show_number2))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>For the above code,  when you click button1, it works as you would expect. But when you click button2, the page restarts, and you only see button1 again.<br>This is not a bug, but what’s supposed to work with Streamlit, probably for a good reason. </p><p>So if we want to achieve our original goal, we need to leverage session state to store some several things. One thing to notice, once we click the second button, the app will restart anyway,<br>we can’t really change that, we just need to cache some knowledge of what already happened, and use those information to guide the work flow.</p><p>So information we need to cache are:<br>(1) if each of the buttons are already clicked<br>(2) the random number generated after the first button click, so it doesn’t get regnerated again during the app restart.  </p><p>The new code is like the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import streamlit as st</span><br><span class="line">import random</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## init states</span><br><span class="line">if &#x27;button1_clicked&#x27; not in st.session_state:</span><br><span class="line">    st.session_state.button1_clicked = False</span><br><span class="line">    </span><br><span class="line">if &#x27;button2_clicked&#x27; not in st.session_state:</span><br><span class="line">    st.session_state.button2_clicked = False</span><br><span class="line">    </span><br><span class="line">if &#x27;show_number1&#x27; not in st.session_state:</span><br><span class="line">    st.session_state.show_number1 = 0</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">def callback1():</span><br><span class="line">    st.session_state.button1_clicked = True</span><br><span class="line">    </span><br><span class="line">def callback2():</span><br><span class="line">    st.session_state.button2_clicked = True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">button1 = st.button(&#x27;Step1&#x27;,on_click=callback1)</span><br><span class="line">if button1 == True or st.session_state.button2_clicked == True:</span><br><span class="line">    </span><br><span class="line">    if button1 == False and st.session_state.button2_clicked  == True:</span><br><span class="line">        show_number1 = st.session_state.show_number1</span><br><span class="line">    else:</span><br><span class="line">        show_number1 = random.random()</span><br><span class="line">        </span><br><span class="line">    st.write(&quot;random number generated from step 1: &#123;&#125;&quot;.format(show_number1))</span><br><span class="line">    st.session_state.show_number1 = show_number1</span><br><span class="line"></span><br><span class="line">    button2 = st.button(&#x27;Step2&#x27;,on_click=callback2)</span><br><span class="line">    if button2 == True:</span><br><span class="line">        show_number2 = random.random()</span><br><span class="line">        st.write(&quot;random number generated from step 2: &#123;&#125;&quot;.format(show_number2))</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> streamlit </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An Early Look at the Labor Market Impact Potential of Large Language Model such as GPT by OpenAI</title>
      <link href="2023/03/20/how-gpt-and-large-language-model-affect-job-market/"/>
      <url>2023/03/20/how-gpt-and-large-language-model-affect-job-market/</url>
      
        <content type="html"><![CDATA[<p>Recently, OpenAI, OpenResearch, and University of Pennsylvania published their studies about how the popular GPT and GPTs models will affect people’s jobs.</p><p>Here is the abstract of the paper:</p><p>We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and<br>related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their<br>correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4.<br>Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work<br>tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their<br>tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater<br>exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We<br>conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies<br>(GPTs), suggesting that as these models could have notable economic, social, and policy implications.</p><p>The link to the full paper is <a href="https://arxiv.org/pdf/2303.10130v2.pdf">Here</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GPT </tag>
            
            <tag> openAI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Easy and neat way to retry function after error in Python</title>
      <link href="2023/03/18/automatic-retry-function-after-error-in-python/"/>
      <url>2023/03/18/automatic-retry-function-after-error-in-python/</url>
      
        <content type="html"><![CDATA[<p>Have you ever written a Python script that needed to handle unreliable network connections or flaky APIs? If so, you might have encountered situations where your script would fail due to transient errors such as network timeouts, HTTP 500 errors, or rate limits. In these cases, it can be helpful to automatically retry the failing operation instead of giving up and terminating the script. This is where the tenacity library comes in.</p><p>tenacity is a Python library that provides a flexible and powerful way to retry operations that might fail due to various reasons. With tenacity, you can retry a function with configurable delay intervals, maximum retries, and exception handling. You can also customize the retry behavior based on the return value of the function.</p><p>Let’s take a look at a simple example here:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import random</span><br><span class="line">from tenacity import retry, stop_after_attempt, wait_random</span><br><span class="line"></span><br><span class="line">@retry(stop=stop_after_attempt(3), wait=wait_random(min=1, max=5))</span><br><span class="line">def my_func():</span><br><span class="line">    if random.random() &gt; 0.3:</span><br><span class="line">        print(&quot;Random number &gt; 0.3, function succeeded!&quot;)</span><br><span class="line">    else:</span><br><span class="line">        print(&quot;Random number &lt;= 0.3, raising ValueError...&quot;)</span><br><span class="line">        raise ValueError(&quot;Random number is too low.&quot;)</span><br><span class="line"></span><br><span class="line">my_func()</span><br></pre></td></tr></table></figure><p>one of the output may like this, where you can see it fails for two time, then succeeded at the 3rd try:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Random number &lt;= 0.3, raising ValueError...</span><br><span class="line">Random number &lt;= 0.3, raising ValueError...</span><br><span class="line">Random number &gt; 0.3, function succeeded!</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> retry </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to bulk index data with Elastic Search engine</title>
      <link href="2023/03/18/elasticsearch-bulk-index-data/"/>
      <url>2023/03/18/elasticsearch-bulk-index-data/</url>
      
        <content type="html"><![CDATA[<p>Previously, we have discussed how to index and query data using elasticsearch in Python<br><a href="https://www.datasciencebyexample.com/2023/03/18/elasticsearch-dense-vector-search/">Here</a></p><p>However, indexing large amounts of data in Elasticsearch can be a challenging task, especially if you need to index millions of documents or more. Fortunately, Elasticsearch provides a powerful API endpoint called _bulk that allows you to index multiple documents in a single request, which can greatly improve indexing performance.</p><p>In this article, we’ll explore how to use the _bulk API endpoint in Elasticsearch to index large amounts of data efficiently. We’ll start by discussing the _bulk API endpoint and its requirements, and then we’ll provide some examples of how to use it in Python using the requests library.</p><h2 id="What-is-the-bulk-API-endpoint"><a href="#What-is-the-bulk-API-endpoint" class="headerlink" title="What is the _bulk API endpoint?"></a>What is the _bulk API endpoint?</h2><p>The _bulk API endpoint in Elasticsearch allows you to index, update, or delete multiple documents in a single request. This can be much more efficient than sending individual requests for each document, especially when dealing with large amounts of data.</p><p>The _bulk endpoint accepts a newline-delimited JSON (NDJSON) payload that specifies the operations to perform on each document. Each line in the payload represents a single operation, and each operation consists of a JSON object that specifies the index, update, or delete action to perform on a single document.</p><p>Here’s an example of what a _bulk payload might look like:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST my_index/_bulk</span><br><span class="line">&#123;&quot;index&quot;:&#123;&quot;_id&quot;:1&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;John Doe&quot;,&quot;age&quot;:35,&quot;city&quot;:&quot;New York&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;:&#123;&quot;_id&quot;:2&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Jane Doe&quot;,&quot;age&quot;:28,&quot;city&quot;:&quot;San Francisco&quot;&#125;</span><br><span class="line">&#123;&quot;index&quot;:&#123;&quot;_id&quot;:3&#125;&#125;</span><br><span class="line">&#123;&quot;name&quot;:&quot;Bob Smith&quot;,&quot;age&quot;:42,&quot;city&quot;:&quot;Chicago&quot;&#125;</span><br></pre></td></tr></table></figure><p>In this example, we’re indexing three documents in the my_index index. Each document is represented as a separate JSON object, and the index action is used to specify the operation type for each document. The _id field is also specified for each document using the index action.</p><p>Note that each document is separated by a newline character (\n) and that the bulk request is wrapped in a single JSON object. You can include multiple index or delete actions in a single _bulk request, and Elasticsearch will process them all in one go.</p><h2 id="Using-bulk-with-Python-and-requests"><a href="#Using-bulk-with-Python-and-requests" class="headerlink" title="Using _bulk with Python and requests"></a>Using _bulk with Python and requests</h2><p>Now that we understand the basics of the _bulk API endpoint, let’s look at how to use it in Python using the requests library.</p><p>Suppose we have a list of data that we want to index in Elasticsearch. Here’s an example of how we might loop through the list of data and call the _bulk API endpoint using requests:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line"># Example list of data to index</span><br><span class="line">data_list = [</span><br><span class="line">    &#123;&quot;name&quot;: &quot;John Doe&quot;, &quot;age&quot;: 35, &quot;city&quot;: &quot;New York&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;: &quot;Jane Doe&quot;, &quot;age&quot;: 28, &quot;city&quot;: &quot;San Francisco&quot;&#125;,</span><br><span class="line">    &#123;&quot;name&quot;: &quot;Bob Smith&quot;, &quot;age&quot;: 42, &quot;city&quot;: &quot;Chicago&quot;&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"># Elasticsearch settings</span><br><span class="line">es_url = &quot;http://localhost:9200&quot;</span><br><span class="line">es_index = &quot;my_index&quot;</span><br><span class="line"></span><br><span class="line"># Bulk index the data</span><br><span class="line">bulk_data = &quot;&quot;</span><br><span class="line">for i, data in enumerate(data_list):</span><br><span class="line">    # Add the index action and ID for each document</span><br><span class="line">    bulk_data += json.dumps(&#123;&quot;index&quot;: &#123;&quot;_id&quot;: i+1&#125;&#125;) + &quot;\n&quot;</span><br><span class="line">    # Add the document data</span><br><span class="line">    bulk_data += json.dumps(data) + &quot;\n&quot;</span><br><span class="line"></span><br><span class="line">    # Send the bulk request every 1000 documents</span><br><span class="line">    if i &gt; 0 and i % 1000 == 0:</span><br><span class="line">        # Send the bulk request</span><br><span class="line">        response = requests.post(f&quot;&#123;es_url&#125;/&#123;es_index&#125;/_bulk&quot;, headers=&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;, data=bulk_data)</span><br><span class="line">        # Reset the bulk_data variable</span><br><span class="line">        bulk_data = &quot;&quot;</span><br><span class="line"></span><br><span class="line"># Send the final bulk request</span><br><span class="line">response = requests.post(f&quot;&#123;es_url&#125;/&#123;es_index&#125;/_bulk&quot;, headers=&#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;, data=bulk_data)</span><br><span class="line"># Print the response content</span><br><span class="line">print(response.content)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>vector search using Elastic Search,  index and search example using python requests library</title>
      <link href="2023/03/18/elasticsearch-dense-vector-search/"/>
      <url>2023/03/18/elasticsearch-dense-vector-search/</url>
      
        <content type="html"><![CDATA[<p>Vector search has becoming very useful in deep learning applications.</p><p>To search dense vectors in Elasticsearch 8.6, you can use the “dense_vector” data type, which was introduced in Elasticsearch 7.10. This data type allows you to store dense vectors as a single field in your documents, which can then be searched using various similarity measures such as cosine similarity or euclidean distance.</p><p>Here’s an example of how to search for similar vectors using cosine similarity:</p><p>First, you need to create an index with a dense_vector field:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">PUT my_index</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;properties&quot;: &#123;</span><br><span class="line">      &quot;my_vector&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;dense_vector&quot;,</span><br><span class="line">        &quot;dims&quot;: 3</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we create an index called “my_index” with a dense_vector field called “my_vector” with three dimensions.</p><p>Next, you can index some documents with vectors:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">POST my_index/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;my_vector&quot;: [0.2, 0.3, 0.4]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST my_index/_doc/2</span><br><span class="line">&#123;</span><br><span class="line">  &quot;my_vector&quot;: [0.1, 0.7, 0.2]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">POST my_index/_doc/3</span><br><span class="line">&#123;</span><br><span class="line">  &quot;my_vector&quot;: [0.8, 0.2, 0.1]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we index three documents with dense vectors.</p><p>Finally, you can search for documents that are similar to a given vector:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET my_index/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;script_score&quot;: &#123;</span><br><span class="line">      &quot;query&quot;: &#123;</span><br><span class="line">        &quot;match_all&quot;: &#123;&#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;script&quot;: &#123;</span><br><span class="line">        &quot;source&quot;: &quot;cosineSimilarity(params.queryVector, &#x27;my_vector&#x27;) + 1.0&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">          &quot;queryVector&quot;: [0.1, 0.5, 0.3]</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>putting all together, corresponding requests code in Python are:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"># Create an index with a dense_vector field</span><br><span class="line">url = &#x27;http://localhost:9200/my_index&#x27;</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;mappings&quot;: &#123;</span><br><span class="line">        &quot;properties&quot;: &#123;</span><br><span class="line">            &quot;my_vector&quot;: &#123;</span><br><span class="line">                &quot;type&quot;: &quot;dense_vector&quot;,</span><br><span class="line">                &quot;dims&quot;: 3</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#x27;Authorization&#x27;: &#x27;Bearer YOUR_ACCESS_TOKEN&#x27;&#125;</span><br><span class="line">response = requests.put(url, data=json.dumps(data), headers=headers)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br><span class="line"># Index some documents with vectors</span><br><span class="line">data = &#123;&quot;my_vector&quot;: [0.2, 0.3, 0.4]&#125;</span><br><span class="line">headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#x27;Authorization&#x27;: &#x27;Bearer YOUR_ACCESS_TOKEN&#x27;&#125;</span><br><span class="line">response = requests.post(url+&#x27;/_doc/1&#x27;, data=json.dumps(data), headers=headers)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br><span class="line">data = &#123;&quot;my_vector&quot;: [0.1, 0.7, 0.2]&#125;</span><br><span class="line">headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#x27;Authorization&#x27;: &#x27;Bearer YOUR_ACCESS_TOKEN&#x27;&#125;</span><br><span class="line">response = requests.post(url+&#x27;/_doc/2&#x27;, data=json.dumps(data), headers=headers)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br><span class="line">data = &#123;&quot;my_vector&quot;: [0.8, 0.2, 0.1]&#125;</span><br><span class="line">headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#x27;Authorization&#x27;: &#x27;Bearer YOUR_ACCESS_TOKEN&#x27;&#125;</span><br><span class="line">response = requests.post(url+&#x27;/_doc/3&#x27;, data=json.dumps(data), headers=headers)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br><span class="line"># Search for documents that are similar to a given vector</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;script_score&quot;: &#123;</span><br><span class="line">            &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;,</span><br><span class="line">            &quot;script&quot;: &#123;</span><br><span class="line">                &quot;source&quot;: &quot;cosineSimilarity(params.queryVector, &#x27;my_vector&#x27;) + 1.0&quot;,</span><br><span class="line">                &quot;params&quot;: &#123;&quot;queryVector&quot;: [0.1, 0.5, 0.3]&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">headers = &#123;&#x27;Content-Type&#x27;: &#x27;application/json&#x27;, &#x27;Authorization&#x27;: &#x27;Bearer YOUR_ACCESS_TOKEN&#x27;&#125;</span><br><span class="line">response = requests.get(url+&#x27;/_search&#x27;, data=json.dumps(data), headers=headers)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Notice that in the cosineSimilarity function, we add 1.0 to the equation to avoid negative error from elastic search something like this:<br><code>Error: &#123;&quot;error&quot;:&#123;&quot;root_cause&quot;:[&#123;&quot;type&quot;:&quot;illegal_argument_exception&quot;,&quot;reason&quot;:&quot;script_score script returned an invalid score [-0.9679827] for doc [0]. Must be a non-negative score!&quot;&#125;]</code></p><p>Another to notice is that, we are using cosineSimilarity to calcualte score. However if the vector is normalized or you simply want to calcuate the dot product<br>score of vectors, we switch to use <code>dotProduct</code>. Reason to use `dotProduct’ is becuase less computing and potential faster in elasticsearch. code is  for example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;script_score&quot;: &#123;</span><br><span class="line">            &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;,</span><br><span class="line">            &quot;script&quot;: &#123;</span><br><span class="line">                &quot;source&quot;: &quot;dotProduct(params.queryVector, &#x27;my_vector&#x27;) + 1.0&quot;,</span><br><span class="line">                &quot;params&quot;: &#123;&quot;queryVector&quot;: [0.1, 0.5, 0.3]&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>One last thing, if you want to limit the number of records to return form search, add <code>size</code> in the query:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data = &#123;</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">        &quot;script_score&quot;: &#123;</span><br><span class="line">            &quot;query&quot;: &#123;&quot;match_all&quot;: &#123;&#125;&#125;,</span><br><span class="line">            &quot;script&quot;: &#123;</span><br><span class="line">                &quot;source&quot;: &quot;dotProduct(params.queryVector, &#x27;my_vector&#x27;) + 1.0&quot;,</span><br><span class="line">                &quot;params&quot;: &#123;&quot;queryVector&quot;: [0.1, 0.5, 0.3]&#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">&quot;size&quot;: 5</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AI and the Future of Jobs, Questions and Answers on ChatGPT and Automation</title>
      <link href="2023/03/17/ai-and-future-job/"/>
      <url>2023/03/17/ai-and-future-job/</url>
      
        <content type="html"><![CDATA[<ol><li><p>What is ChatGPT and how does it work?<br>Answer: ChatGPT is a virtual assistant or chatbot that uses natural language processing (NLP) and generative AI to create human-like responses. It is a user interface for the GPT3 language model developed by OpenAI, which was trained using unsupervised learning on a massive dataset of 175 billion parameters.</p></li><li><p>How will jobs in marketing and advertising be affected by ChatGPT?<br>Answer: ChatGPT can be used to automate routine aspects of marketing work, such as creating lists, defining the structure of content marketing pieces, or generating product descriptions. However, it cannot replicate the authentic voice of a brand or the human connections that come with influencer marketing. Marketers who can apply truly human qualities should have nothing to fear.</p></li><li><p>What can marketers do to avoid becoming redundant due to AI?<br>Answer: Marketers can master using AI to augment their own creativity and human skills, such as creativity, strategic thinking, empathy, and emotional intelligence. They should also keep up-to-date with the latest developments in AI to ensure they remain effective and competitive in their roles.</p></li><li><p>Will AI make all jobs redundant?<br>Answer: Throughout history, new developments in technology have tended to create new jobs as quickly as they make old jobs redundant. The jobs that are created are often more technical, creative, or highly-skilled, meaning that they are higher paying and often more rewarding.</p></li><li><p>What are the limitations of ChatGPT and AI in general?<br>Answer: ChatGPT and AI in general are limited by their training data, which may contain bias, and their inability to replicate original thought or creativity in the same way as humans. However, as AI technology advances, it may be able to come closer to emulating some human qualities, so it is important to stay up-to-date with the latest developments.</p></li><li><p>What is the potential of ChatGPT in various professions?<br>Answer: ChatGPT can be used to augment professional skills in various professions, such as law, copywriting, journalism, computer programming, and human resources.</p></li><li><p>Can ChatGPT replace human educators?<br>Answer: No, human intangibles like patience, discipline skills, friendliness, and genuine concern for students’ well-being cannot be replaced by ChatGPT or other AI tools.</p></li><li><p>What are the risks of using ChatGPT in the legal profession?<br>Answer: ChatGPT may not consider the most recent cases and decisions, and it may sometimes get things wrong. It is also susceptible to dataset bias and less than 100% accuracy, making it unsuitable for rendering judgments.</p></li><li><p>Which jobs are more likely to be replaced by machines in the next few years or decades?<br>Answer: Receptionist, many customer service jobs, driving jobs including taxi drivers and truckers, and soldiers.</p></li><li><p>How can AI tools like ChatGPT be used in human resources?<br>Answer: ChatGPT can be used to write job descriptions, automate routine tasks, and conduct sentiment analytics, while HR professionals can focus on building a better understanding of the individuals that make up the company.</p></li><li><p>What are some uses of ChatGPT in different professions?<br>Answer: ChatGPT can be used to create reports and summaries for analysts, automate customer service inquiries, automate simple tasks for salespeople, automate lesson plans and quizzes for teachers, and create social media posts for PR professionals.</p></li><li><p>Are robots and AI technologies replacing human workers?<br>Answer: No, robots and AI technologies are changing how we work and what kinds of jobs exist. They are not necessarily eliminating jobs, but rather creating new ones.</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatGPT </tag>
            
            <tag> GPT4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scratch Your Head? Be Careful with \r or Carriage Return when Printing Strings</title>
      <link href="2023/03/16/carriage-return-in-string-python/"/>
      <url>2023/03/16/carriage-return-in-string-python/</url>
      
        <content type="html"><![CDATA[<p>Have you ever encountered a situation where you printed a string in Python and found that it didn’t appear as expected? If you have, there’s a chance that the ‘\r’ character, also known as a carriage return, may be the culprit.</p><p>The ‘\r’ character is used to return the cursor to the beginning of the current line without advancing to the next line. This can be useful in certain situations, such as when updating progress bars or printing live output in the terminal. However, if you’re not aware of its presence in a string, it can cause confusion and unexpected results when printing.</p><p>Consider the following code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Define a string with &#x27;\r&#x27;</span><br><span class="line">my_string = &quot;Hello\rworld!&quot;</span><br><span class="line"></span><br><span class="line"># Print the string</span><br><span class="line">print(my_string)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>At first glance, you might expect the output to be “Hello world!”, but instead, you’ll see that only “world!” is printed to the console. This is because the ‘\r’ character causes the cursor to return to the beginning of the line after printing “Hello”, effectively overwriting it with “world!”.</p><p>To avoid unexpected behavior caused by ‘\r’, you can use the repr() function to print the string with escape characters visible:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Define a string with &#x27;\r&#x27;</span><br><span class="line">my_string = &quot;Hello\rworld!&quot;</span><br><span class="line"></span><br><span class="line"># Print the string with escape characters visible</span><br><span class="line">print(repr(my_string))</span><br></pre></td></tr></table></figure><p>This will output:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#x27;Hello\rworld!&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now you can see that the string contains a ‘\r’ character.</p><p>So now your see when working with strings in Python, it’s important to be aware of the presence of ‘\r’ characters, especially when printing. If you encounter unexpected behavior, it’s worth checking for this character and understanding how it can affect your output. By being mindful of this potential issue, you can avoid scratching your head in confusion and ensure that your code behaves as expected.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> carriage return </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decoding JSON String When There are Single Quotes</title>
      <link href="2023/03/16/what-to-do-when-single-quotes-in-json-string/"/>
      <url>2023/03/16/what-to-do-when-single-quotes-in-json-string/</url>
      
        <content type="html"><![CDATA[<p>JSON (JavaScript Object Notation) is a popular format used for transmitting data between applications. It’s a lightweight data interchange format that is easy for humans to read and write and easy for machines to parse and generate. However, JSON data is typically enclosed in double quotes, which can be problematic when dealing with data that contains single quotes.</p><p>Consider the following JSON string:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;John&quot;,</span><br><span class="line">    &quot;age&quot;: 30,</span><br><span class="line">    &quot;hobbies&quot;: [&quot;reading&quot;, &quot;swimming&quot;, &quot;playing guitar&quot;],</span><br><span class="line">    &quot;address&quot;: &#123;</span><br><span class="line">        &quot;street&quot;: &quot;123 Main St&quot;,</span><br><span class="line">        &quot;city&quot;: &quot;New York&quot;,</span><br><span class="line">        &quot;state&quot;: &quot;NY&quot;,</span><br><span class="line">        &quot;zipcode&quot;: &quot;10001&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;description&quot;: &quot;John&#x27;s favorite quote is &#x27;Carpe diem.&#x27;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This JSON string is perfectly valid and can be decoded using Python’s built-in json module with the <code>loads()</code> function:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">json_str = &#x27;&#123;&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30, &quot;hobbies&quot;: [&quot;reading&quot;, &quot;swimming&quot;, &quot;playing guitar&quot;], &quot;address&quot;: &#123;&quot;street&quot;: &quot;123 Main St&quot;, &quot;city&quot;: &quot;New York&quot;, &quot;state&quot;: &quot;NY&quot;, &quot;zipcode&quot;: &quot;10001&quot;&#125;, &quot;description&quot;: &quot;John\&#x27;s favorite quote is \&#x27;Carpe diem.\&#x27;&quot;&#125;&#x27;</span><br><span class="line"></span><br><span class="line">data = json.loads(json_str)</span><br><span class="line"></span><br><span class="line">print(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The output is:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 30, &#x27;hobbies&#x27;: [&#x27;reading&#x27;, &#x27;swimming&#x27;, &#x27;playing guitar&#x27;], &#x27;address&#x27;: &#123;&#x27;street&#x27;: &#x27;123 Main St&#x27;, &#x27;city&#x27;: &#x27;New York&#x27;, &#x27;state&#x27;: &#x27;NY&#x27;, &#x27;zipcode&#x27;: &#x27;10001&#x27;&#125;, &#x27;description&#x27;: &quot;John&#x27;s favorite quote is &#x27;Carpe diem.&#x27;&quot;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>However, if the JSON string uses single quotes instead of double quotes, the json module will raise a JSONDecodeError exception because the string is not valid JSON.</p><p>For example, consider the following JSON string:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &#x27;name&#x27;: &#x27;John&#x27;,</span><br><span class="line">    &#x27;age&#x27;: 30,</span><br><span class="line">    &#x27;hobbies&#x27;: [&#x27;reading&#x27;, &#x27;swimming&#x27;, &#x27;playing guitar&#x27;],</span><br><span class="line">    &#x27;address&#x27;: &#123;</span><br><span class="line">        &#x27;street&#x27;: &#x27;123 Main St&#x27;,</span><br><span class="line">        &#x27;city&#x27;: &#x27;New York&#x27;,</span><br><span class="line">        &#x27;state&#x27;: &#x27;NY&#x27;,</span><br><span class="line">        &#x27;zipcode&#x27;: &#x27;10001&#x27;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#x27;description&#x27;: &#x27;John\&#x27;s favorite quote is &quot;Carpe diem.&quot;&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If we try to decode this string using the json module, we will get a JSONDecodeError:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line">json_str = &quot;&#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 30, &#x27;hobbies&#x27;: [&#x27;reading&#x27;, &#x27;swimming&#x27;, &#x27;playing guitar&#x27;], &#x27;address&#x27;: &#123;&#x27;street&#x27;: &#x27;123 Main St&#x27;, &#x27;city&#x27;: &#x27;New York&#x27;, &#x27;state&#x27;: &#x27;NY&#x27;, &#x27;zipcode&#x27;: &#x27;10001&#x27;&#125;, &#x27;description&#x27;: &#x27;John\\&#x27;s favorite quote is \&quot;Carpe diem.\&quot;&#x27;&#125;&quot;</span><br><span class="line"></span><br><span class="line">data = json.loads(json_str)</span><br><span class="line"></span><br><span class="line">print(data)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>with error:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;</span><br><span class="line">  File &quot;/usr/lib/python3.8/json/__init__.py&quot;, line 357, in loads</span><br><span class="line">    return _default_decoder.decode(s)</span><br><span class="line">  File &quot;/usr/lib/python3.8/json/decoder.py&quot;, line 337, in decode</span><br><span class="line">    obj, end = self.raw_decode(s, idx=_w(s, 0).end())</span><br><span class="line">  File &quot;/usr/lib/python3.</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now the <code>ast</code> function come to rescue, if we do this instead:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import ast</span><br><span class="line"></span><br><span class="line"># A string with single quotes</span><br><span class="line">my_string = &quot;&#123;&#x27;name&#x27;: &#x27;John&#x27;, &#x27;age&#x27;: 30, &#x27;city&#x27;: &#x27;New York&#x27;&#125;&quot;</span><br><span class="line"></span><br><span class="line"># Use ast.literal_eval() to evaluate the string into a dictionary</span><br><span class="line">my_dict = ast.literal_eval(my_string)</span><br><span class="line"></span><br><span class="line"># Print the resulting dictionary</span><br><span class="line">print(my_dict[&#x27;name&#x27;])</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>it will work!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> json string decoding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Expanding Array Columns in Pandas DataFrames and wrap up in scikit-learn transformer</title>
      <link href="2023/03/15/expand-array-column-in-pandas/"/>
      <url>2023/03/15/expand-array-column-in-pandas/</url>
      
        <content type="html"><![CDATA[<p>Sometimes, you may find yourself working with a Pandas DataFrame that contains a column of arrays with the same length. In some cases, it may be more useful to “explode” this column of arrays into multiple columns, with one column for each value in the arrays. This can make it easier to perform analysis or modeling on the data.</p><p>In this blog post, we’ll explore how to use Pandas to expand an array column into multiple columns, and how to encapsulate this functionality into a scikit-learn transformer for use in machine learning pipelines.</p><h2 id="Expanding-an-Array-Column-with-Pandas"><a href="#Expanding-an-Array-Column-with-Pandas" class="headerlink" title="Expanding an Array Column with Pandas"></a>Expanding an Array Column with Pandas</h2><p>To illustrate how to expand an array column in a Pandas DataFrame, let’s start with an example DataFrame that contains a column of arrays:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;&#x27;array_col&#x27;: [[1, 2, 3], [4, 5, 6], [7, 8, 9]]&#125;)</span><br></pre></td></tr></table></figure><p>This DataFrame has a single column named array_col with three rows, each containing an array of three integers. To expand this column into multiple columns, we can use the apply method to apply a function to each row of the DataFrame. This function will return a new DataFrame with the values of the array column, which will be automatically assigned to new columns in the resulting DataFrame.</p><p>Here’s an example of how to do this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def explode_array_column(row):</span><br><span class="line">    return pd.Series(row[&#x27;array_col&#x27;])</span><br><span class="line"></span><br><span class="line">expanded_cols = df.apply(explode_array_column, axis=1)</span><br><span class="line">expanded_cols.columns = [&#x27;col_&#123;&#125;&#x27;.format(i) for i in range(expanded_cols.shape[1])]</span><br><span class="line"></span><br><span class="line">df = pd.concat([df, expanded_cols], axis=1)</span><br><span class="line">df = df.drop(&#x27;array_col&#x27;, axis=1)</span><br><span class="line"></span><br><span class="line">print(df)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This will output a new DataFrame with three columns (col_0, col_1, and col_2) that contain the values from the original array column:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">   col_0  col_1  col_2</span><br><span class="line">0      1      2      3</span><br><span class="line">1      4      5      6</span><br><span class="line">2      7      8      9</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Creating-a-Custom-Transformer-with-scikit-learn"><a href="#Creating-a-Custom-Transformer-with-scikit-learn" class="headerlink" title="Creating a Custom Transformer with scikit-learn"></a>Creating a Custom Transformer with scikit-learn</h2><p>While the above approach works well for a single DataFrame, it can be cumbersome to repeat the same steps for multiple DataFrames. One way to simplify this process is to encapsulate the functionality into a custom transformer that can be used in scikit-learn pipelines.</p><p>Here’s an example implementation of a custom transformer that expands an array column into multiple columns:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line">class ArrayExpander(BaseEstimator, TransformerMixin):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        pass</span><br><span class="line">        </span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line">        return self</span><br><span class="line">    </span><br><span class="line">    def transform(self, X):</span><br><span class="line">        def explode_array_column(row):</span><br><span class="line">            return pd.Series(row[&#x27;array_col&#x27;])</span><br><span class="line">        </span><br><span class="line">        df = X.copy()</span><br><span class="line">        expanded_cols = df.apply(explode_array_column, axis=1)</span><br><span class="line">        expanded_cols.columns = [&#x27;embed_col_&#123;&#125;&#x27;.format(i) for i in range(expanded_cols.shape[1])]</span><br><span class="line">        df = pd.concat([df, expanded_cols], axis=1)</span><br><span class="line">        df = df.drop(&#x27;array_col&#x27;, axis=1)</span><br><span class="line">        return df</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">array_expander = ArrayExpander()</span><br><span class="line">df_transformed = array_expander.fit_transform(df)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Longer and more, the one minute takeaways of GPT4 model</title>
      <link href="2023/03/14/takeaway-of-gpt4/"/>
      <url>2023/03/14/takeaway-of-gpt4/</url>
      
        <content type="html"><![CDATA[<p>Most impressive thing:<br>The token limits goes from 4K up to 32K tokens (or roughly 50 pages of texts).<br>Now we can use GPT4 to write long novel.</p><p>Other important takeaways of GPT4:</p><ol><li>GPT-4 is more reliable, creative, and capable of handling more nuanced instructions than GPT-3.5, especially for complex tasks.</li><li>GPT-4 can accept both text and image inputs and generate text outputs in various domains.</li><li>Steerability allows API users to customize their AI’s style and task by describing the directions in the “system” message.</li><li>GPT-4 still has limitations, including hallucinating facts and making reasoning errors, and language model outputs should be used with caution, especially in high-stakes contexts.</li><li>GPT-4 significantly reduces hallucinations compared to previous models and scores 40% higher than GPT-3.5 on internal adversarial factuality evaluations.</li></ol>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gp4 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Adding progress bars and parallelize tasks in Python</title>
      <link href="2023/03/13/parrallelizing-and-visualizing-tasks-with-pqdm/"/>
      <url>2023/03/13/parrallelizing-and-visualizing-tasks-with-pqdm/</url>
      
        <content type="html"><![CDATA[<p>use tqdm and pqdm in Python to add progress bars to your code and parallelize tasks.</p><p>Projects in Python often involve long-running tasks like training models or processing large datasets. To make these tasks more manageable, it’s helpful to add progress bars to your code and parallelize tasks to take advantage of multiple CPU cores.</p><p>Two popular Python libraries for achieving these goals are <code>tqdm</code> and <code>pqdm</code>. tqdm provides a simple way to add progress bars to your code, while pqdm is a wrapper around tqdm and concurrent.futures that allows you to parallelize tasks while also showing a progress bar.</p><p>In this blog post, we’ll walk through how to use tqdm and pqdm in Python to add progress bars and parallelize tasks.</p><h2 id="Adding-Progress-Bars-with-tqdm"><a href="#Adding-Progress-Bars-with-tqdm" class="headerlink" title="Adding Progress Bars with tqdm"></a>Adding Progress Bars with tqdm</h2><p><code>tqdm</code> is a Python library that provides a simple way to add progress bars to your code. To use tqdm, you first need to install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install tqdm</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Once you’ve installed tqdm, you can use it in your code like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from tqdm import tqdm</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">for i in tqdm(range(100)):</span><br><span class="line">    time.sleep(0.1) # simulate a longer-running task</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we use tqdm to add a progress bar to a loop that runs 100 times. We also use the time.sleep function to simulate a longer-running task. When you run this code, you’ll see a progress bar that updates in real-time as the loop runs:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">100%|███████████████████████████████████████████████████| 100/100 [00:10&lt;00:00,  9.87it/s]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The progress bar shows you the percentage of the loop that’s completed, as well as an estimate of the time remaining to complete the loop. You can customize the appearance of the progress bar with different colors, styles, and other options to match your preferences and the needs of your project.</p><h2 id="Parallelizing-Tasks-with-pqdm"><a href="#Parallelizing-Tasks-with-pqdm" class="headerlink" title="Parallelizing Tasks with pqdm"></a>Parallelizing Tasks with pqdm</h2><p>pqdm is a Python library that builds on top of tqdm and concurrent.futures to provide a simple way to parallelize tasks while also showing a progress bar. To use pqdm, you first need to install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install pqdm</span><br></pre></td></tr></table></figure><p>Once you’ve installed pqdm, you can use it in your code like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pqdm.processes import pqdm</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">def process_data(data):</span><br><span class="line">    time.sleep(0.1) # simulate a longer-running task</span><br><span class="line">    return data * 2</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    data = range(100)</span><br><span class="line">    processed_data = pqdm(data, process_data, n_jobs=4, desc=&#x27;Processing data&#x27;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we define a function process_data that takes in a piece of data and returns a processed version of that data. We then use pqdm to apply this function to each piece of data in parallel, using four processes (n_jobs=4) and displaying a progress bar with the label “Processing data” (desc=’Processing data’).</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tqdm </tag>
            
            <tag> pqdm </tag>
            
            <tag> progress bars </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to convert CURL to python requests and vice versa</title>
      <link href="2023/03/12/conver-curl-to-pyton-requests-and-vice-versa/"/>
      <url>2023/03/12/conver-curl-to-pyton-requests-and-vice-versa/</url>
      
        <content type="html"><![CDATA[<p><code>curl</code> and Python <code>requests</code> are both powerful tools for sending HTTP requests. While curl is a command-line tool that allows you to send requests directly from your terminal, Python’s requests library provides a more programmatic way to send requests from within Python code. In this article, we’ll explore how to convert between curl and Python requests, so you can use the tool that makes the most sense for your workflow.</p><h2 id="Converting-curl-to-Python-requests"><a href="#Converting-curl-to-Python-requests" class="headerlink" title="Converting curl to Python requests"></a>Converting <code>curl</code> to Python <code>requests</code></h2><p>The basic syntax of a curl command looks like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl [OPTIONS] URL</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>When converting a curl command to Python requests, we need to translate the options and URL into Python code.</p><p>Here’s an example curl command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -X POST https://example.com/api/v1/users \</span><br><span class="line">    -H &#x27;Content-Type: application/json&#x27; \</span><br><span class="line">    -H &#x27;Authorization: Bearer YOUR_API_KEY&#x27; \</span><br><span class="line">    -d &#x27;&#123;&quot;username&quot;: &quot;john_doe&quot;, &quot;email&quot;: &quot;john_doe@example.com&quot;&#125;&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To convert this curl command to Python requests, we can write the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &#x27;https://example.com/api/v1/users&#x27;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;,</span><br><span class="line">    &#x27;Authorization&#x27;: &#x27;Bearer YOUR_API_KEY&#x27;</span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    &#x27;username&#x27;: &#x27;john_doe&#x27;,</span><br><span class="line">    &#x27;email&#x27;: &#x27;john_doe@example.com&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we use the requests.post() method to send a POST request to the URL <code>https://example.com/api/v1/users</code> with the JSON payload <code>&#123;&quot;username&quot;: &quot;john_doe&quot;, &quot;email&quot;: &quot;john_doe@example.com&quot;&#125;</code>. We also include the Content-Type and Authorization headers.</p><h2 id="Converting-Python-requests-to-curl"><a href="#Converting-Python-requests-to-curl" class="headerlink" title="Converting Python requests to curl"></a>Converting Python requests to curl</h2><p>Converting Python requests code to a curl command is a bit trickier, as there’s no direct equivalent for the requests library on the command line. However, we can use the –data or -d option to pass data to the curl command, and the -H option to set headers.</p><p>Here’s an example Python GET requests script:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &#x27;https://example.com/api/v1/users&#x27;</span><br><span class="line">headers = &#123;</span><br><span class="line">    &#x27;Content-Type&#x27;: &#x27;application/json&#x27;,</span><br><span class="line">    &#x27;Authorization&#x27;: &#x27;Bearer YOUR_API_KEY&#x27;</span><br><span class="line">&#125;</span><br><span class="line">params = &#123;</span><br><span class="line">    &#x27;username&#x27;: &#x27;john_doe&#x27;,</span><br><span class="line">    &#x27;sort&#x27;: &#x27;name&#x27;,</span><br><span class="line">    &#x27;order&#x27;: &#x27;asc&#x27;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">response = requests.get(url, headers=headers, params=params)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.json())</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To convert this Python requests code to a curl command, we can use the following command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">curl -X GET &#x27;https://example.com/api/v1/users?username=john_doe&amp;sort=name&amp;order=asc&#x27; \</span><br><span class="line">    -H &#x27;Content-Type: application/json&#x27; \</span><br><span class="line">    -H &#x27;Authorization: Bearer YOUR_API_KEY&#x27;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we use the -X GET option to specify that we’re sending a GET request, and we pass the URL and query parameters as a string. We also include the Content-Type and Authorization headers.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> curl </tag>
            
            <tag> requests </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between `json` and `data` parameter of the HTTP POST Requests with JSON Data in Python using the Requests Library</title>
      <link href="2023/03/12/using-json-or-data-parameter-in-requests-post/"/>
      <url>2023/03/12/using-json-or-data-parameter-in-requests-post/</url>
      
        <content type="html"><![CDATA[<p>HTTP POST requests are a common way to send data to web servers from client applications. When sending JSON data as the body of a POST request, the requests library in Python provides two options: using the json parameter or manually converting the payload dictionary to a JSON string and using the data parameter. In this post, we’ll discuss both options and when to use each one.</p><h2 id="Using-the-json-Parameter"><a href="#Using-the-json-Parameter" class="headerlink" title="Using the json Parameter"></a>Using the json Parameter</h2><p>The json parameter is the easiest way to send JSON data as the body of a POST request using the requests library. Here’s an example code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;https://example.com/api&quot;</span><br><span class="line">payload = &#123;&quot;key&quot;: &quot;value&quot;&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, json=payload)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we define a url variable that represents the endpoint where we want to send the POST request. We also define a payload variable that contains the JSON data we want to send. To send the POST request, we use the requests.post() method and pass the url and json parameters. The json parameter automatically sets the Content-Type header to application/json and serializes the payload dictionary to a JSON string. The response variable stores the server’s response, which we can then inspect using response.status_code and response.text.</p><p>Using the json parameter is the preferred way to send JSON data in a POST request because it’s more concise and Pythonic. It also automatically sets the Content-Type header to application/json, which is the recommended way to send JSON data in an HTTP POST request.</p><h2 id="Using-the-data-Parameter"><a href="#Using-the-data-Parameter" class="headerlink" title="Using the data Parameter"></a>Using the data Parameter</h2><p>The data parameter is an alternative way to send JSON data in a POST request using the requests library. Here’s an example code snippet:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">url = &quot;https://example.com/api&quot;</span><br><span class="line">payload = &#123;&quot;key&quot;: &quot;value&quot;&#125;</span><br><span class="line"></span><br><span class="line">headers = &#123;&quot;Content-Type&quot;: &quot;application/json&quot;&#125;</span><br><span class="line">data = json.dumps(payload)</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, data=data)</span><br><span class="line"></span><br><span class="line">print(response.status_code)</span><br><span class="line">print(response.text)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we define a headers dictionary that sets the Content-Type header to application/json. We also convert the payload dictionary to a JSON string using the json.dumps() method and store it in a data variable. To send the POST request, we use the requests.post() method and pass the url, headers, and data parameters. The data parameter expects a byte string, which is why we convert the JSON string to a byte string using json.dumps().</p><p>Using the data parameter gives you more control over the headers and payload of the POST request, but it’s less concise and requires more manual work than using the json parameter. You need to manually set the Content-Type header and serialize the payload dictionary to a JSON string using the json.dumps() method.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this post, we discussed two ways to send JSON data as the body of a POST request in Python using the requests library: using the json parameter and using the data parameter. Using the json parameter is the recommended way because it’s more concise, Pythonic, and sets the Content-Type header automatically. However, the data parameter gives you more control over the headers and payload of the POST request, which can be useful in certain scenarios.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> requests </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Prettify and Visualize JSON Data in Python</title>
      <link href="2023/03/11/how-to-prettify-and-visualize-json-data/"/>
      <url>2023/03/11/how-to-prettify-and-visualize-json-data/</url>
      
        <content type="html"><![CDATA[<p>JSON (JavaScript Object Notation) is a lightweight data interchange format that is widely used in web applications and APIs. JSON data can be difficult to read and visualize, especially if it contains nested structures and arrays. In this blog post, we will learn how to use Python to prettify and visualize JSON data.</p><h2 id="Using-json-dumps"><a href="#Using-json-dumps" class="headerlink" title="Using json.dumps()"></a>Using <code>json.dumps()</code></h2><p>The easiest way to prettify and visualize JSON data in Python is to use the json.dumps() function. This function converts a Python data structure to a JSON string and applies indentation to make it more readable.</p><p>Here’s an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line"># Define a JSON data structure</span><br><span class="line">data = &#123;</span><br><span class="line">    &quot;name&quot;: &quot;John&quot;,</span><br><span class="line">    &quot;age&quot;: 30,</span><br><span class="line">    &quot;city&quot;: &quot;New York&quot;,</span><br><span class="line">    &quot;pets&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Fluffy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;cat&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Buddy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;dog&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># Use the json.dumps() function to prettify and visualize the data</span><br><span class="line">print(json.dumps(data, indent=4))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>output</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;John&quot;,</span><br><span class="line">    &quot;age&quot;: 30,</span><br><span class="line">    &quot;city&quot;: &quot;New York&quot;,</span><br><span class="line">    &quot;pets&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Fluffy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;cat&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Buddy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;dog&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As you can see, the json.dumps() function takes two arguments: the data structure to be prettified and the number of spaces to use for indentation. In this example, the indent argument is set to 4, which means that each level of nesting is indented by four spaces. This makes the data structure easier to read and visualize.</p><h2 id="Converting-a-JSON-String-to-a-Python-Data-Structure"><a href="#Converting-a-JSON-String-to-a-Python-Data-Structure" class="headerlink" title="Converting a JSON String to a Python Data Structure"></a>Converting a JSON String to a Python Data Structure</h2><p>If you have a JSON string instead of a Python data structure, you need to use the json.loads() function to convert it to a Python data structure before you can use json.dumps() to prettify and visualize it.</p><p>Here’s an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import json</span><br><span class="line"></span><br><span class="line"># Define a JSON string</span><br><span class="line">json_string = &#x27;&#123;&quot;name&quot;: &quot;John&quot;, &quot;age&quot;: 30, &quot;city&quot;: &quot;New York&quot;, &quot;pets&quot;: [&#123;&quot;name&quot;: &quot;Fluffy&quot;, &quot;species&quot;: &quot;cat&quot;&#125;, &#123;&quot;name&quot;: &quot;Buddy&quot;, &quot;species&quot;: &quot;dog&quot;&#125;]&#125;&#x27;</span><br><span class="line"></span><br><span class="line"># Use json.loads() to convert the JSON string to a Python data structure</span><br><span class="line">data = json.loads(json_string)</span><br><span class="line"></span><br><span class="line"># Use json.dumps() to prettify and visualize the data</span><br><span class="line">print(json.dumps(data, indent=4))</span><br></pre></td></tr></table></figure><p>output</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;name&quot;: &quot;John&quot;,</span><br><span class="line">    &quot;age&quot;: 30,</span><br><span class="line">    &quot;city&quot;: &quot;New York&quot;,</span><br><span class="line">    &quot;pets&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Fluffy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;cat&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;name&quot;: &quot;Buddy&quot;,</span><br><span class="line">            &quot;species&quot;: &quot;dog&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, we first defined a JSON string json_string. We then used json.loads() to convert it to a Python data structure, which we stored in the variable data. Finally, we used json.dumps() to prettify and visualize the data variable.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> json data visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What impact can we expect on the advertising market in light of the ChatGPT era?</title>
      <link href="2023/03/05/how-chatgpt-affect-advertising-market/"/>
      <url>2023/03/05/how-chatgpt-affect-advertising-market/</url>
      
        <content type="html"><![CDATA[<p>We know this formula:<br>Ads revenue = Traffic x Ad impressions x Effective cost per mille (eCPM)</p><p>Let’s first look at traffic.<br>Compared with search, products like ChatGPT that use natural language interaction can meet richer scenarios and more specific needs, and the multi-round interaction mode of dialogue will undoubtedly increase overall traffic. If these types of products are subsequently integrated into life scenarios such as smart speakers, their user value and frequency of use are expected to further increase.</p><p>Now let’s look at Impressions.<br>Compared with search and other internet advertising, dialogue is actually a more natural user interaction interface, and users’ tolerance for ads will naturally decrease. Users will definitely be resentful of forced or excessive ads.Therefore, there will be higher requirements for ad accuracy and native degree, which presents a practical challenge for new-generation ad product managers in designing ad products in the era of ChatGPT. One possible approach is that there will be multiple forms of ads.</p><p>For example, if a user asks for platform recommendations for a suitable car, then it is acceptable to directly insert car ads that meet the requirements. When users are engaged in a conversation about lifestyle, brand ads can be subtly embedded in the content.</p><p>Finally, let’s look at eCPM.<br>Undoubtedly, compared with search and other internet advertising, ads in the ChatGPT era will be better able to meet user needs and directly improve eCPM. Dialogue commands are intent inputs that are of higher dimensionality than search keywords and browsing behaviors.</p><p>This means that the platform can directly obtain user intent rather than relying on algorithmic prediction and calculation, making it more likely to match more suitable ads, which is the key to improving eCPM.</p><p>So in the end which direction do you think the ads revenues will go? Probably the key is how to get more personalized ads in the era of ChatGPT!</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> advertising </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>One-line Code to Remove Duplicates in a List While Maintaining the Original Order of Elements</title>
      <link href="2023/03/01/one-line-of-python-code-to-remove-duplicates-in-list-and-maintain-order/"/>
      <url>2023/03/01/one-line-of-python-code-to-remove-duplicates-in-list-and-maintain-order/</url>
      
        <content type="html"><![CDATA[<p>Python is a popular programming language for its simplicity and ease of use. When working with lists in Python, you may encounter a scenario where you need to remove duplicates while maintaining the original order of the elements. In this blog post, we’ll explore how to achieve this using a one-liner Python code and why we can do it in Python 3.7 and above.</p><p>Consider the following list of integers:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">my_list = [1, 2, 3, 4, 2, 1, 5, 6, 7, 6]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>If we want to remove the duplicates in this list, we can use the built-in set() function. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new_list = list(set(my_list))</span><br></pre></td></tr></table></figure><p>This will create a new list that contains only the unique elements of my_list. However, this method does not maintain the original order of the elements. The order of the elements in the new list will be arbitrary.</p><p>To maintain the original order of the elements, we can use a one-liner Python code that uses the dict.fromkeys() method and the fact that dictionaries in Python 3.7 and above maintain the order of insertion. Here’s the code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">new_list = list(dict.fromkeys(my_list))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This code creates a dictionary from the elements of my_list using dict.fromkeys(). Since dictionaries in Python 3.7 and above maintain the order of insertion, the order of the keys in the dictionary will be the same as the order of the elements in the original list. We then convert the dictionary keys back to a list using the built-in list() function. The resulting new_list will contain only the unique elements of my_list, while maintaining their original order.</p><p>This method is efficient and easy to use, making it a useful tool for working with lists in Python. The ability of dictionaries to maintain the order of insertion is a relatively new feature in Python, introduced in version 3.7. Prior to version 3.7, dictionaries were unordered, and this method would not have worked.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How does two tower system work in the recommender system</title>
      <link href="2023/02/26/two-tower-system-in-recommender-system/"/>
      <url>2023/02/26/two-tower-system-in-recommender-system/</url>
      
        <content type="html"><![CDATA[<p>The two-tower recommendation system is a type of collaborative filtering algorithm used in recommendation systems. It is called a “two-tower” system because it consists of two neural networks or “towers” that work together to generate personalized recommendations for users.</p><h2 id="How-does-it-work"><a href="#How-does-it-work" class="headerlink" title="How does it work"></a>How does it work</h2><p>The first tower is called the “user tower.” It takes as input a user’s historical interactions with items, such as the products they have purchased or the movies they have watched, and converts this information into a fixed-length embedding vector that represents the user’s preferences. This embedding vector is then passed to the second tower.</p><p>The second tower is called the “item tower.” It takes as input the metadata of all items in the catalog, such as the title, description, genre, and other features. The item tower also converts this information into a fixed-length embedding vector that represents each item.</p><p>The two embedding vectors from the user and item towers are then compared using a similarity function, such as cosine similarity. The similarity score indicates how similar the user’s preferences are to each item in the catalog. The items with the highest similarity scores are recommended to the user.</p><p>The two-tower recommendation system is a popular approach to personalized recommendation because it can handle large-scale and sparse data sets, and can capture complex user-item interactions. It has been used in a variety of applications, such as e-commerce, streaming services, and social media platforms.</p><h2 id="How-training-is-done-using-deep-learning"><a href="#How-training-is-done-using-deep-learning" class="headerlink" title="How training is done using deep learning"></a>How training is done using deep learning</h2><p> In the two-tower recommendation system, the neural networks that generate the user and item embeddings need to be optimized in such a way that the dot product of user embeddings and item embeddings are higher for user purchased items and lower for not purchased items. This is typically achieved through a process called training, where the model is presented with a set of user-item interactions and learns to predict the likelihood of each user interacting with each item in the future.</p><p>During training, the model is optimized to minimize a loss function, which measures the difference between the predicted and actual user-item interactions. The most commonly used loss function in recommendation systems is the binary cross-entropy loss, which penalizes the model for making incorrect predictions.</p><p>To optimize the neural networks, backpropagation is used to compute the gradients of the loss with respect to the model parameters. The gradients are then used to update the model parameters using an optimization algorithm such as stochastic gradient descent (SGD) or Adam. The process of updating the model parameters is repeated for multiple epochs until the model converges to a set of optimal parameters.</p><p>By optimizing the neural networks in this way, the model learns to generate user and item embeddings that capture the underlying patterns and relationships in the data, and can make accurate predictions of user-item interactions. This allows the two-tower recommendation system to provide personalized recommendations that are tailored to the preferences of each individual user.</p><h2 id="Efficient-search-during-inference-time"><a href="#Efficient-search-during-inference-time" class="headerlink" title="Efficient search during inference time"></a>Efficient search during inference time</h2><p>When calculating the dot product of a user embedding with all the item embeddings in the item tower, there are several techniques that can be used to make the computation more efficient and faster. Here are a few approaches:</p><p>Use matrix multiplication: Rather than calculating the dot product between the user embedding and each item embedding one-by-one, it is more efficient to perform a matrix multiplication between the user embedding and the entire item embedding matrix. This can be done using the numpy or PyTorch library, which are optimized for matrix computations.</p><p>Use approximate nearest neighbor (ANN) search: When the number of items is very large, it can be computationally expensive to calculate the dot product between the user embedding and all the item embeddings. One approach to speed up the search is to use an approximate nearest neighbor search algorithm, such as locality-sensitive hashing (LSH) or k-d trees. These algorithms allow us to quickly identify a smaller set of candidate items that are most similar to the user’s preferences.</p><p>Use a cache: Since the user embedding is fixed during inference, we can cache the dot products between the user embedding and all item embeddings. This can be done ahead of time during training or on-the-fly during inference. By caching the dot products, we can avoid having to compute them every time a user requests recommendations, which can significantly speed up the recommendation process.</p><p>Use parallelization: If the hardware allows for it, we can parallelize the computation of the dot products between the user embedding and all item embeddings. This can be done using multi-threading or GPUs to perform the computation in parallel, which can further speed up the recommendation process.</p><p>By using these techniques, we can make the computation of the dot product between the user embedding and all item embeddings more efficient and faster, which can help to improve the performance of the recommendation system.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> recommender system </tag>
            
            <tag> two tower system </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AlphaGo and ChatGPT, The Similarities Between Two AI Titans</title>
      <link href="2023/02/15/alphago-and-chatgpt/"/>
      <url>2023/02/15/alphago-and-chatgpt/</url>
      
        <content type="html"><![CDATA[<p>AlphaGo and ChatGPT are two of the most famous and groundbreaking AI systems ever created.<br>While they were designed for different purposes, they share some important similarities, including the use of deep learning, neural networks, and a focus on achieving human-like performance.<br>And now, thanks to a recent announcement from DeepMind, we can add one more similarity to the list: the open-sourcing of AlphaGo’s Monte Carlo Tree Search algorithm.</p><p>Monte Carlo Tree Search is a search algorithm that solves for the best move in turn-based games by selecting, expanding, simulating, and updating the nodes in a strategy tree.<br>It is the heart of AlphaGo and AlphaZero, and it is arguably the most complex component of these AI systems.<br>Making it efficient is even more nontrivial. But with the release of the “mctx” repository on Github,<br>developers can now access JAX-native Monte Carlo Tree Search that runs on batches of inputs in parallel and blazing fast.</p><p>But what makes AlphaGo and ChatGPT so similar in the first place? </p><ul><li>Deep Learning: Both AlphaGo and ChatGPT use deep learning techniques to process and understand large amounts of data. AlphaGo was trained on thousands of human expert games, while ChatGPT was trained on a massive amount of text data from the internet.</li><li>Neural Networks: Both systems use neural networks to process data and make predictions. AlphaGo uses a combination of convolutional neural networks and recurrent neural networks to analyze the board state and predict the best moves, while ChatGPT uses a transformer-based neural network to generate human-like text.</li><li>Reinforcement Learning: AlphaGo uses reinforcement learning to train its neural network by playing millions of games against itself and learning from its mistakes. ChatGPT does not use reinforcement learning but instead uses unsupervised learning to train its neural network on a massive amount of text data.</li><li>Human-like performance: Both AlphaGo and ChatGPT are designed to achieve human-like performance in their respective domains. AlphaGo defeated the world champion Go player, Lee Sedol, in a historic match, while ChatGPT can generate text that is difficult to distinguish from that written by humans.</li><li>Groundbreaking achievements: AlphaGo and ChatGPT have both achieved groundbreaking achievements in their respective fields. AlphaGo was the first AI system to defeat a human world champion in Go, while ChatGPT is one of the most advanced natural language processing systems in the world.</li></ul><p>With the release of the “mctx” repository, developers can now explore the inner workings of AlphaGo and AlphaZero,<br>and build upon this groundbreaking research to create their own AI systems.<br>The open-sourcing of this code is a testament to the importance of collaboration and the power of open science.</p><p><strong>mctx</strong> <a href="https://github.com/deepmind/mctx">Link</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> chatgpt </tag>
            
            <tag> alphago </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transforming One or More Columns of a Pandas DataFrame using ColumnTransformer</title>
      <link href="2023/02/14/sklearn-columntransformer-one-columns-to-many-columns/"/>
      <url>2023/02/14/sklearn-columntransformer-one-columns-to-many-columns/</url>
      
        <content type="html"><![CDATA[<p>When working with tabular data, it’s common to have to transform one or more columns to make them more amenable to analysis or modeling. In many cases, these transformations can be easily accomplished using the pandas library. However, when working with large datasets or building machine learning pipelines, it can be more efficient to use scikit-learn’s ColumnTransformer class to apply transformations to specific columns of the data.</p><p>In this blog post, we’ll demonstrate how to use a custom transformer with scikit-learn’s <strong>ColumnTransformer</strong> to transform one or more columns of a Pandas DataFrame.</p><h2 id="Example-1-Transforming-NumPy-arrays"><a href="#Example-1-Transforming-NumPy-arrays" class="headerlink" title="Example 1: Transforming NumPy arrays"></a>Example 1: Transforming NumPy arrays</h2><p>Let’s start with a simple example where we have a NumPy array with three columns, and we want to transform the first two columns into two new columns.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line">from sklearn.compose import ColumnTransformer</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">class CustomTransformer(BaseEstimator, TransformerMixin):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        pass</span><br><span class="line">    </span><br><span class="line">    def transform(self, X):</span><br><span class="line">        # Here, X is a 2D numpy array or pandas DataFrame</span><br><span class="line">        # Transform columns 0 and 1 into multiple columns</span><br><span class="line">        transformed_cols = np.column_stack([X[:, 0]**2, np.sqrt(X[:, 1])])</span><br><span class="line">        # Return the transformed columns as a 2D numpy array</span><br><span class="line">        return transformed_cols</span><br><span class="line">    </span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line">        return self</span><br><span class="line">    </span><br><span class="line"># Example usage</span><br><span class="line">X = np.array([[1, 4, 7], [2, 9, 8], [3, 16, 9]])</span><br><span class="line">transformer = ColumnTransformer(</span><br><span class="line">    transformers=[(&#x27;custom&#x27;, CustomTransformer(), [0, 1])],</span><br><span class="line">    remainder=&#x27;passthrough&#x27;)</span><br><span class="line"># The &#x27;remainder&#x27; parameter preserves any columns not transformed</span><br><span class="line">transformed_X = transformer.fit_transform(X)</span><br><span class="line">print(transformed_X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, the CustomTransformer class takes two input columns and transforms them into two output columns. The ColumnTransformer applies this transformer to columns 0 and 1 of the input data, and preserves column 2. The “passthrough” option has been used to preserve the remaining column in its original form.</p><h2 id="Example-2-Transforming-Pandas-DataFrames"><a href="#Example-2-Transforming-Pandas-DataFrames" class="headerlink" title="Example 2: Transforming Pandas DataFrames"></a>Example 2: Transforming Pandas DataFrames</h2><p>Now, let’s modify the previous example to work with a Pandas DataFrame instead of a NumPy array.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line">from sklearn.compose import ColumnTransformer</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line"></span><br><span class="line">class CustomTransformer(BaseEstimator, TransformerMixin):</span><br><span class="line">    def __init__(self):</span><br><span class="line">        pass</span><br><span class="line">    </span><br><span class="line">    def transform(self, X):</span><br><span class="line">        # Here, X is a pandas DataFrame</span><br><span class="line">        # Transform columns &#x27;A&#x27; and &#x27;B&#x27; into multiple columns</span><br><span class="line">        transformed_cols = pd.DataFrame(&#123;&#x27;A_squared&#x27;: X[&#x27;A&#x27;]**2, </span><br><span class="line">                                         &#x27;B_sqrt&#x27;: X[&#x27;B&#x27;]**0.5&#125;)</span><br><span class="line">        # Return the transformed columns as a pandas DataFrame</span><br><span class="line">        return transformed_cols</span><br><span class="line">    </span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line">        return self</span><br><span class="line">    </span><br><span class="line"># Example usage</span><br><span class="line">df = pd.DataFrame(&#123;&#x27;A&#x27;: [1, 2, 3], &#x27;B&#x27;: [4, 9, 16], &#x27;C&#x27;: [7, 8, 9]&#125;)</span><br><span class="line">transformer = ColumnTransformer(</span><br><span class="line">    transformers=[(&#x27;custom&#x27;, CustomTransformer(), [&#x27;A&#x27;, &#x27;B&#x27;])], </span><br><span class="line">    remainder=&#x27;passthrough&#x27;)</span><br><span class="line"># The &#x27;remainder&#x27; parameter preserves any columns not transformed</span><br><span class="line">transformed_df = transformer.fit_transform(df)</span><br><span class="line">print(transformed_df)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, the CustomTransformer class takes two input columns (‘A’ and ‘B’) and transforms them into two output columns (‘A_squared’ and ‘B_sqrt’) in a pandas DataFrame. The ColumnTransformer applies this transformer to columns ‘A’ and ‘B’ of the input data, and preserves column ‘C’. The “passthrough” option has been used to preserve the remaining column ‘C’ in its original form.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unveiling the Future Challenges for ChatGPT, Navigating the Risks Ahead</title>
      <link href="2023/02/13/potential-risk-of-chatgpt/"/>
      <url>2023/02/13/potential-risk-of-chatgpt/</url>
      
        <content type="html"><![CDATA[<p><img src="/content/images/2023-02-13-1.jpg"></p><p>ChatGPT, the popular language model developed by OpenAI, has revolutionized the way we interact with AI. However, as the use of language models like ChatGPT becomes more widespread, it is important to consider the potential risks and challenges that they may face in the future.</p><h2 id="1-Quality-of-Data-and-Fairness"><a href="#1-Quality-of-Data-and-Fairness" class="headerlink" title="1. Quality of Data and Fairness"></a>1. Quality of Data and Fairness</h2><p>One of the major risks associated with the use of ChatGPT is the quality of the data used to train the model. As more people use the model to generate content, there is a risk that the quality of the generated content could decrease, especially if the model is used to produce low-quality, unreliable, or misleading information.<br>The use of free online data to train language models raises important questions about the distribution of value and the fairness of the system, as the people who generate and publish this data do not receive any compensation for their contributions.</p><h2 id="2-Bias-and-Inaccuracies"><a href="#2-Bias-and-Inaccuracies" class="headerlink" title="2. Bias and Inaccuracies"></a>2. Bias and Inaccuracies</h2><p>Like any AI model, GPT-3 is trained on a dataset that reflects the biases and inaccuracies present in the data it was trained on. This can lead to biased and inaccurate responses from the model, especially when it comes to sensitive topics such as race, gender, and politics.<br>For instance, a study by MIT Technology Review found that AI models trained on biased data sets were more likely to produce biased results. The same is true for ChatGPT and other language models, which can perpetuate and amplify existing biases in the data they are trained on.</p><h2 id="3-Lack-of-Common-Sense"><a href="#3-Lack-of-Common-Sense" class="headerlink" title="3. Lack of Common Sense"></a>3. Lack of Common Sense</h2><p>Despite its impressive language generation capabilities, GPT-3 still lacks a true understanding of the world and the ability to apply common sense reasoning to new situations. This can result in nonsensical or incorrect answers to questions.<br>For example, GPT-3 may generate an answer that is technically correct but not aligned with common sense or everyday understanding of the world.</p><h2 id="4-Limited-Context"><a href="#4-Limited-Context" class="headerlink" title="4. Limited Context"></a>4. Limited Context</h2><p>ChatGPT is designed to respond to individual prompts, so it may struggle to maintain a consistent conversational context across multiple turns or to understand the implications of what it has said in the past.<br>This can lead to inconsistent or misleading answers, especially in situations where the context is important, such as in medical or legal advice.</p><h2 id="5-Ethical-Concerns"><a href="#5-Ethical-Concerns" class="headerlink" title="5. Ethical Concerns"></a>5. Ethical Concerns</h2><p>The use of GPT-3 and other large language models raises important ethical concerns, including the potential for the technology to be used to spread misinformation and propaganda, and the impact it may have on jobs and employment in industries such as writing and journalism.<br>As the use of language models like ChatGPT becomes more widespread, there is a risk that it could be used to spread false information or manipulate public opinion. There is also the potential for the technology to displace jobs in industries that rely on human writing and journalism, which raises important ethical questions about the distribution of value and the future of work.</p><h2 id="6-High-Computational-Requirements"><a href="#6-High-Computational-Requirements" class="headerlink" title="6. High Computational Requirements"></a>6. High Computational Requirements</h2><p>Finally, GPT-3 requires significant computational resources to run, making it less accessible to individual researchers and developers who might want to use it.<br>This limits the ability of smaller organizations and researchers to experiment with and build on the technology, which could slow its development and limit its potential impact.</p><p>The development and widespread adoption of web3 technology, which is based on decentralized data, could also have an impact on the future of ChatGPT. If web3 technology becomes widely adopted, it could become more difficult for ChatGPT to access the large amounts of publicly available data that it needs to train its models.</p><p>In conclusion, while ChatGPT has the potential to revolutionize the way we interact with AI, it is important to consider the potential risks and challenges that it may face in the future, and to take steps to mitigate these risks and ensure its continued success. This includes addressing issues such as bias and inaccuracies in training data, the lack of common sense, limited context, ethical concerns, and high computational requirements.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt3 </tag>
            
            <tag> chatgpt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>General Rule of Maximizing Ad Ranking, Balancing Relevance and Bid for Optimal Results</title>
      <link href="2023/02/12/website-ad-ranking-general-rule/"/>
      <url>2023/02/12/website-ad-ranking-general-rule/</url>
      
        <content type="html"><![CDATA[<p>Ad ranking is an essential aspect of online advertising that impacts both the website that displays the ads and the advertisers who create them. To optimize ad ranking, websites need to balance different factors, such as ad relevance and bid, to ensure that the most valuable and relevant ads are displayed to users.</p><p>One of the most important factors in ad ranking is ad relevance. Users are more likely to engage with ads that are relevant to their interests, needs, and search queries. Thus, websites need to evaluate the quality of ads and prioritize those that are most relevant to their users. This can be achieved by using algorithms that take into account several factors, such as click-through rate, ad quality score, and user demographics.</p><p>Another important factor to consider is bid. Advertisers typically bid on the cost per click (CPC) or cost per impression (CPM) for their ads. Websites can use this bid amount to determine the ad’s placement and frequency. However, simply selecting the highest bid could lead to irrelevant ads being shown to users, resulting in a poor user experience.</p><p>To balance ad relevance and bid, websites need to evaluate their algorithm and determine the best way to weigh these factors. This may involve regularly updating the algorithm, monitoring user behavior, and adjusting the weight given to different factors as needed. Additionally, websites can encourage advertisers to create high-quality, relevant ads by providing guidelines and resources.</p><p>A basic formula that can be used to determine ad ranking is the Ad Rank formula:</p><p><strong>Ad Rank = Bid x Quality Score</strong></p><p>This formula takes into account both the advertiser’s bid amount and the quality of the ad, with the quality score being a measure of the ad’s relevance and overall value to users.</p><p>When optimizing ad ranking, websites need to balance user experience, advertiser objectives, and revenue generation. By finding the right balance between ad relevance and bid, websites can provide a better user experience, attract high-quality advertisers, and maximize their revenue potential.</p><p>In conclusion, ad ranking is a complex process that involves balancing multiple factors, including ad relevance and bid. By using algorithms that take into account these factors and utilizing the Ad Rank formula, websites can optimize their ad ranking and provide the best user experience while generating revenue from advertising.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> adranking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between &#39;python -m pip install&#39; and &#39;pip install&#39;</title>
      <link href="2023/02/08/different-ways-to-install-python-packages-with-pip/"/>
      <url>2023/02/08/different-ways-to-install-python-packages-with-pip/</url>
      
        <content type="html"><![CDATA[<p>Installing packages in Python can be done using the package manager pip. There are two ways to run pip to install packages: python -m pip install and pip install. In this article, we’ll discuss the difference between the two.</p><h2 id="python-m-pip-install"><a href="#python-m-pip-install" class="headerlink" title="python -m pip install"></a>python -m pip install</h2><p>The python -m pip install command runs the pip module as a script using the python executable. The -m option stands for “module” and it tells Python to run the specified module as a script. This allows you to specify which Python interpreter you want to use, even if you have multiple versions of Python installed on your system. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python3 -m pip install &lt;package_name&gt;</span><br></pre></td></tr></table></figure><p>This is a more flexible and portable way of installing packages, as it allows you to specify which Python interpreter you want to use, even if you have multiple versions of Python installed on your system. This can be useful if you want to install a package into a specific Python environment, or if you want to use a different version of Python than the default one on your system.</p><h2 id="pip-install"><a href="#pip-install" class="headerlink" title="pip install"></a>pip install</h2><p>The pip install command assumes that pip is in your system’s PATH, and that it corresponds to the correct version of Python that you want to use. For example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip3 install &lt;package_name&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This is a more convenient option, as it does not require you to specify the Python interpreter to use. However, if you have multiple versions of Python installed, or if the pip executable is not in your PATH, then pip install may not work as expected.</p><p>In conclusion, both python -m pip install and pip install achieve the same result, but python -m pip install is a more flexible and portable option, while pip install is more convenient but may not work in all scenarios. It is important to understand the difference between the two to choose the right approach when installing packages in Python.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logging metrics and tags of multiple models into one mlflow in Databricks</title>
      <link href="2023/02/07/logging-multiple-models-in-one-mlflow-experiment-with-databricks/"/>
      <url>2023/02/07/logging-multiple-models-in-one-mlflow-experiment-with-databricks/</url>
      
        <content type="html"><![CDATA[<p>There are situations where we need to run multiple notebooks or algorithms that are closely related. In this case, it makes sense to log all the information into one central place.</p><p>We can leverage MLflow with Databricks to achieve this goal.</p><p>First, let’s see what information we can log with MLflow?</p><h2 id="Information-to-log-in-MLflow"><a href="#Information-to-log-in-MLflow" class="headerlink" title="Information to log in MLflow"></a>Information to log in MLflow</h2><ul><li><p>mlflow.log_param() logs a single key-value param in the currently active run. The key and value are both strings. Use mlflow.log_params() to log multiple params at once.</p></li><li><p>mlflow.log_metric() logs a single key-value metric. The value must always be a number. MLflow remembers the history of values for each metric. Use mlflow.log_metrics() to log multiple metrics at once.</p></li><li><p>mlflow.set_tag() sets a single key-value tag in the currently active run. The key and value are both strings. Use mlflow.set_tags() to set multiple tags at once.</p></li><li><p>mlflow.log_artifact() logs a local file or directory as an artifact, optionally taking an artifact_path to place it in within the run’s artifact URI. Run artifacts can be organized into directories, so you can place the artifact in a directory this way.</p></li><li><p>mlflow.log_artifacts() logs all the files in a given directory as artifacts, again taking an optional artifact_path.</p></li></ul><p>To distinguish different algorithm names, we can use the mlflow.set_tag()function to log the algorithm name into the experiments.</p><h2 id="Example-code-to-use-in-Databricks-notebooks"><a href="#Example-code-to-use-in-Databricks-notebooks" class="headerlink" title="Example code to use in Databricks notebooks"></a>Example code to use in Databricks notebooks</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import mlflow</span><br><span class="line">import mlflow.sklearn</span><br><span class="line">from sklearn.ensemble import RandomForestRegressor</span><br><span class="line">from sklearn.metrics import mean_squared_error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Import the dataset from scikit-learn and create the training and test datasets. </span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">from sklearn.datasets import load_diabetes</span><br><span class="line"></span><br><span class="line">db = load_diabetes()</span><br><span class="line">X = db.data</span><br><span class="line">y = db.target</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br><span class="line"></span><br><span class="line"># This run uses mlflow.set_experiment() to specify an experiment in the workspace where runs should be logged. </span><br><span class="line"># If the experiment specified by experiment_name does not exist in the workspace, MLflow creates it.</span><br><span class="line"># Access these runs using the experiment name in the workspace file tree. </span><br><span class="line"></span><br><span class="line">experiment_name = &quot;/xxxx/test-experiment&quot;  # pluging your path at Databricks, where the &quot;test-experiment&quot; is the  actual experiment name</span><br><span class="line">mlflow.set_experiment(experiment_name)</span><br><span class="line"></span><br><span class="line">with mlflow.start_run():</span><br><span class="line">  n_estimators = 100</span><br><span class="line">  max_depth = 6</span><br><span class="line">  max_features = 3</span><br><span class="line">  # Create and train model</span><br><span class="line">  rf = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, max_features = max_features)</span><br><span class="line">  rf.fit(X_train, y_train)</span><br><span class="line">  # Make predictions</span><br><span class="line">  predictions = rf.predict(X_test)</span><br><span class="line">  </span><br><span class="line">  # Log algorithm name</span><br><span class="line">  mlflow.set_tag(&quot;model_name&quot;, &quot;user_test1&quot;)</span><br><span class="line">    </span><br><span class="line">  # Log parameters</span><br><span class="line">  mlflow.log_param(&quot;num_trees&quot;, n_estimators)</span><br><span class="line">  mlflow.log_param(&quot;maxdepth&quot;, max_depth)</span><br><span class="line">  mlflow.log_param(&quot;max_feat&quot;, max_features)</span><br><span class="line">  </span><br><span class="line">  # Log model</span><br><span class="line">  mlflow.sklearn.log_model(rf, &quot;random-forest-model&quot;)</span><br><span class="line">  </span><br><span class="line">  # Create metrics</span><br><span class="line">  mse = mean_squared_error(y_test, predictions)</span><br><span class="line">    </span><br><span class="line">  # Log metrics</span><br><span class="line">  mlflow.log_metric(&quot;mse&quot;, mse)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Results-in-MLflow-experiment-dashboard"><a href="#Results-in-MLflow-experiment-dashboard" class="headerlink" title="Results in MLflow experiment dashboard"></a>Results in MLflow experiment dashboard</h2><p><img src="/content/images/2023-02-07-1.png"></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Difference between fit, transform, fit_transform, predict, and predict_proba in a sklearn pipeline</title>
      <link href="2023/02/02/operators-in-sklearn-pipeline/"/>
      <url>2023/02/02/operators-in-sklearn-pipeline/</url>
      
        <content type="html"><![CDATA[<h2 id="What-is-pipeline-in-scikit-learn"><a href="#What-is-pipeline-in-scikit-learn" class="headerlink" title="What is pipeline in scikit-learn"></a>What is pipeline in scikit-learn</h2><p>Pipeline in scikit-learn is a utility class that helps to assemble several steps of an ML workflow into a single scikit-learn estimator. A pipeline consists of a sequence of transformations or pre-processing steps, followed by an estimator that makes predictions based on the transformed data. The pipeline helps to simplify the ML process by automating the steps involved in transforming the data and training the model. The pipeline also ensures that the data is processed consistently throughout the entire workflow and helps to prevent data leakage between the different stages of the pipeline. The pipeline class is a convenient tool for encapsulating the entire ML process, making it easier to manage and share the code, and reduces the risk of errors in the implementation.</p><h2 id="Pipeline-with-or-without-estimator"><a href="#Pipeline-with-or-without-estimator" class="headerlink" title="Pipeline with or without estimator"></a>Pipeline with or without estimator</h2><p>Pipeline doesn’t necessarily need to have a machine learning model ast the estimator in the final step for various reasons.<br>For example, we just want to create a data pipeline for preprocessing data to divide the tasks between preprocessing and modelinng.</p><p>In both cases, the operators we are going talk below work the same way.</p><h2 id="what-is-a-transformer-in-sklearn"><a href="#what-is-a-transformer-in-sklearn" class="headerlink" title="what is a transformer in sklearn"></a>what is a transformer in sklearn</h2><p> A transformer is an estimator that implements the fit(), transform() and/or fit_transform() methods. TransformerMixin is the default implementation and a Mixin class that provides a consistent interface across transformers of sklearn.</p><p> In fit() function, we get the input data and perform the required computations to the specific transform function we will then apply. For example,it can calculate the average and standard deviation of the input data, and get them ready for later use.</p><p> In transform(),  we will transform the input data into some new formats. The output is usually an array or a sparse matrix with equal number of samples (n_samples) as the input data. The parameters obtaind from fit() function will be used in this step.<br> For eample, if we want to transform the input data to be a normalized version, we will subtract every data points with mean and divide by the standard deviation obtained from fit() step. </p><p> fit_transform() is just a more efficient way to call fit() and transform() together. It’s implemented by default.</p><h2 id="Difference-between-fit-transform-fit-transform-predict-and-predict-proba-in-a-pipeline"><a href="#Difference-between-fit-transform-fit-transform-predict-and-predict-proba-in-a-pipeline" class="headerlink" title="Difference between fit() , transform(), fit_transform(), predict(), and predict_proba() in a pipeline"></a>Difference between fit() , transform(), fit_transform(), predict(), and predict_proba() in a pipeline</h2><p>In a pipeline, we have multiple transformers, and each transformer has it’s own fit() and transform() methods,<br>so there are usually confusions about the exact differences among several similar functions with pipeline, and when to use them.<br>Here are first discuss the differences, then show some examples to demonstrate that.</p><h3 id="fit"><a href="#fit" class="headerlink" title="fit()"></a>fit()</h3><p>Fit all the transformers one after the other and transform the data. Finally, fit the transformed data using the final estimator. Notice that, it will not call the the transform() method of the last transformer. This make sense, because in a typical pipeline, the last step is just a model estimator, and transform() is probably not the correct concept. Instead, a predict() operation will make more sense.  </p><p>The return value of fit() is the pipeline object itself with all steps fitted.</p><h3 id="transform"><a href="#transform" class="headerlink" title="transform()"></a>transform()</h3><p>transform() of pipeline will call only transform method of all the transformers one by one including the last one.</p><h3 id="fit-transform"><a href="#fit-transform" class="headerlink" title="fit_transform()"></a>fit_transform()</h3><p>fit_transform() of pipeline will call both fit and transform method of all the transformers on by one including the last one.</p><h3 id="predict"><a href="#predict" class="headerlink" title="predict()"></a>predict()</h3><p>predict() of pipeline only works wthen the last step of the pipeline has predict() method defined, which is usually true if the last step is a model estimator. predict() will call transform of each transformer in the pipeline before the last step. Then, the transformed data are finally passed to the final estimator that calls predict method. </p><h3 id="predict-proba"><a href="#predict-proba" class="headerlink" title="predict_proba()"></a>predict_proba()</h3><p>For example, sometimes we want to get a predicted probability instead of class in a classification, so we call predict_proba()<br>instead of predict(). It will only work if the estimator of the last step has predict_proba() defined. Otherwise, the precedure is the same as predict().</p><h2 id="Demo-1"><a href="#Demo-1" class="headerlink" title="Demo 1"></a>Demo 1</h2><h3 id="first-define-two-custom-transformers-and-put-them-in-a-pipeline"><a href="#first-define-two-custom-transformers-and-put-them-in-a-pipeline" class="headerlink" title="first define two custom transformers, and put them in a pipeline"></a>first define two custom transformers, and put them in a pipeline</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> TransformerMixin, BaseEstimator</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom transformer 1: Log Transformer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, log_base=np.e</span>):</span></span><br><span class="line">        self.flag = <span class="string">&#x27;N&#x27;</span></span><br><span class="line">        self.log_base = log_base</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.flag = <span class="string">&#x27;Y&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;hello from transformer 1 fit&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;hello from transformer 1 transform&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;check fit value from transformer 1:<span class="subst">&#123;self.flag&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> np.log(X) / np.log(self.log_base)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Custom transformer 2: Square Root Transformer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqrtTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.flag = <span class="string">&#x27;N&#x27;</span></span><br><span class="line">        </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        self.flag = <span class="string">&#x27;Y&#x27;</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;hello from transformer 2 fit&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;hello from transformer 2 transform&#x27;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;check fit value from transformer 2:<span class="subst">&#123;self.flag&#125;</span>&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> np.sqrt(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating the pipeline</span></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;log&#x27;</span>, LogTransformer()),</span><br><span class="line">    (<span class="string">&#x27;sqrt&#x27;</span>, SqrtTransformer()),</span><br><span class="line">])</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="with-fit-method-the-transform-method-of-the-last-transformer-is-not-called"><a href="#with-fit-method-the-transform-method-of-the-last-transformer-is-not-called" class="headerlink" title="with fit() method, the transform() method of the last transformer is not called"></a>with fit() method, the transform() method of the last transformer is not called</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">pipe_new = pipe.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(pipe_new)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>hello from transformer 1 fithello from transformer 1 transformcheck fit value from transformer 1:Yhello from transformer 2 fitPipeline(steps=[(&#39;log&#39;, LogTransformer()), (&#39;sqrt&#39;, SqrtTransformer())])</code></pre><h3 id="with-transform-method-all-transfom-methods-are-called-and-parameters-from-previous-fit-are-cached"><a href="#with-transform-method-all-transfom-methods-are-called-and-parameters-from-previous-fit-are-cached" class="headerlink" title="with transform() method, all transfom() methods are called; and parameters from previous fit() are cached"></a>with transform() method, all transfom() methods are called; and parameters from previous fit() are cached</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">X_transformed = pipe.transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_transformed)</span><br></pre></td></tr></table></figure><pre><code>hello from transformer 1 transformcheck fit value from transformer 1:Yhello from transformer 2 transformcheck fit value from transformer 2:Y[0.         0.83255461 1.04814707 1.17741002 1.26863624]</code></pre><p>if no previous fit() applied, with transform() method, all transform() methods are still called, but parameters from previous fit() are not there</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># redefine a new pipe </span></span><br><span class="line"></span><br><span class="line">pipe = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;log&#x27;</span>, LogTransformer()),</span><br><span class="line">    (<span class="string">&#x27;sqrt&#x27;</span>, SqrtTransformer()),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">X_transformed = pipe.transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_transformed)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>hello from transformer 1 transformcheck fit value from transformer 1:Nhello from transformer 2 transformcheck fit value from transformer 2:N[0.         0.83255461 1.04814707 1.17741002 1.26863624]</code></pre><h3 id="with-fit-transform-all-fit-and-transform-methods-are-called-and-return-value-is-transformed-data-not-the-pipeline"><a href="#with-fit-transform-all-fit-and-transform-methods-are-called-and-return-value-is-transformed-data-not-the-pipeline" class="headerlink" title="with fit_transform(), all fit() and transform() methods are called, and return value is transformed data, not the pipeline"></a>with fit_transform(), all fit() and transform() methods are called, and return value is transformed data, not the pipeline</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">X_transformed = pipe.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_transformed)</span><br></pre></td></tr></table></figure><pre><code>hello from transformer 1 fithello from transformer 1 transformcheck fit value from transformer 1:Yhello from transformer 2 fithello from transformer 2 transformcheck fit value from transformer 2:Y[0.         0.83255461 1.04814707 1.17741002 1.26863624]</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pipeline </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark SQL - Collecting Columns into Lists after Groupby</title>
      <link href="2023/02/01/collect-columns-into-lists-after-groupby-in-pyspark/"/>
      <url>2023/02/01/collect-columns-into-lists-after-groupby-in-pyspark/</url>
      
        <content type="html"><![CDATA[<p>In Spark SQL, you may want to collect the values of one or more columns into lists after grouping the data by one or more columns. This can be accomplished using the collect_list aggregate function in Spark SQL.</p><h2 id="Collecting-a-Single-Column-into-a-List"><a href="#Collecting-a-Single-Column-into-a-List" class="headerlink" title="Collecting a Single Column into a List"></a>Collecting a Single Column into a List</h2><p>The following code shows an example of how to collect the values of a single column column3 into a list named list_column3 after grouping the data by columns column1 and column2:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pyspark.sql.functions import collect_list</span><br><span class="line"></span><br><span class="line">df.groupBy(&quot;column1&quot;, &quot;column2&quot;).agg(</span><br><span class="line">  collect_list(&quot;column3&quot;).alias(&quot;list_column3&quot;)</span><br><span class="line">).show()</span><br></pre></td></tr></table></figure><p>The result of this code will be a dataframe with three columns: column1, column2, and list_column3. The values of column column3 will be collected into a list named list_column3 for each unique combination of values in columns column1 and column2.</p><h2 id="Collecting-Multiple-Columns-into-Lists"><a href="#Collecting-Multiple-Columns-into-Lists" class="headerlink" title="Collecting Multiple Columns into Lists"></a>Collecting Multiple Columns into Lists</h2><p>You can collect the values of multiple columns into multiple lists after grouping the data by one or more columns, as shown in the following code:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pyspark.sql.functions import collect_list</span><br><span class="line"></span><br><span class="line">df.groupBy(&quot;column1&quot;, &quot;column2&quot;).agg(</span><br><span class="line">  collect_list(&quot;columnA&quot;).alias(&quot;listA&quot;),</span><br><span class="line">  collect_list(&quot;columnB&quot;).alias(&quot;listB&quot;)</span><br><span class="line">).show()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The result of this code will be a dataframe with four columns: column1, column2, listA, and listB. The values of columns columnA and columnB will be collected into lists named listA and listB respectively, for each unique combination of values in columns column1 and column2.</p><h2 id="Collecting-Columns-into-Lists-using-Spark-SQL"><a href="#Collecting-Columns-into-Lists-using-Spark-SQL" class="headerlink" title="Collecting Columns into Lists using Spark SQL"></a>Collecting Columns into Lists using Spark SQL</h2><p>You can also collect the values of one or more columns into lists after grouping the data by one or more columns using Spark’s spark.sql method. The following code shows an example of how to do this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df.createOrReplaceTempView(&quot;df_view&quot;)</span><br><span class="line"></span><br><span class="line">spark.sql(&quot;SELECT column1, column2, collect_list(column3) AS list_column3 FROM df_view GROUP BY column1, column2&quot;).show()</span><br></pre></td></tr></table></figure><p>This code first creates a temporary view df_view from the df dataframe, and then executes a Spark SQL query that groups the data by columns column1 and column2, and collects the values of column column3 into a list named list_column3. The result of the query will be displayed using the show method.</p><h2 id="Order-of-Elements-in-the-Lists"><a href="#Order-of-Elements-in-the-Lists" class="headerlink" title="Order of Elements in the Lists"></a>Order of Elements in the Lists</h2><p>It’s important to note that the order of elements in the lists collected by the collect_list function will be the same, as they are collected based on the same grouping conditions. For example, if you collect two columns columnA and columnB into two lists listA and listB respectively, the order of elements in listA and listB will correspond to each other for each group defined by the groupBy clause.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to define mulitple endpoints in one API using AWS API gateway</title>
      <link href="2023/01/25/define-multi-endpoints-in-one-api-aws/"/>
      <url>2023/01/25/define-multi-endpoints-in-one-api-aws/</url>
      
        <content type="html"><![CDATA[<p>By default, when we create API using the AWS API Gateway service, the default resource generated is the root path “/“.<br>And then two steps are needed:</p><ol><li>first, create a method under the “/“ resource, for example a POST method</li><li>deploy the API to a stage with any stage name you like,e.g. prod</li></ol><p>In this case, an API endpoint will be created in this format: url/stage_name.<br>For example, this is one valid endpoint: “mxxggeoyvj.execute-api.us-east-1.amazonaws.com/prod”</p><p>The above steps are shown in more detail here:<br><a href="/2022/09/02/2022-09-02-1/">Create API with API Gateway</a></p><p>Now, how to create multiple endpoints in the same API?</p><h2 id="step-1-create-a-new-resource-under-the-“-“-resource"><a href="#step-1-create-a-new-resource-under-the-“-“-resource" class="headerlink" title="step 1, create a new resource under the “/“ resource"></a>step 1, create a new resource under the “/“ resource</h2><p>Clicking the “/“ path, then click “Action” and choose “create resource” as the following screenshot:<br><img src="/content/images/2023-01-25-1.png"></p><p>This will lead to the details of creating the child resource as our first endpoint as the following screenshot:<br><img src="/content/images/2023-01-25-2.png"></p><p>so this will be our first endpoint. But we need to continue to create an actual method, such as a POST method for the first endpoint like this:<br><img src="/content/images/2023-01-25-3.png"><br>Creating method is similar as before, just hook up the mehod with a lamda function, for example.</p><h2 id="step-2-create-a-second-resource-under-the-“-“-resource"><a href="#step-2-create-a-second-resource-under-the-“-“-resource" class="headerlink" title="step 2, create a second resource under the “/“ resource"></a>step 2, create a second resource under the “/“ resource</h2><p>Clicking the “/“ path again, then click “Action” and choose “create resource” again, to create the second endpoint.<br>Notice that, we need to click back the “/“ path in order to create parallel endpoints under “/“ path,<br>such as “/endpoint1” and “/endpoint2”.</p><p>This also means, if needed, we could click the first endpoint path, and then create another resource, it will create an hierarchical path<br>like this “/endpoint1/endpoint2”.</p><p>After creating the second point, again, we need to create a POST method for the second endopint.</p><p>After all the above steps, the results of two parallel resources ad two endpoints are as the following:<br><img src="/content/images/2023-01-25-4.png"></p><h2 id="step-3-deploy-the-API-to-stage"><a href="#step-3-deploy-the-API-to-stage" class="headerlink" title="step 3, deploy the API to stage"></a>step 3, deploy the API to stage</h2><p>After we created two endopints as child resources, and each resources has a POST method, we are ready to deploy the<br>API to a stage, for example a stage named “prod”:</p><p><img src="/content/images/2023-01-25-5.png"><br><img src="/content/images/2023-01-25-6.png"></p><p>After the prod stage is deployed, we can check indvidual endpoint and its invoking url like this:<br><img src="/content/images/2023-01-25-7.png"></p><p>Great job done!</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> api gateway </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>In PySpark how to filter dataframe column using unique values from another dataframe</title>
      <link href="2023/01/22/spark-dataframe-filter-by-column-from-another-dataframe/"/>
      <url>2023/01/22/spark-dataframe-filter-by-column-from-another-dataframe/</url>
      
        <content type="html"><![CDATA[<p>Here is one common task in PySpark: how to filter one dataframe column are from unique values from anther dataframe?</p><h2 id="Method-1"><a href="#Method-1" class="headerlink" title="Method 1"></a>Method 1</h2><p>Say we have two dataframes df1 and df2, and we want to filter df1 by column called “id”, where its values need to be from<br>column “id” in df2. If the unique values of column “id” from df2 is not too big, we can do the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pyspark.sql.functions import col</span><br><span class="line"></span><br><span class="line"># Create the first DataFrame</span><br><span class="line">df1 = spark.createDataFrame([(1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;), (4, &quot;d&quot;)], [&quot;id&quot;, &quot;value&quot;])</span><br><span class="line"></span><br><span class="line"># Create the second DataFrame</span><br><span class="line">df2 = spark.createDataFrame([(1, &quot;x&quot;), (2, &quot;y&quot;)], [&quot;id&quot;, &quot;other_value&quot;])</span><br><span class="line"></span><br><span class="line"># Get the unique values of the second DataFrame&#x27;s column</span><br><span class="line">unique_values = df2.select(&quot;id&quot;).distinct().rdd.flatMap(lambda x: x).collect()</span><br><span class="line"></span><br><span class="line"># Filter the first DataFrame&#x27;s column based on the unique values</span><br><span class="line">filtered_df1 = df1.filter(col(&quot;id&quot;).isin(unique_values))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In the above example, filtered_df1 will only contain the rows from df1 where the id column is in the list of unique values from df2’s id column.</p><h2 id="Method-2"><a href="#Method-2" class="headerlink" title="Method 2"></a>Method 2</h2><p>However the above example using collect might be not optimal for large datasize.<br>in the example provided, the unique values of the second DataFrame’s column are collected using the .collect() method. This is necessary because the .isin() function that is used to filter the first DataFrame’s column takes in an iterable (e.g. a list, set, or tuple) of values to check against.<br>However, it’s worth mentioning that this may cause performance issues if the second DataFrame is very large and the unique values are a lot of, it’s better to use the join or subquery method to filter the first DataFrame based on the second Dataframe instead of collecting the unique values.</p><p>For example, if you want to filter the first DataFrame based on the second DataFrame’s id column:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">filtered_df1 = df1.join(df2, df1.id == df2.id, &#x27;inner&#x27;).select(df1.columns)</span><br><span class="line">This will give you the same result of the previous example but without collecting the unique values.</span><br></pre></td></tr></table></figure><h2 id="Possible-error-if-two-dataframe-share-many-columns-names-that-are-the-same"><a href="#Possible-error-if-two-dataframe-share-many-columns-names-that-are-the-same" class="headerlink" title="Possible error if two dataframe share many columns names that are the same"></a>Possible error if two dataframe share many columns names that are the same</h2><p>how about df1 and df2 have column names that are the same? Then you will get error using code above.<br>But don’t worry, it’s easy to fix. We just need to rename the columns of one othe columns. </p><p>If both DataFrames have columns with the same name, you will need to use the alias() function to assign a new name to one of the columns before performing the join.<br>For example, if both DataFrames have a column named “id”:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pyspark.sql.functions import col</span><br><span class="line"></span><br><span class="line"># Create the first DataFrame</span><br><span class="line">df1 = spark.createDataFrame([(1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;), (4, &quot;d&quot;)], [&quot;id&quot;, &quot;value&quot;])</span><br><span class="line"></span><br><span class="line"># Create the second DataFrame</span><br><span class="line">df2 = spark.createDataFrame([(1, &quot;x&quot;), (2, &quot;y&quot;)], [&quot;id&quot;, &quot;other_value&quot;])</span><br><span class="line"></span><br><span class="line"># Assign a new name to the second DataFrame&#x27;s &#x27;id&#x27; column</span><br><span class="line">df2 = df2.selectExpr(&quot;id as df2_id&quot;, &quot;other_value&quot;)</span><br><span class="line"></span><br><span class="line"># Perform the join</span><br><span class="line">filtered_df1 = df1.join(df2, df1.id == df2.df2_id, &#x27;inner&#x27;).select(df1.columns)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="Replace-dataframe-columns-at-once"><a href="#Replace-dataframe-columns-at-once" class="headerlink" title="Replace dataframe columns at once"></a>Replace dataframe columns at once</h2><p>Another probably quicker method is to replace all the columns at once.<br>So in pyspark how to rename all of the dataframe columns by adding a prefix?</p><p>In PySpark, you can use the selectExpr() function along with a list of string expressions to rename all of the DataFrame’s columns by adding a prefix.</p><p>Here is an example of how you can add a prefix “prefix_” to all of the columns in a DataFrame:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from pyspark.sql.functions import col</span><br><span class="line"></span><br><span class="line"># Create a DataFrame</span><br><span class="line">df = spark.createDataFrame([(1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;), (4, &quot;d&quot;)], [&quot;id&quot;, &quot;value&quot;])</span><br><span class="line"></span><br><span class="line"># Get the current column names</span><br><span class="line">old_columns = df.columns</span><br><span class="line"></span><br><span class="line"># Create a list of string expressions to rename the columns</span><br><span class="line">new_columns = [&quot;prefix_&quot; + col for col in old_columns]</span><br><span class="line"></span><br><span class="line"># Use the selectExpr() function to rename the columns</span><br><span class="line">df = df.selectExpr(*[f&quot;&#123;old&#125; as &#123;new&#125;&quot; for old, new in zip(old_columns, new_columns)])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, the selectExpr() function is used to rename all of the columns by adding the prefix “prefix_” to the original column name. It’s done by creating a list of string expressions that include the original column name and the new column name with the prefix “prefix_” using the list comprehension.</p><p>Also, you can use the withColumnRenamed() method to rename all columns one by one.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">for col in old_columns:</span><br><span class="line">    df = df.withColumnRenamed(col, f&quot;prefix_&#123;col&#125;&quot;)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>This method will rename all the columns one by one by passing the original column name and the new column name with the prefix “prefix_”</p><h2 id="how-to-deep-copy-datafame-in-pyspark"><a href="#how-to-deep-copy-datafame-in-pyspark" class="headerlink" title="how to deep copy datafame in pyspark"></a>how to deep copy datafame in pyspark</h2><p>So if you don’t want to change the orignal columns of the dataframe, just operate on a copy.<br>In PySpark, you can create a deep copy of a DataFrame by using the .copy() method or by creating a new DataFrame from the original DataFrame’s data.</p><p>Here is an example of how you can create a deep copy of a DataFrame using the .copy() method:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Create the original DataFrame</span><br><span class="line">df = spark.createDataFrame([(1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;), (4, &quot;d&quot;)], [&quot;id&quot;, &quot;value&quot;])</span><br><span class="line"></span><br><span class="line"># Create a deep copy of the original DataFrame</span><br><span class="line">df_copy = df.copy()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>In this example, the df_copy variable will contain a deep copy of the original DataFrame, which is independent of the original DataFrame and any modifications made to it will not affect the original DataFrame.</p><p>Please note that .copy() method is not available in all versions of PySpark, so you can use the second method to create a deep copy of DataFrame.<br>So another way to create a deep copy of a DataFrame is by creating a new DataFrame from the original DataFrame’s data.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Create a new DataFrame from the original DataFrame&#x27;s data</span><br><span class="line">df_copy = spark.createDataFrame(df.rdd, df.schema)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
            <tag> pyspark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to generate access token in Databricks</title>
      <link href="2023/01/20/generate-access-token-in-databricks/"/>
      <url>2023/01/20/generate-access-token-in-databricks/</url>
      
        <content type="html"><![CDATA[<p>In Databricks, personal access tokens can be used for secure authentication to the Databricks API instead of passwords.</p><p>The process of generating a new access token is very easy.</p><p>Step1. Click your user profile (email address) on the top right. Then click User Settings.<br><img src="/content/images/2023-01-20-1.png"></p><p>Step2.  Choose the “Access tokens” menu, and click “Generate new token”.<br>Give it a new name, and choose the expiration date. Then remember copy the token and store somewhere.<br><img src="/content/images/2023-01-20-2.png"></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FAISS index and normalization</title>
      <link href="2023/01/18/fais-index-and-normalization/"/>
      <url>2023/01/18/fais-index-and-normalization/</url>
      
        <content type="html"><![CDATA[<p>Previously, we have discussed how to implement a real time semantic search using sentence transformer and FAISS.<br><a href="https://www.datasciencebyexample.com/2023/01/14/embedding-plus-faiss-semantic-search/">real time semantic search</a></p><p>Here, we talk more about indexing in FAISS.<br>The most popular indexes we should look at are the simplest — flat indexes.</p><p>Flat indexes are ‘flat’ because we do not modify the vectors that we feed into them.</p><p>Because there is no approximation or clustering of our vectors — these indexes produce the most accurate results. While we have perfect search quality, this comes at the cost of significant search times.</p><h2 id="Two-flat-indexes"><a href="#Two-flat-indexes" class="headerlink" title="Two flat indexes"></a>Two flat indexes</h2><p>Two common flat index:</p><ul><li>IndexFlatL2, which uses Euclidean/L2 distance</li><li>IndexFlatIP, which uses inner product distance (similar as cosine distance but without normalization)</li></ul><p>The search speed between these two flat indexes are very similar, and IndexFlatIP is slightly faster for larger datasets.<br>See the following query time vs dataset size comparison:<br><img src="/content/images/2023-01-18-1.png"></p><h2 id="how-to-normalize-similarity-metrics"><a href="#how-to-normalize-similarity-metrics" class="headerlink" title="how to normalize similarity metrics"></a>how to normalize similarity metrics</h2><p>If the vectors we indexed are not normalized, the similarity metrics came out from FAISS are not normalized either.<br>For example, sometimes we want to have a cosine similarity metrics, where we can have a more meaningful threshold to compare.</p><p>It’s very easy to do it with FAISS, just need to make sure vectors are normalized before indexing, and before sending the query vector.</p><p>Example code, during indexing time:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">index = faiss.IndexIDMap(faiss.IndexFlatIP(768))</span><br><span class="line">faiss.normalize_L2(encoded_data)</span><br><span class="line">index.add_with_ids(encoded_data, np.array(range(0, len(encoded_data))))</span><br></pre></td></tr></table></figure><p>during query time:</p><pre><code>query_vector = model.encode([query])k = 3faiss.normalize_L2(query_vector)top_k = index.search(query_vector, k)</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> semantic search </tag>
            
            <tag> FAISS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>The fanaticism and reality of Web3</title>
      <link href="2023/01/15/the-fanaticism-and-reality-of-Web3/"/>
      <url>2023/01/15/the-fanaticism-and-reality-of-Web3/</url>
      
        <content type="html"><![CDATA[<p>In the past two years, the concept of “Web3” has become increasingly popular in the IT industry. In the past 2022, web3 can be said to be on the cusp of public opinion, and it is really popular.</p><p>But today, many people still feel confused when facing this term, and they don’t know the similarities and differences between it and existing Internet technologies. On the other hand, web3.0 evangelists believe that this technology will become the infrastructure of the next generation Internet, and many companies have also begun to invest in early research and development in related fields. In addition, the relationship between the concept of the metaverse of the fire and web3.0 has also made people curious.</p><p>Recently, InfoQ interviewed Kaiyun Labs CTO Yang Weili (Wiliam Yang). Kaiyun Lab is a start-up company, determined to build a trust-free database for web3. In the interview, Yang Weili shared his many views on the technology of web3, and described a blueprint for a new generation of Internet industry based on web3.</p><h2 id="Web3-boom"><a href="#Web3-boom" class="headerlink" title="Web3 boom"></a>Web3 boom</h2><p><mark> InfoQ: Please explain what web3 is in plain language. At present, is there a generally accepted definition of this technology? </mark><br>Weili Yang: From a technical point of view, we generally think that Web3 is a new generation of Internet ecosystem with decentralized technology as its core. Web3 uses blockchain technology as the core to build a new generation of decentralized Internet components, and then build the services and applications we want to provide based on them. The purpose of building these network components with blockchain technology is to allow users to truly own the Internet, to make the identity and data of Internet users our own, and to make us the masters of the data. We can display personal data through this network and use them to trade and realize.</p><p>From another perspective, in the Web1 era, we can only read data, and read information and information by browsing the web. The biggest change in the Web2 era is that each of our Internet users has become a creator of content, and we are constantly exporting content to the Internet.</p><p>In this process, at first we used a browser to operate on the computer, such as opening a blog page to write an article. With the maturity of the mobile Internet, we are now accustomed to operating on mobile phones, a large number of mobile applications have emerged, and Internet companies have begun to record personal content and data.</p><p>The difference that Web3 brings is that we can make our own decisions about the Internet. For example, Weibo is a product promoted to us by Sina, and all data information is centralized under the management of Sina. And Web3 is a decentralized network, without such a single point of control or management, what these resources in the network should do will be decided by a mechanism similar to public voting. We also have full control over how the data posted by individuals is managed and who has access to our data. At the same time, Web3 resources can always be accessed, and there will be no problem that a certain manufacturer’s server cannot be accessed due to a failure.</p><p>Web3 is an Internet ecosystem centered on decentralized technology. We can compare the ecology of the earth we live in now, and these elements will also be necessary on the Internet. We may only be able to do some limited things on the Internet now, so in the new generation of Internet ecosystem, it can not only meet the traditional needs of our Internet access, but also meet the needs that are getting closer to our real life. some needs. For example, the concept of Metaverse means that the Internet is more like a virtual world, bringing us better and better experiences.</p><p><mark> InfoQ: From the perspective of historical development, do you think the Internet has evolved to the stage of Web3? </mark><br>Weili Yang: We are still in the early stages of the third iteration. First of all, there is a consensus that blockchain technology is the core of the third iteration. This technology has been proposed for many years, and many excellent products and projects have been launched. But from the perspective of end Internet users, these products, as Web3 infrastructure, are not enough to match the Internet products we use every day. From user experience to performance, they are still different from Web2 products in all aspects. In order for the third iteration to enter a relatively mature stage, it must first rely on the further improvement of the underlying infrastructure. For example, for the very important distributed storage technology, although we have seen many promising products at present, compared with the existing storage services, these current Web3 products still need better improvement.</p><p>In addition, for the entire Web3, these future products need to redefine the economic model and user behavior of the Internet. Web3 believes that the personal data of users is valuable. For example, a Weibo article or a video posted by a user is valuable in itself. How is this value reflected in Web3? We can think that readers need to pay, which may sound like a subversion of everyone’s usage habits, but this may be the user behavior pattern of the Internet in the future. It will take a long time for the developers of enterprise products and Internet users to continue to practice and adapt to the new economic model.</p><p><mark> InfoQ: The concept of Web3 has probably become popular in the past year, but it is not a particularly new concept. Then it has suddenly become popular in the past year. What do you think is the main reason?</mark><br>Yang Weili: The concept of Web3 has been particularly popular in the past year, and I can attribute it to two aspects. On the one hand, it is because of the concept of metaverse, and on the other hand, it is NFT. After these two concepts became popular, everyone began to pay attention to and explore, and touched the concept of Web3 behind it.</p><p>What exactly are NFTs? Why is everyone so interested? That’s because NFT allows us to see the so-called digital assets, that is, personal data on the Internet, such as articles and pictures, which have channels for realizing value in Web3. It gives us a possibility. We may be used to using various applications on the Internet to communicate, post articles, pictures, videos… but we may never have thought that NFT can turn the content data we retain on the Internet into value. NFT marks the uniqueness of user assets. For example, Xiao Ming’s book is the only one, which belongs to Xiao Ming. Then I define this book in the form of NFT and record it on the blockchain. Maybe it has some special features. value, so that we can realize it through NFT channels.</p><p>In addition, the epidemic has changed our way of life a lot in the past few years, which has brought the concept of metaverse into flames. The metaverse gives us the possibility of virtualizing office, entertainment and life. When everyone learns about NFT and metaverse, they finally find that the very big concept behind it is Web3, which is why Web3 has become more and more popular in this year. The reason for getting hotter.</p><h2 id="Web3-0-infrastructure-construction"><a href="#Web3-0-infrastructure-construction" class="headerlink" title="Web3.0 infrastructure construction"></a>Web3.0 infrastructure construction</h2><p><mark> InfoQ: What are the differences and advantages of Web3 compared to Web2?</mark><br>Weili Yang: The biggest advantage of Web3 is that Internet users have data ownership. A lot of our personal data is stored on the servers of Internet companies, and these companies completely own and control our user data. Although we use these applications for free, the business always needs to make a profit. Where does the money come from? A large part of it comes from marketing and advertising based on user data, making money in this way. In this mode of single data storage and enterprise side, there are some privacy issues of personal data. We cannot fully control the use of personal data by these enterprises, and we do not know whether they have modified the data, etc.</p><p>A very important point is that in the Web2 era, we cannot guarantee that our personal data can be accessed at any time, because enterprises may cut off access to some resources for some reason. For example, if we upload a picture, the server may be turned off and it will no longer be accessible. In the era of Web3, data is not owned by a certain company, it is securely encrypted and stored on the network. When a party needs to access our data, the user needs to authorize, so that the user can safely manage personal data. There will be a concept of key management in the middle. The user has his own key, he can decide whether the data can be accessed by other services, and this ownership is given to the user, which is the ownership of the data.</p><p>Another aspect is the change of user identity mechanism. With the development of Web3 technology, decentralized identity becomes possible. Decentralized identity is the ability for users to fully manage their identities and data in a decentralized manner. Because blockchain technology provides the functions of non-deletable and ledger, it is distributed, so users can prove themselves through cryptographically verifiable digital identities, which is a very safe way of proof without revealing our personal information.</p><p>Another very important difference is the form of organizational governance. The organizational form of Web3 will change fundamentally. The current mainstream organizational form is a company, which needs to be registered and has employees and different roles. The management is responsible for the company’s future development according to the company’s system and their decisions. In Web3, there is a decentralized autonomous organization. In this form of organization, there will be no CEO, but the holders of tokens. The organization will pre-set its structure and rules, define them in the form of smart contracts, publish them on the network, and all rules will be strictly enforced. In this way, some privileges of a small group of people will not cause company decisions to be made or implemented in a democratic manner, and promote the democratization of Web3 organizational management.</p><p>Finally, just because there is no single entity in Web3, all architectures are decentralized. Therefore, on such an ecosystem, data and services can always be accessed and cannot be blocked, and transaction data cannot be blocked. All are stored on the blockchain and cannot be tampered with. The above is the biggest difference between Web3 and Web2.</p><p><mark> InfoQ: Will Web3 coexist with Web2 next, or will it completely replace the latter after a certain stage?</mark><br>Weili Yang: I think it depends on how we define Web3 and Web2. From a technical point of view, both Web3 and Web2 make full use of the achievements of human beings in the fields of computer science and non-computer science. They just present different user experiences for Internet users, define different roles of users, and serve users. In Web2, we can think that users are only using some functions provided by the Internet, and do not really fully own the Internet. Personal data is not completely for their own use, and cannot be traded for cash, and cannot reflect its more value.</p><p>When the Internet develops to Web3, users have different roles, and we become the masters of these data, we can perform some operations on it, and even sell it. From a technical point of view, Web3 is not the opposite of Web2, so it does not mean that with Web3, Web2 will be subverted, and there will be no more. Up to now, Web3 has not had any disruptive new technologies. Instead, it still uses some relatively mature technologies and concepts in the Internet, and has carried out certain innovations and reorganizations to bring a new concept to the Internet. The Web3 community also draws on a lot of Web2 technical achievements.</p><p>In the near future, we should be able to see that entrepreneurs in the Web3 field launch a large number of innovative products. For example, the decentralized blog in Web3 can always be accessed. Even if one or two host servers in the network are shut down, or even leave the network permanently, our access to these articles will not be affected. From the perspective of users, we will see that some Internet application products have been successfully copied from 2.0 to 3.0. As for whether the game between them will eventually lead to the demise of 2.0 products, it will take time to tell us. There is a very high probability that they will coexist, it all depends on the user. If our users can adapt to different economic models or user behaviors, this model may coexist.</p><p><mark> InfoQ: Facing the current upsurge, how should Web2 companies deploy Web3?</mark><br>Weili Yang: At present, a large number of Web2 companies have begun to pay attention to this emerging field. To start the layout in this field, the first thing is to do research based on the company’s own business model. At least you should understand what Web3 is and what different experiences it can bring to the company’s users. The company should once again think about the issue of user value from the perspective of users, and help users realize value transfer. At the same time, after all, Web2 companies provide users with services and products to make profits, so we have to consider how to make the same money under the new technical architecture and business model. In the Web3 era, companies can no longer control all user data, so can advertising still be done? Does this kind of money-making channel still exist? If you can no longer make money in this way, how can you make money? This is a very important point that Web2 companies have to consider.</p><p>We believe that companies should take the initiative to investigate the feasibility of blockchain technology in the company’s business and consider how to reconstruct our business model. A change that is required in the future is to provide a transaction and realization platform for user data through the blockchain, and earn service fees for it. Still take the blog system as an example. In the era of Web3, the articles posted by bloggers may already have value, and users will naturally have to pay to read his articles, so that bloggers can earn profits. So how will this profit be given to this enterprise? The publisher of the blog system can charge a certain handling fee from this transaction to obtain profits. Of course, whether this income situation can support the company’s operations and ensure that the company makes money is something that Web2 companies need to investigate.</p><p>On the other hand, many Web2 companies are originally engaged in Internet infrastructure, and it is necessary for such companies to deploy Web3. Because Web3 has a very big goal in decentralized storage and distributed storage, Web2 companies can use existing mature technology and experience to quickly enter the Web3 field, and in this field, earn the first pot of gold as a pioneer , to establish their industry status in this field.</p><p><mark> InfoQ: Decentralization is considered to be the core feature of Web3. Can you explain to us in detail how web3.0 makes the Internet decentralized?</mark><br>Weili Yang: The technology stack of Web3 can be divided into five layers from bottom to top. The first is the infrastructure and network layer, which provides the communication mechanism, interface, protocol, etc. of the entire Internet. The most famous one is P2P, a peer-to-peer network protocol. Blockchain technology is also established based on P2P, which provides a decentralized foundation for Web3 from the bottom layer. There is a layer above it called the middle layer, or the off-chain layer, and then the protocol layer, which can also be called the on-chain layer. The purpose of the middle layer is to solve the scalability and performance problems of the protocol layer, and transfer part of the computing work from the blockchain to the off-chain solution. Above these three layers are the API layer and the application layer, which provide developers and ordinary Internet users with blockchain access interfaces. These layers provide us with a decentralized mechanism layer by layer, thus realizing the decentralization of the entire Internet.</p><p><mark> InfoQ: Blockchain is the key technology behind Web3.0, why will blockchain become the core of this Internet iteration (revolution)?</mark><br>Weili Yang: When we defined Web3, we expected the Internet to undergo another revolution, expecting users to truly own their data and determine the shape of the Internet. We found that decentralization can achieve this goal for us, and decentralization is just the product of blockchain technology, so blockchain has become the very core infrastructure of Web3.</p><p>Blockchain is an implementation of a distributed database. Once the data is connected to the chain, it cannot be modified, so it also solves the problem of data exchange between different websites. There is only one copy of user data, which is stored on the Web3 network and on the blockchain; different products and applications that need to access the user’s personal data require user authorization. But there is only one piece of data, as long as it can be authorized, it can use its data under the authorization of the user, which solves the problem of data exchange between websites or applications to a certain extent.</p><p><mark> InfoQ: Is blockchain the only technical support for realizing Web3? What role do these technologies such as artificial intelligence, machine learning, and cloud play in the Web3 world? Now, has the infrastructure construction of Web3 entered a mature stage?</mark><br>Yang Weili: The blockchain only provides a means of decentralized distributed accounting, and the core components necessary for Web3 include storage, computing, and networking. Then the blockchain can solve some problems for us, such as storage, and it may also Solve some computational problems. At present, with the development of IPFS technology, decentralized storage is gradually taking shape, but it has not yet reached the level in the vision and needs further development.</p><p>Next is calculation. Smart contracts have brought us computing power. Developers of Web3 applications can define business logic based on smart contracts. Smart contracts help with specific calculations and implementations to support Web3 applications. Artificial intelligence, machine learning, and cloud technology are relatively higher-level applications, and they will also be applied to Web3. For example, artificial intelligence can distribute data more efficiently and intelligently, and optimize network performance. Behind cloud computing is a distributed technical architecture, which is not particularly different from Web3 in terms of technology. These mature cloud vendors now have a lot of experience and technology accumulation, which will greatly promote the development of Web3. At the same time, these cloud service providers are very likely to play the role of pioneers in the Web3 field, and they may be the first to usher in industrial changes.</p><p>In terms of Web3 infrastructure construction, from the perspective of the five-tier architecture, each field has star products. Web3 is still in the early stage of development, so artificial intelligence and cloud technology can be applied to the field of Web3. There are no disruptive new technologies in the Web3 field. It is more about the integration of current mature technologies and achievements, so as to solve some pain points in the Internet.</p><p><mark> InfoQ: What is the current status of Web3 application development?</mark><br>Yang Weili: The current development work is mainly divided into two aspects. On the one hand, some innovations based on decentralized technology are still in the prototype stage to a certain extent. In this aspect, relatively few people will use or try them.</p><p>Another major direction is to move the mature products of Web2 to Web3. For example, products such as Youtube and Facebook have competing products on Web3. Because the community ecology of Web3 is very active, developers are also happy to deploy these mature products and concepts on Web3. This in itself is very important to the infrastructure development of Web3, because the needs of users and the functional requirements of products can reflect the insufficiency of the infrastructure, thereby promoting the further development of the infrastructure, which is a very good promotion for Web3 itself.</p><p><mark> InfoQ: What is the employment situation in the Web3 space?</mark><br>Weili Yang: Over the past year, the Web3 entrepreneurial boom has continued to heat up. There are indeed a large number of Web2 engineers transitioning to Web3. This trend must exist. As for more and less, this is indeed a relative concept, which is not easy to measure.</p><p>There are also many domestic teams dedicated to the technical research and development of Web3-related fields, but there is still no established ecosystem of Web3 in China. On the other hand, how to define the practitioners of Web3 is also a problem. We cannot say that someone is researching the blockchain, he is a practitioner of Web3, and this definition cannot be clarified.</p><p><mark> InfoQ: If there is an engineer who wants to become a Web3 developer, what skills should he learn?</mark><br>Yang Weili: In the field of Web3, especially the block chain, the entire community prefers programming languages, one is Go and the other is Rust, and these two are also the preferred languages ​​for development. The programming skills of Web3 developers are important in this area. JavaScript also has a decent percentage of usage. Of course, as a developer, basic computer science knowledge is necessary, and you must be familiar with data, structures, algorithms, and networks. You also need to understand the fields of distributed systems, blockchain technology, and smart contracts.</p><p><mark> InfoQ: For today’s technology companies, since Web3 is the future form of the Internet, should everyone be aware of it from now on, and more or less start to prepare for the future?</mark><br>Yang Weili: Yes. Technology companies should examine Web3 technology from the perspective of user value to see what kind of user experience it can bring to users, including data integration, personal privacy data protection, and high security. Blockchain technologies are very important. Important, but not the highest priority. After all, technology companies still consider corporate profits and business models, so they have to think about what role blockchain technology can play in the business model, and what changes other components in Web3 can bring to the business model of technology companies. What form will the company’s products be provided to users.</p><p>The other is the subject of marketing. At present, marketing is still a very important channel to acquire customers, but Web3 will be different. The focus in the Web3 world is to build consumer and customer communities, and to drive the arrival of new customers by motivating the first batch of users. In the age of Web3, all enterprises need to consider this question.</p><h2 id="next-generation-Internet"><a href="#next-generation-Internet" class="headerlink" title="next generation Internet?"></a>next generation Internet?</h2><p><mark> InfoQ: As the next generation of Internet, Web3 has now entered the initial stage. So will its actual realization be anything iconic?</mark><br>Yang Weili: It should be difficult to have a clear sign, event, and time point. The consensus we can reach on Web3 is that Internet users can truly own personal data, which is a very different Internet ecology. Then as we feel more and more changes in our own permissions in our daily Internet use, we may gradually feel the arrival of Web3.</p><p>Another very important point is the value of users. In the era of Web2, we may not realize this concept. We don’t pay to use the Internet now, and most apps are free to use. It is very likely that in the Web3 ecology, we have to pay for using the Internet, which is very different from the current usage habits, but this is likely to be a reality in the Web3 ecology.</p><p>So how do users get income to pay for this fee? In Web3, it is very likely that everyone is valuable on the Internet. We can create value in this virtual world. Everyone’s value is not only reflected in offline work, but also online. If we have writing skills, we can monetize our works. If we are good at music, we can monetize music works. Everyone not only consumes content on the Internet, but also has the ability to monetize our own value. When it comes to payment channels, there are also digital wallets in Web3. Digital wallets can facilitate the management of our digital assets and personal identities. Things that seem troublesome now will have very convenient solutions.</p><p><mark> InfoQ: What are the differences and connections between the concepts of Web3 and Metaverse?</mark><br>Weili Yang: There is no necessary connection between Web3 and the Metaverse. The reason why people often compare them is that Metaverse and Web3 have many intersections at the core technology level. But Metaverse puts more emphasis on the concept of virtual reality, mainly to allow users to interact virtualized. Web3 is not limited to this direction, it is a decentralized network based on blockchain technology. Metaverse will use some very important technologies and infrastructure in Web3, so we can regard Metaverse as a form of Web3 to a certain extent.</p><p>On the other hand, many giants are betting on the metaverse field, so how decentralized are the metaverse products or concepts they launched? This is a very worthwhile question. If they are still making very centralized products, they will be very different from Web3. From this perspective, it is also difficult for us to define the future of the Internet as a metaverse. What route these companies hope to take in the field of Web3 and Metaverse must also follow their own research and decision-making. It is difficult to say that they will definitely develop in this direction in the future.</p><p><mark> InfoQ: Why did Kaiyun Lab decide to make a layout in this direction? What actions has your company made around this strategy?</mark><br>Weili Yang: Kaiyun Lab has been doing research on storage and computing for a long time. We are also optimistic about and recognize the field of decentralized distributed storage and computing, and believe that they are the two cornerstones of Web3. Because we are also optimistic about the future of Web3, we have conducted long-term research in this field and finally determined the main battlefield of decentralized storage.</p><p>In this field, we also found that there is still a long way to go for decentralized storage, so we have invested a lot of research and development resources to improve the scalability of storage and retrieval in the form of open source solutions, combined with decentralized databases and Decentralized computing provides computing power for the entire storage, and expects to provide data processing capabilities for the decentralized network, making data truly valuable.</p><p>What we are currently focusing on building is a Web3 database with ledger functions, aiming to provide storage and computing solutions for Web3. Judging from the current development, because the entire Web3 ecology is still a bottom-up layered architecture, we should first consolidate the bottom layer, and then gradually develop to the application layer. Only with a complete infrastructure can we provide a complete user experience interface.</p><h2 id="Controversy-and-the-future"><a href="#Controversy-and-the-future" class="headerlink" title="Controversy and the future"></a>Controversy and the future</h2><p><mark> InfoQ: There are also some opposing voices about the concept of Web3, and many articles say it is a hoax. What do you think of some doubting voices that exist today?</mark><br>Yang Weili: First of all, this concept is very popular. At the same time, because Web3 is gradually improving and maturing based on some technologies we now recognize, there will be a small number of people who take advantage of the loopholes in technology and supervision to carry out the so-called behavior of cutting leeks. Of course, I think the emergence of these scams and problems is also an education for the public. The Web3 ecology is definitely not a scam game, and the emergence of these scams or problems reflects some of the current problems in Web3. Feedback from the market can help Web3 continue to mature and develop, and this so-called behavior of cutting leeks will become less and less.</p><p>Specifically, from the perspective of the economic model of Web3, the rules of the game in this ecosystem are not perfect, and these loopholes are just used by these illegal people to carry out some leek-cutting behaviors. Just like in the process of making software in the early days, there will be many fatal flaws and loopholes in the IT industry. It is through the feedback from the market and users that we discover these loopholes, thereby promoting the continuous development of the technical field of the software industry, and then making up for these loopholes to provide better products. I believe that Web3 will also go through the same process.</p><p>In the end, there are still many areas to be explored in the development of Web3. The reason why the mainstream of the Internet is centralized technology is also because the technology itself is not enough to make these platforms decentralized. Since it is currently centralized, it still has something we can explore. Practitioners of Web3 can think about how to turn it into a decentralized architecture. At present, many organizations in the world, including domestic industries and universities, are actively exploring, researching and trying technologies in the Web3 field, and many innovative achievements have emerged.</p>]]></content>
      
      
      <categories>
          
          <category> web3 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> web3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Real time in memory semantic search with deep learning embedding and FAISS</title>
      <link href="2023/01/14/embedding-plus-faiss-semantic-search/"/>
      <url>2023/01/14/embedding-plus-faiss-semantic-search/</url>
      
        <content type="html"><![CDATA[<p>We use sentence transformer to encode short texts, and then index the results using in memory search engine FAISS;<br>Togehter we can achieve real time performance of the semantic search simply on CPU platforms.</p><h1 id="install-packages-if-necessary"><a href="#install-packages-if-necessary" class="headerlink" title="install packages if necessary"></a>install packages if necessary</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install faiss-cpu</span><br><span class="line">!pip install -U sentence-transformers</span><br></pre></td></tr></table></figure><h1 id="import-libraries"><a href="#import-libraries" class="headerlink" title="import libraries"></a>import libraries</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_20newsgroups</span><br><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = SentenceTransformer(<span class="string">&#x27;distilbert-base-nli-mean-tokens&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.encode([<span class="string">&#x27;how are you&#x27;</span>])[<span class="number">0</span>].shape</span><br></pre></td></tr></table></figure><pre><code>(768,)</code></pre><h1 id="get-data"><a href="#get-data" class="headerlink" title="get data"></a>get data</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = fetch_20newsgroups()[<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">subjects = [item.split(<span class="string">&#x27;\n&#x27;</span>)[<span class="number">1</span>] <span class="keyword">for</span> item <span class="keyword">in</span> data]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">subjects[:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>[&#39;Subject: WHAT car is this!?&#39;, &#39;Subject: SI Clock Poll - Final Call&#39;, &#39;Subject: PB questions...&#39;, &#39;Subject: Re: Weitek P9000 ?&#39;, &#39;Subject: Re: Shuttle Launch Question&#39;, &#39;Subject: Re: Rewording the Second Amendment (ideas)&#39;, &#39;Subject: Brain Tumor Treatment (thanks)&#39;, &#39;Subject: Re: IDE vs SCSI&#39;, &#39;Subject: WIn 3.0 ICON HELP PLEASE!&#39;, &#39;Subject: Re: Sigma Designs Double up??&#39;]</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoded_data = model.encode(subjects)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">encoded_data.shape</span><br></pre></td></tr></table></figure><pre><code>(11314, 768)</code></pre><h1 id="indexing-the-dataset"><a href="#indexing-the-dataset" class="headerlink" title="indexing the dataset"></a>indexing the dataset</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index = faiss.IndexIDMap(faiss.IndexFlatIP(<span class="number">768</span>))</span><br><span class="line">index.add_with_ids(encoded_data, np.array(<span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(encoded_data))))</span><br></pre></td></tr></table></figure><h2 id="serializing-the-index-to-disk-The-serialized-index-can-be-then-exported-into-any-machine-for-hosting-the-search-engine"><a href="#serializing-the-index-to-disk-The-serialized-index-can-be-then-exported-into-any-machine-for-hosting-the-search-engine" class="headerlink" title="serializing the index to disk, The serialized index can be then exported into any machine for hosting the search engine"></a>serializing the index to disk, The serialized index can be then exported into any machine for hosting the search engine</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">faiss.write_index(index, <span class="string">&#x27;20news&#x27;</span>)</span><br></pre></td></tr></table></figure><h2 id="read-the-index-back-from-disk-for-demo-purpose-so-called-desearializing"><a href="#read-the-index-back-from-disk-for-demo-purpose-so-called-desearializing" class="headerlink" title="read the index back from disk for demo purpose,  so called desearializing"></a>read the index back from disk for demo purpose,  so called desearializing</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">index = faiss.read_index(<span class="string">&#x27;20news&#x27;</span>)</span><br></pre></td></tr></table></figure><h1 id="Now-do-the-semantic-search"><a href="#Now-do-the-semantic-search" class="headerlink" title="Now do the semantic search"></a>Now do the semantic search</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search</span>(<span class="params">query</span>):</span></span><br><span class="line">    start=time.time()</span><br><span class="line">    query_vector = model.encode([query])</span><br><span class="line">    k = <span class="number">5</span></span><br><span class="line">    top_k = index.search(query_vector, k)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;spent time: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(time.time()-start))</span><br><span class="line">    <span class="keyword">return</span> [subjects[_<span class="built_in">id</span>] <span class="keyword">for</span> _<span class="built_in">id</span> <span class="keyword">in</span> top_k[<span class="number">1</span>].tolist()[<span class="number">0</span>]]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># type the query</span></span><br><span class="line"><span class="comment"># query=str(input())</span></span><br><span class="line"></span><br><span class="line">query = <span class="string">&quot;auto&quot;</span></span><br><span class="line">results=search(query)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;results :&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> result <span class="keyword">in</span> results:</span><br><span class="line">   <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>,result)</span><br></pre></td></tr></table></figure><pre><code>spent time: 0.035505056381225586results :     Subject: (w)rec.autos     Subject: Re: DRIVE     Subject: WHAT car is this!?     Subject: Re: WHAT car is this!?     Subject: Car AMP [Forsale]</code></pre><h1 id="Code-link"><a href="#Code-link" class="headerlink" title="Code link"></a>Code link</h1><p><a href="https://github.com/robotlearner001/blog/blob/main/embedding-plus-faiss/2023-01-13-sentence-transformer-ebmedding-faiss-semantic-search.ipynb">github link</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> deep learning </tag>
            
            <tag> semantic search </tag>
            
            <tag> vector search </tag>
            
            <tag> FAISS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Feature reduction and visualization using autoencoder with Pytorch</title>
      <link href="2022/12/19/autoencoder-feature-reduction-and-visualization-pytorch/"/>
      <url>2022/12/19/autoencoder-feature-reduction-and-visualization-pytorch/</url>
      
        <content type="html"><![CDATA[<p>In this notebook, we are going to use autoencoder architecture in Pytorch to reduce feature dimensions and visualiations.</p><p>First, to install PyTorch, you may use the following pip command,</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ pip install torch torchvision</span><br></pre></td></tr></table></figure><p>The <code>torchvision</code> package contains the image data sets that are ready for use in PyTorch.</p><p>More details on its installation through <a href="https://pytorch.org/get-started/locally/">this guide</a> from <a href="pytorch.org">pytorch.org</a>.</p><h2 id="Setup"><a href="#Setup" class="headerlink" title="Setup"></a>Setup</h2><p>importing relevant dependencies.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br></pre></td></tr></table></figure><p>Set seed and other configurations for reproducibility.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">seed = <span class="number">21</span></span><br><span class="line">torch.manual_seed(seed)</span><br><span class="line">torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br></pre></td></tr></table></figure><p>Set the batch size, the number of training epochs, and the learning rate.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">batch_size = <span class="number">512</span></span><br><span class="line">epochs = <span class="number">50</span></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br></pre></td></tr></table></figure><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>load the MNIST dataset as a convienient exampe using the <code>torchvision</code> package. </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])</span><br><span class="line"></span><br><span class="line">train_dataset = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&quot;~/torch_datasets&quot;</span>, train=<span class="literal">True</span>, transform=transform, download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">    train_dataset, batch_size=batch_size, shuffle=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>check one example data</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">examples = <span class="built_in">enumerate</span>(train_loader)</span><br><span class="line">batch_idx, (example_data, example_targets) = <span class="built_in">next</span>(examples)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">example_data[<span class="number">0</span>][<span class="number">0</span>].<span class="built_in">max</span>()</span><br></pre></td></tr></table></figure><pre><code>tensor(1.)</code></pre><h2 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h2><p>An autoencoder is a type of neural network that finds the function mapping the features x to itself. This objective is known as reconstruction, and an autoencoder accomplishes this through the following process:<br>(1) an encoder learns the data representation in lower-dimension space,<br>(2) a decoder learns to reconstruct the original data based on the learned representation by the encoder.</p><p>In the following we define our autoencoder class with fully connected layers and activation functions for both its encoder and decoder components.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> Tensor</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AE</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.encoder_hidden_layer = nn.Linear(</span><br><span class="line">            in_features=kwargs[<span class="string">&quot;input_shape&quot;</span>], out_features=<span class="number">1000</span></span><br><span class="line">        )</span><br><span class="line">        self.encoder_output_layer = nn.Linear(</span><br><span class="line">            in_features=<span class="number">1000</span>, out_features=<span class="number">128</span></span><br><span class="line">        )</span><br><span class="line">        self.encoder_output_layer2 = nn.Linear(</span><br><span class="line">            in_features=<span class="number">128</span>, out_features=<span class="number">32</span></span><br><span class="line">        )</span><br><span class="line">        self.encoder_output_layer3 = nn.Linear(</span><br><span class="line">            in_features=<span class="number">32</span>, out_features=<span class="number">2</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">            </span><br><span class="line">        self.decoder_hidden_layer = nn.Linear(</span><br><span class="line">            in_features=<span class="number">2</span>, out_features=<span class="number">32</span></span><br><span class="line">        )</span><br><span class="line">        self.decoder_hidden_layer2 = nn.Linear(</span><br><span class="line">            in_features=<span class="number">32</span>, out_features=<span class="number">128</span></span><br><span class="line">        )</span><br><span class="line">        self.decoder_hidden_layer3 = nn.Linear(</span><br><span class="line">            in_features=<span class="number">128</span>, out_features=<span class="number">1000</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.decoder_output_layer = nn.Linear(</span><br><span class="line">            in_features=<span class="number">1000</span>, out_features=kwargs[<span class="string">&quot;input_shape&quot;</span>]</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">encode</span>(<span class="params">self, features: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        code = self.encoder_hidden_layer(features)</span><br><span class="line">        code = torch.relu(code)</span><br><span class="line">        code = self.encoder_output_layer(code)</span><br><span class="line">        code = torch.relu(code)</span><br><span class="line">        code = self.encoder_output_layer2(code)</span><br><span class="line">        code = torch.relu(code)</span><br><span class="line">        code = self.encoder_output_layer3(code)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> code</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, encoded: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        </span><br><span class="line">        encode = self.decoder_hidden_layer(encoded)</span><br><span class="line">        encode = torch.relu(encode)</span><br><span class="line">        encode = self.decoder_hidden_layer2(encode)</span><br><span class="line">        encode = torch.relu(encode)</span><br><span class="line">        encode = self.decoder_hidden_layer3(encode)</span><br><span class="line">        encode = torch.relu(encode)</span><br><span class="line">        encode = self.decoder_output_layer(encode)</span><br><span class="line">        reconstructed = torch.sigmoid(encode)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> reconstructed   </span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, features: Tensor</span>) -&gt; Tensor:</span></span><br><span class="line">        encoded = self.encode(features)</span><br><span class="line">        <span class="keyword">return</span> self.decode(encoded)</span><br></pre></td></tr></table></figure><p>Before using our defined autoencoder class, we have the following things to do:<br>    1. We configure which device we want to run on.<br>    2. We instantiate an <code>AE</code> object.<br>    3. We define our optimizer.<br>    4. We define our reconstruction loss.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#  use gpu if available</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a model from `AE` autoencoder class</span></span><br><span class="line"><span class="comment"># load it to the specified device, either gpu or cpu</span></span><br><span class="line">model = AE(input_shape=<span class="number">784</span>).to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create an optimizer object</span></span><br><span class="line"><span class="comment"># Adam optimizer with learning rate 1e-3</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mean-squared error loss</span></span><br><span class="line">criterion = nn.MSELoss()</span><br></pre></td></tr></table></figure><p>We train our autoencoder for our specified number of epochs.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_features, _ <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># reshape mini-batch data to [N, 784] matrix</span></span><br><span class="line">        <span class="comment"># load it to the active device</span></span><br><span class="line">        batch_features = batch_features.view(-<span class="number">1</span>, <span class="number">784</span>).to(device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># reset the gradients back to zero</span></span><br><span class="line">        <span class="comment"># PyTorch accumulates gradients on subsequent backward passes</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute reconstructions</span></span><br><span class="line">        outputs = model(batch_features)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute training reconstruction loss</span></span><br><span class="line">        train_loss = criterion(outputs, batch_features)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># compute accumulated gradients</span></span><br><span class="line">        train_loss.backward()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># perform parameter update based on current gradients</span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># add the mini-batch training loss to epoch loss</span></span><br><span class="line">        loss += train_loss.item()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># compute the epoch training loss</span></span><br><span class="line">    loss = loss / <span class="built_in">len</span>(train_loader)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># display the epoch training loss</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;epoch : &#123;&#125;/&#123;&#125;, recon loss = &#123;:.8f&#125;&quot;</span>.<span class="built_in">format</span>(epoch + <span class="number">1</span>, epochs, loss))</span><br></pre></td></tr></table></figure><pre><code>epoch : 1/50, recon loss = 0.07389576epoch : 2/50, recon loss = 0.05296296epoch : 3/50, recon loss = 0.04810659epoch : 4/50, recon loss = 0.04541392epoch : 5/50, recon loss = 0.04336656epoch : 6/50, recon loss = 0.04195889epoch : 7/50, recon loss = 0.04092639epoch : 8/50, recon loss = 0.04033839epoch : 9/50, recon loss = 0.03984492epoch : 10/50, recon loss = 0.03948938epoch : 11/50, recon loss = 0.03939159epoch : 12/50, recon loss = 0.03877884epoch : 13/50, recon loss = 0.03859487epoch : 14/50, recon loss = 0.03825530epoch : 15/50, recon loss = 0.03797148epoch : 16/50, recon loss = 0.03789599epoch : 17/50, recon loss = 0.03754379epoch : 18/50, recon loss = 0.03740290epoch : 19/50, recon loss = 0.03735819epoch : 20/50, recon loss = 0.03729593epoch : 21/50, recon loss = 0.03699356epoch : 22/50, recon loss = 0.03768872epoch : 23/50, recon loss = 0.03694447epoch : 24/50, recon loss = 0.03680794epoch : 25/50, recon loss = 0.03654349epoch : 26/50, recon loss = 0.03630730epoch : 27/50, recon loss = 0.03620429epoch : 28/50, recon loss = 0.03615394epoch : 29/50, recon loss = 0.03615029epoch : 30/50, recon loss = 0.03593704epoch : 31/50, recon loss = 0.03589566epoch : 32/50, recon loss = 0.03570651epoch : 33/50, recon loss = 0.03599412epoch : 34/50, recon loss = 0.03587519epoch : 35/50, recon loss = 0.03641265epoch : 36/50, recon loss = 0.03615064epoch : 37/50, recon loss = 0.03541873epoch : 38/50, recon loss = 0.03545310epoch : 39/50, recon loss = 0.03534035epoch : 40/50, recon loss = 0.03541123epoch : 41/50, recon loss = 0.03511182epoch : 42/50, recon loss = 0.03499481epoch : 43/50, recon loss = 0.03487989epoch : 44/50, recon loss = 0.03506399epoch : 45/50, recon loss = 0.03487079epoch : 46/50, recon loss = 0.03481269epoch : 47/50, recon loss = 0.03454635epoch : 48/50, recon loss = 0.03444027epoch : 49/50, recon loss = 0.03448961epoch : 50/50, recon loss = 0.03482613</code></pre><p>Let’s extract some test examples to reconstruct using our trained autoencoder.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">test_dataset = torchvision.datasets.MNIST(</span><br><span class="line">    root=<span class="string">&quot;~/torch_datasets&quot;</span>, train=<span class="literal">False</span>, transform=transform, download=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">    test_dataset, batch_size=<span class="number">10</span>, shuffle=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">test_examples = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># loop the test data once to get the first batch of 10 datapoints for reconstruction quality check</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch_features <span class="keyword">in</span> test_loader:</span><br><span class="line">        batch_features = batch_features[<span class="number">0</span>]</span><br><span class="line">        test_examples = batch_features.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        reconstruction = model(test_examples)</span><br><span class="line">        reconstruction_hidden = model.encode(test_examples)</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure><h2 id="Visualize-Reconstruction-Quality"><a href="#Visualize-Reconstruction-Quality" class="headerlink" title="Visualize Reconstruction Quality"></a>Visualize Reconstruction Quality</h2><p>Let’s try to reconstruct some test images using our trained autoencoder.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    number = <span class="number">10</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">20</span>, <span class="number">4</span>))</span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(number):</span><br><span class="line">        <span class="comment"># display original</span></span><br><span class="line">        ax = plt.subplot(<span class="number">2</span>, number, index + <span class="number">1</span>)</span><br><span class="line">        plt.imshow(test_examples[index].numpy().reshape(<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">        plt.gray()</span><br><span class="line">        ax.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># display reconstruction</span></span><br><span class="line">        ax = plt.subplot(<span class="number">2</span>, number, index + <span class="number">1</span> + number)</span><br><span class="line">        plt.imshow(reconstruction[index].numpy().reshape(<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">        plt.gray()</span><br><span class="line">        ax.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">        ax.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-12-19-1.png" alt="png"></p><h2 id="Analysis-so-far"><a href="#Analysis-so-far" class="headerlink" title="Analysis so far"></a>Analysis so far</h2><p>as we can see the reconstruciton is good, but not super great; this is mainly because we use only 2 nodes for the<br>middle hidden layer. Using only 2 nodes is easy for us to see the reduced dimensions, but probably not good enough<br>to capture all the sailent features. For pure feature reduction purpose, we can choose a bigger number of nodes<br>for the middel hidden layer.</p><h2 id="Visualize-the-middel-hidden-layer-with-2-nodes-for-lower-dimension-reduction"><a href="#Visualize-the-middel-hidden-layer-with-2-nodes-for-lower-dimension-reduction" class="headerlink" title="Visualize the middel hidden layer with 2 nodes for lower dimension reduction"></a>Visualize the middel hidden layer with 2 nodes for lower dimension reduction</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># reduce dimension example</span></span><br><span class="line"></span><br><span class="line">results =[]</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> batch_features <span class="keyword">in</span> test_loader:</span><br><span class="line">        batch_features = batch_features[<span class="number">0</span>]</span><br><span class="line">        test_examples = batch_features.view(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">        reconstruction_hidden = model.encode(test_examples)</span><br><span class="line">        results.append(reconstruction_hidden.numpy())</span><br><span class="line">        </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">all_hidden = np.concatenate(results)</span><br><span class="line">all_hidden</span><br></pre></td></tr></table></figure><pre><code>array([[-0.43485078,  0.31671965],       [ 1.5935664 ,  4.4088674 ],       [ 9.075943  ,  4.4781566 ],       ...,       [-0.90027434,  0.3994102 ],       [-2.9567816 ,  2.2586362 ],       [-4.884531  ,  1.9589175 ]], dtype=float32)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">labels = test_dataset.targets.numpy()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">rdf = pd.DataFrame(all_hidden)</span><br><span class="line">rdf[<span class="string">&#x27;lable&#x27;</span>] = labels</span><br><span class="line">rdf.columns = [<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;y&#x27;</span>,<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))  </span><br><span class="line">sns.scatterplot(data=rdf,x=<span class="string">&#x27;x&#x27;</span>,y=<span class="string">&#x27;y&#x27;</span>,hue=<span class="string">&#x27;label&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;AxesSubplot:xlabel=&#39;x&#39;, ylabel=&#39;y&#39;&gt;</code></pre><p><img src="/content/images/2022-12-19-2.png" alt="png"></p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/autoencoder-pytorch-feature-reduction-and-visualization/2022-12-19-autoencoder-feature-reduction-visualization-in-pytorch.ipynb">code link</a></p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> deep learning </tag>
            
            <tag> autoencoder </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Revolutionizing Robotics, How RT-1 is Paving the Way for Intelligent Machines</title>
      <link href="2022/12/15/revolutionizing-robotics-the-emergence-of-RT-1/"/>
      <url>2022/12/15/revolutionizing-robotics-the-emergence-of-RT-1/</url>
      
        <content type="html"><![CDATA[<p>Great progress of robotics and learning from language models.</p><p>Researchers at OpenAI have developed the Robotics Transformer 1 (RT-1), a machine learning model that’s designed to help robots learn from large and diverse datasets. </p><p>RT-1, which is built on a transformer architecture, takes a short history of images from a robot’s camera alongside task descriptions, expressed in natural language, as inputs and directly outputs tokenized actions such as motor commands. </p><p>RT-1 is trained on a real-world robotics dataset of 130,000 episodes covering over 700 tasks, and is able to exhibit improved zero-shot generalisation to new tasks and environments compared to previous techniques. </p><p>RT-1 is also able to compress image tokens and adaptively select soft combinations that can be compressed based on their impact towards learning, resulting in a more than 2.4x inference speed-up.</p><h2 id="open-source-links"><a href="#open-source-links" class="headerlink" title="open source links"></a>open source links</h2><p><a href="https://github.com/google-research/robotics_transformer">code link</a><br><a href="https://robotics-transformer.github.io/assets/rt1.pdf">paper link</a><br><a href="https://robotics-transformer.github.io/">project website</a></p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> news </tag>
            
            <tag> robotics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A list of poupular but hard leetcode problems for interview</title>
      <link href="2022/12/14/populad-and-hard-leetcode-problems-and-solutions/"/>
      <url>2022/12/14/populad-and-hard-leetcode-problems-and-solutions/</url>
      
        <content type="html"><![CDATA[<p>Here is the list most popular but hard interview questions that according to leetcode.</p><h2 id="List-of-problems-and-solutions"><a href="#List-of-problems-and-solutions" class="headerlink" title="List of problems and solutions"></a>List of problems and solutions</h2><ol><li><a href="https://www.datasciencebyexample.com/2022/10/24/2022-10-24-1/">find median of two sorted arrays</a></li><li><a href="https://www.datasciencebyexample.com/2022/10/25/2022-10-25-1/">Regular expression matching</a></li><li><a href="https://www.datasciencebyexample.com/2022/10/27/2022-10-27-1/">Merge K sorted lists</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/01/2022-11-01-1/">Find first missing positive </a></li><li><a href="https://www.datasciencebyexample.com/2022/11/02/2022-11-02-1/">Trapping rain water</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/06/2022-11-05-01/">Wildcard matching</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/08/2022-11-08-01/">Minimum window substring</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/12/largest-rectangle-in-histogram/">Largest rectangle in histogram</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/17/binary-tree-maximum-path-sum/">Binary Tree Maximum Path Sum</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/17/word-ladder/">Word ladder</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/28/word-break-II/">word break II</a></li><li><a href="https://www.datasciencebyexample.com/2022/11/30/max-points-on-a-line/">Max points on a line</a></li></ol><h2 id="link-in-github"><a href="#link-in-github" class="headerlink" title="link in github"></a>link in github</h2><p><a href="https://github.com/robotlearner001/blog/tree/main/leetcode-popular-and-hard-problems">github link</a></p><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A custom scikit-learn transformer example with parameter input and avoid possible None error</title>
      <link href="2022/12/12/custom-sckikt-learn-transformer-with-parameter-and-avoid-none-error/"/>
      <url>2022/12/12/custom-sckikt-learn-transformer-with-parameter-and-avoid-none-error/</url>
      
        <content type="html"><![CDATA[<p>A customer scikit-learn transfomer example with parameter.<br>It takes a pandas DataFrame with a column called input as input, and returns a DataFrame with a column called output containing the transformed data:</p><h2 id="define-custom-transformer"><a href="#define-custom-transformer" class="headerlink" title="define custom transformer"></a>define custom transformer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PowerTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, power=<span class="number">1</span></span>):</span></span><br><span class="line">        self.power = power</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># Fit simply returns self, nothing else to do</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X</span>):</span></span><br><span class="line">        <span class="comment"># Check if input is a DataFrame</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(X, pd.DataFrame):</span><br><span class="line">            <span class="comment"># If so, return a DataFrame with the transformed data</span></span><br><span class="line">            <span class="keyword">return</span> pd.DataFrame(&#123;<span class="string">&#x27;output&#x27;</span>: X[<span class="string">&#x27;input&#x27;</span>] ** self.power&#125;)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># If not, return a plain NumPy array with the transformed data</span></span><br><span class="line">            <span class="keyword">return</span> X[<span class="string">&#x27;input&#x27;</span>] ** self.power</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>To use this transformer, we would first instantiate it with the desired power parameter, and then call its fit_transform method on a DataFrame with an input column. For example:</p><h2 id="generate-some-sample-data-and-dataframe"><a href="#generate-some-sample-data-and-dataframe" class="headerlink" title="generate some sample data and dataframe"></a>generate some sample data and dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate some sample data</span></span><br><span class="line">data = np.random.randn(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DataFrame with an &#x27;input&#x27; column</span></span><br><span class="line">X = pd.DataFrame(&#123;<span class="string">&#x27;input&#x27;</span>: data&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the DataFrame</span></span><br><span class="line"><span class="built_in">print</span>(X)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>      input0 -1.4601351  0.1885322 -0.2726003  0.3068804 -0.221020</code></pre><h2 id="use-the-transfomer-with-parameter-intilization"><a href="#use-the-transfomer-with-parameter-intilization" class="headerlink" title="use the transfomer, with parameter intilization"></a>use the transfomer, with parameter intilization</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Create the transformer</span></span><br><span class="line">transformer = PowerTransformer(power=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fit and transform the data</span></span><br><span class="line">X_transformed = transformer.fit_transform(X)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(X_transformed)</span><br></pre></td></tr></table></figure><pre><code>     output0  2.1319941  0.0355442  0.0743113  0.0941754  0.048850</code></pre><h2 id="possible-error-Parameter-sees-none-value"><a href="#possible-error-Parameter-sees-none-value" class="headerlink" title="possible error:  Parameter sees none value"></a>possible error:  Parameter sees none value</h2><p>the above example shows some practices: (1) the transformer have an default value in the init function (2) when calling the transfomer make sure to have the ···parameter=value format</p><p>   For example,  We can skip the parameter name like this, it will work for one time use:<br><code>transformer = PowerTransformer(2)   </code></p><p>But it will raise None error in using with other methos such as <code>sklearn.model_selection.cross_val_score</code></p><h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/custom_scikit-learn_transformer/2022-12-12-custom-transformer-with-parameter.ipynb">github link</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenAI&#39;s chatbot has just changed the search engine industry</title>
      <link href="2022/12/02/openai-chatbot-has-just-changed-the-search-engine-industry/"/>
      <url>2022/12/02/openai-chatbot-has-just-changed-the-search-engine-industry/</url>
      
        <content type="html"><![CDATA[<p><img src="/content/images/2022-12-02-1.png"></p><p>The game of the search engine has just been changed!</p><p>The reaction to OpenAI’s new chatbot product on Dec 1st 2022 has been overwhelmingly positive. The sheer volume of people wanting to experience it crashed the website for one whole day, demonstrating just how popular the product has become. It’s clear that people are excited to explore the potential of this technology, and to see how it can be used to help improve their lives. </p><p>For years, the search engines giant, such as Google and Baidu have been trying to better understand user’s intention. instead of just searching for specific keywords, search engines are getting better to understand more natural language inquiries. This means that instead of just typing in specific keywords, users can ask questions in their own words and get more accurate results. This shift has created greater accuracy and relevance for search engine results, which can help people find what they’re looking for more quickly and easily.</p><p>But the introduction of OpenAI’s chatbot has further changed the game of search engine. Chatbots are capable of understanding natural language queries, which means users can ask questions in their own words and get more accurate results. Chatbots can also be used to provide personalized recommendations, further improving the search experience. </p><p>Two fundamental differences of OpenAI’s chatbot and Google’s search engine:<br>(1) cost to the users:  Search engine is free in general if you don’t consider the ad.  OpenAI’s chatbot will charge people for its usage.<br>(2)  with search engine, the most relevant raw data can be found for the users to further digest and research; but with OpenAI’s chatbot, the users don’t need to any research and simply get the answers right away.</p><p>The question people need to ask themselves now is: do you want to pay for results and enjoy being lazy or doing some research based on relevant raw data.</p><p>Here we ask OpenAI one question for fun, the question is “ who will win the 2022 FIFA world cup?”<br>and the answer is very honest:<br>“I’m sorry, but I am a large language model trained by OpenAI and my knowledge cutoff is 2021, so I do not have any information about the 2022 FIFA World Cup. My training only goes up until 2021, and I am not currently able to browse the internet, so I cannot provide any updated information on current events or future events. My function is to assist with general knowledge and provide information based on my training, not to provide current or future event predictions.”</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Max points on a line</title>
      <link href="2022/11/30/max-points-on-a-line/"/>
      <url>2022/11/30/max-points-on-a-line/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given an array of points where points[i] = [xi, yi] represents a point on the X-Y plane, return the maximum number of points that lie on the same straight line.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br><img src="/content/images/2022-11-30-1.jpg"></p><p>Input: points = [[1,1],[2,2],[3,3]]<br>Output: 3</p><p>Example 2:<br><img src="/content/images/2022-11-30-2.jpg"></p><p>Input: points = [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]<br>Output: 4</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= points.length &lt;= 300<br>points[i].length == 2<br>-104 &lt;= xi, yi &lt;= 104<br>All the points are unique.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def maxPoints(self, points):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type points: List[List[int]]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        n = len(points)</span><br><span class="line">        if n &lt; 3:</span><br><span class="line">            return n</span><br><span class="line">        </span><br><span class="line">        ans = 0</span><br><span class="line">        for i in range(n):</span><br><span class="line">            dic = &#123;&#x27;inf&#x27;:0&#125;</span><br><span class="line">            samePointsNum = 0</span><br><span class="line">            x1, y1 = points[i]</span><br><span class="line">            for j in range(i+1, n):</span><br><span class="line">                x2, y2 = points[j]</span><br><span class="line">                if x1 == x2 and y1 == y2:</span><br><span class="line">                    samePointsNum += 1</span><br><span class="line">                    continue</span><br><span class="line">                if x1 == x2:</span><br><span class="line">                    slope = &#x27;inf&#x27;</span><br><span class="line">                else:</span><br><span class="line">                    slope = (y2-y1)*1.0/(x2-x1)</span><br><span class="line">                if slope not in dic:</span><br><span class="line">                    dic[slope] = 1</span><br><span class="line">                else:</span><br><span class="line">                    dic[slope] += 1</span><br><span class="line">            ans = max(ans, max(dic.values())+samePointsNum+1)</span><br><span class="line">        return ans</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">points = [[1,1],[3,2],[5,3],[4,1],[2,3],[1,4]]</span><br><span class="line">print(Solution().maxPoints(points))</span><br><span class="line"></span><br><span class="line">points = [[1,1],[2,2],[3,3]]</span><br><span class="line">print(Solution().maxPoints(points))</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word break II</title>
      <link href="2022/11/28/word-break-II/"/>
      <url>2022/11/28/word-break-II/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given a string s and a dictionary of strings wordDict, add spaces in s to construct a sentence where each word is a valid dictionary word. Return all such possible sentences in any order.<br>Note that the same word in the dictionary may be reused multiple times in the segmentation.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: s = “catsanddog”, wordDict = [“cat”,”cats”,”and”,”sand”,”dog”]<br>Output: [“cats and dog”,”cat sand dog”]</p><p>Example 2:<br>Input: s = “pineapplepenapple”, wordDict = [“apple”,”pen”,”applepen”,”pine”,”pineapple”]<br>Output: [“pine apple pen apple”,”pineapple pen apple”,”pine applepen apple”]<br>Explanation: Note that you are allowed to reuse a dictionary word.</p><p>Example 3:<br>Input: s = “catsandog”, wordDict = [“cats”,”dog”,”sand”,”and”,”cat”]<br>Output: []</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= s.length &lt;= 20<br>1 &lt;= wordDict.length &lt;= 1000<br>1 &lt;= wordDict[i].length &lt;= 10<br>s and wordDict[i] consist of only lowercase English letters.<br>All the strings of wordDict are unique.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def wordBreak(self, s, wordDict):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type s: str</span><br><span class="line">        :type wordDict: List[str]</span><br><span class="line">        :rtype: List[str]</span><br><span class="line">        &quot;&quot;&quot;        </span><br><span class="line">        dp = [None] * (len(s) + 1)</span><br><span class="line">        dp[0] = [&#x27;&#x27;]</span><br><span class="line">        for i in range(1, len(s) + 1):</span><br><span class="line">            dp[i] = []</span><br><span class="line">            for j in range(i):</span><br><span class="line">                if dp[j] and s[j:i] in wordDict:</span><br><span class="line">                    for word in dp[j]:</span><br><span class="line">                        dp[i].append(word + (&#x27; &#x27; if word else &#x27;&#x27;) + s[j:i])</span><br><span class="line">        return dp[-1]</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = &quot;catsanddog&quot;</span><br><span class="line">wordDict = [&quot;cat&quot;,&quot;cats&quot;,&quot;and&quot;,&quot;sand&quot;,&quot;dog&quot;]</span><br><span class="line">print(Solution().wordBreak(s, wordDict))</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Predict the 2022 world cup using Poisson distribution</title>
      <link href="2022/11/20/fifa-world-cup-2022-prediction-with-poisson-distribution/"/>
      <url>2022/11/20/fifa-world-cup-2022-prediction-with-poisson-distribution/</url>
      
        <content type="html"><![CDATA[<p>There are many ways to predict the world cup or any games, complex machine learning models are one of those choices.<br>Here we introduct one simple way but many of the times robust way to predict by using Poisson distribution.</p><p>The code can be found here:<br><a href="https://github.com/robotlearner001/blog/tree/main/world-cup-2022-prediction">github link</a></p><p>And the following image shows the prediction:<br><img src="/content/images/2022-11-20-1.jpg"></p><p>and the predicted final winner of 2022 world cup is:<br>Brazil</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> poisson distribution </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to fix google cloud API user rate limit exceeded error</title>
      <link href="2022/11/19/google-cloud-user-rate-limit-exceeded/"/>
      <url>2022/11/19/google-cloud-user-rate-limit-exceeded/</url>
      
        <content type="html"><![CDATA[<p>We are using the google cloud translate API, and suddenly noticed it gives error.<br>A closer check of the logs shows there is some 403 error, which complains “user rate limit exceeded”.</p><p>Then checking the google cloud console, and realized it’s because of the credit card associated with billing expired.</p><p>So the the steps to fix the error are:<br>(1) go to billing page, and update the credit card information.<br>(2) go the account management, and reactivate the account.</p><p>Now the API works correctly again.</p><p><img src="/content/images/2022-11-19-1.png"></p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> google cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Binary Tree Maximum Path Sum</title>
      <link href="2022/11/17/binary-tree-maximum-path-sum/"/>
      <url>2022/11/17/binary-tree-maximum-path-sum/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>A path in a binary tree is a sequence of nodes where each pair of adjacent nodes in the sequence has an edge connecting them. A node can only appear in the sequence at most once. Note that the path does not need to pass through the root.<br>The path sum of a path is the sum of the node’s values in the path.<br>Given the root of a binary tree, return the maximum path sum of any non-empty path.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p><img src="/content/images/2022-11-16-1.jpg"></p><p>Example 1:<br>Input: root = [1,2,3]<br>Output: 6<br>Explanation: The optimal path is 2 -&gt; 1 -&gt; 3 with a path sum of 2 + 1 + 3 = 6.</p><p><img src="/content/images/2022-11-16-2.jpg"></p><p>Example 2:<br>Input: root = [-10,9,20,null,null,15,7]<br>Output: 42<br>Explanation: The optimal path is 15 -&gt; 20 -&gt; 7 with a path sum of 15 + 20 + 7 = 42.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>The number of nodes in the tree is in the range [1, 3 * 104].<br>-1000 &lt;= Node.val &lt;= 1000</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Definition for a binary tree node.</span><br><span class="line"># class TreeNode(object):</span><br><span class="line">#     def __init__(self, val=0, left=None, right=None):</span><br><span class="line">#         self.val = val</span><br><span class="line">#         self.left = left</span><br><span class="line">#         self.right = right</span><br><span class="line">class Solution(object):</span><br><span class="line">    def maxPathSum(self, root):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type root: TreeNode</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.max_sum = float(&#x27;-inf&#x27;)</span><br><span class="line">        self.max_gain(root)</span><br><span class="line">        return self.max_sum</span><br><span class="line">    </span><br><span class="line">    def max_gain(self, node):</span><br><span class="line">        if not node:</span><br><span class="line">            return 0</span><br><span class="line">        </span><br><span class="line">        left_gain = max(self.max_gain(node.left), 0)</span><br><span class="line">        right_gain = max(self.max_gain(node.right), 0)</span><br><span class="line">        </span><br><span class="line">        price_newpath = node.val + left_gain + right_gain</span><br><span class="line">        </span><br><span class="line">        self.max_sum = max(self.max_sum, price_newpath)</span><br><span class="line">        </span><br><span class="line">        return node.val + max(left_gain, right_gain)</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class TreeNode(object):</span><br><span class="line">    def __init__(self, val=0, left=None, right=None):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line">root = TreeNode(1)</span><br><span class="line">root.left = TreeNode(2)</span><br><span class="line">root.right = TreeNode(3)</span><br><span class="line"></span><br><span class="line">solution = Solution()</span><br><span class="line">print( solution.maxPathSum(root) )</span><br><span class="line"></span><br><span class="line"># output: 6</span><br><span class="line"></span><br><span class="line"># test the solution      </span><br><span class="line">class TreeNode(object):</span><br><span class="line">    def __init__(self, val=0, left=None, right=None):</span><br><span class="line">        self.val = val</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line">root = TreeNode(-10)</span><br><span class="line">root.left = TreeNode(9)</span><br><span class="line">root.right = TreeNode(20)</span><br><span class="line">root.right.left = TreeNode(15)</span><br><span class="line">root.right.right = TreeNode(7)</span><br><span class="line"></span><br><span class="line">solution = Solution()</span><br><span class="line">print(solution.maxPathSum(root))</span><br><span class="line"></span><br><span class="line"># output: 42</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word ladder</title>
      <link href="2022/11/17/word-ladder/"/>
      <url>2022/11/17/word-ladder/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>A transformation sequence from word beginWord to word endWord using a dictionary wordList is a sequence of words beginWord -&gt; s1 -&gt; s2 -&gt; … -&gt; sk such that:<br>Every adjacent pair of words differs by a single letter.<br>Every si for 1 &lt;= i &lt;= k is in wordList. Note that beginWord does not need to be in wordList.<br>sk == endWord<br>Given two words, beginWord and endWord, and a dictionary wordList, return the number of words in the shortest transformation sequence from beginWord to endWord, or 0 if no such sequence exists.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: beginWord = “hit”, endWord = “cog”, wordList = [“hot”,”dot”,”dog”,”lot”,”log”,”cog”]<br>Output: 5<br>Explanation: One shortest transformation sequence is “hit” -&gt; “hot” -&gt; “dot” -&gt; “dog” -&gt; cog”, which is 5 words long.</p><p>Example 2:<br>Input: beginWord = “hit”, endWord = “cog”, wordList = [“hot”,”dot”,”dog”,”lot”,”log”]<br>Output: 0<br>Explanation: The endWord “cog” is not in wordList, therefore there is no valid transformation sequence.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= beginWord.length &lt;= 10<br>endWord.length == beginWord.length<br>1 &lt;= wordList.length &lt;= 5000<br>wordList[i].length == beginWord.length<br>beginWord, endWord, and wordList[i] consist of lowercase English letters.<br>beginWord != endWord<br>All the words in wordList are unique.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import collections</span><br><span class="line">class Solution(object):</span><br><span class="line">    def ladderLength(self, beginWord, endWord, wordList):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type beginWord: str</span><br><span class="line">        :type endWord: str</span><br><span class="line">        :type wordList: List[str]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if endWord not in wordList:</span><br><span class="line">            return 0</span><br><span class="line">        wordList = set(wordList)</span><br><span class="line">        queue = collections.deque([(beginWord, 1)])</span><br><span class="line">        while queue:</span><br><span class="line">            word, length = queue.popleft()</span><br><span class="line">            if word == endWord:</span><br><span class="line">                return length</span><br><span class="line">            for i in range(len(word)):</span><br><span class="line">                for c in &#x27;abcdefghijklmnopqrstuvwxyz&#x27;:</span><br><span class="line">                    next_word = word[:i] + c + word[i+1:]</span><br><span class="line">                    if next_word in wordList:</span><br><span class="line">                        wordList.remove(next_word)</span><br><span class="line">                        queue.append((next_word, length + 1))</span><br><span class="line">        return 0</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">beginWord = &quot;hit&quot;</span><br><span class="line">endWord = &quot;cog&quot;</span><br><span class="line">wordList = [&quot;hot&quot;,&quot;dot&quot;,&quot;dog&quot;,&quot;lot&quot;,&quot;log&quot;,&quot;cog&quot;]</span><br><span class="line">solution = Solution()</span><br><span class="line">print(solution.ladderLength(beginWord, endWord, wordList))</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Largest rectangle in histogram</title>
      <link href="2022/11/12/largest-rectangle-in-histogram/"/>
      <url>2022/11/12/largest-rectangle-in-histogram/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given an array of integers heights representing the histogram’s bar height where the width of each bar is 1, return the area of the largest rectangle in the histogram.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br><img src="/content/images/2022-11-12-1.jpg"></p><p>Input: heights = [2,1,5,6,2,3]<br>Output: 10<br>Explanation: The above is a histogram where width of each bar is 1.<br>The largest rectangle is shown in the red area, which has an area = 10 units.</p><p>Example 2:<br><img src="/content/images/2022-11-12-2.jpg"><br>Input: heights = [2,4]<br>Output: 4</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= heights.length &lt;= 105<br>0 &lt;= heights[i] &lt;= 104</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def largestRectangleArea(self, heights):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type heights: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not heights:</span><br><span class="line">            return 0</span><br><span class="line">        stack = []</span><br><span class="line">        max_area = 0</span><br><span class="line">        for i in range(len(heights)):</span><br><span class="line">            if not stack or heights[i] &gt;= heights[stack[-1]]:</span><br><span class="line">                stack.append(i)</span><br><span class="line">            else:</span><br><span class="line">                while stack and heights[i] &lt; heights[stack[-1]]:</span><br><span class="line">                    h = heights[stack.pop()]</span><br><span class="line">                    w = i if not stack else i - stack[-1] - 1</span><br><span class="line">                    max_area = max(max_area, h * w)</span><br><span class="line">                stack.append(i)</span><br><span class="line">        while stack:</span><br><span class="line">            h = heights[stack.pop()]</span><br><span class="line">            w = len(heights) if not stack else len(heights) - stack[-1] - 1</span><br><span class="line">            max_area = max(max_area, h * w)</span><br><span class="line">        return max_area</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = Solution()</span><br><span class="line">print(s.largestRectangleArea([2,1,5,6,2,3]))</span><br><span class="line">print(s.largestRectangleArea([2,4]))</span><br><span class="line">print(s.largestRectangleArea([2,4,5,6,7,8,9]))</span><br><span class="line">print(s.largestRectangleArea([9,8,7,6,5,4,3,2,1]))</span><br><span class="line">print(s.largestRectangleArea([1,2,3,4,5,6,7,8,9]))</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Minimum window substring</title>
      <link href="2022/11/08/2022-11-08-01/"/>
      <url>2022/11/08/2022-11-08-01/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given two strings s and t of lengths m and n respectively, return the minimum window substring of s such that every character in t (including duplicates) is included in the window.<br>If there is no such substring, return the empty string “”.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: s = “ADOBECODEBANC”, t = “ABC”<br>Output: “BANC”<br>Explanation: The minimum window substring “BANC” includes ‘A’, ‘B’, and ‘C’ from string t.</p><p>Example 2:<br>Input: s = “a”, t = “a”<br>Output: “a”<br>Explanation: The entire string s is the minimum window.</p><p>Example 3:<br>Input: s = “a”, t = “aa”<br>Output: “”<br>Explanation: Both ‘a’s from t must be included in the window.<br>Since the largest window of s only has one ‘a’, return empty string.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>m == s.length<br>n == t.length<br>1 &lt;= m, n &lt;= 105<br>s and t consist of uppercase and lowercase English letters.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def minWindow(self, s, t):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type s: str</span><br><span class="line">        :type t: str</span><br><span class="line">        :rtype: str</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not s or not t or len(s) &lt; len(t):</span><br><span class="line">            return &quot;&quot;</span><br><span class="line">        n = len(s)</span><br><span class="line">        m = len(t)</span><br><span class="line">        hash_map = &#123;&#125;</span><br><span class="line">        for i in t:</span><br><span class="line">            hash_map[i] = hash_map.get(i, 0) + 1</span><br><span class="line">        left = right = 0</span><br><span class="line">        count = len(hash_map)</span><br><span class="line">        min_len = n+1</span><br><span class="line">        min_start = 0</span><br><span class="line">        while right &lt; n:</span><br><span class="line">            if s[right] in hash_map:</span><br><span class="line">                hash_map[s[right]] -= 1</span><br><span class="line">                if hash_map[s[right]] == 0:</span><br><span class="line">                    count -= 1</span><br><span class="line">            right += 1</span><br><span class="line">            while count == 0:</span><br><span class="line">                if right - left &lt; min_len:</span><br><span class="line">                    min_len = right - left</span><br><span class="line">                    min_start = left</span><br><span class="line">                if s[left] in hash_map:</span><br><span class="line">                    hash_map[s[left]] += 1</span><br><span class="line">                    if hash_map[s[left]] &gt; 0:</span><br><span class="line">                        count += 1</span><br><span class="line">                left += 1</span><br><span class="line">        if min_len == n+1:</span><br><span class="line">            return &quot;&quot;</span><br><span class="line">        return s[min_start:min_start+min_len]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import unittest</span><br><span class="line"></span><br><span class="line">class TestSolution(unittest.TestCase):</span><br><span class="line">    def setUp(self):</span><br><span class="line">        self.s = Solution()</span><br><span class="line"></span><br><span class="line">    def test_case1(self):</span><br><span class="line">        str1 = &quot;ADOBECODEBANC&quot;</span><br><span class="line">        str2 = &quot;ABC&quot;</span><br><span class="line">        result = &quot;BANC&quot;</span><br><span class="line">        self.assertEqual(self.s.minWindow(str1,str2),result)</span><br><span class="line"></span><br><span class="line">    def test_case2(self):</span><br><span class="line">        str1 = &quot;a&quot;</span><br><span class="line">        str2 = &quot;b&quot;</span><br><span class="line">        result = &quot;&quot;</span><br><span class="line">        self.assertEqual(self.s.minWindow(str1,str2),result)</span><br><span class="line"></span><br><span class="line">    def test_case3(self):</span><br><span class="line">        str1 = &quot;a&quot;</span><br><span class="line">        str2 = &quot;aa&quot;</span><br><span class="line">        result = &quot;&quot;</span><br><span class="line">        self.assertEqual(self.s.minWindow(str1,str2),result)</span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    unittest.main()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wildcard matching using dynamic programming</title>
      <link href="2022/11/06/2022-11-05-01/"/>
      <url>2022/11/06/2022-11-05-01/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given an input string (s) and a pattern (p), implement wildcard pattern matching with support for ‘?’ and ‘<em>‘ where:<br>‘?’ Matches any single character.<br>‘</em>‘ Matches any sequence of characters (including the empty sequence).<br>The matching should cover the entire input string (not partial).</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: s = “aa”, p = “a”<br>Output: false<br>Explanation: “a” does not match the entire string “aa”.</p><p>Example 2:<br>Input: s = “aa”, p = “<em>“<br>Output: true<br>Explanation: ‘</em>‘ matches any sequence.</p><p>Example 3:<br>Input: s = “cb”, p = “?a”<br>Output: false<br>Explanation: ‘?’ matches ‘c’, but the second letter is ‘a’, which does not match ‘b’.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>0 &lt;= s.length, p.length &lt;= 2000<br>s contains only lowercase English letters.<br>p contains only lowercase English letters, ‘?’ or ‘*’.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def isMatch(self, s, p):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type s: str</span><br><span class="line">        :type p: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        # dp[i][j] = dp[i-1][j-1] if s[i] == p[j] or p[j] == &#x27;?&#x27;</span><br><span class="line">        # dp[i][j] = dp[i-1][j] or dp[i][j-1] if p[j] == &#x27;*&#x27;</span><br><span class="line">        # dp[0][0] = True</span><br><span class="line">        # dp[i][0] = False</span><br><span class="line">        # dp[0][j] = dp[0][j-1] if p[j] == &#x27;*&#x27;</span><br><span class="line">        # dp[0][j] = False if p[j] != &#x27;*&#x27;</span><br><span class="line">        # return dp[-1][-1]</span><br><span class="line">        dp = [[False] * (len(p) + 1) for _ in range(len(s) + 1)]</span><br><span class="line">        dp[0][0] = True</span><br><span class="line">        for j in range(1, len(p) + 1):</span><br><span class="line">            if p[j - 1] == &#x27;*&#x27;:</span><br><span class="line">                dp[0][j] = dp[0][j - 1]</span><br><span class="line">        for i in range(1, len(s) + 1):</span><br><span class="line">            for j in range(1, len(p) + 1):</span><br><span class="line">                if p[j - 1] == &#x27;?&#x27; or p[j - 1] == s[i - 1]:</span><br><span class="line">                    dp[i][j] = dp[i - 1][j - 1]</span><br><span class="line">                elif p[j - 1] == &#x27;*&#x27;:</span><br><span class="line">                    dp[i][j] = dp[i - 1][j] or dp[i][j - 1]</span><br><span class="line">        return dp[-1][-1]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = Solution()</span><br><span class="line">assert s.isMatch(&quot;aa&quot;, &quot;a&quot;) == False</span><br><span class="line">assert s.isMatch(&quot;aa&quot;, &quot;*&quot;) == True</span><br><span class="line">assert s.isMatch(&quot;cb&quot;, &quot;?a&quot;) == False</span><br><span class="line">assert s.isMatch(&quot;adceb&quot;, &quot;*a*b&quot;) == True</span><br><span class="line">assert s.isMatch(&quot;acdcb&quot;, &quot;a*c?b&quot;) == False</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Compute how much water are trapped after raining (trapping rain water)</title>
      <link href="2022/11/02/2022-11-02-1/"/>
      <url>2022/11/02/2022-11-02-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given n non-negative integers representing an elevation map where the width of each bar is 1, compute how much water it can trap after raining.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br><img src="/content/images/2022-11-02-1.png"><br>Input: height = [0,1,0,2,1,0,1,3,2,1,2,1]<br>Output: 6<br>Explanation: The above elevation map (black section) is represented by array [0,1,0,2,1,0,1,3,2,1,2,1]. In this case, 6 units of rain water (blue section) are being trapped.</p><p>Example 2:<br>Input: height = [4,2,0,3,2,5]<br>Output: 9</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>n == height.length<br>1 &lt;= n &lt;= 2 * 104<br>0 &lt;= height[i] &lt;= 105</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def trap(self, height):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type height: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not height:</span><br><span class="line">            return 0</span><br><span class="line">        left = 0</span><br><span class="line">        right = len(height) - 1</span><br><span class="line">        left_max = height[left]</span><br><span class="line">        right_max = height[right]</span><br><span class="line">        res = 0</span><br><span class="line">        while left &lt; right:</span><br><span class="line">            if height[left] &lt; height[right]:</span><br><span class="line">                if height[left] &gt;= left_max:</span><br><span class="line">                    left_max = height[left]</span><br><span class="line">                else:</span><br><span class="line">                    res += left_max - height[left]</span><br><span class="line">                left += 1</span><br><span class="line">            else:</span><br><span class="line">                if height[right] &gt;= right_max:</span><br><span class="line">                    right_max = height[right]</span><br><span class="line">                else:</span><br><span class="line">                    res += right_max - height[right]</span><br><span class="line">                right -= 1</span><br><span class="line">        return res</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = Solution()</span><br><span class="line">print(s.trap([0,1,0,2,1,0,1,3,2,1,2,1]))</span><br><span class="line">print(s.trap([4,2,0,3,2,5]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Find first missing positive integer in an unsorted array</title>
      <link href="2022/11/01/2022-11-01-1/"/>
      <url>2022/11/01/2022-11-01-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given an unsorted integer array nums, return the smallest missing positive integer.<br>You must implement an algorithm that runs in O(n) time and uses constant extra space.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: nums = [1,2,0]<br>Output: 3<br>Explanation: The numbers in the range [1,2] are all in the array.</p><p>Example 2:<br>Input: nums = [3,4,-1,1]<br>Output: 2<br>Explanation: 1 is in the array but 2 is missing.</p><p>Example 3:<br>Input: nums = [7,8,9,11,12]<br>Output: 1<br>Explanation: The smallest positive integer 1 is missing.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= nums.length &lt;= 105<br>-231 &lt;= nums[i] &lt;= 231 - 1</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># runs in O(n) time and uses constant extra space.</span><br><span class="line">class Solution(object):</span><br><span class="line">    def firstMissingPositive(self, nums):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if len(nums) == 0:</span><br><span class="line">            return 1</span><br><span class="line">        </span><br><span class="line">        # mark the number from 0 to n</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            if nums[i] &lt;= 0:</span><br><span class="line">                nums[i] = len(nums) + 1</span><br><span class="line"></span><br><span class="line">        # mark the number that exists in nums</span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            num = abs(nums[i])</span><br><span class="line">            if num &lt;= len(nums):</span><br><span class="line">                nums[num - 1] = -abs(nums[num - 1])</span><br><span class="line">                </span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            if nums[i] &gt; 0:</span><br><span class="line">                return i + 1</span><br><span class="line">        </span><br><span class="line">        return len(nums) + 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># just do the same thing without using extra space</span><br><span class="line">class Solution2(object):</span><br><span class="line">    def firstMissingPositive(self, nums):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums: List[int]</span><br><span class="line">        :rtype: int</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if len(nums) == 0:</span><br><span class="line">            return 1</span><br><span class="line"></span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            n = nums[i]</span><br><span class="line">            while 0 &lt; n &lt;= len(nums) and n != nums[n-1]:</span><br><span class="line">                nums[i], nums[n-1] = nums[n-1], nums[i]</span><br><span class="line">                n = nums[i]</span><br><span class="line"></span><br><span class="line">        for i in range(len(nums)):</span><br><span class="line">            if nums[i] != i + 1:</span><br><span class="line">                return i + 1</span><br><span class="line">        return len(nums) + 1</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Test solution</span><br><span class="line">print(Solution().firstMissingPositive([1,2,0]))</span><br><span class="line">print(Solution().firstMissingPositive([3,4,-1,1]))</span><br><span class="line">print(Solution().firstMissingPositive([7,8,9,11,12]))</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to call a function multi-times with try and catch before raise error in Python</title>
      <link href="2022/10/28/2022-10-28-1/"/>
      <url>2022/10/28/2022-10-28-1/</url>
      
        <content type="html"><![CDATA[<p>Here is one task that we might be doing: scraping some webpages in a for loop. Then some error happens for one of the pages,<br>and then the whole process breaks, and we didn’t store results to disk yet. So we have to restart the process again.</p><p>Why don’t we just retry one of the steps multiple times, we may get it right at the second time?</p><p>Here is a function example to return the same task multiple times with try and catch block, before finally raise the error.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_try_function_call</span>(<span class="params"> num_retries = <span class="number">5</span> </span>):</span></span><br><span class="line">    <span class="keyword">for</span> attempt_no <span class="keyword">in</span> <span class="built_in">range</span>(num_retries):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&#x27;************* try time at <span class="subst">&#123;attempt_no&#125;</span>*************&#x27;</span>)</span><br><span class="line">            failure_chance =  random.random()</span><br><span class="line">                        </span><br><span class="line">            <span class="keyword">if</span> failure_chance&gt;<span class="number">0.6</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;there is going to be error&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>/<span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;no error&#x27;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>/<span class="number">1</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> error:</span><br><span class="line">            <span class="keyword">if</span> attempt_no &lt; (num_retries - <span class="number">1</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="built_in">str</span>(error))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">raise</span> error</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">multi_try_function_call()</span><br></pre></td></tr></table></figure><pre><code>************* try time at 0*************there is going to be errordivision by zero************* try time at 1*************there is going to be errordivision by zero************* try time at 2*************there is going to be errordivision by zero************* try time at 3*************there is going to be errordivision by zero************* try time at 4*************no error0.0</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Merge K sorted lists</title>
      <link href="2022/10/27/2022-10-27-1/"/>
      <url>2022/10/27/2022-10-27-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>You are given an array of k linked-lists lists, each linked-list is sorted in ascending order.<br>Merge all the linked-lists into one sorted linked-list and return it.</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: lists = [[1,4,5],[1,3,4],[2,6]]<br>Output: [1,1,2,3,4,4,5,6]<br>Explanation: The linked-lists are:<br>[<br>  1-&gt;4-&gt;5,<br>  1-&gt;3-&gt;4,<br>  2-&gt;6<br>]<br>merging them into one sorted list:<br>1-&gt;1-&gt;2-&gt;3-&gt;4-&gt;4-&gt;5-&gt;6</p><p>Example 2:<br>Input: lists = []<br>Output: []</p><p>Example 3:<br>Input: lists = [[]]<br>Output: []</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>k == lists.length<br>0 &lt;= k &lt;= 104<br>0 &lt;= lists[i].length &lt;= 500<br>-104 &lt;= lists[i][j] &lt;= 104<br>lists[i] is sorted in ascending order.<br>The sum of lists[i].length will not exceed 104.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Definition for singly-linked list.</span><br><span class="line">class ListNode(object):</span><br><span class="line">     def __init__(self, val=0, next=None):</span><br><span class="line">         self.val = val</span><br><span class="line">         self.next = next</span><br><span class="line">         </span><br><span class="line">class Solution(object):</span><br><span class="line">    def mergeKLists(self, lists):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type lists: List[ListNode]</span><br><span class="line">        :rtype: ListNode</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not lists:</span><br><span class="line">            return None</span><br><span class="line">        if len(lists) == 1:</span><br><span class="line">            return lists[0]</span><br><span class="line">        if len(lists) == 2:</span><br><span class="line">            return self.mergeTwoLists(lists[0], lists[1])</span><br><span class="line">        mid = len(lists) // 2</span><br><span class="line">        left = self.mergeKLists(lists[:mid])</span><br><span class="line">        right = self.mergeKLists(lists[mid:])</span><br><span class="line">        return self.mergeTwoLists(left, right)</span><br><span class="line">    </span><br><span class="line">    def mergeTwoLists(self, l1, l2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type l1: ListNode</span><br><span class="line">        :type l2: ListNode</span><br><span class="line">        :rtype: ListNode</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if not l1:</span><br><span class="line">            return l2</span><br><span class="line">        if not l2:</span><br><span class="line">            return l1</span><br><span class="line">        if l1.val &lt; l2.val:</span><br><span class="line">            l1.next = self.mergeTwoLists(l1.next, l2)</span><br><span class="line">            return l1</span><br><span class="line">        else:</span><br><span class="line">            l2.next = self.mergeTwoLists(l1, l2.next)</span><br><span class="line">            return l2</span><br><span class="line">            </span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = Solution()</span><br><span class="line">l1 = ListNode(1, ListNode(4, ListNode(5)))</span><br><span class="line">l2 = ListNode(1, ListNode(3, ListNode(4)))</span><br><span class="line">l3 = ListNode(2, ListNode(6))</span><br><span class="line">lists = [l1, l2, l3]</span><br><span class="line">result = s.mergeKLists(lists)</span><br><span class="line"></span><br><span class="line">#loop and print each element of the result</span><br><span class="line">while result:</span><br><span class="line">    print(result.val)</span><br><span class="line">    result = result.next</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Regular expression matching</title>
      <link href="2022/10/25/2022-10-25-1/"/>
      <url>2022/10/25/2022-10-25-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given an input string s and a pattern p, implement regular expression matching with support for ‘.’ and ‘*’ where:</p><ul><li>‘.’ Matches any single character.​​​​</li><li>‘*’ Matches zero or more of the preceding element.<br>The matching should cover the entire input string (not partial).</li></ul><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: s = “aa”, p = “a”<br>Output: false<br>Explanation: “a” does not match the entire string “aa”.</p><p>Example 2:<br>Input: s = “aa”, p = “a*”<br>Output: true<br>Explanation: ‘*’ means zero or more of the preceding element, ‘a’. Therefore, by repeating ‘a’ once, it becomes “aa”.</p><p>Example 3:<br>Input: s = “ab”, p = “.<em>“<br>Output: true<br>Explanation: “.</em>“ means “zero or more (*) of any character (.)”.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>1 &lt;= s.length &lt;= 20<br>1 &lt;= p.length &lt;= 30<br>s contains only lowercase English letters.<br>p contains only lowercase English letters, ‘.’, and ‘<em>‘.<br>It is guaranteed for each appearance of the character ‘</em>‘, there will be a previous valid character to match.</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def isMatch(self, s, p):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type s: str</span><br><span class="line">        :type p: str</span><br><span class="line">        :rtype: bool</span><br><span class="line">        &quot;&quot;&quot;        </span><br><span class="line">        if not p:</span><br><span class="line">            return not s</span><br><span class="line">        first_match = bool(s) and p[0] in &#123;s[0], &#x27;.&#x27;&#125;</span><br><span class="line">        if len(p) &gt;= 2 and p[1] == &#x27;*&#x27;:</span><br><span class="line">            return self.isMatch(s, p[2:]) or first_match and self.isMatch(s[1:], p)</span><br><span class="line">        else:</span><br><span class="line">            return first_match and self.isMatch(s[1:], p[1:])</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">s = &quot;aa&quot;</span><br><span class="line">p = &quot;a&quot;</span><br><span class="line">print(Solution().isMatch(s, p))</span><br><span class="line"></span><br><span class="line">s = &quot;aa&quot;</span><br><span class="line">p = &quot;a*&quot;</span><br><span class="line">print(Solution().isMatch(s, p))</span><br><span class="line"></span><br><span class="line">s = &quot;ab&quot;</span><br><span class="line">p = &quot;.*&quot;</span><br><span class="line">print(Solution().isMatch(s, p))</span><br><span class="line"></span><br><span class="line">s = &quot;aab&quot;</span><br><span class="line">p = &quot;c*a*b&quot;</span><br><span class="line">print(Solution().isMatch(s, p))</span><br><span class="line"></span><br><span class="line">s = &quot;mississippi&quot;</span><br><span class="line">p = &quot;mis*is*p*.&quot;</span><br><span class="line">print(Solution().isMatch(s, p))</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Find median of two sorted arrays</title>
      <link href="2022/10/24/2022-10-24-1/"/>
      <url>2022/10/24/2022-10-24-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Problem"><a href="#Problem" class="headerlink" title="Problem:"></a>Problem:</h2><p>Given two sorted arrays nums1 and nums2 of size m and n respectively, return the median of the two sorted arrays. </p><p>The overall run time complexity should be O(log (m+n)).</p><h2 id="Input-and-output-examples"><a href="#Input-and-output-examples" class="headerlink" title="Input and output examples"></a>Input and output examples</h2><p>Example 1:<br>Input: nums1 = [1,3], nums2 = [2]<br>Output: 2.00000<br>Explanation: merged array = [1,2,3] and median is 2.</p><p>Example 2:<br>Input: nums1 = [1,2], nums2 = [3,4]<br>Output: 2.50000<br>Explanation: merged array = [1,2,3,4] and median is (2 + 3) / 2 = 2.5.</p><h2 id="Constraints"><a href="#Constraints" class="headerlink" title="Constraints:"></a>Constraints:</h2><p>nums1.length == m<br>nums2.length == n<br>0 &lt;= m &lt;= 1000<br>0 &lt;= n &lt;= 1000<br>1 &lt;= m + n &lt;= 2000<br>-106 &lt;= nums1[i], nums2[i] &lt;= 106</p><h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution(object):</span><br><span class="line">    def findMedianSortedArrays(self, nums1, nums2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :type nums1: List[int]</span><br><span class="line">        :type nums2: List[int]</span><br><span class="line">        :rtype: float</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        nums = nums1 + nums2</span><br><span class="line">        nums.sort()</span><br><span class="line">        if len(nums) % 2 == 0:</span><br><span class="line">            return (nums[len(nums)//2] + nums[len(nums)//2 - 1])/2.0</span><br><span class="line">        else:</span><br><span class="line">            return nums[len(nums)//2]</span><br></pre></td></tr></table></figure><h3 id="Test-the-the-solution"><a href="#Test-the-the-solution" class="headerlink" title="Test the the solution:"></a>Test the the solution:</h3><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">nums1 = [1,2,5,]</span><br><span class="line">nums2 = [3,4,8,10]</span><br><span class="line">solution = Solution()</span><br><span class="line">result = solution.findMedianSortedArrays(nums1,nums2)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p>More interview questions can be found here:<br><a href="/categories/algorithm/">Algorithm</a></p>]]></content>
      
      
      <categories>
          
          <category> algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> leetcode </tag>
            
            <tag> interview questions </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Create secrets scopes and keys for databricks, and how to give scope access to someone else</title>
      <link href="2022/10/16/2022-10-16-1/"/>
      <url>2022/10/16/2022-10-16-1/</url>
      
        <content type="html"><![CDATA[<p>In previous blogs, we have discussed about how to install the databricks command line tool (CLI),<br>and how to add authentication to the databricks website host.</p><p>Steps to install databricks command line for both linux and windows system can be found here:<br><a href="https://www.datasciencebyexample.com/2022/10/11/2022-10-11-1/">https://www.datasciencebyexample.com/2022/10/11/2022-10-11-1/</a></p><p>Steps to add authentication to the databricks website host can be found here:<br><a href="https://www.datasciencebyexample.com/2022/10/12/2022-10-12-1/">https://www.datasciencebyexample.com/2022/10/12/2022-10-12-1/</a></p><p>Now back to how to create secrets scopes and keys uisng databricks command line tool.<br>Go to your terminal where you have installed the CLT.</p><h2 id="1-add-scope-in-databricks"><a href="#1-add-scope-in-databricks" class="headerlink" title="1. add scope in databricks"></a>1. add scope in databricks</h2><p>Step 1: Creating a New Scope<br>scope name that make sense to yourself, where token will live</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks secrets create-scope --scope &lt;scope-name&gt;</span><br></pre></td></tr></table></figure><p>Step 2: Pushing Token to Scope as the key for the scope</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks secrets put --scope &lt;scope-name&gt; --key &lt;key-name&gt;</span><br></pre></td></tr></table></figure><p>Step 3: Access token in Databricks Notebook</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dbutils.secrets.get(scope=&lt;scope-name&gt;, key=&lt;key-name&gt;)</span><br></pre></td></tr></table></figure><p>if you try to print out the key obtained by dbutils in the databricks notebook, it will be something that doesn’t make sense.<br>This is expected behavior to protect the key; Just define some variable to hold the key and use in later operations that will expect the key.</p><h2 id="2-Managing-Scope-Giving-Scope-Access-to-Someone-Else"><a href="#2-Managing-Scope-Giving-Scope-Access-to-Someone-Else" class="headerlink" title="2. Managing Scope - Giving Scope Access to Someone Else"></a>2. Managing Scope - Giving Scope Access to Someone Else</h2><p>check what users have access to scope:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks secrets list-acls --scope &lt;name-of-scope&gt;</span><br></pre></td></tr></table></figure><p>Now give access to other users:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks secrets put-acl --scope &lt;name-of-scope&gt; --principal &lt;email-of-user&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to authenticate databricks command line tool</title>
      <link href="2022/10/12/2022-10-12-1/"/>
      <url>2022/10/12/2022-10-12-1/</url>
      
        <content type="html"><![CDATA[<p>Once we have installed the databricks command line tool (CLI), we still need to add authentication to the databricks website host.</p><p>Steps to install databricks command line for both linux and windows system can be found here:<br><a href="https://www.datasciencebyexample.com/2022/10/11/2022-10-11-1/">https://www.datasciencebyexample.com/2022/10/11/2022-10-11-1/</a></p><p>First, To authenticate to the CLI you use a Databricks personal access token. In order to generate a personal token, go to the workspace and click user setting as the following:</p><p><img src="/content/images/2022-10-12-1.png"><br><img src="/content/images/2022-10-12-2.png"></p><p>Following the above screen shots, generated a token and saved it somewhere for later use.</p><p>Second, on your local comuter, type the following databricks command:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks configure --token</span><br></pre></td></tr></table></figure><p>The command begins by issuing the prompt:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Databricks Host (should begin with https://):</span><br></pre></td></tr></table></figure><p>Now enter your workspace URL, with the format https://<instance-name>.cloud.databricks.com.<br>The command continues by issuing the prompt to enter your personal access token you saved from the first step:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Token:</span><br></pre></td></tr></table></figure><p>After you complete the prompts, your access credentials are stored in the file ~/.databrickscfg on Unix, Linux, or macOS, or %USERPROFILE%.databrickscfg on Windows.<br>The file contains a default profile entry:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">host = &lt;workspace-URL&gt;</span><br><span class="line">token = &lt;personal-access-token&gt;</span><br></pre></td></tr></table></figure><p>Now you should be able to continue other work, such as create secret scope and key.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to install databricks command line tool in linux and windows</title>
      <link href="2022/10/11/2022-10-11-1/"/>
      <url>2022/10/11/2022-10-11-1/</url>
      
        <content type="html"><![CDATA[<p>Sometimes we need to interact with databricks command line to get work done, for example, create new secrets scope and key.</p><p>Install the databricks command line in linux or MAC OS is pretty easy:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install databricks-cli</span><br></pre></td></tr></table></figure><p>or if you want to install the latest version, do this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install databricks-cli --upgrade</span><br></pre></td></tr></table></figure><p>After the the package is installed, check to see if it’s working:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks --version</span><br></pre></td></tr></table></figure><p>Install the databricks command line in Windows will take longer time.<br>First, we still need to install the databricks-cli library, go to your python environment in windows, such as Anaconda:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install databricks-cli</span><br></pre></td></tr></table></figure><p>Now if we try the databricks command, it might complain with this error:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">databricks : The term &#x27;databricks&#x27; is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was</span><br><span class="line">included, verify that the path is correct and try again.</span><br></pre></td></tr></table></figure><p>This is because the command is not in the default system PATH. To add the databricks command in the system path in windows,<br>First observe where the databricks-cli library is installed, you might see this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Requirement already satisfied: databricks-cli in c:\users\&lt;yourusername&gt;\appdata\roaming\python\python38\site-packages (0.17.3)</span><br></pre></td></tr></table></figure><p>Or just run </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip --version</span><br></pre></td></tr></table></figure><p>You will see the pip path under windows; Usually the python package is installed under the same directory as pip.  However this is not true all the time,<br>so the safest method is still trying to figure out the path by checking the specific package you just installed.</p><p>Now, the databricks command installation path should be as the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Users\&lt;yourusername&gt;\AppData\Roaming\Python\Python38\Scripts\databricks.exe</span><br></pre></td></tr></table></figure><p>We can use the command with the whole path, or we could put the following path in the system path in windows:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">C:\Users\&lt;yourusername&gt;\AppData\Roaming\Python\Python38\Scripts\</span><br></pre></td></tr></table></figure><p>Search “Edit the system environment variable” in windows, and add the above path as a new variable under PATH:<br><img src="/content/images/2022-10-11-1.png"></p><p>After the above step, you can try this command again, and it should be working:</p><figure class="highlight plaintext"><figcaption><span>--version</span></figcaption><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to add watermark using Python</title>
      <link href="2022/10/08/2022-10-08-1/"/>
      <url>2022/10/08/2022-10-08-1/</url>
      
        <content type="html"><![CDATA[<p>The main idea of adding watermark with PIL library in Python is this：</p><p>(1) First create new empty image RGBA with the same size as original image but with transparent background (…, …, …, 0), e.g. a totally transparent black image (255,255,255,0).</p><p>(2) Next draw on this new image text with different transparency (e.g. 180 out of 255) and in different place. </p><p>(3) Finally use Image.alpha_composite(original_image, text_image) to put text on image with expected transparency.</p><h2 id="1-Put-a-watermark-text-in-the-center-of-an-image"><a href="#1-Put-a-watermark-text-in-the-center-of-an-image" class="headerlink" title="1. Put a watermark text in the center of an image"></a>1. Put a watermark text in the center of an image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"></span><br><span class="line">watermark = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- original image ---</span></span><br><span class="line">original_image = Image.<span class="built_in">open</span>(<span class="string">&#x27;lena.png&#x27;</span>).convert(<span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">original_image_size = original_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- watermarks image ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># image with the same size and transparent color (..., ..., ..., 0)</span></span><br><span class="line">watermarks_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, original_image_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">watermarks_draw = ImageDraw.Draw(watermarks_image)</span><br><span class="line"></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;arial.ttf&#x27;</span>, <span class="number">55</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate text size in pixels (width, height)</span></span><br><span class="line">text_size = font.getsize(watermark) </span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate top/left corner for centered text</span></span><br><span class="line">x = original_image_size[<span class="number">0</span>]//<span class="number">2</span> - text_size[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">y = original_image_size[<span class="number">1</span>]//<span class="number">2</span> - text_size[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># draw text </span></span><br><span class="line">watermarks_draw.text((x, y), watermark, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">180</span>), font=font)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- put watermarks image on original image ---</span></span><br><span class="line">combined_image = Image.alpha_composite(original_image, watermarks_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- result ---</span></span><br><span class="line">display(combined_image)</span><br><span class="line"><span class="comment"># save result to a new image</span></span><br><span class="line">combined_image.save(<span class="string">f&#x27;lena_watermark_1.png&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/content/images/lena_watermark_1.png"></p><h2 id="2-Put-multiple-watermark-texts-in-the-center-of-an-image"><a href="#2-Put-multiple-watermark-texts-in-the-center-of-an-image" class="headerlink" title="2. Put multiple watermark texts in the center of an image"></a>2. Put multiple watermark texts in the center of an image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"></span><br><span class="line">watermark = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line"><span class="comment"># --- original image ---</span></span><br><span class="line">original_image = Image.<span class="built_in">open</span>(<span class="string">&#x27;lena.png&#x27;</span>).convert(<span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">original_image_size = original_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- watermarks image ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># image with the same size and transparent color (..., ..., ..., 0)</span></span><br><span class="line">watermarks_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, original_image_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">watermarks_draw = ImageDraw.Draw(watermarks_image)</span><br><span class="line"></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;arial.ttf&#x27;</span>, <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate text size in pixels (width, height)</span></span><br><span class="line">text_size = font.getsize(watermark) </span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate top/left corner for centered text</span></span><br><span class="line">parts = <span class="number">8</span></span><br><span class="line">offset_x = original_image_size[<span class="number">0</span>]//parts</span><br><span class="line">offset_y = original_image_size[<span class="number">1</span>]//parts</span><br><span class="line"></span><br><span class="line">start_x = original_image_size[<span class="number">0</span>]//parts - text_size[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">start_y = original_image_size[<span class="number">1</span>]//parts - text_size[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, parts, <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, parts, <span class="number">2</span>):</span><br><span class="line">        x = start_x + a*offset_x</span><br><span class="line">        y = start_y + b*offset_y</span><br><span class="line">        watermarks_draw.text((x, y), watermark, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">200</span>), font=font)</span><br><span class="line">        </span><br><span class="line"><span class="comment"># --- put watermarks image on original image ---</span></span><br><span class="line"></span><br><span class="line">combined_image = Image.alpha_composite(original_image, watermarks_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- result ---</span></span><br><span class="line"></span><br><span class="line">display(combined_image)</span><br><span class="line">combined_image.save(<span class="string">f&#x27;lena_watermark_2.png&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/content/images/lena_watermark_2.png"></p><h2 id="3-Put-a-rotated-watermark-text-in-the-center-of-an-image"><a href="#3-Put-a-rotated-watermark-text-in-the-center-of-an-image" class="headerlink" title="3. Put a rotated watermark text in the center of an image"></a>3. Put a rotated watermark text in the center of an image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">watermark = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line"><span class="comment"># --- original image ---</span></span><br><span class="line">original_image = Image.<span class="built_in">open</span>(<span class="string">&#x27;lena.png&#x27;</span>).convert(<span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">original_image_size = original_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- text image ---</span></span><br><span class="line"></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;arial.ttf&#x27;</span>, <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate text size in pixels (width, height)</span></span><br><span class="line">text_size = font.getsize(watermark) </span><br><span class="line"></span><br><span class="line"><span class="comment"># create image for text</span></span><br><span class="line">text_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, text_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">text_draw = ImageDraw.Draw(text_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw text on image</span></span><br><span class="line">text_draw.text((<span class="number">0</span>, <span class="number">0</span>), watermark, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">129</span>), font=font)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rotate text image and fill with transparent color</span></span><br><span class="line">rotated_text_image = text_image.rotate(<span class="number">45</span>, expand=<span class="literal">True</span>, fillcolor=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">rotated_text_image_size = rotated_text_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment">#rotated_text_image.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- watermarks image ---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># image with the same size and transparent color (..., ..., ..., 0)</span></span><br><span class="line">watermarks_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, original_image_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate top/left corner for centered text</span></span><br><span class="line">x = original_image_size[<span class="number">0</span>]//<span class="number">2</span> - rotated_text_image_size[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">y = original_image_size[<span class="number">1</span>]//<span class="number">2</span> - rotated_text_image_size[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># put text on watermarks image</span></span><br><span class="line">watermarks_image.paste(rotated_text_image, (x, y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- put watermarks image on original image ---</span></span><br><span class="line"></span><br><span class="line">combined_image = Image.alpha_composite(original_image, watermarks_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- result ---</span></span><br><span class="line"></span><br><span class="line">display(combined_image)</span><br><span class="line">combined_image.save(<span class="string">f&#x27;lena_watermark_3.png&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/content/images/lena_watermark_3.png"></p><h2 id="4-Put-multiple-rotated-watermark-text-in-the-center-of-an-image"><a href="#4-Put-multiple-rotated-watermark-text-in-the-center-of-an-image" class="headerlink" title="4. Put multiple rotated watermark text in the center of an image"></a>4. Put multiple rotated watermark text in the center of an image</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image, ImageDraw, ImageFont</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">watermark = <span class="string">&#x27;demo&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- original image ---</span></span><br><span class="line">original_image = Image.<span class="built_in">open</span>(<span class="string">&#x27;lena.png&#x27;</span>).convert(<span class="string">&quot;RGBA&quot;</span>)</span><br><span class="line">original_image_size = original_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- text image ---</span></span><br><span class="line"></span><br><span class="line">font = ImageFont.truetype(<span class="string">&#x27;arial.ttf&#x27;</span>, <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate text size in pixels (width, height)</span></span><br><span class="line">text_size = font.getsize(watermark) </span><br><span class="line"></span><br><span class="line"><span class="comment"># create image for text</span></span><br><span class="line">text_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, text_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">text_draw = ImageDraw.Draw(text_image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># draw text on image</span></span><br><span class="line">text_draw.text((<span class="number">0</span>, <span class="number">0</span>), watermark, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>, <span class="number">129</span>), font=font)</span><br><span class="line"></span><br><span class="line"><span class="comment"># rotate text image and fill with transparent color</span></span><br><span class="line">rotated_text_image = text_image.rotate(<span class="number">45</span>, expand=<span class="literal">True</span>, fillcolor=(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">rotated_text_image_size = rotated_text_image.size</span><br><span class="line"></span><br><span class="line"><span class="comment">#rotated_text_image.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- watermarks image ---</span></span><br><span class="line"></span><br><span class="line">combined_image = original_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># calculate top/left corner for centered text</span></span><br><span class="line">parts = <span class="number">8</span></span><br><span class="line">offset_x = original_image_size[<span class="number">0</span>]//parts</span><br><span class="line">offset_y = original_image_size[<span class="number">1</span>]//parts</span><br><span class="line"></span><br><span class="line">start_x = original_image_size[<span class="number">0</span>]//parts - rotated_text_image_size[<span class="number">0</span>]//<span class="number">2</span></span><br><span class="line">start_y = original_image_size[<span class="number">1</span>]//parts - rotated_text_image_size[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> a <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, parts, <span class="number">2</span>):</span><br><span class="line">    <span class="keyword">for</span> b <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, parts, <span class="number">2</span>):</span><br><span class="line">        x = start_x + a*offset_x</span><br><span class="line">        y = start_y + b*offset_y</span><br><span class="line">        <span class="comment"># image with the same size and transparent color (..., ..., ..., 0)</span></span><br><span class="line">        watermarks_image = Image.new(<span class="string">&#x27;RGBA&#x27;</span>, original_image_size, (<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>,<span class="number">0</span>))</span><br><span class="line">        <span class="comment"># put text in expected place on watermarks image</span></span><br><span class="line">        watermarks_image.paste(rotated_text_image, (x, y))</span><br><span class="line">        <span class="comment"># put watermarks image on original image</span></span><br><span class="line">        combined_image = Image.alpha_composite(combined_image, watermarks_image)</span><br><span class="line">        </span><br><span class="line"><span class="comment">#combined_image.show()</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- result ---</span></span><br><span class="line"></span><br><span class="line">display(combined_image)</span><br><span class="line">combined_image.save(<span class="string">f&#x27;lena_watermark_4.png&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="/content/images/lena_watermark_4.png"></p>]]></content>
      
      
      <categories>
          
          <category> image processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> PIL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to make AWS S3 buckets public through bucket policy</title>
      <link href="2022/10/01/2022-10-01-1/"/>
      <url>2022/10/01/2022-10-01-1/</url>
      
        <content type="html"><![CDATA[<p>The first step is to create a S3 buckets, here is how we can create one on AWS website through UI:<br><img src="/content/images/2022-10-01-1.png" alt="create S3 bucket"></p><p>After clicking the “create bucket”, in the following configuration page, give it a name.<br>Notice that the bucket name should be unique among all AWS buckets, not only unique to your own buckets.<br>We call it “datasciencebyexample-demo” here.</p><p>If we want to make part or all of the contents in this bucket public, unlick the “Block all public access”<br>option in this step as the following screenshot:<br><img src="/content/images/2022-10-01-2.png" alt="allow public access in S3"></p><p>Clicking into this new bucket, and let’s create one new folder called “test”:<br><img src="/content/images/2022-10-01-3.png" alt="create folder in S3"></p><p>Get into the test folder, and upload some files, for example, we uploaded a file called text.txt.<br>But if we open the url of this just uploaded file in the bucket, you will find the file is still not publicly accessible.<br><img src="/content/images/2022-10-01-4.png"></p><p>Why is that? The previous step we did actually only make this bucket “Objects can be public”.<br>We still need to add bucket policy to make all of the bucket or part of bucket contents really public.</p><p>Now go back to the bucket level, find Permission tab, and click the Edit button under bucket policy:<br><img src="/content/images/2022-10-01-5.png"></p><p>If we want to make all objects under this bucket to be public, put the following policy statement in the text box:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Principal&quot;: &quot;*&quot;,</span><br><span class="line">            &quot;Action&quot;: &quot;s3:GetObject&quot;,</span><br><span class="line">            &quot;Resource&quot;: &quot;arn:aws:s3:::datasciencebyexample-demo/*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>If we only want to make objects under the “test” directory to be public, put the following policy statement in the text box:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;Version&quot;: &quot;2012-10-17&quot;,</span><br><span class="line">    &quot;Statement&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;Effect&quot;: &quot;Allow&quot;,</span><br><span class="line">            &quot;Principal&quot;: &quot;*&quot;,</span><br><span class="line">            &quot;Action&quot;: &quot;s3:GetObject&quot;,</span><br><span class="line">            &quot;Resource&quot;: &quot;arn:aws:s3:::datasciencebyexample-demo/test/*&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>After saving the bucket policy, you will find your file urls are now public!</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> s3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Human&#39;s first asteroid impact experiment, NASA Double Asteroid Redirection Test (DART)</title>
      <link href="2022/09/28/2022-09-28-1/"/>
      <url>2022/09/28/2022-09-28-1/</url>
      
        <content type="html"><![CDATA[<p>Dinosaurs would dream of getting this capability.</p><p>In NASA’s “Double Asteroid Redirection Test (DART)”, an unmanned spacecraft launched from Earth successfully hit an asteroid,<br>which NASA says is the world’s first “planet” Defense” test. At the time of impact, they were relatively close to Earth—just under 11 million kilometers.</p><p>Dimorphos is an asteroid moon orbiting the near-Earth asteroid Didymos, which astronomers discovered more than two decades ago, meaning “twin” in Greek.<br>Didymos is about 780 meters wide, and the Dimorphos running around it have a diameter of 160 meters, and its name means “two forms”<br><img src="/content/images/2022-09-28-1.png" alt="DART size comparison"></p><p>Despite the small target chosen, the fast impact would change only 1% of the speed at which Dimorphos orbits Didymos, which doesn’t sound like much,<br>but it’s enough change to be seen through a telescope.</p><p>According to its original orbit, Dimorphos circles Didymos every 11 hours and 55 minutes.<br>It is expected that the period may change to 11 hours and 45 minutes after the impact.<br>DART team members say it will take scientists about two months to determine whether the asteroid’s orbit has changed.</p><p><img src="/content/images/2022-09-28-2.png" alt="orbit change expected for DART"></p>]]></content>
      
      
      <categories>
          
          <category> physics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DART </tag>
            
            <tag> Asteroid </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OpenCV and Pillow color code order difference, BGR vs RGB</title>
      <link href="2022/09/19/2022-09-19-1/"/>
      <url>2022/09/19/2022-09-19-1/</url>
      
        <content type="html"><![CDATA[<p>We can use opencv to read in a video, and extract each frame as an image.  Then we can use Python Image Library to further analyze or edit<br>the image. But when you open the image, you might find it the color seems totally wrong.</p><p>This is just because opencv and Python Image Library (PIL) use different order of color code. The order in opencv is BGR, while in PIL it’s RGB. Fortunately, it’s easy to correct it as you will find in the following example</p><h2 id="Read-a-short-video-of-Robin-bird-using-opencv"><a href="#Read-a-short-video-of-Robin-bird-using-opencv" class="headerlink" title="Read a short video of Robin bird using opencv"></a>Read a short video of Robin bird using opencv</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">vidcap = cv2.VideoCapture(<span class="string">&#x27;robin.mp4&#x27;</span>)</span><br><span class="line"></span><br><span class="line">success = <span class="literal">True</span></span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> success <span class="keyword">and</span> count&lt;<span class="number">100</span>:</span><br><span class="line">    success,image = vidcap.read()</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># we got the 100th frame in the video as an image array</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Now we use PIL to view the image, and find the Robin in blue color. If you checked the original video, or you have seen a Robin before,<br>you know there is something wrong.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">display(Image.fromarray(image))</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-09-19-1.png"></p><h2 id="Switch-from-BGR-to-RGB-using-opencv"><a href="#Switch-from-BGR-to-RGB-using-opencv" class="headerlink" title="Switch from BGR to RGB using opencv"></a>Switch from BGR to RGB using opencv</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)</span><br></pre></td></tr></table></figure><p>Now we get the correct image of Robin, much better!</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display(Image.fromarray(image_rgb))</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-09-19-2.png"></p><h2 id="code-link-in-github"><a href="#code-link-in-github" class="headerlink" title="code link in github"></a>code link in github</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/opencv-and-pillow-color-order-difference/2022-09-20-opencv-and-pillow-channel-difference.ipynb">https://github.com/robotlearner001/blog/blob/main/opencv-and-pillow-color-order-difference/2022-09-20-opencv-and-pillow-channel-difference.ipynb</a></p>]]></content>
      
      
      <categories>
          
          <category> image processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PIL </tag>
            
            <tag> image editing </tag>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to remove part of an image using Python</title>
      <link href="2022/09/18/2022-09-18-1/"/>
      <url>2022/09/18/2022-09-18-1/</url>
      
        <content type="html"><![CDATA[<p>What does remove part of an image mean? mathematically and programmatically, most of the times, it means convert the array for corresponding area of the image to be<br>(0,0,0) for the RGB channel value,  or a black color.</p><p>Removing part of an image is useful in many situations. For example, in the text to image application, using Dallie2,<br>we can edit part of the image with inpainting. </p><h2 id="Image-processing-using-Python-Image-Library"><a href="#Image-processing-using-Python-Image-Library" class="headerlink" title="Image processing using Python Image Library"></a>Image processing using Python Image Library</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np      </span><br><span class="line">  </span><br><span class="line"><span class="comment"># Opening the image and converting </span></span><br><span class="line"><span class="comment"># it to RGB color mode</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(<span class="string">&quot;lena.png&quot;</span>).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line"></span><br><span class="line">display(img)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-09-18-1.png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Extracting the image data &amp;</span></span><br><span class="line"><span class="comment"># creating an numpy array out of it</span></span><br><span class="line">img_arr = np.array(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the pixel size of this image</span></span><br><span class="line"><span class="built_in">print</span>(img_arr.shape)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check some of the values</span></span><br><span class="line">img_arr</span><br></pre></td></tr></table></figure><pre><code>(225, 400, 3)array([[[108,  73,  53],        [109,  72,  54],        [109,  67,  51],        ...,        [ 56,  41,  44],        [ 62,  47,  50],        [ 70,  54,  57]],       [[110,  75,  55],        [110,  73,  55],        [109,  67,  51],        ...,        [ 54,  39,  42],        [ 57,  42,  45],        [ 64,  48,  51]],       [[109,  74,  55],        [110,  73,  55],        [107,  68,  53],        ...,        [ 51,  39,  41],        [ 56,  41,  44],        [ 61,  46,  49]],       ...,       [[130,  75,  54],        [128,  74,  50],        [130,  78,  56],        ...,        [123,  75,  61],        [122,  74,  60],        [128,  80,  66]],       [[129,  72,  52],        [127,  70,  50],        [131,  77,  53],        ...,        [126,  78,  64],        [122,  74,  60],        [126,  78,  64]],       [[134,  72,  49],        [133,  72,  51],        [126,  71,  51],        ...,        [140,  87,  71],        [135,  79,  64],        [140,  84,  67]]], dtype=uint8)</code></pre><p>from the the above results, we can see that, the image has 225 (y direction) times 400 (x direction) pixels; At each pixel, there is a tuple of size 3 represents the RGB color. </p><p>One thing to be careful is the channel difference between RGB and RGBA. If you take a snapshot of the screen. The pixels inside the bounding box are returned as an “RGB” image on Windows or “RGBA” on macOS. For RGB channel, it needs 24 bits to encode the pixel, while for RGBA, it needs 32 bits to encode the pixel.</p><p>If the original image has RGB channel, but we need RGBA channel, we can use the following function to convert the image from RGB channel to RGBA channel, or vice versa.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">img_rgba = Image.<span class="built_in">open</span>(<span class="string">&quot;lena.jpg&quot;</span>).convert(<span class="string">&#x27;RGBA&#x27;</span>)</span><br><span class="line">img_arr_rgba = np.array(img_rgba)</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the pixel size of this new image</span></span><br><span class="line"><span class="built_in">print</span>(img_arr_rgba.shape)</span><br></pre></td></tr></table></figure><pre><code>(225, 400, 4)</code></pre><p>Let us get back to our task of removing parts of the image as the following</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  </span><br><span class="line"><span class="comment"># Turning part of the the pixel values of to be black</span></span><br><span class="line"><span class="comment"># the order is  ymin: ymax,  xmin:xmax</span></span><br><span class="line">img_arr[<span class="number">150</span> : <span class="number">225</span>, <span class="number">0</span> : <span class="number">100</span>] = (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creating an image out of the previously modified array</span></span><br><span class="line">img_edit = Image.fromarray(img_arr)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Displaying the image</span></span><br><span class="line">display(img_edit)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-09-18-2.png"></p><p>Now you can see the left corner of the image is black now.</p><h2 id="code-link-in-github"><a href="#code-link-in-github" class="headerlink" title="code link in github"></a>code link in github</h2><p><a href="https://github.com/robotlearner001/blog/blob/main/remove-part-of-image-using-python/2022-09-17-remove-part-of-image-using-python.ipynb">https://github.com/robotlearner001/blog/blob/main/remove-part-of-image-using-python/2022-09-17-remove-part-of-image-using-python.ipynb</a></p>]]></content>
      
      
      <categories>
          
          <category> image processing </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PIL </tag>
            
            <tag> image editing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ways to make your Pandas operatoins 100 times faster</title>
      <link href="2022/09/10/2022-09-10-1/"/>
      <url>2022/09/10/2022-09-10-1/</url>
      
        <content type="html"><![CDATA[<p>Today we discusses three ways to speed up operations in the Python data analysis library Pandas. Pandas is useful for working with tabular data stored in spreadsheets and databases. It provides many functions for manipulating and transforming dataframes, or structural data.</p><h2 id="Method-1-using-itertuples-to-iterate-over-dataframe-rows"><a href="#Method-1-using-itertuples-to-iterate-over-dataframe-rows" class="headerlink" title="Method 1: using itertuples() to iterate over dataframe rows"></a>Method 1: using itertuples() to iterate over dataframe rows</h2><p>explain: There are many places that we need to iterate rows of a dataframe and do some manipulations.  Naively, we just check each row of data without thinking too much, here we use a simple sum operations to show the performance change</p><h3 id="generate-a-dataframe-from-random-numbers"><a href="#generate-a-dataframe-from-random-numbers" class="headerlink" title="generate a dataframe from random numbers"></a>generate a dataframe from random numbers</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;a&quot;</span>: [random.randint(<span class="number">0</span>,<span class="number">100</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>)],</span><br><span class="line">    <span class="string">&quot;b&quot;</span>: [random.randint(<span class="number">100</span>,<span class="number">200</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100000</span>)],</span><br><span class="line">&#125;</span><br><span class="line">)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>a</th>      <th>b</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>17</td>      <td>161</td>    </tr>    <tr>      <th>1</th>      <td>77</td>      <td>150</td>    </tr>    <tr>      <th>2</th>      <td>30</td>      <td>121</td>    </tr>    <tr>      <th>3</th>      <td>18</td>      <td>130</td>    </tr>    <tr>      <th>4</th>      <td>31</td>      <td>178</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>99995</th>      <td>5</td>      <td>183</td>    </tr>    <tr>      <th>99996</th>      <td>46</td>      <td>138</td>    </tr>    <tr>      <th>99997</th>      <td>9</td>      <td>133</td>    </tr>    <tr>      <th>99998</th>      <td>25</td>      <td>162</td>    </tr>    <tr>      <th>99999</th>      <td>98</td>      <td>144</td>    </tr>  </tbody></table><p>100000 rows × 2 columns</p></div><h3 id="situation-1-naive-loop"><a href="#situation-1-naive-loop" class="headerlink" title="situation 1:  naive loop"></a>situation 1:  naive loop</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(df)):</span><br><span class="line">    results.append(df.iloc[i][<span class="string">&#x27;a&#x27;</span>]+df.iloc[i][<span class="string">&#x27;b&#x27;</span>])</span><br></pre></td></tr></table></figure><pre><code>11.9 s ± 297 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><h3 id="situation-2-using-iterrows"><a href="#situation-2-using-iterrows" class="headerlink" title="situation 2: using iterrows()"></a>situation 2: using iterrows()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> index, row <span class="keyword">in</span> df.iterrows():</span><br><span class="line">    results.append(row[<span class="string">&#x27;a&#x27;</span>]+row[<span class="string">&#x27;b&#x27;</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>3.61 s ± 156 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><h3 id="situation-3-using-itertuples"><a href="#situation-3-using-itertuples" class="headerlink" title="situation 3: using itertuples()"></a>situation 3: using itertuples()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples():</span><br><span class="line">    results.append(row.a+row.b)</span><br></pre></td></tr></table></figure><pre><code>69.4 ms ± 3.16 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><p>conclusion:  using itertuples() is 52 times faster than iterrows(), and  171 times than naive looping!</p><h2 id="Method-2-how-to-create-new-dataframe-efficiently"><a href="#Method-2-how-to-create-new-dataframe-efficiently" class="headerlink" title="Method 2: how to create new dataframe efficiently"></a>Method 2: how to create new dataframe efficiently</h2><p>explain: when we need to make a new dataframe and add a new column to it, for example, a sum of two existing columns</p><h3 id="situation-1-get-data-list-then-make-a-new-dataframe-from-the-data"><a href="#situation-1-get-data-list-then-make-a-new-dataframe-from-the-data" class="headerlink" title="situation 1: get data list, then make a new dataframe from the data"></a>situation 1: get data list, then make a new dataframe from the data</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">results = []</span><br><span class="line"><span class="keyword">for</span> row <span class="keyword">in</span> df.itertuples():</span><br><span class="line">    results.append( (row.a, row.b, row.a+row.b) )</span><br><span class="line">    </span><br><span class="line">new_df = pd.DataFrame(data=results)    </span><br></pre></td></tr></table></figure><pre><code>133 ms ± 2.43 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)</code></pre><h3 id="situation-2-copy-the-old-dataframe-to-a-new-one-then-make-new-column-using-apply-function"><a href="#situation-2-copy-the-old-dataframe-to-a-new-one-then-make-new-column-using-apply-function" class="headerlink" title="situation 2:  copy the old dataframe to a new one, then make new column using apply() function"></a>situation 2:  copy the old dataframe to a new one, then make new column using apply() function</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">new_df = df.copy()</span><br><span class="line"></span><br><span class="line">new_df[<span class="string">&#x27;c&#x27;</span>] = new_df.apply(<span class="keyword">lambda</span> row: row[<span class="string">&#x27;a&#x27;</span>]+row[<span class="string">&#x27;b&#x27;</span>],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><pre><code>988 ms ± 106 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><h3 id="situation-3-copy-the-old-dataframe-to-a-new-one-then-make-new-column-using-native-dataframe-vectorized-operation"><a href="#situation-3-copy-the-old-dataframe-to-a-new-one-then-make-new-column-using-native-dataframe-vectorized-operation" class="headerlink" title="situation 3:  copy the old dataframe to a new one, then make new column using native dataframe vectorized operation"></a>situation 3:  copy the old dataframe to a new one, then make new column using native dataframe vectorized operation</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line">new_df = df.copy()</span><br><span class="line"></span><br><span class="line">new_df[<span class="string">&#x27;c&#x27;</span>] = new_df[<span class="string">&#x27;a&#x27;</span>]+new_df[<span class="string">&#x27;b&#x27;</span>]</span><br></pre></td></tr></table></figure><pre><code>956 µs ± 14 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)</code></pre><p>conclusion:  if we can leverage native df operations without for looping, it’s the best, could be 1000 times faster; if we have complex<br>manipulation, making new data then create dataframe out of it, might be a better choice </p><h2 id="Method-3-how-to-use-apply-function-efficiently"><a href="#Method-3-how-to-use-apply-function-efficiently" class="headerlink" title="Method 3: how to use apply() function efficiently"></a>Method 3: how to use apply() function efficiently</h2><p>under the hood, the popular apply() function is a for loop with some overhead; when possible, we can leverage benefits of vectorized operations;  we take a conditional multiplication as an example here</p><h3 id="situation-1-use-apply-directly"><a href="#situation-1-use-apply-directly" class="headerlink" title="situation 1:  use apply() directly"></a>situation 1:  use apply() directly</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">condition_multi</span>(<span class="params">a</span>):</span></span><br><span class="line">    <span class="keyword">if</span> a&gt;<span class="number">30</span>:</span><br><span class="line">        <span class="keyword">return</span> a*<span class="number">2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> a*<span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">df[<span class="string">&#x27;c&#x27;</span>] = df[<span class="string">&#x27;a&#x27;</span>].apply(condition_multi)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>33.4 ms ± 2.41 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><h3 id="situation-2-leverage-NumPy-operations"><a href="#situation-2-leverage-NumPy-operations" class="headerlink" title="situation 2: leverage NumPy operations"></a>situation 2: leverage NumPy operations</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%%timeit</span><br><span class="line"></span><br><span class="line">df[<span class="string">&#x27;c&#x27;</span>] = np.where(df[<span class="string">&#x27;a&#x27;</span>]&gt;<span class="number">30</span>, df[<span class="string">&#x27;a&#x27;</span>]*<span class="number">2</span>,df[<span class="string">&#x27;a&#x27;</span>]*<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>1.18 ms ± 304 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)</code></pre><p>conclusion: using numpy makes it 28 times faster!</p><h2 id="Link-to-the-code"><a href="#Link-to-the-code" class="headerlink" title="Link to the code:"></a>Link to the code:</h2><p><a href="https://github.com/robotlearner001/blog/tree/main/Make-pandas-100-times-faster">https://github.com/robotlearner001/blog/tree/main/Make-pandas-100-times-faster</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Be an AWS lambda expert No.3 - How to handle CORS errors when calling AWS API Gateway endpoint from frontend</title>
      <link href="2022/09/03/2022-09-03-1/"/>
      <url>2022/09/03/2022-09-03-1/</url>
      
        <content type="html"><![CDATA[<p>We have developed a AWS lambda function, then we put it online by using AWS API gateway in our previous blogs:<br><a href="https://www.datasciencebyexample.com/2022/09/01/2022-09-01-1/">https://www.datasciencebyexample.com/2022/09/01/2022-09-01-1/</a><br><a href="https://www.datasciencebyexample.com/2022/09/02/2022-09-02-1/">https://www.datasciencebyexample.com/2022/09/02/2022-09-02-1/</a>  </p><p>We could also test the API and verify it is working as what we expect, things really look awesome.</p><p>So we are ready to send the API endpoint to our frontend and call it from the website.</p><p>As soon as we did that, we found some CORS errors like this:</p><p>Error message:<br><em>Access to XMLHttpRequest at ‘<a href="https://xxx.execute-api.us-east-1.amazonaws.com/prod/&#39;">https://xxx.execute-api.us-east-1.amazonaws.com/prod/&#39;</a> from origin ‘<a href="https://www.datasciencebyexample.com&/#39;">https://www.datasciencebyexample.com&#39;</a> has been blocked by CORS policy: No ‘Access-Control-Allow-Origin’ header is present on the requested resource.</em></p><p>The CORS error stands for Cross-Origin Resource Sharing (CORS) error. By default, the server side and web side can’t be talking from different domains.<br>So we need to relax this requirement from both sides. </p><h2 id="Enable-CORS-on-AWS-API-gateway"><a href="#Enable-CORS-on-AWS-API-gateway" class="headerlink" title="Enable CORS on AWS API gateway"></a>Enable CORS on AWS API gateway</h2><p>First, on the server side, we need to make sure the response from the server side, i.e. the API, should have the header including ‘Access-Control-Allow-Origin’ key, to tell which domain is allowed to call the API. We can put specific domain to be the value for the value, or ‘*’ to stand for all sites are allowed. Notice that this header is provided by the API side, not from the frontend side.</p><p>With AWS API gateway, one can check the official guide here,<br><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html</a>, or just follow the following steps:</p><ol><li><p>Sign in to the API Gateway console at <a href="https://console.aws.amazon.com/apigateway">https://console.aws.amazon.com/apigateway</a>.</p></li><li><p>In the API Gateway console, choose an API under APIs.</p></li><li><p>Choose a resource under Resources. This will enable CORS for all the methods on the resource. Alternatively, you could choose a method under the resource to enable CORS for just this method.</p></li><li><p>Choose Enable CORS from the Actions drop-down menu.<br>and Choose Enable CORS</p></li><li><p>In the Enable CORS form, don’t need to change anyting, just use the default suggestions, basicaly they are:</p><ol><li><p>In the Access-Control-Allow-Headers input field, type a static string of a comma-separated list of headers that the client must submit in the actual request of the resource. Use the console-provided header list of Content-Type,X-Amz-Date,Authorization,X-Api-Key,X-Amz-Security-Token or specify your own headers.</p></li><li><p>Use the console-provided value of ‘*’ as the Access-Control-Allow-Origin header value to allow access requests from all domains, or specify a named domain to all access requests from the specified domain.</p></li></ol><p> Choose Enable CORS and replace existing CORS headers.</p></li><li><p>Last but not least, remember to deploy the API again, otherwise it might not take effect.</p></li></ol><p>Here is screen shot of what described in the above steps:<br><img src="/content/images/2022-09-03-1.png"></p><p>We can check if the CORS setting is working by calling the API using requests library in python, and print out the headers from response:<br><img src="/content/images/2022-09-03-2.png"></p><h2 id="Calling-API-using-javascript-fetch"><a href="#Calling-API-using-javascript-fetch" class="headerlink" title="Calling API using javascript fetch"></a>Calling API using javascript fetch</h2><p>After verifing the header has the “Access-Control-Allow-Origin” key and value from the response header, we are sure the CORS is all set up on the server side.<br>To call the endpoint on the frontend, we don’t need to much work, just find the method that you like. </p><p>Here is an example of calling the POST endpoint using fetch in javascript:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">function CORSexample(input) &#123;</span><br><span class="line"> </span><br><span class="line">    // call api</span><br><span class="line">    // POST request using fetch()</span><br><span class="line">fetch(&quot;https://xxx.execute-api.us-east-1.amazonaws.com/prod/&quot;, &#123;</span><br><span class="line">method: &quot;POST&quot;,</span><br><span class="line">// Adding body or contents to send</span><br><span class="line">body: JSON.stringify(&#123;</span><br><span class="line">&quot;data&quot;:&#123;&quot;query&quot;:input&#125;</span><br><span class="line">&#125;),</span><br><span class="line"> </span><br><span class="line">// Adding headers to the request if necessary</span><br><span class="line">headers: &#123;</span><br><span class="line">&quot;Content-type&quot;: &quot;application/json; charset=UTF-8&quot;</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line"> </span><br><span class="line">// Converting result to JSON</span><br><span class="line">.then(response =&gt; response.json())</span><br><span class="line"> </span><br><span class="line">.then(function(result)</span><br><span class="line">  &#123;</span><br><span class="line">   console.log(result)</span><br><span class="line">   // in some older jquery version, one has to parse the json to be object</span><br><span class="line">   // it&#x27;s not necessary in new versions, just try it or not</span><br><span class="line">   //var result = $.parseJSON(result)</span><br><span class="line">  </span><br><span class="line">   // continue other custome stuff you have </span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line">&#125;).catch(error =&gt; console.error(&#x27;Error:&#x27;, error)); </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> CORS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Be an AWS lambda expert No.2 - How to expose lambda function as API endpoint using API Gateway service</title>
      <link href="2022/09/02/2022-09-02-1/"/>
      <url>2022/09/02/2022-09-02-1/</url>
      
        <content type="html"><![CDATA[<p>We have created a lambda function using custom layer in the previous blog:<br><a href="https://www.datasciencebyexample.com/2022/09/01/2022-09-01-1/">https://www.datasciencebyexample.com/2022/09/01/2022-09-01-1/</a></p><p>In order to use it, we have to expose it as an API endpoint using the API Gateway service.<br>First search the “API Gateway” in your AWS console like this:<br><img src="/content/images/2022-09-02-1.png"></p><p>Then click the “Create API” button like this:<br><img src="/content/images/2022-09-02-2.png"></p><p>Of the list of options, we can choose to build a REST API:<br><img src="/content/images/2022-09-02-3.png"></p><p>For “choose the protocal”, give the API a name and some description, and all the other options can be default:<br><img src="/content/images/2022-09-02-4.png"></p><p>Now we are ready to create a method as the following:<br><img src="/content/images/2022-09-02-5.png"></p><p>and we will choose to create a “POST” method as an example like this:<br><img src="/content/images/2022-09-02-6.png"></p><p>Now we are ready to integrate the lambda function we set up in the previous blog named as “gpt3_lambda” with this POST method as the following,<br>where we begin to type in the “Lambda Function” black space, and would see the options to choose the lambda function list:<br><img src="/content/images/2022-09-02-7.png"></p><p>As a last step, we should click “Actions” and choose the “Deploy API” from the drop down list:<br><img src="/content/images/2022-09-02-8.png"></p><p>Choose “[new state]” for the Deployment Stage, and give a name to that stage, such as “prod”, then hit the deploy button:<br><img src="/content/images/2022-09-02-9.png"></p><p>Now we will be successful, and just copy the url form the following screen shot, and use it somewhere!<br><img src="/content/images/2022-09-02-10.png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> lambda function </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Be an AWS lambda expert No.1 - How to install python packages by adding custom layer to lambda function easliy</title>
      <link href="2022/09/01/2022-09-01-1/"/>
      <url>2022/09/01/2022-09-01-1/</url>
      
        <content type="html"><![CDATA[<p>AWS lambda function is an agile way to implement serverless API endpoint; It is also quite easy to spin up some python models as long as the model is<br>not very complex. </p><h2 id="1-Try-adding-existing-AWS-layers-to-see-if-it-satisfies-your-needs"><a href="#1-Try-adding-existing-AWS-layers-to-see-if-it-satisfies-your-needs" class="headerlink" title="1. Try adding existing AWS layers to see if it satisfies your needs"></a>1. Try adding existing AWS layers to see if it satisfies your needs</h2><p>To include some very common pythons packages, such as pandas, numpy and requests,<br>you can add the aws layer called “AWSDataWranger”.</p><p>The following screen shows how to add it after you create the lambda function, here we are using the python3.9 environment.</p><p><img src="/content/images/2022-09-01-1.png"></p><p>Most of the time, we may want to more custom python packages to be installed, so we have to create a custom layer to have those packages specifically installed.<br>Notice that, there is a 250M size limit for each layer, so this means we usually can only install one or two packages in each layer. And those packages cann’t<br>be too large.</p><p>The easiest way to create a custom layer and ship it to AWS, such that you can choose to add it in your lambda fuctions, is by using The AWS Command Line Interface (AWS CLI).</p><h2 id="2-Install-AWS-CLI"><a href="#2-Install-AWS-CLI" class="headerlink" title="2. Install AWS CLI"></a>2. Install AWS CLI</h2><p>AWS CLI provides direct access to the Amazon Web Services public API. So it’s likely you already have aws cli set up.<br>If not, you can follow the offical instructions to install it on different platforms.<br>Here we take the Unbuntu linux as an example.</p><p>There are couples of ways to install AWS CLI, one way is to install AWS CLI is by using pip:<br>Step 1: Install pip (on Ubuntu OS), if you don’t have it already<br>$ sudo apt install python3-pip</p><p>Step 2: Install CLI<br>$ pip install awscli –upgrade –user</p><p>Step 3: Check installation<br>$ aws –version</p><p>Step 4: Configure AWS CLI to link your own credentials<br>$ aws configure</p><p>As a result of the above command, the AWS CLI will prompt you for four pieces of information.<br>The first two are your AWS Access Key ID and AWS Secret Access Key, which serve as your account credentials.<br><img src="/content/images/2022-09-01-2.png"></p><p>The passkey ID and a secret passkey ID can be created from AWS Management Console.<br>For the region name, it is something like us-west-2 in this example.<br>We also need to write our default output format. We can choose between the default value, in which case we will only have to press Enter.<br>Or we can also select the JSON format (JavaScript Object Notation), in which case we’ll type json and hit Enter.</p><p>The AWS CLI configuration is stored in ~ /.aws / config and ~ /.aws / credentials, as you can see in the following screenshot.<br><img src="/content/images/2022-09-01-3.png"></p><h2 id="3-Create-a-virtual-python-environment"><a href="#3-Create-a-virtual-python-environment" class="headerlink" title="3. Create a virtual python environment"></a>3. Create a virtual python environment</h2><p>Taking python 3.9 as an example, we first need to install the latest version of Python 3.9 and the development libraries:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install python3.9</span><br><span class="line">sudo  apt-get install python3.9-dev python3.9-venv</span><br></pre></td></tr></table></figure><p>Now we can create a Python 3.9 virtual environment called sandbox or any name you like using the following command.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python3.9 -m venv sandbox</span><br></pre></td></tr></table></figure><p>and activate the virtual environment:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source sandbox/bin/activate</span><br></pre></td></tr></table></figure><p>Now we have the sandbox virtual environment turned on, we are ready to install python packages that we want to use in lambda functions.</p><h2 id="4-Install-python-packages-and-zip-it-and-upload-to-AWS"><a href="#4-Install-python-packages-and-zip-it-and-upload-to-AWS" class="headerlink" title="4. Install python packages and zip it and upload to AWS"></a>4. Install python packages and zip it and upload to AWS</h2><p>Here we take installing openai package as an example, with the sandbox virtual environment, we install it using pip:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install openai</span><br></pre></td></tr></table></figure><p>after the openai is successfully installed in the sandbox virtual environment, we deactivate and environment by typing:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">deactivate</span><br></pre></td></tr></table></figure><p>Now, we make a new folder called ‘python’, copy the sandbox virtual environment packages into this directory, and then zip it:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">mkdir python</span><br><span class="line">cd python</span><br><span class="line">cp -r ../sandbox/lib64/python3.9/site-packages/* .</span><br><span class="line">cd ..</span><br><span class="line">zip -r openai_layer.zip python</span><br></pre></td></tr></table></figure><p>Now, we should be able to use the aws cli command to upload the zip file as a custom layer with name “openai” like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">aws lambda publish-layer-version --layer-name openai --zip-file fileb://openai_layer.zip --compatible-runtimes python3.9</span><br></pre></td></tr></table></figure><p>Notice that, “fileb://“ is necessary to make it work in the above command.<br>with successful upload, the command will output info about relevant messages telling your layer name and version.</p><h2 id="5-Choose-the-custom-layer-for-lambda-function"><a href="#5-Choose-the-custom-layer-for-lambda-function" class="headerlink" title="5. Choose the custom layer for lambda function"></a>5. Choose the custom layer for lambda function</h2><p>If we go back the layer adding part of the lambda function, after refreshing, we should be able to see the custom layer called openai and its version at 1.<br>Just choose it and add it. Now we are ready to import it in the code!<br><img src="/content/images/2022-09-01-4.png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> lambda function </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understand the NaN and None difference in Pandas once for all</title>
      <link href="2022/08/24/2022-08-24-1/"/>
      <url>2022/08/24/2022-08-24-1/</url>
      
        <content type="html"><![CDATA[<p>Pandas and Numpy are widely used formats for data mining and data sciences, but sometimes people get confused by None and NaN, which are very similar but slightly different data types. Here we figure it out once for all with some examples. </p><h2 id="main-difference"><a href="#main-difference" class="headerlink" title="main difference"></a>main difference</h2><p>The distinction between None and NaN in Pandas can be summarized as:</p><ol><li>None represents a missing entry, but its type is not numeric. So any column (ad Pandas Series) that contains a None value is definately not a numeric type, such as int or float.</li><li>NaN which stands for not-a-number, is on the other hand a numeric type. This means that NaN can be found in a numeric column of int or float type.</li></ol><h2 id="tests-in-action"><a href="#tests-in-action" class="headerlink" title="tests in action"></a>tests in action</h2><p>in the following test, a None value is automatically transferred as a NaN value, because Pandas automatically converted None to NaN<br>given that the other value in the series is a numeric. The will make the series a numeric type and will be much easier for many<br>following operations.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.Series([<span class="number">1</span>,<span class="literal">None</span>])</span><br></pre></td></tr></table></figure><pre><code>0    1.01    NaNdtype: float64</code></pre><p>in the following test, the other value in the series is a string, so the None value stay as None value. This make the whole<br>series an object type.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.Series([<span class="string">&quot;1&quot;</span>,<span class="literal">None</span>])</span><br></pre></td></tr></table></figure><pre><code>0       11    Nonedtype: object</code></pre><h2 id="None-type-can-lead-to-more-arithmetic-errors"><a href="#None-type-can-lead-to-more-arithmetic-errors" class="headerlink" title="None type can lead to more arithmetic errors"></a>None type can lead to more arithmetic errors</h2><p>Why did we claim with NaN type, it will be much easier for many other operations useful to data science?<br>It just gives less error for many arithmetic operations. For example, the following operation will give an error:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="literal">None</span> + <span class="number">1</span></span><br></pre></td></tr></table></figure><pre><code>---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)&lt;ipython-input-8-3fd8740bf8ab&gt; in &lt;module&gt;----&gt; 1 None + 1TypeError: unsupported operand type(s) for +: &#39;NoneType&#39; and &#39;int&#39;</code></pre><p>while the following operations with NaN type is fine, we just get another NaN type, but no error.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np.nan + <span class="number">1</span></span><br></pre></td></tr></table></figure><pre><code>nan</code></pre><h2 id="How-to-check-the-None-and-NaN-type"><a href="#How-to-check-the-None-and-NaN-type" class="headerlink" title="How to check the None and NaN type"></a>How to check the None and NaN type</h2><p>There are several different ways to check if a data type is None or NaN values;<br>First using numpy, the function np.isnan() can check if a value is a NaN value, but it won’t work with None values.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.isnan(np.nan)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><p>in Pandas, there are functions that are isnull() and isna(), which are literally does the same things. isnull() is just an alias of the isna() method; Basically isnull() detects missing values, so both nan or None will be True.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.isnull(np.nan)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pd.isnull(<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Maximize your credit cards cashback using AI optimization</title>
      <link href="2022/08/21/2022-08-21-1/"/>
      <url>2022/08/21/2022-08-21-1/</url>
      
        <content type="html"><![CDATA[<p>In general, there are many credit cards that offer good cash back rates for certain spending categories ranging from 1% to 5%. </p><p>The main spending categories include gas, dining, grocery, online shopping (not including amazon, target, walmart etc), and amazon shopping. There are other smaller and more specific categories, but the above categories are the main stream ones and matter to most of us.</p><p>One conventional wisdom of choosing the credit cards is trying to think about your biggest spending category, and then choose the credit cards that offers maximum cashback percentage on that category. </p><p>With optimization algorithms in AI, we could do it better. First, we estimate our annual spending in different spending categories. Then, the algorithm will figure out the best spending strategy for us using different credit cards.</p><p>The default list of credit cards are chosen with the following conditions:</p><ol><li>no annual fee, to save you money</li><li>it offers the best cash back in certain main spending categories</li><li>one of the credit cards is for Amazon shopping only, and it has 3% cashback rate; however, if you happend to have the paid Amazon prime membership, the rate is 5%. In our optimization algorithm, we use the 3% for calculation, but keep in mind, you might get more than that if you have the membership.</li><li>Some cards offer very good cashback but with a spending cap, so if your spending in that category is higher than the cap, say 500 per month or 6000 per year, the algorithm will tell you how to allocate some percentage of your spending using another credit card.</li></ol><p>With this AI based optimization algorithm, your cashback rate is usually higher than 3%!</p><p>One thing to notice is that, the best cashback outcome comes from having all the listed credit cards; however, you can also specify a subset of the credit cards, for example, you only have 2 or 3 cards, you can checkbox them, and run the algorithm. The algorithm will tell you the best scenario using the credit cards you pick.</p><p>Now try your luck!</p><h2 id="cashback-optimizer-demo"><a href="#cashback-optimizer-demo" class="headerlink" title="cashback optimizer demo"></a>cashback optimizer demo</h2><p>or go to the direct link <a href="https://www.datasciencebyexample.com/scripts/cashback-optimizer/cashback-optimizer.html">cashback optimizer</a><br> <iframe src="/scripts/cashback-optimizer/cashback-optimizer.html" width="100%" height="2000" style="border:1px solid black;"></p></iframe>]]></content>
      
      
      <categories>
          
          <category> optimization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> credit card </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Common python errors and how to fix them</title>
      <link href="2022/08/15/2022-08-15-1/"/>
      <url>2022/08/15/2022-08-15-1/</url>
      
        <content type="html"><![CDATA[<p>The following is a list of common errors in Python, and knowing how to fix them will save your day.</p><h2 id="1-IndentationError"><a href="#1-IndentationError" class="headerlink" title="1. IndentationError"></a>1. IndentationError</h2><p>In Python, all code is lined up with correct spaces. So, whether there are extra spaces or missing spaces, the entire code doesn’t run and just returns an error function.</p><p>Python code follows the PEP8 whitespace specification, using 4 spaces for each level of indentation.<br>example:<br><img src="/content/images/2022-08-15-1.png"><br>how to fix:<br><img src="/content/images/2022-08-15-2.png"></p><h2 id="2-Mixing-Tab-and-Space-TabError"><a href="#2-Mixing-Tab-and-Space-TabError" class="headerlink" title="2. Mixing Tab and Space (TabError)"></a>2. Mixing Tab and Space (TabError)</h2><p>This type of error is caused by using both tabs and spaces for encoding. The tab key is essentially a tab, not an indent. Spaces are recommended due to the varying width of spaces represented by tabs in different text editors.</p><h2 id="3-SyntaxError"><a href="#3-SyntaxError" class="headerlink" title="3. SyntaxError"></a>3. SyntaxError</h2><p>There are three reasons for syntax errors:</p><ol><li>invalid syntax</li></ol><p>Missing punctuation, mixed use of Chinese and English symbols, misspelling, variable name or function name using keywords.</p><ol start="2"><li>Invalid character in identifier</li></ol><p>Unrecognized characters appear in the code, check for extra characters or Chinese characters.</p><ol start="3"><li><p>Incomplete string detected (EOL while scanning string literal)</p><p> In many cases, it is due to inconsistent quotation marks around the string.</p></li></ol><p>some examples:<br><img src="/content/images/2022-08-15-3.png"><br>Error reason: the comma is a Chinese comma<br>Error message: SyntaxError: invalid character in identifier</p><p><img src="/content/images/2022-08-15-4.png"><br>Error reason: parentheses are not paired</p><p>Error message: SyntaxError:unexpected EOF while parsing</p><p><img src="/content/images/2022-08-15-5.png"><br>Cause of error: Forgot to add a colon at the end of statements such as if/elif/else/while/for/def/class</p><p>Error message: SyntaxError: invalid syntax</p><h2 id="4-NameError"><a href="#4-NameError" class="headerlink" title="4. NameError"></a>4. NameError</h2><p>Variable name error is the most common and most commonly encountered built-in error type. It often occurs in Python variable naming. If the variable cannot be found, a NameError will be raised. There are a few things to keep in mind about the rules for variable names:</p><ol><li>The variable name can only contain letters, numbers and underscores, and cannot start with numbers;</li><li>Variable names cannot contain spaces, but underscores can be used to separate words within them;</li><li>Do not use Python keywords and function names as variable names, such as print;</li><li>Variable names should be short and descriptive;</li><li>Use lowercase l and uppercase O with caution, because it is easy to be mistaken for numbers 1 and 0.</li></ol><p>If there is a variable name error, you can check whether the variable is assigned a value, whether there is a case of inconsistent capitalization or a wrong variable name, and correct it after finding it.</p><h2 id="5-IndexError"><a href="#5-IndexError" class="headerlink" title="5. IndexError"></a>5. IndexError</h2><p>An index is the position of an item in an array or list, this exception occurs when we try to access an element from a list or a tuple from an index that does not exist in the list.</p><p>For example, if you have a list of 10 elements with indices between 0 and 9, if you try to access an element at index 10 or 11 or more, an IndexError will be raised.</p><h2 id="6-KeyError"><a href="#6-KeyError" class="headerlink" title="6. KeyError"></a>6. KeyError</h2><p>When reading the key and value in the dictionary, if the key does not exist, a KeyError will be triggered.</p><h2 id="7-TypeError"><a href="#7-TypeError" class="headerlink" title="7. TypeError"></a>7. TypeError</h2><p>This error is raised when an incorrect or unsupported object type is used in a program. This error is also raised if you try to call a non-callable object or iterate through a non-iterable identifier.</p><p>example<br><img src="/content/images/2022-08-15-6.png"><br>Reason for error: When using “+” for splicing, you must use a string, or use the str() function to convert the number into a string<br>Error message: TypeError: can only concatenate str(not “int”) to str</p><h2 id="8-AttributeError"><a href="#8-AttributeError" class="headerlink" title="8. AttributeError"></a>8. AttributeError</h2><p>Property errors are raised when attribute references and assignments fail.<br>The cause of such errors is an attempt to access an unknown object property, in other words the property of the corresponding object cannot be found. You can check whether the constructor <strong>init</strong>() in the class is written correctly, with two underscores on the left and right sides.</p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What are the risks when more people start to use AI writers such as GPT3 to produce articles?</title>
      <link href="2022/08/11/2022-08-11-1/"/>
      <url>2022/08/11/2022-08-11-1/</url>
      
        <content type="html"><![CDATA[<p><img src="/content/images/2022-08-11-1.png" alt="AI writting GPT3"></p><p>GPT3 is currently arguably the best AI tool for text generation, a machine learning model that uses deep learning to produce human-like text.</p><p>As more and more people are using AI assisted tool to generate contents, there are some potential risks that could arise.</p><h2 id="Risk-1"><a href="#Risk-1" class="headerlink" title="Risk 1"></a>Risk 1</h2><p>The first one is that over-reliance on AI tools could lead to contents that are not original and lack creativity. </p><p>If people become too reliant on AI tools to generate content, the originality and creativity of that content may suffer. This is because AI tools often lack the ability to generate truly original ideas, and instead simply regurgitate information that they have been fed. As a result, the content that is produced may be dull and uninspired. Additionally, if people rely too heavily on AI tools to generate content, they may begin to lose their own ability to generate original ideas. This could have a negative impact on their own creativity and originality, as well as on the quality of the content that they produce. </p><p>Additionally, contents generated by AI tools may not be able to pass plagiarism detection software. This is because AI tools often simply rearrange or paraphrase existing information, rather than generate truly original content. As a result, the content they produce may be flagged as plagiarized by software designed to detect plagiarism. This could lead to the content being rejected or removed, which would be a waste of time and effort.</p><h2 id="Risk-2"><a href="#Risk-2" class="headerlink" title="Risk 2"></a>Risk 2</h2><p>The second potential risk of using AI to generate content is that the content may be inaccurate or misleading. </p><p>This is because AI tools often do not have the same level of understanding of the topic as a human would. As a result, they may inadvertently include inaccurate information in the content they generate. This could lead to people being misinformed or misled by the content. Additionally, if the AI tool is not configured properly, it may generate content that is biased or discriminatory. For example, if a tool is trained on data that is biased against a certain group of people, it may produce content that is similarly biased. This could lead to the further marginalization of already marginalized groups.</p><h2 id="Risk-3"><a href="#Risk-3" class="headerlink" title="Risk 3"></a>Risk 3</h2><p>The third one is that the use of AI tools could lead to job losses for content creators.</p><p>If people begin to rely too heavily on AI tools to generate content, it could lead to job losses for content creators. This is because AI tools can often do the same job as a human content creator, but at a fraction of the cost. As a result, companies may begin to prefer to use AI tools rather than human content creators, in order to save money. This could lead to content creators losing their jobs, as well as to a decline in the quality of content, as AI tools are not yet able to match humans in terms of creativity and originality.</p><h2 id="Risk-4"><a href="#Risk-4" class="headerlink" title="Risk 4"></a>Risk 4</h2><p>The fourth potential risk is that the AI tool could be used to create contents that are offensive or hateful in nature.</p><p>If an AI tool is not configured properly, it may generate content that is offensive or hateful in nature. This is because AI tools often simply regurgitate information that they have been fed. As a result, if the AI tool is fed offensive or hateful information, it may generate content that is similarly offensive or hateful. This could lead to people being offended or hurt by the content and could potentially cause social unrest. Additionally, if the AI tool is used to genera a large amount of content, the offensive or hateful content could be spread widely, potentially causing harm to many people. </p><h2 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h2><p>As we can see, there could be some potential risks associated with using AI to generate content. However, on the other hand, these risks might be mitigated by ensuring that the AI tool is configured properly and used in conjunction with human input.</p>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> gpt3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Best tips for data scientists, most common option setup in python pandas</title>
      <link href="2022/08/06/2022-08-06-1/"/>
      <url>2022/08/06/2022-08-06-1/</url>
      
        <content type="html"><![CDATA[<p>The most commonly used data table tool for data scientists is pandas; through pandas tables, tables (dataframes) and analysis tables can be easily displayed. The formatting of the table is not good, which will affect the analysis efficiency.</p><p>If a worker wants to do well, he must first sharpen his tools. Here are some commonly used setting methods:</p><ol><li>Show more lines</li><li>Show more columns</li><li>Change the column width</li><li>Set the precision of the float column</li><li>Number formatted display</li><li>Change the drawing method</li><li>Configure the output of info()</li><li>Print out current settings and reset all options</li></ol><h2 id="1-show-more-rows"><a href="#1-show-more-rows" class="headerlink" title="1. show more rows"></a>1. show more rows</h2><p>By default, pandas does not exceed the display range of the screen. If there are many rows in the table, it will truncate the middle row and display only part of it. Sometimes, if the total number of rows of data to be viewed is not many, you can control the maximum number of rows displayed by setting display.max_rows, for example, setting the display of 200 rows of data to view the data at one time:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">pd.set_option(&#x27;display.max_rows&#x27;, 200)</span><br><span class="line"># or set like this</span><br><span class="line"># pd.options.display.max_rows = 200</span><br></pre></td></tr></table></figure><p>But when the number of rows of data exceeds display.max_rows, then display.min_rows will determine how many rows are in the displayed part. Because the default number of rows in display.min_rows is 10, the data is generally displayed as the first 5 rows of data and the last 5 rows of data.</p><p>In the same way, you can also display the number of rows that can be displayed according to your own habits, such as 20</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.min_rows&#x27;, 20)</span><br><span class="line"># pd.options.display.min_rows = 20</span><br></pre></td></tr></table></figure><p>If you need to restore the default settings, you can reset to default conditions like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># reset</span><br><span class="line">pd.reset_option(&#x27;display.max_rows&#x27;)</span><br></pre></td></tr></table></figure><h2 id="2-Show-more-columns"><a href="#2-Show-more-columns" class="headerlink" title="2. Show more columns"></a>2. Show more columns</h2><p>The row can be set, the same column can be set, display.max_columns controls the number of columns that can be displayed, the default value is 20.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.get_option(&#x27;display.max_columns&#x27;)</span><br><span class="line"># pd.options.display.max_columns</span><br><span class="line">20</span><br></pre></td></tr></table></figure><h2 id="3-Change-column-width"><a href="#3-Change-column-width" class="headerlink" title="3. Change column width"></a>3. Change column width</h2><p>pandas has some limits on the number of characters displayed in a column, the default is 50 characters. Therefore, if some value characters are too long, an ellipsis will be displayed. If you want to display all, you can set display.max_colwidth, for example, set it to 500.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.max_colwidth&#x27;,500)</span><br><span class="line"># pd.options.display.max_colwidth = 500</span><br></pre></td></tr></table></figure><h2 id="4-Set-the-precision-of-the-float-column"><a href="#4-Set-the-precision-of-the-float-column" class="headerlink" title="4. Set the precision of the float column"></a>4. Set the precision of the float column</h2><p>For float data, pandas displays only 6 decimal places by default. We can set display.precision in advance to display only 2 digits to avoid repeated operations later.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.precision&#x27;,2)</span><br><span class="line"># pd.options.display.precision = 2</span><br></pre></td></tr></table></figure><p>This setting does not affect the underlying data, it only affects the display of floating columns.</p><h2 id="5-Format-the-numbers"><a href="#5-Format-the-numbers" class="headerlink" title="5. Format the numbers"></a>5. Format the numbers</h2><p>There is an option display.float_formatoption in pandas which can be used to format any float column. This only applies to floating point columns, for other data types, they must be converted to floating point numbers.</p><h4 id="Format-large-value-numbers-with-commas"><a href="#Format-large-value-numbers-with-commas" class="headerlink" title="Format large value numbers with commas"></a>Format large value numbers with commas</h4><p>Large numbers like 1200000 seem inconvenient, so we separate them with commas.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.float_format&#x27;,&#x27;&#123;:,&#125;&#x27;.format)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-08-06-1.png" alt="pandas display format example"></p><h4 id="set-number-precision"><a href="#set-number-precision" class="headerlink" title="set number precision"></a>set number precision</h4><p>Similar to display.precision above, if we only care about 2 digits after the decimal point, we can format it like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.float_format&#x27;, &#x27;&#123;:,.2f&#125;&#x27;.format)</span><br></pre></td></tr></table></figure><h4 id="percentage-sign-formatting"><a href="#percentage-sign-formatting" class="headerlink" title="percentage sign formatting"></a>percentage sign formatting</h4><p>If we want to display a percentage column, we can set it like this.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.float_format&#x27;, &#x27;&#123;:.2f&#125;%&#x27;.format)</span><br></pre></td></tr></table></figure><h2 id="6-Change-the-drawing-method"><a href="#6-Change-the-drawing-method" class="headerlink" title="6. Change the drawing method"></a>6. Change the drawing method</h2><p>By default, pandas uses matplotlib as the plotting backend. Starting from version 0.25, pandas provides options for using different backends, such as plotly, bokeh and other third-party libraries, but the premise is that you need to install them first.</p><p>The setup is very simple, as long as the third-party library is installed, it also only needs one line.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">pd.set_option(&#x27;plotting.backend&#x27;, &#x27;altair&#x27;)</span><br><span class="line">data = pd.Series(np.random.randn(100).cumsum())</span><br><span class="line">data.plot()</span><br></pre></td></tr></table></figure><h2 id="7-Configure-the-output-of-info"><a href="#7-Configure-the-output-of-info" class="headerlink" title="7. Configure the output of info()"></a>7. Configure the output of info()</h2><p>In pandas, we often use info() to quickly view the data of the DataFrame. However, the info method has a default limit on the maximum number of columns to analyze, and if there are nulls in the dataset, it will be very slow when counting statistics on large datasets.</p><p>pandas provides two options:</p><p>display.max_info_columns: Set the maximum number of columns to analyze, the default is 100.<br>display.max_info_rows: Set the threshold when the count is null, the default is 1690785.</p><p>For example, when analyzing a dataset with 150 features, we can set display.max_info_columns to a value that covers all columns, such as setting it to 200:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.max_info_columns&#x27;, 200)</span><br></pre></td></tr></table></figure><p>When analyzing large datasets, df.info() is slow due to calculating all nulls. So we can simply set display.max_info_rows to a small value to avoid counting, e.g. only count null if the number of rows does not exceed 5:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.max_info_rows&#x27;, 5)</span><br></pre></td></tr></table></figure><h2 id="8-Print-out-current-settings-and-reset-all-options"><a href="#8-Print-out-current-settings-and-reset-all-options" class="headerlink" title="8. Print out current settings and reset all options"></a>8. Print out current settings and reset all options</h2><p>pd.describe_option() will print out the description of the setting and its current value.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.describe_option()</span><br></pre></td></tr></table></figure><p>You can also print certain options, such as line display.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># specific search</span><br><span class="line">pd.describe_option(&#x27;rows&#x27;)</span><br></pre></td></tr></table></figure><p>Finally, we can also reset all directly.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.reset_option(&#x27;all&#x27;)</span><br></pre></td></tr></table></figure><h2 id="Summarize"><a href="#Summarize" class="headerlink" title="Summarize"></a>Summarize</h2><p>The above is the use of commonly used set_option, you can set it as follows at one time:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pd.set_option(&#x27;display.max_rows&#x27;,xxx) # maximum number of rows</span><br><span class="line">pd.set_option(&#x27;display.min_rows&#x27;,xxx) # Minimum display rows</span><br><span class="line">pd.set_option(&#x27;display.max_columns&#x27;,xxx) # Maximum number of display columns</span><br><span class="line">pd.set_option (&#x27;display.max_colwidth&#x27;,xxx) #Maximum column characters</span><br><span class="line">pd.set_option( &#x27;display.precision&#x27;,2) # floating point precision</span><br><span class="line">pd.set_option(&#x27;display.float_format&#x27;,&#x27;&#123;:,&#125;&#x27;.format) #Comma-separated numbers</span><br><span class="line">pd.set_option(&#x27;display.float_format&#x27;, &#x27;&#123;:,.2f&#125;&#x27;.format) #Set floating point precision</span><br><span class="line">pd.set_option(&#x27;display.float_format&#x27;, &#x27;&#123;:.2f&#125;%&#x27;.format) #Percent sign formatting</span><br><span class="line">pd.set_option(&#x27;plotting.backend&#x27;, &#x27;altair&#x27;) # Change the backend drawing method</span><br><span class="line">pd.set_option(&#x27;display.max_info_columns&#x27;, 200) # info output maximum number of columns</span><br><span class="line">pd.set_option(&#x27;display.max_info_rows&#x27;, 5) # info count threshold when null</span><br><span class="line">pd.describe_option() #Show all settings and descriptions</span><br><span class="line">pd.reset_option(&#x27;all&#x27;) #Reset all setting options</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>StagingBucket already exists error during cdk Bootstrap</title>
      <link href="2022/07/28/2022-07-28-1/"/>
      <url>2022/07/28/2022-07-28-1/</url>
      
        <content type="html"><![CDATA[<p>During my practice of fllowing the AWS Cloud Development Kit (AWS CDK) script that automatically provisions container<br>image-based Lambda functions that perform ML inference using pre-trained Hugging Face modelsWhen using the aws cdk stack deployment,<br>I got some error like this:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>:<span class="number">45</span>:<span class="number">26</span> AM CREATE_FAILED        | AWS::S3::Bucket       | StagingBucket</span><br><span class="line">cdk-hnb659fds-assets-xxxxxxxxxxx-us-east-<span class="number">1</span> already exists</span><br><span class="line"></span><br><span class="line">Environment aws://xxxxxxxxx/us-east-<span class="number">1</span> failed bootstrapping: Error: </span><br><span class="line">The stack named CDKToolkit failed creation, it may need to be manually deleted <span class="keyword">from</span> the AWS console: ROLLBACK_COMPLETE: </span><br><span class="line">cdk-hnb659fds-assets-xxxxxxx-us-east-<span class="number">1</span> already exists</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>As a reminder, here are the steps I was doing:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cdk bootstrap</span><br><span class="line">cdk deploy</span><br></pre></td></tr></table></figure><p>When I reach the cdk bootstrap I get the above error:</p><p>The error happens probably because I previously mannually removed some images on the aws ECR.<br>So to fix that error, first I go to the aws console and find the CloudFormation dashboard, and delete the stack called “CDKToolkit”,<br>shown as below</p><p><img src="/content/images/2022-07-28-1.png" alt="lift plot"></p><p>Then I went to S3 buckets in aws console, and delete the bucket named as “cdk-hnb659fds-assets-xxxxxxxxxxx-us-east-1”, as shown below:</p><p><img src="/content/images/2022-07-28-2.png" alt="lift plot"></p><p>After it’s done, I rerun cdk bootstrap, it works.</p>]]></content>
      
      
      <categories>
          
          <category> stack overflow </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reveal secrets of early Universe, three questions to understand the James Webb telescope</title>
      <link href="2022/07/17/2022-07-17-1/"/>
      <url>2022/07/17/2022-07-17-1/</url>
      
        <content type="html"><![CDATA[<p><img src="/content/images/2022-07-17-1.png" alt="James Webb telescope"></p><h2 id="1-Why-does-the-Webb-Telescope-fly-so-far"><a href="#1-Why-does-the-Webb-Telescope-fly-so-far" class="headerlink" title="1. Why does the Webb Telescope fly so far?"></a>1. Why does the Webb Telescope fly so far?</h2><p>The Webb telescope has been launched into space 1.5 million kilometers from Earth, a place known as the “Earth-Sun Lagrange L2 point”. The average distance between the moon and the earth is 380,000 kilometers, which is 4 times farther than the Webb telescope!</p><p>Hubble is a visible light telescope, basically what we see with the human eye is what the telescope captures; Webb is an infrared telescope, it mainly accepts light in infrared wavelengths - redder than red, a kind of invisible to our eyes” red”.</p><p>According to the theory of physics, all matter in the universe has a temperature and emits photons outward. Only objects that are hot enough to emit visible light outwards allow our eyes to see these photons. More objects emit infrared light, and we need infrared sensors to detect them.</p><p>The James Webb Telescope is a device used to detect cosmic infrared light. The sensors on it are extremely sensitive and can distinguish extremely small temperature changes, thereby detecting faint light from tens of billions of light-years away.</p><p>Just as we can’t see stars during the day, it’s not that the star isn’t there, but that the sun’s rays are so strong that its photons scatter in the atmosphere, drowning out the dim starlight. The Earth and Moon also emit infrared light, and the closer you get to Earth, the more it interferes with the Webb telescope. How to do it? Scientists should keep the telescope as far away from the Earth and the moon as possible, and block the sunlight with a heat shield to minimize the interference of thermal radiation.</p><p>So, is the Webb telescope sent to the L2 point of the Earth’s Lagrange because it is cold enough there? Not really.</p><p>Compared with the distance from the sun, the temperature difference caused by 1.5 million kilometers is almost negligible. On the one hand, the L2 point is selected because it can avoid the interference of the thermal radiation of the earth and the moon, and another important reason is that it is in a stable area between the sun and the earth. , the spacecraft can “float” here for a long time without consuming much fuel. Weber orbits the L2 point under the gravitational force of the earth and the sun.</p><h2 id="2-Why-does-the-telescope-need-to-work-in-the-infrared-range"><a href="#2-Why-does-the-telescope-need-to-work-in-the-infrared-range" class="headerlink" title="2. Why does the telescope need to work in the infrared range?"></a>2. Why does the telescope need to work in the infrared range?</h2><p>Launched in April 1990, the Hubble Space Telescope is a groundbreaking generation of space telescopes.</p><p>In the past 30 years, the Hubble Space Telescope has provided us with the most important images of the universe, such as the famous Pillar of Creations and the nearly 10,000 objects known as the Hubble Ultra Deep Field. Image of galaxies.</p><p>The Webb telescope is designed to look beyond Hubble to see deeper into the universe.</p><p>Although Hubble is expected to continue working for another decade or two, Webb, with its more sophisticated equipment and design, is seen as a future successor to the mission of exploring space.</p><p>The main differences between Hubble and Webb are:</p><p>Hubble was designed to collect visible and ultraviolet light, with only limited infrared capabilities. The Webb telescope collects infrared light (IR);<br>The Hubble telescope is 2.4 meters in diameter, and the Webb telescope is 6.5 meters in diameter, and has a larger light collection area, which means that Webb can see deeper, farther and earlier than Hubble;<br>Hubble is orbiting very close to Earth, while Webb will be 1.5 million kilometers away. That’s four times the distance from Earth to the Moon.</p><p>The figure below shows that the same picture, infrared light can see more details.<br><img src="/content/images/2022-07-17-2.png" alt="more details with infrared lights on the right"></p><h2 id="3-Could-infrared-light-allow-us-to-see-the-beginning-of-the-universe"><a href="#3-Could-infrared-light-allow-us-to-see-the-beginning-of-the-universe" class="headerlink" title="3. Could infrared light allow us to see the beginning of the universe?"></a>3. Could infrared light allow us to see the beginning of the universe?</h2><p><img src="/content/images/2022-07-17-3.png" alt="timeline of universe"></p><p>Hawking said the universe exploded from a point much smaller than an atom, and it took about 13.8 billion years of evolution to become what we see today.</p><p>Astronomers say Webb will examine the entire multibillion-year history of the universe – from the first stars to life in our solar system. NASA Administrator Bill Nelson called the telescope a “keyhole to the past.”</p><p>The Webb Space Telescope has a light-receiving area five times that of Hubble, and can work in the near-infrared range, with an observable spectral wavelength of up to 30 microns. This means that Webb can not only see the older galaxies, that is, the scene when the universe was 200 million years old, but also penetrate through the dust of the universe and see the mysteries behind it.</p><p>But why can see the past? Can time be turned back?<br>Simply put, because the universe is constantly expanding, and the speed of light has a limit, the light we see now from far away is actually the light emitted in the past. The distance that light travels in 1 year, called a light-year, is the unit of distance in the universe, and 1 light-year is nearly 10 trillion kilometers.</p><p>For example, the light emitted from the birth of a celestial body 10 billion light-years away from Earth will only be seen this year. Conversely, if the light we capture comes from an object 10 billion light-years away, it is an event that happened 10 billion years ago. The life of the universe is probably more than 13 billion years old, so if we can observe the light emitted by objects far enough away, we can see exactly what happened in the beginning of the universe.</p><p>The oldest known light in the universe is from the galaxy GN-z11. According to calculations, the galaxy is 13.4 billion light-years away from Earth.<br><img src="/content/images/2022-07-17-4.png" alt="galaxy GN-z11"></p><p>The question is how do we know how far away the light we’re receiving comes from?<br>The answer is spectroscopic analysis! When we observe celestial objects, the spectral lines of the light they emit are not at standard wavelengths. The slope lengths of all spectral lines are lengthened, that is to say, the spectral lines move toward the red end. This phenomenon is called spectral line redshift, and it is caused by the Doppler effect.</p><p>As light changes over time and distance, its wavelength, or color, also changes. And this change is regular. When you see a train coming towards you, its color tends to be blue. Of course, this change is very small, and the human eye cannot distinguish it. This is called blue shift. If you see a train moving away from you, its color is more red, which is called a redshift. When scientists observe the universe, they find that almost all celestial bodies have red shifts, which is why they have the theory of the Big Bang. When you detect a beam of light, record its spectrum, measure its spectrum after a period of time, and know how far it comes from according to its changes.</p>]]></content>
      
      
      <categories>
          
          <category> physics </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cosmology </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A great columnTransformer example to preprocess data for online article SOV prediction</title>
      <link href="2022/07/16/2022-07-16-1/"/>
      <url>2022/07/16/2022-07-16-1/</url>
      
        <content type="html"><![CDATA[<p>ColumnTransformer is one of the most useful data transforming tools for data science projects. Here we show a complete example of applying columnTransformer as the preprocessing tools, and then apply RandomForrest Regression models to predict share of voice (SOV) for online articles. </p><h2 id="data-introduction"><a href="#data-introduction" class="headerlink" title="data introduction"></a>data introduction</h2><p>The original link of data is here: <a href="https://archive-beta.ics.uci.edu/ml/datasets/online+news+popularity">https://archive-beta.ics.uci.edu/ml/datasets/online+news+popularity</a><br>Features are already created, but checking the feature names alone would help us greatly to undrstand what would influence the online article ranking.</p><p>What do the instances that comprise the dataset represent?</p><p>Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)</p><p>Attribute Information:<br>     0. url:                           URL of the article (non-predictive)<br>     1. timedelta:                     Days between the article publication and the dataset acquisition (non-predictive)<br>     2. n_tokens_title:                Number of words in the title<br>     3. n_tokens_content:              Number of words in the content<br>     4. n_unique_tokens:               Rate of unique words in the content<br>     5. n_non_stop_words:              Rate of non-stop words in the content<br>     6. n_non_stop_unique_tokens:      Rate of unique non-stop words in the content<br>     7. num_hrefs:                     Number of links<br>     8. num_self_hrefs:                Number of links to other articles published by Mashable<br>     9. num_imgs:                      Number of images<br>    10. num_videos:                    Number of videos<br>    11. average_token_length:          Average length of the words in the content<br>    12. num_keywords:                  Number of keywords in the metadata<br>    13. data_channel_is_lifestyle:     Is data channel ‘Lifestyle’?<br>    14. data_channel_is_entertainment: Is data channel ‘Entertainment’?<br>    15. data_channel_is_bus:           Is data channel ‘Business’?<br>    16. data_channel_is_socmed:        Is data channel ‘Social Media’?<br>    17. data_channel_is_tech:          Is data channel ‘Tech’?<br>    18. data_channel_is_world:         Is data channel ‘World’?<br>    19. kw_min_min:                    Worst keyword (min. shares)<br>    20. kw_max_min:                    Worst keyword (max. shares)<br>    21. kw_avg_min:                    Worst keyword (avg. shares)<br>    22. kw_min_max:                    Best keyword (min. shares)<br>    23. kw_max_max:                    Best keyword (max. shares)<br>    24. kw_avg_max:                    Best keyword (avg. shares)<br>    25. kw_min_avg:                    Avg. keyword (min. shares)<br>    26. kw_max_avg:                    Avg. keyword (max. shares)<br>    27. kw_avg_avg:                    Avg. keyword (avg. shares)<br>    28. self_reference_min_shares:     Min. shares of referenced articles in Mashable<br>    29. self_reference_max_shares:     Max. shares of referenced articles in Mashable<br>    30. self_reference_avg_sharess:    Avg. shares of referenced articles in Mashable<br>    31. weekday_is_monday:             Was the article published on a Monday?<br>    32. weekday_is_tuesday:            Was the article published on a Tuesday?<br>    33. weekday_is_wednesday:          Was the article published on a Wednesday?<br>    34. weekday_is_thursday:           Was the article published on a Thursday?<br>    35. weekday_is_friday:             Was the article published on a Friday?<br>    36. weekday_is_saturday:           Was the article published on a Saturday?<br>    37. weekday_is_sunday:             Was the article published on a Sunday?<br>    38. is_weekend:                    Was the article published on the weekend?<br>    39. LDA_00:                        Closeness to LDA topic 0<br>    40. LDA_01:                        Closeness to LDA topic 1<br>    41. LDA_02:                        Closeness to LDA topic 2<br>    42. LDA_03:                        Closeness to LDA topic 3<br>    43. LDA_04:                        Closeness to LDA topic 4<br>    44. global_subjectivity:           Text subjectivity<br>    45. global_sentiment_polarity:     Text sentiment polarity<br>    46. global_rate_positive_words:    Rate of positive words in the content<br>    47. global_rate_negative_words:    Rate of negative words in the content<br>    48. rate_positive_words:           Rate of positive words among non-neutral tokens<br>    49. rate_negative_words:           Rate of negative words among non-neutral tokens<br>    50. avg_positive_polarity:         Avg. polarity of positive words<br>    51. min_positive_polarity:         Min. polarity of positive words<br>    52. max_positive_polarity:         Max. polarity of positive words<br>    53. avg_negative_polarity:         Avg. polarity of negative  words<br>    54. min_negative_polarity:         Min. polarity of negative  words<br>    55. max_negative_polarity:         Max. polarity of negative  words<br>    56. title_subjectivity:            Title subjectivity<br>    57. title_sentiment_polarity:      Title polarity<br>    58. abs_title_subjectivity:        Absolute subjectivity level<br>    59. abs_title_sentiment_polarity:  Absolute polarity level<br>    60. shares:                        Number of shares (target)</p><p>Additional Information</p><ul><li>The articles were published by Mashable (<a href="http://www.mashable.com/">www.mashable.com</a>) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.</li><li>Acquisition date: January 8, 2015</li><li>The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method.  See their article for more details on how the relative performance values were set.</li></ul><h2 id="load-libraires"><a href="#load-libraires" class="headerlink" title="load libraires"></a>load libraires</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> KBinsDiscretizer</span><br><span class="line"><span class="keyword">from</span> sklearn.inspection <span class="keyword">import</span> permutation_importance</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="import-data"><a href="#import-data" class="headerlink" title="import data"></a>import data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">&#x27;data/OnlineNewsPopularity.csv&#x27;</span>)</span><br><span class="line">df</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>url</th>      <th>timedelta</th>      <th>n_tokens_title</th>      <th>n_tokens_content</th>      <th>n_unique_tokens</th>      <th>n_non_stop_words</th>      <th>n_non_stop_unique_tokens</th>      <th>num_hrefs</th>      <th>num_self_hrefs</th>      <th>num_imgs</th>      <th>...</th>      <th>min_positive_polarity</th>      <th>max_positive_polarity</th>      <th>avg_negative_polarity</th>      <th>min_negative_polarity</th>      <th>max_negative_polarity</th>      <th>title_subjectivity</th>      <th>title_sentiment_polarity</th>      <th>abs_title_subjectivity</th>      <th>abs_title_sentiment_polarity</th>      <th>shares</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>      <td>731.0</td>      <td>12.0</td>      <td>219.0</td>      <td>0.663594</td>      <td>1.0</td>      <td>0.815385</td>      <td>4.0</td>      <td>2.0</td>      <td>1.0</td>      <td>...</td>      <td>0.100000</td>      <td>0.70</td>      <td>-0.350000</td>      <td>-0.600</td>      <td>-0.200000</td>      <td>0.500000</td>      <td>-0.187500</td>      <td>0.000000</td>      <td>0.187500</td>      <td>593</td>    </tr>    <tr>      <th>1</th>      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>      <td>731.0</td>      <td>9.0</td>      <td>255.0</td>      <td>0.604743</td>      <td>1.0</td>      <td>0.791946</td>      <td>3.0</td>      <td>1.0</td>      <td>1.0</td>      <td>...</td>      <td>0.033333</td>      <td>0.70</td>      <td>-0.118750</td>      <td>-0.125</td>      <td>-0.100000</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.500000</td>      <td>0.000000</td>      <td>711</td>    </tr>    <tr>      <th>2</th>      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>      <td>731.0</td>      <td>9.0</td>      <td>211.0</td>      <td>0.575130</td>      <td>1.0</td>      <td>0.663866</td>      <td>3.0</td>      <td>1.0</td>      <td>1.0</td>      <td>...</td>      <td>0.100000</td>      <td>1.00</td>      <td>-0.466667</td>      <td>-0.800</td>      <td>-0.133333</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.500000</td>      <td>0.000000</td>      <td>1500</td>    </tr>    <tr>      <th>3</th>      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>      <td>731.0</td>      <td>9.0</td>      <td>531.0</td>      <td>0.503788</td>      <td>1.0</td>      <td>0.665635</td>      <td>9.0</td>      <td>0.0</td>      <td>1.0</td>      <td>...</td>      <td>0.136364</td>      <td>0.80</td>      <td>-0.369697</td>      <td>-0.600</td>      <td>-0.166667</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.500000</td>      <td>0.000000</td>      <td>1200</td>    </tr>    <tr>      <th>4</th>      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>      <td>731.0</td>      <td>13.0</td>      <td>1072.0</td>      <td>0.415646</td>      <td>1.0</td>      <td>0.540890</td>      <td>19.0</td>      <td>19.0</td>      <td>20.0</td>      <td>...</td>      <td>0.033333</td>      <td>1.00</td>      <td>-0.220192</td>      <td>-0.500</td>      <td>-0.050000</td>      <td>0.454545</td>      <td>0.136364</td>      <td>0.045455</td>      <td>0.136364</td>      <td>505</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>39639</th>      <td>http://mashable.com/2014/12/27/samsung-app-aut...</td>      <td>8.0</td>      <td>11.0</td>      <td>346.0</td>      <td>0.529052</td>      <td>1.0</td>      <td>0.684783</td>      <td>9.0</td>      <td>7.0</td>      <td>1.0</td>      <td>...</td>      <td>0.100000</td>      <td>0.75</td>      <td>-0.260000</td>      <td>-0.500</td>      <td>-0.125000</td>      <td>0.100000</td>      <td>0.000000</td>      <td>0.400000</td>      <td>0.000000</td>      <td>1800</td>    </tr>    <tr>      <th>39640</th>      <td>http://mashable.com/2014/12/27/seth-rogen-jame...</td>      <td>8.0</td>      <td>12.0</td>      <td>328.0</td>      <td>0.696296</td>      <td>1.0</td>      <td>0.885057</td>      <td>9.0</td>      <td>7.0</td>      <td>3.0</td>      <td>...</td>      <td>0.136364</td>      <td>0.70</td>      <td>-0.211111</td>      <td>-0.400</td>      <td>-0.100000</td>      <td>0.300000</td>      <td>1.000000</td>      <td>0.200000</td>      <td>1.000000</td>      <td>1900</td>    </tr>    <tr>      <th>39641</th>      <td>http://mashable.com/2014/12/27/son-pays-off-mo...</td>      <td>8.0</td>      <td>10.0</td>      <td>442.0</td>      <td>0.516355</td>      <td>1.0</td>      <td>0.644128</td>      <td>24.0</td>      <td>1.0</td>      <td>12.0</td>      <td>...</td>      <td>0.136364</td>      <td>0.50</td>      <td>-0.356439</td>      <td>-0.800</td>      <td>-0.166667</td>      <td>0.454545</td>      <td>0.136364</td>      <td>0.045455</td>      <td>0.136364</td>      <td>1900</td>    </tr>    <tr>      <th>39642</th>      <td>http://mashable.com/2014/12/27/ukraine-blasts/</td>      <td>8.0</td>      <td>6.0</td>      <td>682.0</td>      <td>0.539493</td>      <td>1.0</td>      <td>0.692661</td>      <td>10.0</td>      <td>1.0</td>      <td>1.0</td>      <td>...</td>      <td>0.062500</td>      <td>0.50</td>      <td>-0.205246</td>      <td>-0.500</td>      <td>-0.012500</td>      <td>0.000000</td>      <td>0.000000</td>      <td>0.500000</td>      <td>0.000000</td>      <td>1100</td>    </tr>    <tr>      <th>39643</th>      <td>http://mashable.com/2014/12/27/youtube-channel...</td>      <td>8.0</td>      <td>10.0</td>      <td>157.0</td>      <td>0.701987</td>      <td>1.0</td>      <td>0.846154</td>      <td>1.0</td>      <td>1.0</td>      <td>0.0</td>      <td>...</td>      <td>0.100000</td>      <td>0.50</td>      <td>-0.200000</td>      <td>-0.200</td>      <td>-0.200000</td>      <td>0.333333</td>      <td>0.250000</td>      <td>0.166667</td>      <td>0.250000</td>      <td>1300</td>    </tr>  </tbody></table><p>39644 rows × 61 columns</p></div><h2 id="define-features-and-targets"><a href="#define-features-and-targets" class="headerlink" title="define features and targets"></a>define features and targets</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># be careful the features has a prefix of white space</span></span><br><span class="line">numeric_features = [<span class="string">&#x27; n_tokens_title&#x27;</span>, <span class="string">&#x27; n_tokens_content&#x27;</span>, <span class="string">&#x27; n_unique_tokens&#x27;</span>, <span class="string">&#x27; n_non_stop_words&#x27;</span>, <span class="string">&#x27; n_non_stop_unique_tokens&#x27;</span>, <span class="string">&#x27; num_hrefs&#x27;</span>, <span class="string">&#x27; num_self_hrefs&#x27;</span>, <span class="string">&#x27; num_imgs&#x27;</span>, <span class="string">&#x27; num_videos&#x27;</span>, <span class="string">&#x27; average_token_length&#x27;</span>, <span class="string">&#x27; num_keywords&#x27;</span>, <span class="string">&#x27; data_channel_is_lifestyle&#x27;</span>, <span class="string">&#x27; data_channel_is_entertainment&#x27;</span>, <span class="string">&#x27; data_channel_is_bus&#x27;</span>, <span class="string">&#x27; data_channel_is_socmed&#x27;</span>, <span class="string">&#x27; data_channel_is_tech&#x27;</span>, <span class="string">&#x27; data_channel_is_world&#x27;</span>, <span class="string">&#x27; kw_min_min&#x27;</span>, <span class="string">&#x27; kw_max_min&#x27;</span>, <span class="string">&#x27; kw_avg_min&#x27;</span>, <span class="string">&#x27; kw_min_max&#x27;</span>, <span class="string">&#x27; kw_max_max&#x27;</span>, <span class="string">&#x27; kw_avg_max&#x27;</span>, <span class="string">&#x27; kw_min_avg&#x27;</span>, <span class="string">&#x27; kw_max_avg&#x27;</span>, <span class="string">&#x27; kw_avg_avg&#x27;</span>, <span class="string">&#x27; self_reference_min_shares&#x27;</span>, <span class="string">&#x27; self_reference_max_shares&#x27;</span>, <span class="string">&#x27; self_reference_avg_sharess&#x27;</span>, <span class="string">&#x27; weekday_is_monday&#x27;</span>, <span class="string">&#x27; weekday_is_tuesday&#x27;</span>, <span class="string">&#x27; weekday_is_wednesday&#x27;</span>, <span class="string">&#x27; weekday_is_thursday&#x27;</span>, <span class="string">&#x27; weekday_is_friday&#x27;</span>, <span class="string">&#x27; weekday_is_saturday&#x27;</span>, <span class="string">&#x27; weekday_is_sunday&#x27;</span>, <span class="string">&#x27; is_weekend&#x27;</span>, <span class="string">&#x27; LDA_00&#x27;</span>, <span class="string">&#x27; LDA_01&#x27;</span>, <span class="string">&#x27; LDA_02&#x27;</span>, <span class="string">&#x27; LDA_03&#x27;</span>, <span class="string">&#x27; LDA_04&#x27;</span>, <span class="string">&#x27; global_subjectivity&#x27;</span>, <span class="string">&#x27; global_sentiment_polarity&#x27;</span>, <span class="string">&#x27; global_rate_positive_words&#x27;</span>, <span class="string">&#x27; global_rate_negative_words&#x27;</span>, <span class="string">&#x27; rate_positive_words&#x27;</span>, <span class="string">&#x27; rate_negative_words&#x27;</span>, <span class="string">&#x27; avg_positive_polarity&#x27;</span>, <span class="string">&#x27; min_positive_polarity&#x27;</span>, <span class="string">&#x27; max_positive_polarity&#x27;</span>, <span class="string">&#x27; avg_negative_polarity&#x27;</span>, <span class="string">&#x27; min_negative_polarity&#x27;</span>, <span class="string">&#x27; max_negative_polarity&#x27;</span>, <span class="string">&#x27; title_subjectivity&#x27;</span>, <span class="string">&#x27; title_sentiment_polarity&#x27;</span>, <span class="string">&#x27; abs_title_subjectivity&#x27;</span>, <span class="string">&#x27; abs_title_sentiment_polarity&#x27;</span>]</span><br><span class="line">categorical_features =[]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target = <span class="string">&#x27; shares&#x27;</span></span><br><span class="line"></span><br><span class="line">X = df.drop(target,axis=<span class="number">1</span>)</span><br><span class="line">y = df[target]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="training-and-test-data-split"><a href="#training-and-test-data-split" class="headerlink" title="training and test data split"></a>training and test data split</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><h2 id="define-transformers"><a href="#define-transformers" class="headerlink" title="define transformers"></a>define transformers</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># steps to handle numeric features</span></span><br><span class="line">numeric_transformer = Pipeline(steps=[</span><br><span class="line">        (<span class="string">&#x27;num_imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;mean&#x27;</span>)),</span><br><span class="line">        (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler())</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># steps to handel categorical features</span></span><br><span class="line">categorical_transformer = Pipeline(steps = [        </span><br><span class="line">      (<span class="string">&quot;cat_imputer&quot;</span>, SimpleImputer(strategy=<span class="string">&#x27;constant&#x27;</span>, fill_value = <span class="string">&quot;missing&quot;</span>)),</span><br><span class="line">      (<span class="string">&quot;encoder&quot;</span>, OneHotEncoder(handle_unknown=<span class="string">&#x27;ignore&#x27;</span>))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># TFIDF vectorizer to handel the url only</span></span><br><span class="line">tfidfvectorizer = TfidfVectorizer(max_features = <span class="number">100</span>, stop_words=<span class="string">&#x27;english&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># a customize transformer to be used in the  ColumnTransformer</span></span><br><span class="line"><span class="comment"># which takes a list of one or string features, and return the word counts for the list of features</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Counter</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line">   </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, _X</span>):</span></span><br><span class="line">        X = pd.DataFrame(_X)</span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> X.columns:</span><br><span class="line">            X[col]= X[col].apply(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x.split(<span class="string">&#x27;-&#x27;</span>)))</span><br><span class="line">        <span class="keyword">return</span> X</span><br><span class="line">    </span><br><span class="line">counter = Counter()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># integrate a preprocessor to handel different types of features</span></span><br><span class="line"><span class="comment"># notice in the transformer list of the ColumnTransformer,the parameter is &#x27;url&#x27;, a string type</span></span><br><span class="line"><span class="comment"># while parameters for other transformers are list type</span></span><br><span class="line">preprocessor = ColumnTransformer(</span><br><span class="line">    transformers=[</span><br><span class="line">        (<span class="string">&#x27;url&#x27;</span>, tfidfvectorizer, <span class="string">&#x27;url&#x27;</span>),</span><br><span class="line">        (<span class="string">&#x27;num&#x27;</span>, numeric_transformer, numeric_features),</span><br><span class="line">        (<span class="string">&#x27;count&#x27;</span>, counter, [<span class="string">&#x27;url&#x27;</span>]),</span><br><span class="line">        <span class="comment">#(&#x27;cat&#x27;, categorical_transformer, categorical_features)</span></span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check the format of the training data after preprocessing steps</span></span><br><span class="line"><span class="comment"># preprocessor.fit_transform(X_train)</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="define-a-whole-piple-to-include-model"><a href="#define-a-whole-piple-to-include-model" class="headerlink" title="define a whole piple to include model"></a>define a whole piple to include model</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pipeline = Pipeline(</span><br><span class="line">    steps =[</span><br><span class="line">        (<span class="string">&#x27;preprocessor&#x27;</span>, preprocessor),                            </span><br><span class="line">        (<span class="string">&#x27;model&#x27;</span>,RandomForestRegressor(verbose=<span class="number">0</span>, n_jobs=-<span class="number">1</span>))</span><br><span class="line">    ])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pipeline.fit(X_train,y_train)</span><br></pre></td></tr></table></figure><pre><code>Pipeline(steps=[(&#39;preprocessor&#39;,                 ColumnTransformer(transformers=[(&#39;url&#39;,                                                  TfidfVectorizer(max_features=100,                                                                  stop_words=&#39;english&#39;),                                                  &#39;url&#39;),                                                 (&#39;num&#39;,                                                  Pipeline(steps=[(&#39;num_imputer&#39;,                                                                   SimpleImputer()),                                                                  (&#39;scaler&#39;,                                                                   StandardScaler())]),                                                  [&#39; n_tokens_title&#39;,                                                   &#39; n_tokens_content&#39;,                                                   &#39; n_unique_tokens&#39;,                                                   &#39; n_non_stop_words&#39;,                                                   &#39; n_non_stop_unique_tokens&#39;,                                                   &#39; num_hrefs&#39;,                                                   &#39; num_self_hrefs&#39;,                                                   &#39; n...                                                   &#39; data_channel_is_tech&#39;,                                                   &#39; data_channel_is_world&#39;,                                                   &#39; kw_min_min&#39;, &#39; kw_max_min&#39;,                                                   &#39; kw_avg_min&#39;, &#39; kw_min_max&#39;,                                                   &#39; kw_max_max&#39;, &#39; kw_avg_max&#39;,                                                   &#39; kw_min_avg&#39;, &#39; kw_max_avg&#39;,                                                   &#39; kw_avg_avg&#39;,                                                   &#39; self_reference_min_shares&#39;,                                                   &#39; self_reference_max_shares&#39;,                                                   &#39; &#39;                                                   &#39;self_reference_avg_sharess&#39;,                                                   &#39; weekday_is_monday&#39;, ...]),                                                 (&#39;count&#39;, Counter(),                                                  [&#39;url&#39;])])),                (&#39;model&#39;, RandomForestRegressor(n_jobs=-1))])</code></pre><h2 id="plot-the-lift-chart"><a href="#plot-the-lift-chart" class="headerlink" title="plot the lift chart"></a>plot the lift chart</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_predicted_vs_actuals</span>(<span class="params">y_true, y_predicted, title = <span class="string">&quot;Pred Vs Actual&quot;</span></span>):</span></span><br><span class="line">    </span><br><span class="line">    rmse= mean_squared_error(y_true= y_true, y_pred=y_predicted,squared=<span class="literal">False</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;RMSE:&quot;</span>, rmse)</span><br><span class="line">   </span><br><span class="line">    predVsActual = pd.DataFrame( &#123;<span class="string">&#x27;y_true&#x27;</span>: y_true,<span class="string">&#x27;y_pred&#x27;</span>: y_predicted&#125;)</span><br><span class="line">    predVsActual[<span class="string">&quot;pred_deciles&quot;</span>] = pd.qcut(predVsActual[<span class="string">&quot;y_pred&quot;</span>],q=<span class="number">10</span> ,labels=<span class="literal">False</span>, duplicates = <span class="string">&#x27;drop&#x27;</span>)</span><br><span class="line">   </span><br><span class="line">    aggregations = &#123;</span><br><span class="line">        <span class="string">&#x27;y_true&#x27;</span>:<span class="string">&#x27;mean&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;y_pred&#x27;</span>: <span class="string">&#x27;mean&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    counts = predVsActual.groupby(<span class="string">&quot;pred_deciles&quot;</span>).count()[<span class="string">&quot;y_pred&quot;</span>]</span><br><span class="line"> </span><br><span class="line">    predVsActual= predVsActual.groupby(<span class="string">&quot;pred_deciles&quot;</span>).agg(aggregations)</span><br><span class="line">    predVsActual[<span class="string">&quot;counts&quot;</span>] = counts</span><br><span class="line">    predVsActual[[<span class="string">&quot;y_true&quot;</span>,<span class="string">&quot;y_pred&quot;</span>]].plot(title=title)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="built_in">print</span>(predVsActual)</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">test_preds = pipeline.predict(X_test)</span><br><span class="line">plot_predicted_vs_actuals(y_test, test_preds)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>RMSE: 11240.043547942365</code></pre><p><img src="/content/images/2022-07-16-1.png" alt="lift plot"></p><pre><code>                   y_true        y_pred  countspred_deciles                                   0             1410.976040   1226.474250     7931             2334.482976   1623.177175     7932             2066.532156   1958.356570     7933             2311.552333   2279.694124     7934             2859.727617   2645.796381     7935             2926.430556   3046.564318     7926             3554.585120   3543.738184     7937             4165.051702   4249.105233     7938             4935.030265   5450.838789     7939             6709.374527  11123.519130     793</code></pre><h2 id="code-and-data-link"><a href="#code-and-data-link" class="headerlink" title="code and data link"></a>code and data link</h2><p>please check out this github link: <a href="https://github.com/robotlearner001/blog/tree/main/ColumnTransformer-Example">https://github.com/robotlearner001/blog/tree/main/ColumnTransformer-Example</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>An simple but useful example of online calculator using javascript</title>
      <link href="2022/07/05/2022-07-05-1/"/>
      <url>2022/07/05/2022-07-05-1/</url>
      
        <content type="html"><![CDATA[<h2 id="code-link"><a href="#code-link" class="headerlink" title="code link"></a>code link</h2><p><a href="https://github.com/robotlearner001/blog/tree/main/online-calculator-javascript">github link</a></p><h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2> <iframe src="/scripts/calculator/calculator.html" width="100%" height="1000" style="border:1px solid black;"></iframe>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> scripts </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hyperparameter tuning using Optuna with RandomForestClassifier Example (Python code)</title>
      <link href="2022/07/02/2022-07-02-1/"/>
      <url>2022/07/02/2022-07-02-1/</url>
      
        <content type="html"><![CDATA[<p>For some popular machine learning algorithms, how to set the hyper parameters could affect machine learning algorithm performance greatly.</p><p>One naive way is to loop though different combinations of the hyper parameter space and choose the best configuration. This is called grid search strategy. But this method could be very slow.</p><p>A better way is to use some kind of optimization method to optimize our optimization. Tools such as Optuna and Hyperopt play roles here.</p><p>In the following, we will use the Optuna as example, and apply it on a Random Forrest Classifier.</p><h2 id="1-Import-libraries-and-get-the-newsgroup-data"><a href="#1-Import-libraries-and-get-the-newsgroup-data" class="headerlink" title="1. Import libraries and get the newsgroup data"></a>1. Import libraries and get the newsgroup data</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import os</span><br><span class="line">from sklearn.datasets import fetch_20newsgroups</span><br><span class="line">from sklearn.model_selection import cross_val_score</span><br><span class="line">from sklearn.metrics import f1_score</span><br><span class="line">from sklearn.feature_extraction.text import TfidfVectorizer</span><br><span class="line">from sklearn.pipeline import Pipeline</span><br><span class="line">import joblib</span><br><span class="line">from lightgbm import LGBMClassifier</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line"></span><br><span class="line">import optuna</span><br><span class="line"></span><br><span class="line">data = fetch_20newsgroups()</span><br><span class="line"></span><br><span class="line">X = data[&#x27;data&#x27;][:5000]</span><br><span class="line">y = data[&#x27;target&#x27;][:5000]</span><br></pre></td></tr></table></figure><h2 id="2-Define-a-machine-leaning-pipeline-with-TfidfVectorizer-and-RandomForestClassifie"><a href="#2-Define-a-machine-leaning-pipeline-with-TfidfVectorizer-and-RandomForestClassifie" class="headerlink" title="2. Define a machine leaning pipeline with TfidfVectorizer and RandomForestClassifie"></a>2. Define a machine leaning pipeline with TfidfVectorizer and RandomForestClassifie</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">model = Pipeline([</span><br><span class="line">    (&#x27;tfidf&#x27;, TfidfVectorizer(stop_words=&#x27;english&#x27;)),   </span><br><span class="line">    (&#x27;rf&#x27;, RandomForestClassifier())</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="3-Define-hyper-parameter-space-and-Optuna-objective-to-optimize"><a href="#3-Define-hyper-parameter-space-and-Optuna-objective-to-optimize" class="headerlink" title="3. Define hyper parameter space and Optuna objective to optimize"></a>3. Define hyper parameter space and Optuna objective to optimize</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def objective(trial):    </span><br><span class="line">    </span><br><span class="line">    joblib.dump(study, &#x27;study.pkl&#x27;)</span><br><span class="line">    </span><br><span class="line">    tfidf__analyzer = trial.suggest_categorical(&#x27;tfidf__analyzer&#x27;, [&#x27;word&#x27;, &#x27;char&#x27;, &#x27;char_wb&#x27;]) </span><br><span class="line">    tfidf__lowercase = trial.suggest_categorical(&#x27;tfidf__lowercase&#x27;, [False, True]) </span><br><span class="line">    tfidf__max_features = trial.suggest_int(&#x27;tfidf__max_features&#x27;, 500, 10_000) </span><br><span class="line">    rf__n_estimators = trial.suggest_int(&#x27;rf__num_estimators&#x27;, 300, 500) </span><br><span class="line">    rf__max_depth = trial.suggest_int(&#x27;rf__max_depth&#x27;, 5, 15) </span><br><span class="line">    rf__min_samples_split = trial.suggest_int(&#x27;rf__min_samples_split&#x27;, 10, 30) </span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    params = &#123;</span><br><span class="line">        &#x27;tfidf__analyzer&#x27;: tfidf__analyzer,</span><br><span class="line">        &#x27;tfidf__lowercase&#x27;: tfidf__lowercase,</span><br><span class="line">        &#x27;tfidf__max_features&#x27;: tfidf__max_features,</span><br><span class="line">        &#x27;rf__n_estimators&#x27;: rf__n_estimators,</span><br><span class="line">        &#x27;rf__max_depth&#x27;: rf__max_depth,</span><br><span class="line">        &#x27;rf__min_samples_split&#x27;: rf__min_samples_split,</span><br><span class="line">       </span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    model.set_params(**params)</span><br><span class="line"></span><br><span class="line">    return  -np.mean(cross_val_score(model, X, y, cv=3, n_jobs=-1,scoring=&#x27;neg_log_loss&#x27;))</span><br></pre></td></tr></table></figure><p>Notice that, by default Optuna tries to minimize the objective function, since we use native log loss function to maximize the Random Forrest Classifier, we add another negative sign in in front of the cross-validation scores.</p><h2 id="4-Run-the-Optuna-trials-to-find-the-best-hyper-parameter-configuration"><a href="#4-Run-the-Optuna-trials-to-find-the-best-hyper-parameter-configuration" class="headerlink" title="4. Run the Optuna trials to find the best hyper parameter configuration"></a>4. Run the Optuna trials to find the best hyper parameter configuration</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># by default, the direction is to minimizae, but can set it to maximize too</span><br><span class="line">#study = optuna.create_study(direction=&#x27;minimize&#x27;)</span><br><span class="line">study = optuna.create_study()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#study.optimize(objective, timeout=3600)</span><br><span class="line">study.optimize(objective, n_trials=20)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># to record the value for the last time</span><br><span class="line">joblib.dump(study, &#x27;study.pkl&#x27;)</span><br></pre></td></tr></table></figure><p>Notice that, we are saving the hyper parameter optimization process into a local pickle file, which means we can monitor the process in the middle or at the end by opening another notebook.</p><h2 id="5-how-to-visualize-the-results"><a href="#5-how-to-visualize-the-results" class="headerlink" title="5. how to visualize the results"></a>5. how to visualize the results</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import joblib</span><br><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import optuna</span><br><span class="line"></span><br><span class="line">data = joblib.load(&#x27;study.pkl&#x27;)</span><br><span class="line"></span><br><span class="line">df = data.trials_dataframe()</span><br><span class="line">df.dropna(inplace=True)</span><br><span class="line">df.reset_index(inplace=True)</span><br><span class="line"></span><br><span class="line">df[&#x27;time&#x27;] = (df.datetime_complete - df.datetime_start).dt.total_seconds()</span><br><span class="line">df = df[df.time&gt;=0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(&#x27;best val:&#x27;,  round(df.value.min(),4))</span><br><span class="line">print(&#x27;best params:&#x27;,  data.best_params)</span><br><span class="line"></span><br><span class="line">a = sns.lineplot(x=df.index, y=df.value.cummin())</span><br><span class="line">a.set_xlabel(&#x27;trial number&#x27;)</span><br><span class="line">sns.scatterplot(x=df.index, y=df.value, color=&#x27;red&#x27;)</span><br><span class="line">a.set_ylabel(&#x27;log loss&#x27;)</span><br><span class="line">a.legend([&#x27;best value&#x27;, &quot;trial&#x27;s value&quot;]);</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-07-02-1.png" alt="Optuna tuning results with trails"></p><h2 id="6-download-the-whole-code-notebook"><a href="#6-download-the-whole-code-notebook" class="headerlink" title="6. download the whole code notebook"></a>6. download the whole code notebook</h2><p><a href="https://github.com/robotlearner001/blog/tree/main/hyper-parameter-tunning">github link</a></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hyperparameter tuning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to insert images in DataBricks notebook</title>
      <link href="2022/06/22/2022-06-22-1/"/>
      <url>2022/06/22/2022-06-22-1/</url>
      
        <content type="html"><![CDATA[<p>Following steps show how to upload images to databricks filestore, and insert that path in the notebook cell.</p><h2 id="upload-the-image-to-databricks"><a href="#upload-the-image-to-databricks" class="headerlink" title="upload the image to databricks"></a>upload the image to databricks</h2><p>As shown in the following, in the homepage of your databricks account, click the import and explore data part.</p><p><img src="/content/images/2022-06-22-1.png" alt="step1"></p><p>after the this, in the upload file menu, further click the drop file place to choose image file from your local computer.</p><p><img src="/content/images/2022-06-22-2.png" alt="step2"></p><p>with this being successful, you will see the path of the image you just uploaded, here our file name is called dbx_test.png,<br>as shown in the following:</p><p><img src="/content/images/2022-06-22-3.png" alt="step3"></p><h2 id="insert-the-image-path-using-markdown-in-notebook"><a href="#insert-the-image-path-using-markdown-in-notebook" class="headerlink" title="insert the image path using markdown in notebook"></a>insert the image path using markdown in notebook</h2><p>Notice that, the path begins with /FileStore/, but we need replace with file/, as shown in the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">%md</span><br><span class="line">![test image](files/tables/dbx_test.png)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>in the above code, “test image” is just some description you can name whatever you want to.</p><p>As the result, after your run the cell, you will see the image shown up like this:</p><p><img src="/content/images/2022-06-22-4.png" alt="step4"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> databricks </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to plot mulitiple subplots of barplots using seaborn</title>
      <link href="2022/05/15/2022-05-15-1/"/>
      <url>2022/05/15/2022-05-15-1/</url>
      
        <content type="html"><![CDATA[<p>Sometimes we want to plot multiple barplots in subplots, this examples shows a nice way to do it.</p><h2 id="load-sample-data"><a href="#load-sample-data" class="headerlink" title="load sample data"></a>load sample data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.set_theme(style=<span class="string">&quot;whitegrid&quot;</span>)</span><br><span class="line">tips = sns.load_dataset(<span class="string">&quot;tips&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tips.head(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>total_bill</th>      <th>tip</th>      <th>sex</th>      <th>smoker</th>      <th>day</th>      <th>time</th>      <th>size</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>16.99</td>      <td>1.01</td>      <td>Female</td>      <td>No</td>      <td>Sun</td>      <td>Dinner</td>      <td>2</td>    </tr>    <tr>      <th>1</th>      <td>10.34</td>      <td>1.66</td>      <td>Male</td>      <td>No</td>      <td>Sun</td>      <td>Dinner</td>      <td>3</td>    </tr>  </tbody></table></div><h2 id="aggregate-the-data-by-day-and-sex-and-sum-tips-for-each-dimension"><a href="#aggregate-the-data-by-day-and-sex-and-sum-tips-for-each-dimension" class="headerlink" title="aggregate the data by day and sex, and sum tips for each dimension"></a>aggregate the data by day and sex, and sum tips for each dimension</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tips_agg = tips.groupby([<span class="string">&#x27;day&#x27;</span>,<span class="string">&#x27;sex&#x27;</span>])[<span class="string">&#x27;tip&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">tips_agg</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>day</th>      <th>sex</th>      <th>tip</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Thur</td>      <td>Male</td>      <td>89.41</td>    </tr>    <tr>      <th>1</th>      <td>Thur</td>      <td>Female</td>      <td>82.42</td>    </tr>    <tr>      <th>2</th>      <td>Fri</td>      <td>Male</td>      <td>26.93</td>    </tr>    <tr>      <th>3</th>      <td>Fri</td>      <td>Female</td>      <td>25.03</td>    </tr>    <tr>      <th>4</th>      <td>Sat</td>      <td>Male</td>      <td>181.95</td>    </tr>    <tr>      <th>5</th>      <td>Sat</td>      <td>Female</td>      <td>78.45</td>    </tr>    <tr>      <th>6</th>      <td>Sun</td>      <td>Male</td>      <td>186.78</td>    </tr>    <tr>      <th>7</th>      <td>Sun</td>      <td>Female</td>      <td>60.61</td>    </tr>  </tbody></table></div><h2 id="install-the-latest-seaborn-package-if-not-installed"><a href="#install-the-latest-seaborn-package-if-not-installed" class="headerlink" title="install the latest seaborn package if not installed"></a>install the latest seaborn package if not installed</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">!pip install seaborn==<span class="number">0.11</span><span class="number">.2</span></span><br></pre></td></tr></table></figure><h2 id="visualize-the-total-tips-of-male-vs-female-for-each-different-weekday-in-the-data"><a href="#visualize-the-total-tips-of-male-vs-female-for-each-different-weekday-in-the-data" class="headerlink" title="visualize the total tips of male vs female, for each different weekday in the data"></a>visualize the total tips of male vs female, for each different weekday in the data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># automatically adjust the rows and colums </span></span><br><span class="line">all_days = <span class="built_in">list</span>(<span class="built_in">set</span>(tips_agg[<span class="string">&#x27;day&#x27;</span>]))</span><br><span class="line">n_cols = <span class="number">3</span></span><br><span class="line">n_rows = math.ceil(<span class="built_in">len</span>(all_days)/n_cols)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># give the figsize</span></span><br><span class="line">fig, axes = plt.subplots( n_rows, n_cols, figsize=(<span class="number">20</span>, <span class="number">10</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n_rows):</span><br><span class="line">  <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n_cols):</span><br><span class="line">    index = i*n_cols+j</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> index &gt;= <span class="built_in">len</span>(all_days):</span><br><span class="line">      <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    day = all_days[index]</span><br><span class="line">    bp = sns.barplot(ax=axes[i, j], data=tips_agg[tips_agg[<span class="string">&#x27;day&#x27;</span>]==day], x=<span class="string">&#x27;sex&#x27;</span>, y=<span class="string">&#x27;tip&#x27;</span>)</span><br><span class="line">    bp.<span class="built_in">set</span>(title=day)</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> bp.get_xticklabels():</span><br><span class="line">        item.set_rotation(<span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># if the x axis title is very long, this configuration will be very useful </span></span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure><p><img src="/content/images/2022-05-15-1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> seaborn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to use SentenceTransformer, a powerful and simple text embedding method based on transformer methods</title>
      <link href="2022/04/10/2022-04-10-1/"/>
      <url>2022/04/10/2022-04-10-1/</url>
      
        <content type="html"><![CDATA[<p>SentenceTransfomer is a Python framework developed based on Sentence-BERT that can generate high-quality embedding vectors for sentences and short texts.<br>Many languages including English and Chinese are supported.<br>Compared with the BERT model, the sentencetransformer is simpler to use, and the vector can be directly obtained by passing in the text.</p><h2 id="how-to-install"><a href="#how-to-install" class="headerlink" title="how to install"></a>how to install</h2><p>Recommended Python 3.6 or higher, PyTorch 1.6.0 or higher, and transformers v4.6.0 or higher developed by huggingface.<br>Notice that Python 2.7 environment will not work.</p><p>Sometimes, the installation may not be successful, it may be a pip version compatibility problem, you can upgrade the pip package first</p><p>python3 -m pip install –upgrade pip</p><p>then to this:</p><p>pip install -U sentence-transformers</p><h2 id="import-the-package"><a href="#import-the-package" class="headerlink" title="import the package"></a>import the package</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br></pre></td></tr></table></figure><h1 id="English-embedding-example"><a href="#English-embedding-example" class="headerlink" title="English embedding example"></a>English embedding example</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;paraphrase-MiniLM-L6-v2&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Our sentences we like to encode</span></span><br><span class="line">sentences = [<span class="string">&#x27;This framework generates embeddings for each input sentence&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;Sentences are passed as a list of string.&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;The quick brown fox jumps over the lazy dog.&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Sentences are encoded by calling model.encode()</span></span><br><span class="line">embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Print the embeddings</span></span><br><span class="line"><span class="keyword">for</span> sentence, embedding <span class="keyword">in</span> <span class="built_in">zip</span>(sentences, embeddings):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sentence:&quot;</span>, sentence)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Embedding:&quot;</span>, embedding)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Sentence: This framework generates embeddings for each input sentenceEmbedding: [-1.76214531e-01  1.20601252e-01 -2.93624073e-01 -2.29858026e-01 -8.22923928e-02  2.37709522e-01  3.39984864e-01 -7.80964196e-01  1.18127614e-01  1.63373962e-01 -1.37715712e-01  2.40282789e-01  4.25125599e-01  1.72417849e-01  1.05279692e-01  5.18164098e-01  6.22218400e-02  3.99285793e-01 -1.81652278e-01 -5.85578680e-01  4.49722409e-02 -1.72750309e-01 -2.68443495e-01 -1.47386149e-01 -1.89217970e-01  1.92150578e-01 -3.83842468e-01 -3.96007091e-01  4.30648863e-01 -3.15320134e-01  3.65949631e-01  6.05158620e-02  3.57325703e-01  1.59736529e-01 -3.00983816e-01  2.63250291e-01 -3.94311100e-01  1.84855521e-01 -3.99549276e-01 -2.67889529e-01 -5.45117497e-01 -3.13403942e-02 -4.30644333e-01  1.33278117e-01 -1.74793795e-01 -4.35465544e-01 -4.77379113e-01  7.12555572e-02 -7.37001151e-02  5.69137156e-01 -2.82579720e-01  5.24975285e-02 -8.20007861e-01  1.98296756e-01  1.69511825e-01  2.71780342e-01  2.64610827e-01 -2.55737714e-02 -1.74096107e-01  1.63314253e-01 -3.95260930e-01 -3.17556299e-02 -2.62556046e-01  3.52754712e-01  3.01434875e-01 -1.47197291e-01  2.10075796e-01 -1.84010491e-01 -4.12896037e-01  4.14775789e-01 -1.89769492e-01 -1.35482445e-01 -3.79272133e-01 -4.68020439e-02 -3.33601385e-02  9.00394097e-02 -3.30133140e-01 -3.87316942e-02  3.75082314e-01 -1.46996319e-01  4.34959829e-01  5.38325727e-01 -2.65445173e-01  1.64445907e-01  4.17078644e-01 -4.72508594e-02 -7.48731196e-02 -4.26261097e-01 -1.96994558e-01  6.10316209e-02 -4.74262655e-01 -6.48334742e-01  3.71462464e-01  2.50957102e-01  1.22529611e-01  8.88766572e-02 -1.06724210e-01  5.33984490e-02  9.74507183e-02 -3.46660167e-02 -1.02882817e-01  2.32289001e-01 -2.53739536e-01 -5.13112307e-01  1.85216278e-01 -3.04357797e-01 -3.55209075e-02 -1.26975372e-01 -7.71632940e-02 -5.15330076e-01 -2.28071719e-01  2.03343164e-02  7.38175958e-02 -1.52558655e-01 -4.00837570e-01 -2.47749180e-01  3.97470325e-01 -2.60260701e-01  2.50906169e-01  1.68228924e-01  1.33900508e-01 -2.10833233e-02 -4.70035732e-01  4.78850156e-01  2.80345589e-01 -4.64546800e-01  3.21747035e-01  2.34207422e-01  2.45772451e-01 -4.71482307e-01  5.00400960e-01  4.10190076e-01  5.15216827e-01  2.62549460e-01  2.11593546e-02 -3.89687568e-01 -2.41742760e-01 -2.14834630e-01 -8.62650797e-02 -1.65323570e-01 -5.21895029e-02  3.41874868e-01  4.50314462e-01 -3.06973577e-01 -2.02294186e-01  6.85521722e-01 -5.33892572e-01  3.58471543e-01  1.45286605e-01 -7.07056001e-02 -1.50529072e-01 -8.56279582e-02 -7.67851025e-02  1.89544857e-01 -1.04067773e-01  5.33544004e-01 -5.27887225e-01  2.42332090e-02 -2.64348090e-01 -2.23186895e-01 -3.81208718e-01  7.59914368e-02 -4.64485109e-01 -3.36549252e-01  4.21229839e-01  1.07479207e-01  1.90457791e-01  2.89487489e-03 -1.08513705e-01  1.53545350e-01  3.16023648e-01 -2.70840749e-02 -5.40594459e-01  8.97286758e-02 -1.15549676e-01  3.97803992e-01 -4.97683346e-01 -2.84893364e-01  4.99861799e-02  3.61279696e-01  6.90535665e-01  1.46821439e-01  1.73396602e-01 -1.74582347e-01 -3.15702260e-01  6.72999769e-02  2.17250243e-01  9.78535116e-02 -1.29472464e-01 -1.86929435e-01  1.34878129e-01 -1.53885290e-01  7.44715557e-02 -1.85536250e-01 -2.80628383e-01 -1.14144213e-01  4.12249625e-01  6.39491975e-02 -1.45715117e-01 -9.82065052e-02 -1.33081883e-01 -1.88410461e-01 -2.84838937e-02 -3.49510163e-02  3.34258713e-02  6.98896796e-02  1.90354511e-01 -2.96724051e-01  2.64706067e-03  1.09140947e-01  1.70892701e-02  2.60589242e-01  3.29038620e-01 -6.61560148e-02  2.39665717e-01 -2.26194620e-01 -3.36869545e-02  1.49400130e-01 -3.21265638e-01 -2.68577904e-01  5.72632015e-01 -4.92308497e-01  2.00666577e-01 -3.49261820e-01 -2.89886612e-02  6.09010458e-01 -5.72333157e-01  2.35000670e-01  6.47180574e-03 -3.14952508e-02  2.78108083e-02 -3.90340954e-01 -2.08950117e-01 -3.04452837e-01 -7.20199272e-02 -8.29840004e-02  3.73792857e-01  7.38937110e-02 -2.21076086e-02  9.88139287e-02 -1.51426882e-01 -1.40430734e-01  2.26017952e-01  2.76089966e-01 -8.87747630e-02 -1.12816028e-01 -2.66286045e-01  2.77834296e-01 -4.75609973e-02  6.71005547e-02 -2.78584175e-02 -2.39991937e-02  2.51708686e-01  4.68793899e-01 -5.39325476e-01  1.10598475e-01 -3.44947308e-01  4.15990084e-01  7.28483498e-02 -3.19647521e-01  4.90374565e-01 -7.30331149e-03 -2.64252443e-03  9.63711083e-01  3.23884904e-01 -7.79616535e-02 -2.37589255e-01  2.34038591e-01 -3.16053987e-01 -1.65628293e-03 -1.09070671e+00  3.38409364e-01  4.70607281e-02  1.07435413e-01 -2.06672356e-01  4.26446088e-03 -1.38461241e-03 -5.31455636e-01 -2.75648654e-01 -1.64648622e-01 -3.42916757e-01 -4.26118672e-01  6.01811945e-01  4.55971897e-01 -2.72701889e-01 -3.45802940e-02  2.62752354e-01 -6.34185225e-03  2.79631346e-01 -2.53559053e-01 -1.68626338e-01  3.82934660e-02  2.07763135e-01 -4.31525975e-01 -7.24000558e-02 -1.26854450e-01  2.07029749e-02  5.74441731e-01  3.54672432e-01  9.28300545e-02  6.70508668e-02  1.11520678e-01 -1.86510980e-02  4.62352097e-01  2.72504926e-01 -3.60474110e-01  5.29415369e-01 -1.00318261e-03 -8.81360695e-02  1.49975494e-01  5.25862724e-02  4.63517487e-01 -3.96831542e-01  2.42640823e-01 -2.08912537e-01  3.65672290e-01 -4.73500433e-04  5.33963263e-01 -1.97879612e-01  3.11582744e-01 -6.96715057e-01 -4.29500550e-01 -4.49359566e-01 -2.71372199e-02 -6.98710978e-02  2.06174582e-01 -1.57107800e-01  4.43521202e-01 -6.74267337e-02 -3.00924033e-01  5.14859557e-01  3.36029828e-01  6.63376600e-02 -1.15235247e-01 -2.95982286e-02  2.79471755e-01 -3.48201916e-02 -7.29324743e-02 -4.58472408e-02  1.54262856e-01  8.09356570e-01  5.20328224e-01 -4.02114749e-01 -3.23152021e-02 -1.10364027e-01  7.50505254e-02 -1.51098579e-01  8.45740080e-01 -1.80843890e-01  3.22573632e-01  1.04708321e-01  3.19663644e-01 -1.55085236e-01  1.69236735e-01 -2.56996632e-01  2.01208770e-01  1.77393183e-01 -2.74333179e-01 -3.36944580e-01  5.02356887e-01 -1.18357182e-01 -2.01166973e-01 -5.36485910e-01 -7.69810379e-02  1.15382867e-02 -2.36464664e-01 -2.98771430e-02  1.31366640e-01  2.94184387e-01  9.90917012e-02 -5.43897867e-01  1.40812770e-01  3.66998672e-01  5.04861325e-02  1.99122518e-01 -2.80674815e-01  4.34192061e-01 -1.40274823e-01  5.78048944e-01  1.77715778e-01  8.98364484e-02  3.29651892e-01  6.13008775e-02 -3.24933589e-01]Sentence: Sentences are passed as a list of string.Embedding: [ 0.32208762 -0.00123908  0.17937377 -0.36919138 -0.06460274  0.09153692  0.24119096 -0.29494217  0.07728957  0.11577005 -0.04479983  0.17928234  0.1475363   0.21511652  0.36810791  0.20910913  0.27194238  0.34880087 -0.57251936 -0.18253218  0.4448957   0.27452925  0.04266282 -0.07683562  0.18689147  0.4496505  -0.16932622 -0.24896334 -0.20479265  0.40285036 -0.2101927   0.03775701  0.07848521  0.12848447  0.02593089  0.4715598  0.17853785 -0.07379771  0.08130724 -0.23328738 -0.49801245 -0.04135723 -0.12094605  0.17028998 -0.19154078 -0.38459808 -0.77479136 -0.10622733 -0.2304489   0.4024145  -0.8745089   0.23853712 -0.4712986   0.21262182  0.3340935  -0.24154    -0.1483509  -0.14513564 -0.34830925 -0.08349245 -0.69097275 -0.29845262 -0.12230504  0.07482646 -0.18775596 -0.3754651  0.21369492 -0.10096409 -0.12234445  0.31431514 -0.23989926  0.22460794  0.0399599   0.36034834 -0.5663802   0.21883512  0.11020288 -0.10870823  0.07084075 -0.02608179  0.18370324  0.08465949 -0.20478249 -0.24435617 -0.08180565 -0.01903111 -0.03591372  0.02398443 -0.2855857   0.07374766 -0.29744208 -0.87717843  0.47101936 -0.0494047   0.36394492  0.482644  0.01564615  0.03558917 -0.26203    -0.1121847   0.0241104   0.37477782 -0.09897303 -0.09851858  0.15000843  0.00689534 -0.12652436 -0.3159893  0.31449488 -0.2942561  -0.2694104   0.20221162  0.14329888 -0.19584627 -0.3410443  -0.03172762  0.7365027   0.31923506  0.2438129   0.30732602  0.09933245  0.19010943 -0.10694525  0.05178664  0.03233436 -0.10314636  0.2649922   0.31206465  0.43152618 -0.6426122   0.0840958  -0.0432735 -0.04991193 -0.12718563  0.13789187  0.01306224  0.34383258  0.09234294 -0.09922738 -0.52159905  0.25842273 -0.01057126 -0.00478189  0.03938858  0.19086099  0.32933885 -0.24345161 -0.07328319 -0.39280015  0.14541808  0.32839534 -0.04184634  0.07407122 -0.7386053  -0.09076015  0.15802307 -0.09780021 -0.21605958 -0.30027467  0.2323658   0.01072446  0.49570465  0.04974837  0.29931435 -0.05382248  0.35328114  0.3419176   0.49667245 -0.4860523  -0.19098845  0.8154575   0.22962622 -0.32077783 -0.32726687 -0.367717    0.3452114  -0.02620159 -0.14315048  0.10648432 -0.24638046 -0.09366632  0.17198639 -0.08508814  0.20120296 -0.05879208 -0.34020975 -0.19565329  0.2828087   0.20124306 -0.08207255  0.09779122 -0.26375008  0.12176559 -0.01041479 -0.4385985   0.11058219  0.48010394 -0.10981981 -0.6375458   0.29336807 -0.1920764   0.46536973  0.2704201   0.19388463  0.17379023 -0.3007702  -0.0275121  -0.02291276  0.3678463   0.02492153  0.53705496  0.18851233 -0.13344418  0.08917347  0.05542957 -0.24818316 -0.04199777  0.05767386 -0.18278812 -0.41686475  0.16070595 -0.46362543  0.11769217 -0.3770692   0.02960374  0.692561   -0.4830891   0.21128401  0.18214527 -0.18429579  0.0681766  -0.02460879 -0.19073623 -0.06736984 -0.5670072  -0.23929311 -0.08497227  0.03093951  0.31079894  0.1291628  0.05248259 -0.3344983   0.18810134  0.23547177 -0.00183485  0.45361614  0.24885082 -0.05641065 -0.2977458  -0.43511704 -0.07969435 -0.17670156 -0.13347092  0.1938272   0.22002596 -0.11057543  0.26473755 -0.27179065  0.03410878 -0.4771442   0.44719058 -0.05570416  0.39643747  0.27483267  0.33305615 -0.10890252  0.2788817   0.21596934 -0.05252272 -0.35867548 -0.69062907  0.03960182  0.00652785 -0.01095348 -0.10027702  0.04770013 -0.34146932 -0.16714163  0.07136464 -0.1807847  -0.30248472 -0.68428737 -0.09592848 -0.2141112  -0.6552438   0.5675644   0.26946738 -0.00190061  0.8618065   0.16771556  0.03102748 -0.2677305  -0.07830263 -0.48510885 -0.26737222 -0.33354253 -0.5738254   0.35678256  0.08993588 -0.13057196 -0.1513651  -0.06124125 -0.13037089  0.5585606   0.614175   -0.04804054 -0.0638859   0.08390605 -0.25143692 -0.04359839 -0.18525787  0.04693348 -0.34380865 -0.09738468  0.1683365   0.0752685   0.1769452   0.17727177 -0.03423442  0.14993554 -0.13773166 -0.20949684 -0.6127283   0.3781397  0.3901828  -0.08359346  0.03152128  0.13122386  0.38826075  0.21844254  0.0972431   0.42089358 -0.3264124  -0.26933426 -0.39095107 -0.22648653 -0.32020715 -0.16287427 -0.03581636  0.363739    0.18583307 -0.0291401 -0.46577957  0.2916888   0.37251312 -0.23726626  0.00338617  0.41540965  0.03300428  0.4500395  -0.08159234  0.33990335  0.24497904  0.0235242 -0.1464306  -0.12644552  0.31128627 -0.15182619  0.01009398  0.49108496  0.14362407  0.11589024 -0.23236984  0.2475176   0.1836449  -0.24836856 -0.11220913 -0.23113322  0.0842896  -0.24378659  0.13307258  0.42355725  0.33348376 -0.3437014   0.0344367   0.18795514  0.20037197 -0.05355961  0.2848529   0.0717658   0.05487169 -0.08103789  0.27076873  0.11700248]Sentence: The quick brown fox jumps over the lazy dog.Embedding: [ 0.58979344 -0.23598255 -0.25411725  0.00311624 -0.08485737 -0.26799768 -0.07506651 -0.3002136   0.05151652  0.16585363  0.26076776  0.38256362  0.43732867 -0.09301949 -0.26568803 -0.09716298 -0.48096094  0.11878292  0.13675483  0.04712067 -0.23696537 -0.52332336 -0.01631867  0.06127304 -0.7433302  -0.11898906 -0.7886529  -0.48108855  0.10314927 -0.32372454  0.8144374  -0.39774537 -0.50315547 -0.7972457  -0.6324822   0.32320985 -0.38419437 -0.11186695 -0.1324357   0.02069666 -0.14309539 -0.0370119  0.06116579  0.16332883 -0.11174309  0.25234267 -1.0464071  -0.37252343  0.15601997 -0.29991606  0.19883864  0.2343344  -0.3702577   0.31733564  0.84428644  0.06977719  0.03273663  0.09948339 -0.31141308  0.50517714  0.00309272  0.38013652  0.04582737  0.00633379 -0.00142918 -0.13568659 -0.07611365 -0.25844312 -0.8022129   0.5508589  -0.09124386 -0.21782017 -0.78810936 -0.5118384   0.46672547  0.55274725 -0.37124714 -0.18645377  0.3585697  -0.19586323  0.18042535 -0.42548886 -0.09681422 -0.05536837  0.52489287  0.24481142  0.01934664 -0.29637936 -0.1277783  -0.30534947  0.45349374  0.07469101 -0.07061689  0.2624302   0.37383935  0.14306359  0.00127857 -0.41776088 -0.24014093 -0.2509353   0.34843782  0.31144044  0.0808733  -0.5764053   0.54085284 -0.01802203 -0.12959798 -0.07399664  0.3936979   0.6488388  -0.02030003 -0.5665558   0.2967598   0.520002  0.21538728  0.10369676  0.06199208  0.01896283 -0.15269236 -1.0642662  0.7614961   0.20734386  0.44718924  0.14493968  0.6580228  -0.09440905 -0.23316365  0.4215707   0.1195763  -0.32571068  0.16425563 -0.49508703 -0.19516118 -0.56183213 -0.14933276  0.610941   -0.17897959 -0.01805552 -0.5964053   0.04918591  0.15347804 -0.42829406  0.73295283 -0.35291103 -0.11159656  0.06127812 -0.29704392  0.4396664  -0.09660351  0.65579444 -0.61403424  0.02576606  0.43827453  0.01733282 -0.40002275 -0.08178308 -0.37126926  0.08230279 -0.13104396 -0.5326111  -0.29928368  0.6993657 -0.04398742 -0.15702991  0.09794132 -0.03017466 -0.10002708  0.199966 -0.4818854   0.17949156  0.5656603  -0.11954784 -0.696373    0.05259641 -0.00549608  0.16739355 -0.31692895 -0.09747531  0.33193663  0.4719962  0.12653954  0.19130926  0.42949092  0.55291235  0.31463274 -0.31433088 -0.41508684  0.32897702  0.35702732 -0.1920966   0.22239415 -0.48717856  0.3409156  -0.22137432 -0.1266758   0.21120834 -0.31347895  0.8468937  0.20112668 -0.42598733  0.5131572  -1.2351419   0.76971793 -0.17414267 -0.02181136 -0.0356865  -1.105949   -0.5720654   0.05585196  0.12461495 -0.45065832  0.06428951 -0.16033873  0.39932927 -0.10322935 -0.02025502 -0.18010448  0.06234786 -0.02188883 -0.15795426  0.28316957  0.02385282  0.03098137 -0.07853306  0.29896578 -0.06237327  0.5498678   0.17862315  0.2116474   0.44483367  0.04890747 -0.162381   -0.22669895  0.18871985  0.07943622  0.1359757  -0.18484493  1.113551    0.82809544 -0.31202707  0.09505999  0.05096091  0.38804898  0.25000468  0.558486    0.31088772 -0.05318568 -0.07675371  0.15282278  0.09189964 -0.01429148  0.6657543 -0.03346021 -0.44703493  0.80067486 -0.4799278   0.17478174 -0.30563813  0.5536521   0.42380962  0.48674306 -0.49677992 -0.45194814 -0.95563084 -0.20709987 -0.22605716 -0.0099914   0.98797685  0.5880775   0.08305439 -0.5578132   0.21136862 -0.3607222   0.52668494  0.33983573 -0.15756187  0.00423779 -0.05354516 -0.5777671   0.5595104  -0.05747148  0.16837652  0.37946853 -0.25776428  0.08421484 -0.15229936 -0.03280768  0.10083867 -0.41858304 -0.44499016 -0.29309887  0.6144206   0.08548218 -0.06349564 -0.6152552   0.79544085 -0.24058406  0.2063889  -0.5125261   0.6312013  0.36744294 -0.4400988   0.46913967  0.23087728 -0.13737966  0.21696861  0.40043226 -0.02490607 -1.139676    0.02653918 -0.32730213  0.09984124  0.05725667 -0.84722155  0.06451946  0.45698035  0.63563025  0.45185634 -0.2751906   0.21346165  0.17374253  0.42822042 -0.6584535   0.4000258 -0.02035557 -0.6730788  -1.0269238   0.16877282 -0.09248707 -0.79977626  0.38093373  0.5171233   0.04200954 -0.04867526 -0.18772238  0.16339499 -0.21974911  0.21939285  0.03676502 -0.29750267 -0.37409678 -0.52095085 -0.41314605 -0.489477   -0.8189662   0.08531482  0.34576944  0.12505981  0.24945222 -0.25254658 -0.03156116  0.27573124 -0.60857177  0.3357  0.22913106  0.6607082  -0.30215803 -0.05315317  0.22247504  0.06138719  0.33555198 -0.0848518   0.08764573  0.10872053 -0.40389338 -0.14949782  0.19458489 -0.81060654  0.79730946 -0.41162547  0.01364165  0.23472963 -0.09732277 -0.29044035  0.03843231 -0.07090472 -0.17404465 -0.44859377 -0.31867278  0.4165608  -0.05431673  0.14036179  1.0559161   0.5301814 ]</code></pre><h2 id="English-embedding-example-with-a-new-pretrained-model"><a href="#English-embedding-example-with-a-new-pretrained-model" class="headerlink" title="# English embedding example with a new pretrained model"></a># English embedding example with a new pretrained model</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sentence_transformers <span class="keyword">import</span> SentenceTransformer</span><br><span class="line">model = SentenceTransformer(<span class="string">&#x27;distiluse-base-multilingual-cased-v1&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Our sentences we like to encode</span></span><br><span class="line">sentences = [<span class="string">&#x27;早上好&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;今天天气非常不错&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#Sentences are encoded by calling model.encode()</span></span><br><span class="line">embeddings = model.encode(sentences)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Print the embeddings</span></span><br><span class="line"><span class="keyword">for</span> sentence, embedding <span class="keyword">in</span> <span class="built_in">zip</span>(sentences, embeddings):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Sentence:&quot;</span>, sentence)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Embedding:&quot;</span>, embedding)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&quot;</span>)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><pre><code>Sentence: 早上好Embedding: [ 3.36048612e-03 -7.75053650e-02 -8.92517120e-02 -1.98542904e-02  4.86167856e-02  4.26223725e-02 -3.25850174e-02 -6.78801164e-03  1.58829652e-02 -2.43490171e-02  1.48126129e-02  6.55123359e-03  1.14513915e-02  3.69007923e-02 -1.88735481e-02 -1.18097216e-02  8.75925422e-02  3.32120210e-02  4.86210473e-02 -9.08677280e-03  1.27627403e-02 -3.61923464e-02 -2.31466126e-02 -2.86142770e-02 -2.80293375e-02  2.24176962e-02  9.83241759e-03 -1.76876169e-02 -3.93512547e-02 -6.60814941e-02 -6.64236844e-02  4.22375835e-02 -4.41516377e-02 -4.42469940e-02  5.83056957e-02 -6.70481995e-02 -3.44123617e-02  4.91611883e-02  1.23786274e-02  2.88232397e-02  3.48660462e-02 -9.41558462e-03 -4.65008654e-02  3.93482149e-02 -2.03807093e-02 -9.81736463e-03 -4.63420851e-03  4.13698182e-02 -7.43050594e-03  5.26707247e-02  4.63205464e-02  1.36748822e-02 -4.64802980e-02 -4.62170877e-02 -3.83359045e-02 -7.37623349e-02 -2.15301700e-02 -7.80433509e-03  6.28040507e-02 -1.95669127e-03  2.84056198e-02  5.01199141e-02 -4.69665565e-02  2.13841908e-02 -7.29201362e-02 -1.84297003e-02 -4.63314839e-02 -4.66326475e-02  4.06820178e-02 -2.62741838e-02 -1.58835202e-04  1.89249944e-02 -1.58798080e-02 -5.56862354e-02 -1.65423111e-03  5.79530513e-03 -4.80827615e-02  2.83442414e-03 -3.37332375e-02 -4.59188670e-02  1.84609480e-02 -8.43395516e-02 -4.98937443e-02  1.62628964e-02 -2.68908050e-02  2.94992961e-02  3.94213945e-02  1.89540815e-02 -1.49033396e-02  1.54328160e-02  6.68804627e-03 -2.17899680e-02 -6.37249798e-02  7.95044973e-02 -6.71640486e-02  1.53331548e-01  1.22375209e-02 -7.15146493e-03 -3.51416469e-02 -5.64979836e-02 -7.00105280e-02  2.48073991e-02  1.99343208e-02  3.48216183e-02  3.10467370e-02  4.85268384e-02 -1.48683321e-02 -1.44339688e-02 -1.83274597e-02 -2.25209910e-02 -4.66139577e-02  8.46805871e-02  5.55498600e-02 -4.86311466e-02  8.04354995e-02  5.78265125e-03 -3.33314948e-02  1.41728045e-02  7.46281222e-02  1.23108663e-02  1.29789282e-02  2.50214320e-02 -1.73974875e-02 -2.06596926e-02 -8.00771266e-03 -4.48289625e-02  3.21343057e-02  2.96824127e-02 -2.19148956e-02 -8.40266049e-02  2.08409578e-02 -5.84821552e-02  9.26640071e-03 -7.78274285e-03  3.21653970e-02  1.23768868e-02  1.75757147e-02 -3.77148017e-02 -6.50716200e-02 -2.81811506e-02  6.65956060e-04 -4.59337793e-02 -2.91092098e-02  1.06873184e-01 -9.06297788e-02  5.74022382e-02 -1.92602631e-02 -9.17858677e-04 -1.03564970e-02 -9.53326374e-03 -1.40155749e-02 -5.38532920e-02 -1.45433499e-02 -8.01574625e-03  2.73187216e-02 -1.10087404e-02 -8.70806128e-02 -5.07052056e-02 -1.94765590e-02  3.09801456e-02  5.79237528e-02 -4.61821295e-02 -6.85098171e-02 -1.12160509e-02  1.87328327e-02 -1.87243987e-02  8.43074173e-03  4.05288208e-03 -1.77395716e-03 -4.84117605e-02 -5.69200628e-02  1.21792853e-01 -8.72146338e-05  7.16463253e-02 -7.18049286e-03  1.85641591e-02  1.04164807e-02  2.23379545e-02 -2.17134301e-02  6.76098093e-02 -2.82478407e-02  3.98118485e-04  1.20027447e-02  2.38112584e-02 -3.34599502e-02 -2.01173555e-02 -6.13471568e-02  8.10436439e-03  4.63223346e-02 -7.35998601e-02 -7.45181814e-02  6.75673410e-02  4.85108085e-02  1.16789509e-02 -4.41863127e-02  3.12849879e-04 -3.95977460e-02  1.64179280e-02  7.68655736e-04  2.21294779e-02  1.06583117e-02 -1.15191847e-01 -7.83179179e-02 -9.30088758e-02  2.98546683e-02  1.93161629e-02  4.65247072e-02 -3.15551423e-02  1.53468028e-02 -2.31775502e-03 -3.51057500e-02  6.75020069e-02  3.89646366e-02  2.36660838e-02  8.71817674e-03  2.33443081e-02  6.04395606e-02 -5.73495515e-02 -7.63548985e-02  1.81578603e-02  5.78259937e-02  3.85002531e-02 -9.10978578e-03  6.28609152e-04  4.22109812e-02  2.34038550e-02 -4.32434771e-03  5.22437766e-02  2.75171399e-02  1.14117227e-02 -1.94266194e-03  4.50319564e-03  7.96783529e-03 -1.06010869e-01 -7.45600974e-03  4.15139422e-02 -2.61434522e-02 -3.02400477e-02 -3.69867403e-03  5.33738770e-02  1.22662308e-02 -7.78622404e-02 -1.04798714e-03  4.03577797e-02  1.95078198e-02  6.87584048e-03  1.52195273e-02 -2.21762992e-02  3.01569123e-02  4.99859042e-02  1.69460364e-02  2.80374940e-02 -1.22692548e-01 -6.09890744e-02  6.93573654e-02 -4.64934334e-02 -2.55377777e-03  9.13314708e-03  1.88059080e-02 -3.50909606e-02 -3.23721021e-02 -1.99456997e-02 -1.85215636e-03  9.07397363e-03 -1.60663063e-03  6.72434550e-03  1.51269864e-02 -2.88711805e-02  1.87143628e-02  3.21673527e-02  1.10762762e-02  5.36762364e-02  1.00923926e-02 -6.80830050e-03  1.38309216e-02 -1.52337942e-02 -1.88095886e-02  4.73398566e-02  1.07036727e-02 -1.34082846e-02  2.46402621e-02  4.00331616e-03 -3.54318060e-02 -3.82929370e-02  5.72114717e-03 -3.37869078e-02  9.22745746e-03 -2.55183261e-02 -6.68449188e-03 -2.95727719e-02 -5.30545451e-02 -1.42496992e-02  1.21139064e-01  5.80232916e-03  6.85878769e-02  3.29775847e-02 -3.54181491e-02  1.07272621e-02 -4.35234942e-02 -3.61797120e-03  4.35319096e-02 -7.25895017e-02  2.31458414e-02 -6.59776181e-02 -9.39039700e-03 -5.54487482e-02 -2.98391879e-02  3.77000533e-02 -4.21746895e-02 -6.79707155e-05 -4.04625982e-02  2.20605936e-02 -2.00542212e-02 -3.81324701e-02 -5.30191511e-02 -6.33813813e-02 -5.16399965e-02  9.64144524e-03  5.90449497e-02 -4.24992219e-02  1.87409557e-02  1.59757817e-03  2.12688763e-02  1.16855623e-02  8.13119933e-02 -3.76809179e-03 -4.92693763e-03  3.39190066e-02  7.72952859e-04  4.17263247e-02 -8.17925707e-02  6.20350353e-02 -1.06017468e-02  4.36364748e-02  1.06892204e-02  4.46716622e-02 -1.07703032e-03 -5.20897424e-03  4.62425686e-02 -1.77654065e-02 -4.77417782e-02 -7.94410184e-02 -3.87781188e-02  3.52257639e-02  8.99284892e-03 -4.21371032e-03 -6.33117780e-02  4.79902467e-03 -2.65776385e-02  6.36078864e-02 -2.88559869e-02 -1.88005026e-02 -6.77149072e-02 -1.77993905e-02 -1.74105745e-02  1.92977523e-03 -4.24707830e-02 -3.41327414e-02 -4.64667156e-02  1.20677715e-02 -5.59292501e-03 -2.15960406e-02  1.65096892e-03  4.98509929e-02  6.74478710e-03 -1.25451041e-02 -3.78884971e-02 -2.44544051e-03  1.05662392e-02 -2.92542093e-02 -2.42053512e-02 -4.72463332e-02 -6.66808896e-03  4.26904894e-02 -1.38995284e-02 -7.33149797e-02 -1.20323701e-02  1.33620957e-02 -6.83322325e-02  5.24532832e-02 -3.99674661e-03 -3.77754751e-03 -4.37322631e-02  3.01865134e-02 -2.41089566e-03 -3.91000248e-02 -7.14202551e-03  1.61565766e-02  2.38034059e-03 -1.91217046e-02 -3.19050848e-02  2.66811196e-02 -3.00042611e-02  6.37865961e-02  9.43038985e-03  9.79864821e-02 -1.36476476e-02 -3.53143290e-02 -1.68743916e-02 -1.17210001e-02 -1.76749416e-02 -2.15478931e-02  6.98762164e-02  6.34187385e-02  3.40041555e-02 -2.52816733e-02 -1.68120004e-02 -3.73728164e-02 -4.34950478e-02 -2.41863336e-02  5.50701953e-02 -3.69273759e-02  2.01697815e-02 -5.65395206e-02 -2.43306421e-02  3.90052013e-02  8.88361968e-03 -7.37517253e-02  1.38345852e-01  9.28618014e-02  1.69148594e-02 -6.06265031e-02 -4.29087738e-03  9.36304405e-03 -6.03743903e-02  6.65058494e-02  9.13580060e-02 -4.83134501e-02 -4.48018834e-02 -2.56855302e-02 -1.47311706e-02 -6.67072907e-02  6.08391464e-02 -1.89301930e-02  5.48280887e-02  5.90014877e-03  5.10426983e-02 -2.76698507e-02  5.75542785e-02  1.65380053e-02  4.50753085e-02 -9.17072967e-02 -2.97022844e-03 -1.58547808e-03  4.13699523e-02 -2.40925550e-02 -8.28880146e-02 -4.60402220e-02 -2.80448776e-02  8.56529176e-02  1.21937310e-02 -1.45478994e-02  2.00680923e-03  5.76159284e-02 -8.33749920e-02 -8.44795082e-04  1.61427092e-02  1.06292889e-02  4.48290072e-02  3.78188565e-02  3.22153270e-02  7.65506970e-03  8.83217156e-02  1.07656186e-03 -4.78400923e-02 -1.30352927e-02  2.12222282e-02  4.37513888e-02 -6.99853227e-02 -2.67156325e-02 -5.85299730e-03 -6.67774230e-02 -4.67196293e-02  4.51602452e-02  5.02196476e-02  7.23657906e-02 -3.45671624e-02  1.77986156e-02  6.36029616e-02  6.04263172e-02 -2.88534202e-02  7.63596641e-03  8.91817287e-02 -1.73596032e-02 -1.14401756e-03 -3.49274017e-02 -3.11373584e-02 -1.08088255e-02  6.22482738e-03  6.60343748e-03 -1.39189782e-02 -1.58823899e-03  3.98824885e-02 -1.50108114e-02  2.18414310e-02  2.50639417e-03 -2.71914285e-02  1.23682460e-02 -1.60910413e-02 -5.62604703e-02  3.08268126e-02 -4.40148003e-02  6.95617199e-02 -3.85880619e-02 -1.06069623e-02  6.90155923e-02 -2.68800631e-02 -7.05972910e-02 -8.45417567e-03 -2.90030837e-02  3.72898579e-02 -1.82219949e-02]Sentence: 今天天气非常不错Embedding: [ 0.03000144 -0.09463879 -0.00121101  0.00919701  0.09230766  0.05803198 -0.06870171  0.08796345  0.05131664  0.05268763  0.06015535 -0.01480623  0.01694882  0.02787215 -0.02760382 -0.01491595  0.04491248  0.07975394 -0.01115547 -0.0462878  -0.04860514  0.01397826  0.02661177  0.05335874  0.0504394  -0.03782453  0.00266618 -0.00529313 -0.06329554  0.00695427 -0.01284165  0.01447636 -0.07145751 -0.08993939  0.03178086 -0.06282265 -0.04155265  0.04588643  0.03874922  0.03362899 -0.00353874  0.01936172 -0.00900048  0.02823229 -0.09740815  0.03750192  0.03347944  0.02718792  0.00610443  0.00467336  0.03310893 -0.03908279 -0.0221571  -0.09013501  0.01532278 -0.05820091  0.00157651 -0.09490337  0.0537669  -0.00431305  0.04847043 -0.03206552 -0.026307   -0.01593183 -0.04248736 -0.04010082 -0.01602365 -0.02053563 -0.03909144  0.03412447  0.03093878  0.00616256  0.0216082  -0.06444417  0.080314    0.02742311 -0.06018759  0.02321919 -0.05734614 -0.06155207  0.04174984 -0.10371719  0.03880399 -0.01905175 -0.0258333   0.03389469  0.00468696  0.0387875  -0.00673322  0.00383488 -0.01235164 -0.03161037 -0.02153032  0.00984649  0.01701394  0.0436353 -0.01779626 -0.02531954 -0.04274719  0.04862759 -0.05897103  0.00572619 -0.03335102 -0.01182074  0.02701874 -0.01848033 -0.02704814  0.01843894  0.06236629  0.04626593 -0.03967442  0.06011495  0.01706262 -0.07964841  0.04118946 -0.066379    0.09199478  0.04699905  0.12130994  0.01098934 -0.02470355 -0.06073544  0.0464027   0.00273122 -0.03935859  0.02141794 -0.0184507   0.03051549 -0.00163435 -0.05913333  0.00292336 -0.07933108  0.03902374 -0.06467438 -0.0390107  -0.03507318 -0.07895196 -0.05337577 -0.09487265  0.04261077 -0.04158578 -0.0317761  -0.11317404  0.01106471 -0.05172293  0.03777007  0.00772178  0.11905682  0.01442179 -0.02014892 -0.01100211 -0.03005649 -0.04160342 -0.05648704  0.01849159 -0.05435764 -0.0839037  -0.05800341  0.02173088  0.00356943  0.05853926  0.1246719  0.02843452  0.08176696  0.0147953  -0.00317226  0.04797639 -0.00291128 -0.09280773  0.02252468 -0.05960978  0.12289534 -0.03065114  0.06989978  0.04467983  0.07636563  0.01817677 -0.00636483 -0.01898711  0.01658535  0.00860319  0.0283874  -0.05914772 -0.0190354   0.02753133 -0.05427081 -0.04155286 -0.00551601  0.01740319 -0.0476647  -0.04968425  0.04580819  0.0008345  -0.0295907  -0.03722452  0.01759684 -0.01680085 -0.00452731 -0.03103271  0.01715709 -0.02973793 -0.09613879  0.0051401  -0.05134783  0.05475045 -0.01283471  0.06491576  0.00521481 -0.02848241 -0.01567771 -0.02928415  0.06885134  0.00134412  0.02393162  0.02902504 -0.00551864  0.05685154  0.01001529 -0.00151077  0.03115507  0.06638921 -0.03961314 -0.00163929 -0.01343718  0.0455446   0.09824838 -0.0037222  -0.01031751  0.01817146 -0.00964774 -0.03243448  0.03805481  0.05487118 -0.1405063  0.02105236  0.07209678 -0.01056682  0.05282811  0.02630474  0.0514919  0.00152254 -0.02837622 -0.0032705  -0.00885593 -0.00931489  0.00091267 -0.02325741  0.03074451  0.01638401  0.07488494 -0.03075658  0.04077358  0.0087177  -0.03887089  0.00657461 -0.09632066  0.05145807  0.04517055  0.00689592  0.00326369 -0.0305842  -0.03148292 -0.01074421  0.01959825  0.06533831  0.01840545  0.05983892  0.02591163  0.04609231 -0.00414881  0.03466849 -0.03956919 -0.0252759   0.02703837 -0.05049418  0.00642855 -0.0466234   0.01452899  0.0411245   0.03267844  0.05253888  0.04270734  0.02421789 -0.02808435  0.03788607 -0.03368303  0.04734772 -0.02120586 -0.00403468  0.05083772 -0.03158763  0.00812041  0.14702259  0.05909027  0.06059919  0.04371373  0.05749655 -0.07144133 -0.007153    0.03108226 -0.00512128 -0.04756605 -0.01369407 -0.03663335 -0.02603604 -0.08464614 -0.00882994  0.05349972  0.00743798  0.0450788  -0.00615429 -0.01372156 -0.06049759 -0.00165892  0.00751451 -0.03650769 -0.05391942  0.03810795  0.01484829  0.00219441  0.02152888  0.02753042  0.06430887 -0.03992795  0.04222113 -0.01056078 -0.01716618  0.04329282  0.00051106 -0.00328042  0.00308413  0.04196681 -0.04609387 -0.04296339  0.03102    -0.03701093  0.062396   -0.06226721  0.00538336  0.0229238   0.02155858 -0.07839612  0.04539885  0.03251994 -0.00489039 -0.0397178  -0.00333692  0.00905649 -0.03641601  0.022086    0.01367772  0.00159605 -0.04355391  0.020589 -0.02687901 -0.03018762 -0.0402398  -0.0284971  -0.06481498 -0.02857979  0.02540303 -0.04830561 -0.04924891  0.08158892  0.02544646  0.01573535 -0.05373577 -0.00390225 -0.01128703 -0.00576774 -0.00892009  0.02994281 -0.02264063  0.02417267  0.05122236 -0.02454749  0.00509644 -0.01798263 -0.0216634   0.01950723 -0.08260044 -0.0616743  -0.06187451  0.03843369  0.02321616 -0.01540206  0.01407583 -0.00955944  0.00272941 -0.0103933 -0.01213074 -0.04850604 -0.03309135  0.05800785 -0.01818478  0.02047308  0.01977739  0.01210918 -0.01445558  0.0328787   0.0352786  -0.02208976 -0.02544963  0.05974025 -0.01183659  0.03732854  0.01193697 -0.03702984  0.02314256 -0.03182907 -0.02021193 -0.0746211  -0.01611404  0.01401942 -0.0582074   0.07607199 -0.02163705 -0.05099812  0.08846287  0.06345282  0.01632329  0.03178183  0.0316623   0.02768641 -0.00532748  0.02762542  0.03190726 -0.03167602 -0.06070093  0.06393433  0.04011519 -0.01285412  0.09149878 -0.03832356  0.01962792 -0.05850937 -0.03222102 -0.01088148  0.04417709 -0.05408356  0.00285973  0.01822237  0.05348348 -0.01791644  0.07268931 -0.04405991 -0.02393987 -0.03948317  0.00895625  0.02411424 -0.0503852  -0.04026405 -0.00080167  0.04002957 -0.02202688 -0.04564584  0.00108072 -0.01706082  0.02669973  0.03271101  0.02675862  0.06614731  0.09970568  0.03362572 -0.00678587  0.01031069  0.055266   -0.00729807 -0.04342917 -0.02459758  0.008681    0.03958506 -0.04298421  0.01152243  0.00646575  0.02900903 -0.02682752 -0.03343278 -0.03024075  0.00380493  0.0342885  -0.02285297  0.05209821 -0.00173273 -0.03453593 -0.02917043 -0.00853625  0.01347628 -0.06093422 -0.02397925 -0.02997962 -0.03431647  0.00035183 -0.00620178  0.00593462  0.01080737  0.01387101 -0.04454185 -0.01523613 -0.03633243  0.01572521 -0.04612231  0.08674803 -0.01647091  0.02339503 -0.01340232  0.00376191 -0.06117163  0.01253373 -0.03407933  0.00644007  0.01499923]</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
            <tag> BERT </tag>
            
            <tag> SentenceTransfomer </tag>
            
            <tag> embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>What is Web3? Its relationship with Web1, Web2 and metaverse</title>
      <link href="2022/03/20/2022-03-20-1/"/>
      <url>2022/03/20/2022-03-20-1/</url>
      
        <content type="html"><![CDATA[<p>The Internet has so far been in two phases: Web 1.0 and Web 2.0.</p><p>The next phase, naturally, is Web 3.0 (Web3 for short). This article talks about some of the basic understanding of Web3 and what it is.<br><img src="/content/images/2022-03-20-1.jpg" alt="png"></p><h2 id="1-Historical-review"><a href="#1-Historical-review" class="headerlink" title="1. Historical review"></a>1. Historical review</h2><p>In the Web 1.0 stage, users are pure content consumers, and the content is provided by the website. You can watch whatever the website lets you watch. A typical example is a news portal.<br><img src="/content/images/2022-03-20-2.jpg" alt="png"></p><p>In the Web 2.0 stage, users are the producers of content, and a website is just a platform that provides services to users. Typical Web 2.0 platforms include Wikipedia, Douyin, WeChat, and so on.<br><img src="/content/images/2022-03-20-3.jpg" alt="png"></p><h2 id="2-Characteristics-of-Web3"><a href="#2-Characteristics-of-Web3" class="headerlink" title="2. Characteristics of Web3"></a>2. Characteristics of Web3</h2><p>Many features of Web3 are still unclear, but many experts believe that it is related to the blockchain.<br>Simply speaking, Web 1.0 is users reading the Internet, Web 2.0 is users writing to the Internet, and Web3 is users living on the Internet.</p><p>Entertainment, work, study, consumption, and communication all take place online. A website is not just a service, it is a living space where part of people’s lives can be done online.</p><p>Study on educational sites, meet on conference sites, make friends on social networking sites, and have fun on gaming sites. All websites together form an all-encompassing online world.</p><h2 id="3-Virtual-world"><a href="#3-Virtual-world" class="headerlink" title="3. Virtual world"></a>3. Virtual world</h2><p>All the life functions of a website, if they can be linked together to allow users to move from one scene to another seamlessly, constitute a virtual world. This is probably what the recently popular metaverse looks like.</p><p>At that stage, users no longer visit the website, but enter the virtual world and live a virtual online life.</p><h2 id="4-Web3-is-distributed"><a href="#4-Web3-is-distributed" class="headerlink" title="4. Web3 is distributed"></a>4. Web3 is distributed</h2><p>Such a virtual world obviously cannot be monopolized by one or several giants, otherwise we will rely on these giants and have to abide by the rules set by them. When your life is in the virtual world, one day the giant suddenly decides to close your account, your virtual life will come to an abrupt end!</p><p>This is why many people have proposed that Web3 should be distributed. This has two meanings:</p><p>(1) It is not centralized, so no single company can control it;</p><p>(2) There are multiple providers for any kind of service, which are connected by distributed protocols, and users can transfer from one provider to another service provider with minimal cost.</p><h2 id="5-The-role-of-blockchain"><a href="#5-The-role-of-blockchain" class="headerlink" title="5. The role of blockchain"></a>5. The role of blockchain</h2><p>If Web3 is determined to be distributed, then blockchain is simply natural infrastructure. Because the blockchain is an implementation of a distributed database, it is distributed in itself, and once the information is on the chain, it cannot be modified.</p><p>This solves the core problem of Web3: data exchange between different websites. Different websites can freely read and write the same user’s data, and these data can be trusted, thus ensuring that users entering another website are like entering different regions of the same world.</p><p>Once Web3 is built on the blockchain, according to the design of the blockchain, users need to have a digital wallet, which is your ID card and bank account in the virtual world. Your identity, property, and consumption are all identified through this digital wallet. Websites identify who you are by the ID of your digital wallet.</p><p>On the other hand, with digital wallets, banking and finance become virtual, which makes the virtual world more like the real world.</p><h2 id="6-Digital-Assets"><a href="#6-Digital-Assets" class="headerlink" title="6. Digital Assets"></a>6. Digital Assets</h2><p>The blockchain allows arbitrary data to be written, so inevitably, all digital records of our virtual lives can be placed on the blockchain.</p><p>If each virtual item has its own number and records them in the blockchain, their owner (ie, linked to a digital wallet) can be determined.</p><p>For example, each digital picture in the virtual world can have a unique number on the blockchain, which will not be confused with other pictures, and the owner of each picture can be determined by linking to a digital wallet.</p><p>We could also trade the picture, register it to another digital wallet, and make its owner someone else. Now the very popular NFT is doing this, but it is not simply a picture registered on the blockchain, but a digital collectible.</p><p>Entrepreneur Chris Dixon recently said publicly: “Web3 is ownership”, which is exactly what he means.</p><p>If everyone, all items, and all transactions are registered on the blockchain, the Web3 based on it is simply endless and unimaginable now. The possibility of the virtual world is 100 times, 1000 times more than the real world.</p><p>At present, it is only the initial stage of Web3, the infrastructure has not been completed, and the gameplay is still being explored, but the progress is very fast. Countless innovations and opportunities should emerge in this area, which deserves close attention. </p>]]></content>
      
      
      <categories>
          
          <category> Metaverse </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Web3 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Good use of regular expression, remove spaces between punctuation and words, normalize texts automatically</title>
      <link href="2022/03/19/2022-03-19-1/"/>
      <url>2022/03/19/2022-03-19-1/</url>
      
        <content type="html"><![CDATA[<p>Regular expression (shortened as regex) is very powerful way to improve efficiency and change formats for us.<br>Here are example how we can leverage grouping in regrex, and remove extra spaces between punctuation and chaters in fron of it.<br>Thus we can normalize format errors in texts automatically.</p><h2 id="Engish-sentence-example，remove-spaces-between-punctuation-and-words"><a href="#Engish-sentence-example，remove-spaces-between-punctuation-and-words" class="headerlink" title="Engish sentence example，remove spaces between punctuation and words"></a>Engish sentence example，remove spaces between punctuation and words</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">sent = <span class="string">&quot;What a wonderful day ,I want to go out and have a walk   !&quot;</span></span><br><span class="line">sent=re.sub(<span class="string">r&#x27;\s+([?,.!;&quot;])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, sent)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sent)</span><br></pre></td></tr></table></figure><pre><code>What a wonderful day,I want to go out and have a walk!</code></pre><p>In the above regrex rule definition: r’\s+([?,.!;”])’, the parentheses define a group (the first group), the square brackets in the group represent all the punctuation marks we need to distinguish by regex, and \s+ represents multiple spaces. So the whole rules means we replace the format of spaces + group with group only, so it automatically removes any extra spaces before the punctuations.</p><h2 id="Chinese-sentence-example，remove-spaces-between-punctuation-and-words"><a href="#Chinese-sentence-example，remove-spaces-between-punctuation-and-words" class="headerlink" title="Chinese sentence example，remove spaces between punctuation and words"></a>Chinese sentence example，remove spaces between punctuation and words</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">sent = <span class="string">&quot;天气真好   ! 我要出去散步 。&quot;</span></span><br><span class="line">sent=re.sub(<span class="string">r&#x27;\s+([?,.!;&quot;。，])&#x27;</span>, <span class="string">r&#x27;\1&#x27;</span>, sent)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sent)</span><br></pre></td></tr></table></figure><pre><code>天气真好! 我要出去散步。</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> regex </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ten important Python libraries that Data Scientist must know</title>
      <link href="2022/03/13/2022-03-12-%201/"/>
      <url>2022/03/13/2022-03-12-%201/</url>
      
        <content type="html"><![CDATA[<p>Good libraries are like useful toolbox, and learning these libraries can make you more productive, whether you’re a novice or a data science expert.</p><p>Below is a basic introduction to some of the most popular Python libraries for data science and machine learning.</p><h2 id="1-Scikit-learn"><a href="#1-Scikit-learn" class="headerlink" title="1. Scikit-learn"></a>1. Scikit-learn</h2><p>This is the most basic and popular Python library for machine learning. In fact, Scikit-learn is the main library for machine learning. It has algorithms and modules for preprocessing, cross-validation, and other similar purposes.</p><p>Some of these algorithms involve regression, decision trees, ensemble modeling, and unsupervised learning algorithms such as clustering.</p><p>Project address: <a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a></p><h2 id="2-NumPy"><a href="#2-NumPy" class="headerlink" title="2. NumPy"></a>2. NumPy</h2><p>NumPy is another wonderful Python library for machine learning and heavy computing. NumPy facilitates simple and efficient numerical computation. It has many other libraries built on top of it, such as Pandas.</p><p>You should at least make sure to learn about NumPy arrays, which are fundamental and have many applications in machine learning, data science, and artificial intelligence-based programs.</p><p>Project address: <a href="https://github.com/numpy/numpy">https://github.com/numpy/numpy</a></p><h2 id="3-Pandas"><a href="#3-Pandas" class="headerlink" title="3. Pandas"></a>3. Pandas</h2><p>This is a Python library built on top of NumPy. It is handy in terms of data structures and exploratory analysis. Another important feature it provides is a DataFrame, a two-dimensional data structure with potentially different types of columns.</p><p>Pandas will be one of the most important libraries you will ever need, which is why it is so important to learn Pandas well.</p><p>Project address: <a href="https://github.com/pandas-dev/pandas">https://github.com/pandas-dev/pandas</a></p><h2 id="4-Matplotlib"><a href="#4-Matplotlib" class="headerlink" title="4. Matplotlib"></a>4. Matplotlib</h2><p>If you need to plot, then Matlotlib is an option. It provides a flexible plotting and visualization library, and Matplotlib is powerful. However, it is cumbersome, so, you can choose Seaborn instead.</p><p>Project address: <a href="https://github.com/matplotlib/matplotlib">https://github.com/matplotlib/matplotlib</a></p><h2 id="5-Seaborn"><a href="#5-Seaborn" class="headerlink" title="5. Seaborn"></a>5. Seaborn</h2><p>Like Matplotlib, it’s a great plotting library, but with Seaborn it’s easier than ever to draw common data visualizations.</p><p>It builds on top of Matplotlib and provides a more pleasant high-level wrapper. You should learn effective data visualization.</p><p>Project address: <a href="https://github.com/seaborn">https://github.com/seaborn</a></p><h2 id="6-SciPy"><a href="#6-SciPy" class="headerlink" title="6. SciPy"></a>6. SciPy</h2><p>This is a Python library for scientific and technical computing. It will give you all the tools you need for scientific and technical computing.</p><p>It has modules for optimization, linear algebra, integration, interpolation, special functions, fast Fourier transforms, signal and image processing, independent dependency estimation solvers, and other tasks.</p><p>Project address: <a href="https://github.com/scipy/scipy">https://github.com/scipy/scipy</a></p><h2 id="7-OpenCV"><a href="#7-OpenCV" class="headerlink" title="7. OpenCV"></a>7. OpenCV</h2><p>This is another great library for Python developers in computer vision. In case you didn’t know, computer vision is one of the most exciting fields in machine learning and artificial intelligence.</p><p>It has applications in many industries such as self-driving cars, robotics, augmented reality, etc., and OpenCV is the best computer vision library.</p><p>Although you can use OpenCV in many programming languages ​​like C++, its Python version is beginner friendly and easy to use, which makes it a great library to be included in this list.</p><p>If you want to learn Python and OpenCV for basic image processing, and do image classification and object detection, and need a course, then I highly recommend taking a hands-on course that will teach you an OpenCV through several labs and exercises.</p><p>Project address: <a href="https://github.com/opencv/opencv">https://github.com/opencv/opencv</a></p><h2 id="8-TensorFlow"><a href="#8-TensorFlow" class="headerlink" title="8. TensorFlow"></a>8. TensorFlow</h2><p>This is one of the most popular machine learning libraries, and chances are you’ve already heard of it. You probably know TensorFlow from Google, invented by their Google Brain team, and used in the RankBrain algorithm that powers millions of search questions on Google’s search engine.</p><p>In general, it is a symbolic math library that is also used in machine learning applications such as neural networks. TensorFlow has many applications, and you can find many stories online, such as how a Japanese farmer used TensorFlow to sort cucumbers.</p><p>Project address: <a href="https://github.com/tensorflow/tensorflow">https://github.com/tensorflow/tensorflow</a></p><h2 id="9-PyTorch"><a href="#9-PyTorch" class="headerlink" title="9. PyTorch"></a>9. PyTorch</h2><p>This is another exciting and powerful Python library for data science and machine learning, something every data scientist should learn.</p><p>In case you didn’t know, PyTorch is one of the best deep learning libraries developed by Facebook for deep learning applications such as face recognition self-driving cars and more.</p><p>You can also use PyTorch to build machine learning models like NLP and computer vision, to name a few. You can also use PyTorch to create deep neural networks.</p><p>Project address: <a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></p><h2 id="10-Keras"><a href="#10-Keras" class="headerlink" title="10. Keras"></a>10. Keras</h2><p>One of the main problems with creating machine learning and deep learning-based solutions is that implementing them can be tedious, requiring many lines of complex code. Keras is a library that makes it easier for you to create these deep learning solutions.</p><p>With just a few lines of code, you can create a model that may require hundreds of lines of traditional code.</p><p>Project address: <a href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</a> </p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convert arbitrary date to the date of  Monday or Sundy within the same week in Python</title>
      <link href="2022/02/22/2022-02-22-%201/"/>
      <url>2022/02/22/2022-02-22-%201/</url>
      
        <content type="html"><![CDATA[<p>It is common practice to convert date to the week number for normalization or feature preprocessing. While this is pretty useful, sometimes we want to compare the week number in a more strict fashion, such that we can order week 50 of last year and week 2 of this year, for example.</p><p>Following code shows both<br>(1） how to get week number of datetime object<br>(2) get the exacte date of the Monday or Sunday within the same week</p><h2 id="get-week-number-for-any-date"><a href="#get-week-number-for-any-date" class="headerlink" title="get week number for any date"></a>get week number for any date</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta, datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># example of a date str</span></span><br><span class="line">dt_str = <span class="string">&#x27;2022/02/22 05:23:20&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert string to datetime object</span></span><br><span class="line">dt_obj = datetime.strptime(dt_str, <span class="string">&#x27;%Y/%m/%d %H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Monday </span></span><br><span class="line">year, week_num, day_of_week = dt_obj.isocalendar()</span><br><span class="line"><span class="built_in">print</span>(week_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>8</code></pre><h2 id="get-date-of-Monday-or-Sunday-for-any-date-within-the-same-week"><a href="#get-date-of-Monday-or-Sunday-for-any-date-within-the-same-week" class="headerlink" title="get date of Monday or Sunday for any date within the same week"></a>get date of Monday or Sunday for any date within the same week</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta, datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the date of Monday for that week</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_date_of_week</span>(<span class="params">date</span>):</span></span><br><span class="line">    start = date - timedelta(days=date.weekday())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(start)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the date of Sunday for that week</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">end_date_of_week</span>(<span class="params">date</span>):</span></span><br><span class="line">    start = date - timedelta(days=date.weekday())</span><br><span class="line">    end = start + timedelta(days=<span class="number">6</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(end)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># example of a date str</span></span><br><span class="line">dt_str = <span class="string">&#x27;2022/02/22 05:23:20&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert string to datetime object</span></span><br><span class="line">dt_obj = datetime.strptime(dt_str, <span class="string">&#x27;%Y/%m/%d %H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Monday </span></span><br><span class="line"><span class="built_in">print</span>(start_date_of_week(dt_obj) )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Sunday </span></span><br><span class="line"><span class="built_in">print</span>(end_date_of_week(dt_obj) )</span><br></pre></td></tr></table></figure><pre><code>2022-02-212022-02-27</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A transformer example to maintain same feature order and add missing features back for feature engineering</title>
      <link href="2022/01/27/2022-01-27-1/"/>
      <url>2022/01/27/2022-01-27-1/</url>
      
        <content type="html"><![CDATA[<p>For data science projects, one important steps in feature engineering is to make sure the order of feature columns during training<br>and prediction/test time is the same. Otherwise, we will not get the results as we expect.</p><p>This is usually not a problem in train/test split or cross validation stages, where training and test data are generally split form the<br>same dataframe. However, once model is put online, and the transformer need to handel each single event, which usually comes in the<br>format of json data, then transformed to dataframe. During this process, the orignal order may not hold.</p><p>To ensure the same feature order is used, we could build a transformer for the pipeline; During the fit stage, the orignal order will be<br>remembered, and during the transform stage, the same order will be enforced; Meanwhile, if there is any missing column, we will add a null value<br>column. </p><h2 id="set-up-some-example-dataframe"><a href="#set-up-some-example-dataframe" class="headerlink" title="set up some example dataframe"></a>set up some example dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># training example, where we have 3 features</span></span><br><span class="line">df_train = pd.DataFrame(data=[[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;f&#x27;</span>],[<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>]])</span><br><span class="line">df_train.columns =  [<span class="string">&#x27;cat1&#x27;</span>,<span class="string">&#x27;cat2&#x27;</span>,<span class="string">&#x27;cat3&#x27;</span>]</span><br><span class="line">display(df_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test example where we missing the 3rd feature</span></span><br><span class="line">df_test = pd.DataFrame(data=[[<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;j&#x27;</span>]])</span><br><span class="line">df_test.columns =  [<span class="string">&#x27;cat2&#x27;</span>,<span class="string">&#x27;cat1&#x27;</span>]</span><br><span class="line">display(df_test)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>a</td>      <td>b</td>      <td>f</td>    </tr>    <tr>      <th>1</th>      <td>c</td>      <td>d</td>      <td>e</td>    </tr>  </tbody></table></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat2</th>      <th>cat1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>h</td>      <td>j</td>    </tr>  </tbody></table></div><h2 id="a-transformer-which-can-be-added-to-a-full-pipeline"><a href="#a-transformer-which-can-be-added-to-a-full-pipeline" class="headerlink" title="a transformer which can be added to a full pipeline"></a>a transformer which can be added to a full pipeline</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">orderMaitain_Transformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"></span><br><span class="line">     <span class="comment"># Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.dtype_dict = &#123;&#125; </span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;initialized&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return self</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        </span><br><span class="line">        self.dtype_dict = X.dtypes.apply(<span class="keyword">lambda</span> x: x.name).to_dict()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line">        <span class="comment">#print(self.dtype_dict)</span></span><br><span class="line">        train_columns = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># add missing column if any</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> self.dtype_dict:</span><br><span class="line">            train_columns.append(col)</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> X.columns:</span><br><span class="line">                <span class="comment"># null boolean are treated as False; can also use other strategy as well</span></span><br><span class="line">                <span class="keyword">if</span> self.dtype_dict[col].startswith(<span class="string">&#x27;bool&#x27;</span>):</span><br><span class="line">                    X[col]=<span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[col] = pd.Series(dtype=self.dtype_dict[col])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># apply same order to both training and test</span></span><br><span class="line">        <span class="built_in">print</span>(train_columns)</span><br><span class="line">        X = X[train_columns]  </span><br><span class="line">        <span class="keyword">return</span> X  </span><br><span class="line">    </span><br><span class="line">orderMaitain_transformer = orderMaitain_Transformer()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>initialized</code></pre><h2 id="apply-transfomer-during-training-and-test-stages"><a href="#apply-transfomer-during-training-and-test-stages" class="headerlink" title="apply transfomer during training and test stages"></a>apply transfomer during training and test stages</h2><h3 id="during-training-stage"><a href="#during-training-stage" class="headerlink" title="during training stage"></a>during training stage</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">orderMaitain_transformer.fit_transform(df_train)</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat1&#39;, &#39;cat2&#39;, &#39;cat3&#39;]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>a</td>      <td>b</td>      <td>f</td>    </tr>    <tr>      <th>1</th>      <td>c</td>      <td>d</td>      <td>e</td>    </tr>  </tbody></table></div><h3 id="during-test-and-prediction-stage"><a href="#during-test-and-prediction-stage" class="headerlink" title="during test and prediction stage"></a>during test and prediction stage</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check that the resuls have an emppty column added, the order is the same as training</span></span><br><span class="line">orderMaitain_transformer.transform(df_test)</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat1&#39;, &#39;cat2&#39;, &#39;cat3&#39;]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>j</td>      <td>h</td>      <td>NaN</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pipeline </tag>
            
            <tag> transformer </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convert short texts to numeric vectors with Character ngram tf-idf vectorizer using scikit learn in Python</title>
      <link href="2022/01/15/2022-01-15-1/"/>
      <url>2022/01/15/2022-01-15-1/</url>
      
        <content type="html"><![CDATA[<p>When apply machine learning algorithms, to handel words or short texts, we usually need to get their numeric embedding vecotors first.<br>Some powerfulmethods including using pre-trained deep learning model such as BERT to more semantic embedding. If computation resource is a limit, or we want to have simpler embedding methods, we could try TF-IDF metrics. </p><p>Here we introduce a very simple way to combine character level n-gram methods and TF-IDF to convert short texts such as a few words to numeric vectors.  Within numeric vectors, we could further apply classification methods such as Gradient Boosted Machine for downstream tasks.</p><p>First, let’s review what’s n-gram:</p><p>Quote some definition from the Wikipedia N-Gram page:</p><p>…an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application… An n-gram of size 1 is referred to as a “unigram”; size 2 is a “bigram”; size 3 is a “trigram”. Larger sizes are sometimes referred to by the value of n, e.g., “four-gram”, “five-gram”, and so on.</p><p>The two most common types of N-Grams, by far, are (1) character-level, where the items consist of one or more characters and (2) word-level, where the items consist of one or more words. The size of the item (or token as it’s often called) is defined by n;</p><p>Second, what’s TF-IDF metrics?</p><p>TF-IDF (Term Frequency - Inverse Document Frequency) encoding is an improved way of BOW (bag of words) which is the same as TF. It considers the frequently seen term in various documents to be less of importance.</p><p>TF (Term Frequency): Counts how many term exists in a document<br>IDF (Inverse Document Frequency): Inverse of the number of documents which contains the term</p><p>So TF-IDF is the basically product of TF and IDF metrics.</p><p>It turns out to be very simple to implement character level n-gram TF-IDF encoding of short texts by using scikit-learn package.<br>This means we can easily incorporate the process step in our data process and feature engineering pipeline</p><h2 id="Step-1-Fit-character-n-gram-tf-idf-vectorizer-with-training-data"><a href="#Step-1-Fit-character-n-gram-tf-idf-vectorizer-with-training-data" class="headerlink" title="Step 1 , Fit character n-gram tf idf vectorizer with training data"></a>Step 1 , Fit character n-gram tf idf vectorizer with training data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># using gram of length at 2 for example</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">corpus = [<span class="string">&#x27;great people&#x27;</span>, <span class="string">&#x27;feel happy&#x27;</span>, <span class="string">&#x27;nice work&#x27;</span>,<span class="string">&#x27;warm weather&#x27;</span>]</span><br><span class="line">vectorizer = TfidfVectorizer(analyzer=<span class="string">&#x27;char&#x27;</span>, ngram_range=(<span class="number">2</span>,<span class="number">2</span>)).fit(corpus)</span><br><span class="line"><span class="built_in">print</span>(vectorizer.get_feature_names())</span><br></pre></td></tr></table></figure><pre><code>[&#39; h&#39;, &#39; p&#39;, &#39; w&#39;, &#39;ap&#39;, &#39;ar&#39;, &#39;at&#39;, &#39;ce&#39;, &#39;e &#39;, &#39;ea&#39;, &#39;ee&#39;, &#39;el&#39;, &#39;eo&#39;, &#39;er&#39;, &#39;fe&#39;, &#39;gr&#39;, &#39;ha&#39;, &#39;he&#39;, &#39;ic&#39;, &#39;l &#39;, &#39;le&#39;, &#39;m &#39;, &#39;ni&#39;, &#39;op&#39;, &#39;or&#39;, &#39;pe&#39;, &#39;pl&#39;, &#39;pp&#39;, &#39;py&#39;, &#39;re&#39;, &#39;rk&#39;, &#39;rm&#39;, &#39;t &#39;, &#39;th&#39;, &#39;wa&#39;, &#39;we&#39;, &#39;wo&#39;]</code></pre><h2 id="Step-2-Transform-new-text-to-tf-idf-weighted-vector"><a href="#Step-2-Transform-new-text-to-tf-idf-weighted-vector" class="headerlink" title="Step 2, Transform new text to tf-idf weighted vector"></a>Step 2, Transform new text to tf-idf weighted vector</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_text = [<span class="string">&#x27;pineapple milk&#x27;</span>]</span><br><span class="line">tfidf_vector = vectorizer.transform(new_text).toarray()</span><br><span class="line"><span class="built_in">print</span>(tfidf_vector)</span><br></pre></td></tr></table></figure><pre><code>[[0.         0.         0.         0.42176478 0.         0.  0.         0.42176478 0.3325242  0.         0.         0.  0.         0.         0.         0.         0.         0.  0.         0.42176478 0.         0.         0.         0.  0.         0.42176478 0.42176478 0.         0.         0.  0.         0.         0.         0.         0.         0.        ]]</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> text mining </tag>
            
            <tag> natural language processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Use Python to solve Linear Programming optimization problem</title>
      <link href="2021/12/29/2021-12-29-1/"/>
      <url>2021/12/29/2021-12-29-1/</url>
      
        <content type="html"><![CDATA[<p>Let’s first define the standard format of linear programming problem,<br>in which we will minimize the following equation<br>$$<br>    z=CX<br>$$<br>where， C is the vector of coefficients，X is the vector of variables to be optimized.<br>The constraints of this minimization can be written as:<br>$$<br>AX&lt;=B<br>$$<br>where，A and B are both coefficient matrix。</p><p>Let’s see an specific example：<br>$$<br>min , z=10x_1 +15x_2+25x_3<br>\s.t<br>\-1x_1-1x_2-1x_3&lt;=-1000<br>\-1x_1+2x_2-0x_3&lt;=0<br>\0x_1+0x_2-1x_3&lt;=-300<br>\-1x_1&lt;=0<br>\-1x_2&lt;=0<br>\-1x_3&lt;=0<br>$$</p><p>In Python, we could call the linprog function in Scipy package to solove linear programming problem,<br>and here is a simple demo of coding in Python: </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import required libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linprog</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the inequality constraints matrix</span></span><br><span class="line"><span class="comment"># Note: the inequality constraints must be in the form of &lt;=</span></span><br><span class="line">A = np.array([[-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the inequality constraints vector</span></span><br><span class="line">b = np.array([-<span class="number">1000</span>, <span class="number">0</span>, -<span class="number">300</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the coefficients of the linear objective function vector</span></span><br><span class="line">c = np.array([<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve linear programming problem</span></span><br><span class="line">res = linprog(c, A_ub=A, b_ub=b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Optimal value:&#x27;</span>, <span class="built_in">round</span>(res.fun, ndigits=<span class="number">2</span>),</span><br><span class="line">      <span class="string">&#x27;\nx values:&#x27;</span>, res.x,</span><br><span class="line">      <span class="string">&#x27;\nNumber of iterations performed:&#x27;</span>, res.nit,</span><br><span class="line">      <span class="string">&#x27;\nStatus:&#x27;</span>, res.message)</span><br></pre></td></tr></table></figure><pre><code>Optimal value: 14500.0 x values: [7.0000000e+02 7.1017063e-09 3.0000000e+02] Number of iterations performed: 7 Status: Optimization terminated successfully.</code></pre>]]></content>
      
      
      <categories>
          
          <category> optimization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linear programming </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-class classification using simple deep learning structure in Pytorch with iris dataset</title>
      <link href="2021/12/26/2021-12-26-1/"/>
      <url>2021/12/26/2021-12-26-1/</url>
      
        <content type="html"><![CDATA[<p>As one can see from the following example, it is very easy to apply deep learning structure in Pytorch<br>to perform multi-class classificatoin task. The valuation performance of this iris dataset is almost perfect.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt </span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1121</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;torch._C.Generator at 0x19734b24a70&gt;</code></pre><h2 id="load-iris-dataset-for-training-and-valuation"><a href="#load-iris-dataset-for-training-and-valuation" class="headerlink" title="load iris dataset for training and valuation"></a>load iris dataset for training and valuation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">Y = iris.target</span><br><span class="line">X=pd.DataFrame(X)</span><br><span class="line">Y=pd.DataFrame(Y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display(X)</span><br><span class="line">display(Y)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.1</td>      <td>3.5</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>1</th>      <td>4.9</td>      <td>3.0</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>2</th>      <td>4.7</td>      <td>3.2</td>      <td>1.3</td>      <td>0.2</td>    </tr>    <tr>      <th>3</th>      <td>4.6</td>      <td>3.1</td>      <td>1.5</td>      <td>0.2</td>    </tr>    <tr>      <th>4</th>      <td>5.0</td>      <td>3.6</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>145</th>      <td>6.7</td>      <td>3.0</td>      <td>5.2</td>      <td>2.3</td>    </tr>    <tr>      <th>146</th>      <td>6.3</td>      <td>2.5</td>      <td>5.0</td>      <td>1.9</td>    </tr>    <tr>      <th>147</th>      <td>6.5</td>      <td>3.0</td>      <td>5.2</td>      <td>2.0</td>    </tr>    <tr>      <th>148</th>      <td>6.2</td>      <td>3.4</td>      <td>5.4</td>      <td>2.3</td>    </tr>    <tr>      <th>149</th>      <td>5.9</td>      <td>3.0</td>      <td>5.1</td>      <td>1.8</td>    </tr>  </tbody></table><p>150 rows × 4 columns</p></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>0</td>    </tr>    <tr>      <th>...</th>      <td>...</td>    </tr>    <tr>      <th>145</th>      <td>2</td>    </tr>    <tr>      <th>146</th>      <td>2</td>    </tr>    <tr>      <th>147</th>      <td>2</td>    </tr>    <tr>      <th>148</th>      <td>2</td>    </tr>    <tr>      <th>149</th>      <td>2</td>    </tr>  </tbody></table><p>150 rows × 1 columns</p></div><h2 id="train-and-valuation-dataset-split"><a href="#train-and-valuation-dataset-split" class="headerlink" title="train and valuation dataset split"></a>train and valuation dataset split</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X=X.values</span><br><span class="line">Y=Y.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x, x_val, y, y_val = train_test_split(X, Y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape, y.shape, x_val.shape, y_val.shape</span><br></pre></td></tr></table></figure><pre><code>((100, 4), (100, 1), (50, 4), (50, 1))</code></pre><h3 id="notice-that-y-needs-to-be-one-dimension-indicating-the-class-type-not-encoded-vectors"><a href="#notice-that-y-needs-to-be-one-dimension-indicating-the-class-type-not-encoded-vectors" class="headerlink" title="notice that: y needs to be one dimension indicating the class type, not encoded vectors"></a>notice that: y needs to be one dimension indicating the class type, not encoded vectors</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train = x.reshape(-<span class="number">1</span>, x.shape[<span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_val = x_val.reshape(-<span class="number">1</span>, x_val.shape[<span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_val = y_val.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="put-data-into-dataloader-so-we-could-train-by-batches-easily"><a href="#put-data-into-dataloader-so-we-could-train-by-batches-easily" class="headerlink" title="put data into dataloader, so we could train by batches easily"></a>put data into dataloader, so we could train by batches easily</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.x=torch.from_numpy(x_train)</span><br><span class="line">        self.y=torch.from_numpy(y_train).long()</span><br><span class="line">        self.<span class="built_in">len</span>=self.x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span>      </span><br><span class="line">        <span class="keyword">return</span> self.x[index], self.y[index]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">data_set=Data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define batch sizes here </span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">trainloader=DataLoader(dataset=data_set,batch_size=batch_size)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="build-softmax-classifier"><a href="#build-softmax-classifier" class="headerlink" title="build softmax classifier"></a>build softmax classifier</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># D_in, dimension of input layer</span></span><br><span class="line"><span class="comment"># H , dimension of hidden layer</span></span><br><span class="line"><span class="comment"># D_out, dimension of output layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,D_in,H,D_out</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">        self.linear1=nn.Linear(D_in,H)</span><br><span class="line">        self.linear2=nn.Linear(H,D_out)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x=torch.sigmoid(self.linear1(x))  </span><br><span class="line">        x=self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_dim=<span class="number">4</span>     <span class="comment"># how many features are in the dataset or how many input nodes in the input layer</span></span><br><span class="line">hidden_dim = <span class="number">20</span> <span class="comment"># hidden layer size</span></span><br><span class="line">output_dim=<span class="number">3</span>    <span class="comment"># number of classes</span></span><br><span class="line"><span class="built_in">print</span>(input_dim,hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate model</span></span><br><span class="line">model=Net(input_dim,hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W:&#x27;</span>,<span class="built_in">list</span>(model.parameters())[<span class="number">0</span>].size())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b&#x27;</span>,<span class="built_in">list</span>(model.parameters())[<span class="number">1</span>].size())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>4 20 3W: torch.Size([20, 4])b torch.Size([20])</code></pre><h2 id="define-the-optimizer"><a href="#define-the-optimizer" class="headerlink" title="define the optimizer"></a>define the optimizer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate=<span class="number">0.05</span></span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_epochs=<span class="number">1000</span></span><br><span class="line">loss_list=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#n_epochs</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">if</span> epoch %<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(epoch)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> trainloader:</span><br><span class="line">      </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clear gradient </span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment">#make a prediction </span></span><br><span class="line">        z=model(x)</span><br><span class="line">        <span class="comment">#print(z)</span></span><br><span class="line">        <span class="comment"># calculate loss, da Cross Entropy benutzt wird muss ich in den loss Klassen vorhersagen, </span></span><br><span class="line">        <span class="comment"># also Wahrscheinlichkeit pro Klasse. Das mach torch.max(y,1)[1])</span></span><br><span class="line">        loss=criterion(z,y)</span><br><span class="line">        <span class="comment"># calculate gradients of parameters </span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update parameters </span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        loss_list.append(loss.data)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(&#x27;epoch &#123;&#125;, loss &#123;&#125;&#x27;.format(epoch, loss.item()))</span></span><br></pre></td></tr></table></figure><pre><code>0100200300400500600700800900</code></pre><h2 id="check-performance-using-the-valuation-data"><a href="#check-performance-using-the-valuation-data" class="headerlink" title="check performance using the valuation data"></a>check performance using the valuation data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># predict the class and give probablity for each class label</span></span><br><span class="line">x_val = torch.from_numpy(x_val)</span><br><span class="line">z=model(x_val)</span><br><span class="line">display(z)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-3.1493e+00,  3.6905e+00, -5.0090e-01],        [ 8.6639e+00,  2.5913e+00, -1.1538e+01],        [-7.7513e+00,  4.1365e-01,  7.2997e+00],        [-3.1786e+00,  3.3443e+00, -1.6125e-01],        [-3.0974e+00,  3.8265e+00, -6.9203e-01],        [ 8.4413e+00,  2.6384e+00, -1.1355e+01],        [-1.4718e-02,  4.3311e+00, -4.3462e+00],        [-5.4910e+00,  1.5831e+00,  3.8781e+00],        [-4.7908e+00,  2.2980e+00,  2.5141e+00],        [-1.2132e+00,  4.3453e+00, -3.1134e+00],        [-5.0060e+00,  1.9947e+00,  2.9958e+00],        [ 8.1200e+00,  2.7792e+00, -1.1151e+01],        [ 8.8828e+00,  2.5214e+00, -1.1683e+01],        [ 8.0960e+00,  2.7942e+00, -1.1141e+01],        [ 8.8283e+00,  2.5107e+00, -1.1654e+01],        [-2.5941e+00,  3.7417e+00, -1.1520e+00],        [-7.0112e+00,  6.8188e-01,  6.2887e+00],        [-1.7457e+00,  4.1789e+00, -2.4035e+00],        [-3.3078e+00,  3.3204e+00,  6.4886e-03],        [-7.0187e+00,  6.5605e-01,  6.3214e+00],        [ 7.9682e+00,  2.8094e+00, -1.1046e+01],        [-4.8998e+00,  2.0493e+00,  2.8430e+00],        [ 8.2123e+00,  2.6980e+00, -1.1198e+01],        [-6.9197e+00,  7.3409e-01,  6.1537e+00],        [-5.4359e+00,  2.1912e+00,  3.2544e+00],        [-6.1038e+00,  1.1452e+00,  4.9261e+00],        [-6.9835e+00,  8.1716e-01,  6.1667e+00],        [-6.8937e+00,  7.5489e-01,  6.1001e+00],        [ 8.0317e+00,  2.7680e+00, -1.1061e+01],        [ 7.8392e+00,  2.8607e+00, -1.0952e+01],        [ 9.1619e+00,  2.3973e+00, -1.1890e+01],        [ 9.2721e+00,  2.3691e+00, -1.1973e+01],        [-1.2764e+00,  4.5603e+00, -3.2536e+00],        [ 8.2595e+00,  2.7126e+00, -1.1257e+01],        [ 8.4267e+00,  2.6317e+00, -1.1359e+01],        [-6.1753e+00,  1.1836e+00,  4.9843e+00],        [-1.8449e+00,  4.1870e+00, -2.3373e+00],        [ 8.5922e+00,  2.6133e+00, -1.1488e+01],        [ 8.7905e+00,  2.5362e+00, -1.1630e+01],        [ 9.1236e+00,  2.4451e+00, -1.1888e+01],        [-6.4186e+00,  9.5598e-01,  5.4381e+00],        [-1.9872e+00,  3.8494e+00, -1.8866e+00],        [-2.3989e+00,  4.0929e+00, -1.6734e+00],        [ 9.1132e+00,  2.4113e+00, -1.1844e+01],        [ 8.8116e+00,  2.5441e+00, -1.1646e+01],        [-1.2458e+00,  4.3741e+00, -3.0913e+00],        [-4.9320e+00,  2.3103e+00,  2.6452e+00],        [-5.8854e+00,  1.5035e+00,  4.3829e+00],        [-1.6208e+00,  4.4212e+00, -2.7724e+00],        [-6.7249e+00,  8.5383e-01,  5.8288e+00]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># choose the predicted class to be the one with maximum probablity</span></span><br><span class="line">yhat=torch.<span class="built_in">max</span>(z.data,<span class="number">1</span>)</span><br><span class="line">display(yhat)</span><br></pre></td></tr></table></figure><pre><code>torch.return_types.max(values=tensor([3.6905, 8.6639, 7.2997, 3.3443, 3.8265, 8.4413, 4.3311, 3.8781, 2.5141,        4.3453, 2.9958, 8.1200, 8.8828, 8.0960, 8.8283, 3.7417, 6.2887, 4.1789,        3.3204, 6.3214, 7.9682, 2.8430, 8.2123, 6.1537, 3.2544, 4.9261, 6.1667,        6.1001, 8.0317, 7.8392, 9.1619, 9.2721, 4.5603, 8.2595, 8.4267, 4.9843,        4.1870, 8.5922, 8.7905, 9.1236, 5.4381, 3.8494, 4.0929, 9.1132, 8.8116,        4.3741, 2.6452, 4.3829, 4.4212, 5.8288]),indices=tensor([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2,        2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2,        1, 2]))</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the indicies</span></span><br><span class="line">y_pred=yhat.indices.detach().numpy()</span><br><span class="line">display(y_pred)</span><br></pre></td></tr></table></figure><pre><code>array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,       0, 1, 2, 2, 1, 2], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display(y_val)</span><br></pre></td></tr></table></figure><pre><code>array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,       0, 1, 2, 2, 1, 2])</code></pre><h2 id="check-the-multi-class-confusion-matric"><a href="#check-the-multi-class-confusion-matric" class="headerlink" title="check the multi-class confusion matric"></a>check the multi-class confusion matric</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> multilabel_confusion_matrix</span><br><span class="line">display(multilabel_confusion_matrix(y_val, y_pred))</span><br></pre></td></tr></table></figure><pre><code>array([[[31,  0],        [ 0, 19]],       [[35,  0],        [ 1, 14]],       [[33,  1],        [ 0, 16]]], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-class </tag>
            
            <tag> classification </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to auto run python jobs using crontab</title>
      <link href="2021/10/24/2021-10-24-1/"/>
      <url>2021/10/24/2021-10-24-1/</url>
      
        <content type="html"><![CDATA[<p>In ubuntu Linux，many times we need to run certain tasks regularly, such as every 10 minutes, at 2 o’clock in the morning every day, etc.<br>The tasks can be varied, but as data scientists, many of our tasks are based on python code. </p><h2 id="A-simple-python-job"><a href="#A-simple-python-job" class="headerlink" title="A simple python job"></a>A simple python job</h2><p>Suppose we have a very simple python text that needs to be run at 2 AM every day. The python text is named test.py, and the code is as follows: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a=3</span><br><span class="line">b=5</span><br><span class="line">print(a+b)</span><br></pre></td></tr></table></figure><p>we could write a bash script，run.sh to run the above python job：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">python test.py &gt;&gt; log.txt</span><br></pre></td></tr></table></figure><p>we need to make the run.sh to be excecutable:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod +x run.sh</span><br></pre></td></tr></table></figure><p>We then can run the script，implement the python code，got the answer of a+b and pipeline it to the file log.txt:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./run.sh</span><br></pre></td></tr></table></figure><p>Our job is now to figure out how to run.sh script file automatically。</p><h2 id="Install-crontab"><a href="#Install-crontab" class="headerlink" title="Install crontab"></a>Install crontab</h2><p>If your ubuntu doesn’t have contab</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">install：apt-get install cron</span><br><span class="line">start：service cron start</span><br><span class="line">restart：service cron restart</span><br><span class="line">stop：service cron stop</span><br><span class="line">check status：service cron status</span><br><span class="line">check scheduled jobs：crontab -l</span><br></pre></td></tr></table></figure><h2 id="add-crontab-job"><a href="#add-crontab-job" class="headerlink" title="add crontab job"></a>add crontab job</h2><p>the command is：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure><p>follow the hint and choose the edit environment you like, such as nano or vi.<br>If we want to run the job 2 am every day, the logic is like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0 2 * * * /home/user_name/run.sh</span><br></pre></td></tr></table></figure><p>If we want to run the job every 5 minutes, then rule is this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*/5 * * * * /home/user_name/run.sh</span><br></pre></td></tr></table></figure><p>After save the above work, we have successfully scheduled the jobs.<br>Using the following command,</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crontab -l</span><br></pre></td></tr></table></figure><p>we could confirm all the scheduled jobs including the one we just put in.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> crontab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to serve the Flask app with nginx as the reverse proxy</title>
      <link href="2021/10/01/2021-10-01-1/"/>
      <url>2021/10/01/2021-10-01-1/</url>
      
        <content type="html"><![CDATA[<p>We are going to build a simple and smooth process using the following steps:</p><ol><li><p> if you don’t have uwsgi installed, try this on ubuntu:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo pip3 install uwsgi</span><br></pre></td></tr></table></figure></li><li><p>locate your flask project folder, if you don’t know how to create a a flask app, check this link:<br><a href="https://datasciencebyexample.com/2021/09/29/2021-09-29-1/">flask app creation</a><br><a href="https://datasciencebyexample.com/2021/09/29/2021-09-29-1/">https://datasciencebyexample.com/2021/09/29/2021-09-29-1/</a></p></li><li><p>create a uswgi init file inside your flask project, for example, you can call it myproject.ini</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[uwsgi]</span><br><span class="line">module = main:app</span><br><span class="line"></span><br><span class="line">master = true</span><br><span class="line">processes = 2</span><br><span class="line"></span><br><span class="line">socket = myproject.sock</span><br><span class="line">chmod-socket = 660</span><br><span class="line">vacuum = true</span><br><span class="line"></span><br><span class="line">die-on-term = true</span><br></pre></td></tr></table></figure></li><li><p>Let’s then create the systemd service unit file. Creating a systemd unit file will allow Ubuntu’s init system to automatically start uWSGI and serve the Flask application whenever the server boots.</p></li></ol><p>Create a unit file ending in .service within the /etc/systemd/system directory to begin, for example</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo emacs /etc/systemd/system/myproject.service</span><br></pre></td></tr></table></figure><p>inside the file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=uWSGI instance to serve myproject</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=&lt;your user name&gt;</span><br><span class="line">Group=www-data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WorkingDirectory=&lt;your flask app directory&gt;</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/uwsgi --ini myproject.ini</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Then type these in the command line:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl start myproject</span><br><span class="line">sudo systemctl enable myproject</span><br></pre></td></tr></table></figure><p>you can check the service status</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl status myproject</span><br></pre></td></tr></table></figure><p>if any changes to the project, just do this to reflect new web app:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service myproject restart</span><br></pre></td></tr></table></figure><ol start="5"><li>install the nginx if you don’t have one<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install nginx</span><br></pre></td></tr></table></figure>At the end of the installation process, Ubuntu (18.04) will start Nginx. The web server should already be up and running.</li></ol><p>We can check with the systemd init system to make sure the service is running by typing:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl status nginx</span><br></pre></td></tr></table></figure><ol start="6"><li><p>Begin by creating a new server block configuration file in Nginx’s sites-available directory. Let’s call this myproject to keep in line with the rest of the guide:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo emacs /etc/nginx/sites-available/myproject</span><br></pre></td></tr></table></figure><p>inside the file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name englishlearningbyexample.com www.englishlearningbyexample.com;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_pass unix:&lt;your flask app dir&gt;/myproject.sock;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>above I was using<br><a href="http://www.englishlearningbyexample.com/">www.englishlearningbyexample.com</a><br>as the example for server_name, but you should replace with yours.</p></li><li><p>final steps</p></li></ol><p>To enable the Nginx server block configuration you’ve just created, link the file to the sites-enabled directory:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ln -s /etc/nginx/sites-available/myproject /etc/nginx/sites-enabled</span><br></pre></td></tr></table></figure><p>then do </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart nginx</span><br></pre></td></tr></table></figure><p>now you should see the website running</p><p>If you encounter any errors, trying checking the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo less /var/log/nginx/error.log: checks the Nginx error logs.</span><br><span class="line">sudo less /var/log/nginx/access.log: checks the Nginx access logs.</span><br><span class="line">sudo journalctl -u nginx: checks the Nginx process logs.</span><br><span class="line">sudo journalctl -u myproject: checks your Flask app’s uWSGI logs.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask app </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Set up Flask app to implement GET and POST API</title>
      <link href="2021/09/29/2021-09-29-1/"/>
      <url>2021/09/29/2021-09-29-1/</url>
      
        <content type="html"><![CDATA[<p>Flask app is one of the easiest ways to setup APIs on the server side for data engineering and data science purpose.<br>Here we show an simple set up process, an example python file, and how to run it in an development environment.</p><h2 id="packages-to-install"><a href="#packages-to-install" class="headerlink" title="packages to install"></a>packages to install</h2><p>assume we are using python3 environment</p><ol><li>install flask</li></ol><p>sudo apt install python3-flask</p><ol start="2"><li>to allow Cross-Origin Resource Sharing</li></ol><p>pip3 install flask_cors</p><h2 id="A-simple-example-python-file-main-py-to-setup-flask-GET-and-POST-APIS"><a href="#A-simple-example-python-file-main-py-to-setup-flask-GET-and-POST-APIS" class="headerlink" title="A simple example python file (main.py) to setup flask GET and POST APIS"></a>A simple example python file (main.py) to setup flask GET and POST APIS</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask,request</span><br><span class="line"><span class="keyword">from</span> flask_cors <span class="keyword">import</span> CORS</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">CORS(app)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">helloWorld</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Hello, cross-origin-world!&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># GET method</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/get&quot;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getexample</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># GET data</span></span><br><span class="line">    query = request.args.get(<span class="string">&quot;query&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> query</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#post method</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/post&quot;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postexample</span>():</span></span><br><span class="line">    <span class="comment">##  Get POST input form data</span></span><br><span class="line">    <span class="comment">#data = dict((key, request.form.get(key)) for key in request.form.keys())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Get POST input json data</span></span><br><span class="line">    data = <span class="built_in">dict</span>((key, request.json.get(key)) <span class="keyword">for</span> key <span class="keyword">in</span> request.json.keys())</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;post request with &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data))</span><br><span class="line"></span><br><span class="line">    output=&#123;<span class="string">&quot;status&quot;</span>:<span class="string">&quot;ok&quot;</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> json.dumps(output)</span><br></pre></td></tr></table></figure><h2 id="to-run-the-flask-API-in-the-command-line-to-run-the-following-command"><a href="#to-run-the-flask-API-in-the-command-line-to-run-the-following-command" class="headerlink" title="to run the flask API: in the command line to run the following command"></a>to run the flask API: in the command line to run the following command</h2><p>sudo FLASK_APP=main.py flask run –host=0.0.0.0 –port 3000</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask app </tag>
            
            <tag> API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Prep and Visualization Example using matplotlib in Python</title>
      <link href="2021/08/25/2021-08-25-1/"/>
      <url>2021/08/25/2021-08-25-1/</url>
      
        <content type="html"><![CDATA[<p>Some simple ways to make histogram or line plots using matplotlib.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install the pydataset package. This package gives us data sets to work with very easily</span></span><br><span class="line">! pip install pydataset</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The convention for importing matplotlib with an alias is &quot;plt&quot;. We&#x27;ll also need pandas and numpy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h2 id="The-Air-Passengers-Dataset"><a href="#The-Air-Passengers-Dataset" class="headerlink" title="The Air Passengers Dataset"></a>The Air Passengers Dataset</h2><p>This dataset shows the number of passengers flying United States airlines by month from 1949-1960. </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pydataset <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">passengers = data(<span class="string">&#x27;AirPassengers&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">passengers.head(<span class="number">12</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>AirPassengers</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>1949.000000</td>      <td>112</td>    </tr>    <tr>      <th>2</th>      <td>1949.083333</td>      <td>118</td>    </tr>    <tr>      <th>3</th>      <td>1949.166667</td>      <td>132</td>    </tr>    <tr>      <th>4</th>      <td>1949.250000</td>      <td>129</td>    </tr>    <tr>      <th>5</th>      <td>1949.333333</td>      <td>121</td>    </tr>    <tr>      <th>6</th>      <td>1949.416667</td>      <td>135</td>    </tr>    <tr>      <th>7</th>      <td>1949.500000</td>      <td>148</td>    </tr>    <tr>      <th>8</th>      <td>1949.583333</td>      <td>148</td>    </tr>    <tr>      <th>9</th>      <td>1949.666667</td>      <td>136</td>    </tr>    <tr>      <th>10</th>      <td>1949.750000</td>      <td>119</td>    </tr>    <tr>      <th>11</th>      <td>1949.833333</td>      <td>104</td>    </tr>    <tr>      <th>12</th>      <td>1949.916667</td>      <td>118</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-Add-a-‘year’-column-to-passengers-that-reflects-the-current-year"><a href="#1-Add-a-‘year’-column-to-passengers-that-reflects-the-current-year" class="headerlink" title="#1 Add a ‘year’ column to passengers that reflects the current year"></a>#1 Add a ‘year’ column to passengers that reflects the current year</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">passengers[<span class="string">&#x27;Year&#x27;</span>] = passengers[<span class="string">&#x27;time&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x))</span><br><span class="line">passengers</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>AirPassengers</th>      <th>year</th>      <th>month</th>      <th>Year</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>1949.000000</td>      <td>112</td>      <td>1949</td>      <td>1.0</td>      <td>1949</td>    </tr>    <tr>      <th>2</th>      <td>1949.083333</td>      <td>118</td>      <td>1949</td>      <td>2.0</td>      <td>1949</td>    </tr>    <tr>      <th>3</th>      <td>1949.166667</td>      <td>132</td>      <td>1949</td>      <td>3.0</td>      <td>1949</td>    </tr>    <tr>      <th>4</th>      <td>1949.250000</td>      <td>129</td>      <td>1949</td>      <td>4.0</td>      <td>1949</td>    </tr>    <tr>      <th>5</th>      <td>1949.333333</td>      <td>121</td>      <td>1949</td>      <td>5.0</td>      <td>1949</td>    </tr>    <tr>      <th>6</th>      <td>1949.416667</td>      <td>135</td>      <td>1949</td>      <td>6.0</td>      <td>1949</td>    </tr>    <tr>      <th>7</th>      <td>1949.500000</td>      <td>148</td>      <td>1949</td>      <td>7.0</td>      <td>1949</td>    </tr>    <tr>      <th>8</th>      <td>1949.583333</td>      <td>148</td>      <td>1949</td>      <td>8.0</td>      <td>1949</td>    </tr>    <tr>      <th>9</th>      <td>1949.666667</td>      <td>136</td>      <td>1949</td>      <td>9.0</td>      <td>1949</td>    </tr>    <tr>      <th>10</th>      <td>1949.750000</td>      <td>119</td>      <td>1949</td>      <td>10.0</td>      <td>1949</td>    </tr>    <tr>      <th>11</th>      <td>1949.833333</td>      <td>104</td>      <td>1949</td>      <td>11.0</td>      <td>1949</td>    </tr>    <tr>      <th>12</th>      <td>1949.916667</td>      <td>118</td>      <td>1949</td>      <td>12.0</td>      <td>1949</td>    </tr>    <tr>      <th>13</th>      <td>1950.000000</td>      <td>115</td>      <td>1950</td>      <td>1.0</td>      <td>1950</td>    </tr>    <tr>      <th>14</th>      <td>1950.083333</td>      <td>126</td>      <td>1950</td>      <td>2.0</td>      <td>1950</td>    </tr>    <tr>      <th>15</th>      <td>1950.166667</td>      <td>141</td>      <td>1950</td>      <td>3.0</td>      <td>1950</td>    </tr>    <tr>      <th>16</th>      <td>1950.250000</td>      <td>135</td>      <td>1950</td>      <td>4.0</td>      <td>1950</td>    </tr>    <tr>      <th>17</th>      <td>1950.333333</td>      <td>125</td>      <td>1950</td>      <td>5.0</td>      <td>1950</td>    </tr>    <tr>      <th>18</th>      <td>1950.416667</td>      <td>149</td>      <td>1950</td>      <td>6.0</td>      <td>1950</td>    </tr>    <tr>      <th>19</th>      <td>1950.500000</td>      <td>170</td>      <td>1950</td>      <td>7.0</td>      <td>1950</td>    </tr>    <tr>      <th>20</th>      <td>1950.583333</td>      <td>170</td>      <td>1950</td>      <td>8.0</td>      <td>1950</td>    </tr>    <tr>      <th>21</th>      <td>1950.666667</td>      <td>158</td>      <td>1950</td>      <td>9.0</td>      <td>1950</td>    </tr>    <tr>      <th>22</th>      <td>1950.750000</td>      <td>133</td>      <td>1950</td>      <td>10.0</td>      <td>1950</td>    </tr>    <tr>      <th>23</th>      <td>1950.833333</td>      <td>114</td>      <td>1950</td>      <td>11.0</td>      <td>1950</td>    </tr>    <tr>      <th>24</th>      <td>1950.916667</td>      <td>140</td>      <td>1950</td>      <td>12.0</td>      <td>1950</td>    </tr>    <tr>      <th>25</th>      <td>1951.000000</td>      <td>145</td>      <td>1951</td>      <td>1.0</td>      <td>1951</td>    </tr>    <tr>      <th>26</th>      <td>1951.083333</td>      <td>150</td>      <td>1951</td>      <td>2.0</td>      <td>1951</td>    </tr>    <tr>      <th>27</th>      <td>1951.166667</td>      <td>178</td>      <td>1951</td>      <td>3.0</td>      <td>1951</td>    </tr>    <tr>      <th>28</th>      <td>1951.250000</td>      <td>163</td>      <td>1951</td>      <td>4.0</td>      <td>1951</td>    </tr>    <tr>      <th>29</th>      <td>1951.333333</td>      <td>172</td>      <td>1951</td>      <td>5.0</td>      <td>1951</td>    </tr>    <tr>      <th>30</th>      <td>1951.416667</td>      <td>178</td>      <td>1951</td>      <td>6.0</td>      <td>1951</td>    </tr>    <tr>      <th>31</th>      <td>1951.500000</td>      <td>199</td>      <td>1951</td>      <td>7.0</td>      <td>1951</td>    </tr>    <tr>      <th>32</th>      <td>1951.583333</td>      <td>199</td>      <td>1951</td>      <td>8.0</td>      <td>1951</td>    </tr>    <tr>      <th>33</th>      <td>1951.666667</td>      <td>184</td>      <td>1951</td>      <td>9.0</td>      <td>1951</td>    </tr>    <tr>      <th>34</th>      <td>1951.750000</td>      <td>162</td>      <td>1951</td>      <td>10.0</td>      <td>1951</td>    </tr>    <tr>      <th>35</th>      <td>1951.833333</td>      <td>146</td>      <td>1951</td>      <td>11.0</td>      <td>1951</td>    </tr>    <tr>      <th>36</th>      <td>1951.916667</td>      <td>166</td>      <td>1951</td>      <td>12.0</td>      <td>1951</td>    </tr>    <tr>      <th>37</th>      <td>1952.000000</td>      <td>171</td>      <td>1952</td>      <td>1.0</td>      <td>1952</td>    </tr>    <tr>      <th>38</th>      <td>1952.083333</td>      <td>180</td>      <td>1952</td>      <td>2.0</td>      <td>1952</td>    </tr>    <tr>      <th>39</th>      <td>1952.166667</td>      <td>193</td>      <td>1952</td>      <td>3.0</td>      <td>1952</td>    </tr>    <tr>      <th>40</th>      <td>1952.250000</td>      <td>181</td>      <td>1952</td>      <td>4.0</td>      <td>1952</td>    </tr>    <tr>      <th>41</th>      <td>1952.333333</td>      <td>183</td>      <td>1952</td>      <td>5.0</td>      <td>1952</td>    </tr>    <tr>      <th>42</th>      <td>1952.416667</td>      <td>218</td>      <td>1952</td>      <td>6.0</td>      <td>1952</td>    </tr>    <tr>      <th>43</th>      <td>1952.500000</td>      <td>230</td>      <td>1952</td>      <td>7.0</td>      <td>1952</td>    </tr>    <tr>      <th>44</th>      <td>1952.583333</td>      <td>242</td>      <td>1952</td>      <td>8.0</td>      <td>1952</td>    </tr>    <tr>      <th>45</th>      <td>1952.666667</td>      <td>209</td>      <td>1952</td>      <td>9.0</td>      <td>1952</td>    </tr>    <tr>      <th>46</th>      <td>1952.750000</td>      <td>191</td>      <td>1952</td>      <td>10.0</td>      <td>1952</td>    </tr>    <tr>      <th>47</th>      <td>1952.833333</td>      <td>172</td>      <td>1952</td>      <td>11.0</td>      <td>1952</td>    </tr>    <tr>      <th>48</th>      <td>1952.916667</td>      <td>194</td>      <td>1952</td>      <td>12.0</td>      <td>1952</td>    </tr>    <tr>      <th>49</th>      <td>1953.000000</td>      <td>196</td>      <td>1953</td>      <td>1.0</td>      <td>1953</td>    </tr>    <tr>      <th>50</th>      <td>1953.083333</td>      <td>196</td>      <td>1953</td>      <td>2.0</td>      <td>1953</td>    </tr>    <tr>      <th>51</th>      <td>1953.166667</td>      <td>236</td>      <td>1953</td>      <td>3.0</td>      <td>1953</td>    </tr>    <tr>      <th>52</th>      <td>1953.250000</td>      <td>235</td>      <td>1953</td>      <td>4.0</td>      <td>1953</td>    </tr>    <tr>      <th>53</th>      <td>1953.333333</td>      <td>229</td>      <td>1953</td>      <td>5.0</td>      <td>1953</td>    </tr>    <tr>      <th>54</th>      <td>1953.416667</td>      <td>243</td>      <td>1953</td>      <td>6.0</td>      <td>1953</td>    </tr>    <tr>      <th>55</th>      <td>1953.500000</td>      <td>264</td>      <td>1953</td>      <td>7.0</td>      <td>1953</td>    </tr>    <tr>      <th>56</th>      <td>1953.583333</td>      <td>272</td>      <td>1953</td>      <td>8.0</td>      <td>1953</td>    </tr>    <tr>      <th>57</th>      <td>1953.666667</td>      <td>237</td>      <td>1953</td>      <td>9.0</td>      <td>1953</td>    </tr>    <tr>      <th>58</th>      <td>1953.750000</td>      <td>211</td>      <td>1953</td>      <td>10.0</td>      <td>1953</td>    </tr>    <tr>      <th>59</th>      <td>1953.833333</td>      <td>180</td>      <td>1953</td>      <td>11.0</td>      <td>1953</td>    </tr>    <tr>      <th>60</th>      <td>1953.916667</td>      <td>201</td>      <td>1953</td>      <td>12.0</td>      <td>1953</td>    </tr>    <tr>      <th>61</th>      <td>1954.000000</td>      <td>204</td>      <td>1954</td>      <td>1.0</td>      <td>1954</td>    </tr>    <tr>      <th>62</th>      <td>1954.083333</td>      <td>188</td>      <td>1954</td>      <td>2.0</td>      <td>1954</td>    </tr>    <tr>      <th>63</th>      <td>1954.166667</td>      <td>235</td>      <td>1954</td>      <td>3.0</td>      <td>1954</td>    </tr>    <tr>      <th>64</th>      <td>1954.250000</td>      <td>227</td>      <td>1954</td>      <td>4.0</td>      <td>1954</td>    </tr>    <tr>      <th>65</th>      <td>1954.333333</td>      <td>234</td>      <td>1954</td>      <td>5.0</td>      <td>1954</td>    </tr>    <tr>      <th>66</th>      <td>1954.416667</td>      <td>264</td>      <td>1954</td>      <td>6.0</td>      <td>1954</td>    </tr>    <tr>      <th>67</th>      <td>1954.500000</td>      <td>302</td>      <td>1954</td>      <td>7.0</td>      <td>1954</td>    </tr>    <tr>      <th>68</th>      <td>1954.583333</td>      <td>293</td>      <td>1954</td>      <td>8.0</td>      <td>1954</td>    </tr>    <tr>      <th>69</th>      <td>1954.666667</td>      <td>259</td>      <td>1954</td>      <td>9.0</td>      <td>1954</td>    </tr>    <tr>      <th>70</th>      <td>1954.750000</td>      <td>229</td>      <td>1954</td>      <td>10.0</td>      <td>1954</td>    </tr>    <tr>      <th>71</th>      <td>1954.833333</td>      <td>203</td>      <td>1954</td>      <td>11.0</td>      <td>1954</td>    </tr>    <tr>      <th>72</th>      <td>1954.916667</td>      <td>229</td>      <td>1954</td>      <td>12.0</td>      <td>1954</td>    </tr>    <tr>      <th>73</th>      <td>1955.000000</td>      <td>242</td>      <td>1955</td>      <td>1.0</td>      <td>1955</td>    </tr>    <tr>      <th>74</th>      <td>1955.083333</td>      <td>233</td>      <td>1955</td>      <td>2.0</td>      <td>1955</td>    </tr>    <tr>      <th>75</th>      <td>1955.166667</td>      <td>267</td>      <td>1955</td>      <td>3.0</td>      <td>1955</td>    </tr>    <tr>      <th>76</th>      <td>1955.250000</td>      <td>269</td>      <td>1955</td>      <td>4.0</td>      <td>1955</td>    </tr>    <tr>      <th>77</th>      <td>1955.333333</td>      <td>270</td>      <td>1955</td>      <td>5.0</td>      <td>1955</td>    </tr>    <tr>      <th>78</th>      <td>1955.416667</td>      <td>315</td>      <td>1955</td>      <td>6.0</td>      <td>1955</td>    </tr>    <tr>      <th>79</th>      <td>1955.500000</td>      <td>364</td>      <td>1955</td>      <td>7.0</td>      <td>1955</td>    </tr>    <tr>      <th>80</th>      <td>1955.583333</td>      <td>347</td>      <td>1955</td>      <td>8.0</td>      <td>1955</td>    </tr>    <tr>      <th>81</th>      <td>1955.666667</td>      <td>312</td>      <td>1955</td>      <td>9.0</td>      <td>1955</td>    </tr>    <tr>      <th>82</th>      <td>1955.750000</td>      <td>274</td>      <td>1955</td>      <td>10.0</td>      <td>1955</td>    </tr>    <tr>      <th>83</th>      <td>1955.833333</td>      <td>237</td>      <td>1955</td>      <td>11.0</td>      <td>1955</td>    </tr>    <tr>      <th>84</th>      <td>1955.916667</td>      <td>278</td>      <td>1955</td>      <td>12.0</td>      <td>1955</td>    </tr>    <tr>      <th>85</th>      <td>1956.000000</td>      <td>284</td>      <td>1956</td>      <td>1.0</td>      <td>1956</td>    </tr>    <tr>      <th>86</th>      <td>1956.083333</td>      <td>277</td>      <td>1956</td>      <td>2.0</td>      <td>1956</td>    </tr>    <tr>      <th>87</th>      <td>1956.166667</td>      <td>317</td>      <td>1956</td>      <td>3.0</td>      <td>1956</td>    </tr>    <tr>      <th>88</th>      <td>1956.250000</td>      <td>313</td>      <td>1956</td>      <td>4.0</td>      <td>1956</td>    </tr>    <tr>      <th>89</th>      <td>1956.333333</td>      <td>318</td>      <td>1956</td>      <td>5.0</td>      <td>1956</td>    </tr>    <tr>      <th>90</th>      <td>1956.416667</td>      <td>374</td>      <td>1956</td>      <td>6.0</td>      <td>1956</td>    </tr>    <tr>      <th>91</th>      <td>1956.500000</td>      <td>413</td>      <td>1956</td>      <td>7.0</td>      <td>1956</td>    </tr>    <tr>      <th>92</th>      <td>1956.583333</td>      <td>405</td>      <td>1956</td>      <td>8.0</td>      <td>1956</td>    </tr>    <tr>      <th>93</th>      <td>1956.666667</td>      <td>355</td>      <td>1956</td>      <td>9.0</td>      <td>1956</td>    </tr>    <tr>      <th>94</th>      <td>1956.750000</td>      <td>306</td>      <td>1956</td>      <td>10.0</td>      <td>1956</td>    </tr>    <tr>      <th>95</th>      <td>1956.833333</td>      <td>271</td>      <td>1956</td>      <td>11.0</td>      <td>1956</td>    </tr>    <tr>      <th>96</th>      <td>1956.916667</td>      <td>306</td>      <td>1956</td>      <td>12.0</td>      <td>1956</td>    </tr>    <tr>      <th>97</th>      <td>1957.000000</td>      <td>315</td>      <td>1957</td>      <td>1.0</td>      <td>1957</td>    </tr>    <tr>      <th>98</th>      <td>1957.083333</td>      <td>301</td>      <td>1957</td>      <td>2.0</td>      <td>1957</td>    </tr>    <tr>      <th>99</th>      <td>1957.166667</td>      <td>356</td>      <td>1957</td>      <td>3.0</td>      <td>1957</td>    </tr>    <tr>      <th>100</th>      <td>1957.250000</td>      <td>348</td>      <td>1957</td>      <td>4.0</td>      <td>1957</td>    </tr>    <tr>      <th>101</th>      <td>1957.333333</td>      <td>355</td>      <td>1957</td>      <td>5.0</td>      <td>1957</td>    </tr>    <tr>      <th>102</th>      <td>1957.416667</td>      <td>422</td>      <td>1957</td>      <td>6.0</td>      <td>1957</td>    </tr>    <tr>      <th>103</th>      <td>1957.500000</td>      <td>465</td>      <td>1957</td>      <td>7.0</td>      <td>1957</td>    </tr>    <tr>      <th>104</th>      <td>1957.583333</td>      <td>467</td>      <td>1957</td>      <td>8.0</td>      <td>1957</td>    </tr>    <tr>      <th>105</th>      <td>1957.666667</td>      <td>404</td>      <td>1957</td>      <td>9.0</td>      <td>1957</td>    </tr>    <tr>      <th>106</th>      <td>1957.750000</td>      <td>347</td>      <td>1957</td>      <td>10.0</td>      <td>1957</td>    </tr>    <tr>      <th>107</th>      <td>1957.833333</td>      <td>305</td>      <td>1957</td>      <td>11.0</td>      <td>1957</td>    </tr>    <tr>      <th>108</th>      <td>1957.916667</td>      <td>336</td>      <td>1957</td>      <td>12.0</td>      <td>1957</td>    </tr>    <tr>      <th>109</th>      <td>1958.000000</td>      <td>340</td>      <td>1958</td>      <td>1.0</td>      <td>1958</td>    </tr>    <tr>      <th>110</th>      <td>1958.083333</td>      <td>318</td>      <td>1958</td>      <td>2.0</td>      <td>1958</td>    </tr>    <tr>      <th>111</th>      <td>1958.166667</td>      <td>362</td>      <td>1958</td>      <td>3.0</td>      <td>1958</td>    </tr>    <tr>      <th>112</th>      <td>1958.250000</td>      <td>348</td>      <td>1958</td>      <td>4.0</td>      <td>1958</td>    </tr>    <tr>      <th>113</th>      <td>1958.333333</td>      <td>363</td>      <td>1958</td>      <td>5.0</td>      <td>1958</td>    </tr>    <tr>      <th>114</th>      <td>1958.416667</td>      <td>435</td>      <td>1958</td>      <td>6.0</td>      <td>1958</td>    </tr>    <tr>      <th>115</th>      <td>1958.500000</td>      <td>491</td>      <td>1958</td>      <td>7.0</td>      <td>1958</td>    </tr>    <tr>      <th>116</th>      <td>1958.583333</td>      <td>505</td>      <td>1958</td>      <td>8.0</td>      <td>1958</td>    </tr>    <tr>      <th>117</th>      <td>1958.666667</td>      <td>404</td>      <td>1958</td>      <td>9.0</td>      <td>1958</td>    </tr>    <tr>      <th>118</th>      <td>1958.750000</td>      <td>359</td>      <td>1958</td>      <td>10.0</td>      <td>1958</td>    </tr>    <tr>      <th>119</th>      <td>1958.833333</td>      <td>310</td>      <td>1958</td>      <td>11.0</td>      <td>1958</td>    </tr>    <tr>      <th>120</th>      <td>1958.916667</td>      <td>337</td>      <td>1958</td>      <td>12.0</td>      <td>1958</td>    </tr>    <tr>      <th>121</th>      <td>1959.000000</td>      <td>360</td>      <td>1959</td>      <td>1.0</td>      <td>1959</td>    </tr>    <tr>      <th>122</th>      <td>1959.083333</td>      <td>342</td>      <td>1959</td>      <td>2.0</td>      <td>1959</td>    </tr>    <tr>      <th>123</th>      <td>1959.166667</td>      <td>406</td>      <td>1959</td>      <td>3.0</td>      <td>1959</td>    </tr>    <tr>      <th>124</th>      <td>1959.250000</td>      <td>396</td>      <td>1959</td>      <td>4.0</td>      <td>1959</td>    </tr>    <tr>      <th>125</th>      <td>1959.333333</td>      <td>420</td>      <td>1959</td>      <td>5.0</td>      <td>1959</td>    </tr>    <tr>      <th>126</th>      <td>1959.416667</td>      <td>472</td>      <td>1959</td>      <td>6.0</td>      <td>1959</td>    </tr>    <tr>      <th>127</th>      <td>1959.500000</td>      <td>548</td>      <td>1959</td>      <td>7.0</td>      <td>1959</td>    </tr>    <tr>      <th>128</th>      <td>1959.583333</td>      <td>559</td>      <td>1959</td>      <td>8.0</td>      <td>1959</td>    </tr>    <tr>      <th>129</th>      <td>1959.666667</td>      <td>463</td>      <td>1959</td>      <td>9.0</td>      <td>1959</td>    </tr>    <tr>      <th>130</th>      <td>1959.750000</td>      <td>407</td>      <td>1959</td>      <td>10.0</td>      <td>1959</td>    </tr>    <tr>      <th>131</th>      <td>1959.833333</td>      <td>362</td>      <td>1959</td>      <td>11.0</td>      <td>1959</td>    </tr>    <tr>      <th>132</th>      <td>1959.916667</td>      <td>405</td>      <td>1959</td>      <td>12.0</td>      <td>1959</td>    </tr>    <tr>      <th>133</th>      <td>1960.000000</td>      <td>417</td>      <td>1960</td>      <td>1.0</td>      <td>1960</td>    </tr>    <tr>      <th>134</th>      <td>1960.083333</td>      <td>391</td>      <td>1960</td>      <td>2.0</td>      <td>1960</td>    </tr>    <tr>      <th>135</th>      <td>1960.166667</td>      <td>419</td>      <td>1960</td>      <td>3.0</td>      <td>1960</td>    </tr>    <tr>      <th>136</th>      <td>1960.250000</td>      <td>461</td>      <td>1960</td>      <td>4.0</td>      <td>1960</td>    </tr>    <tr>      <th>137</th>      <td>1960.333333</td>      <td>472</td>      <td>1960</td>      <td>5.0</td>      <td>1960</td>    </tr>    <tr>      <th>138</th>      <td>1960.416667</td>      <td>535</td>      <td>1960</td>      <td>6.0</td>      <td>1960</td>    </tr>    <tr>      <th>139</th>      <td>1960.500000</td>      <td>622</td>      <td>1960</td>      <td>7.0</td>      <td>1960</td>    </tr>    <tr>      <th>140</th>      <td>1960.583333</td>      <td>606</td>      <td>1960</td>      <td>8.0</td>      <td>1960</td>    </tr>    <tr>      <th>141</th>      <td>1960.666667</td>      <td>508</td>      <td>1960</td>      <td>9.0</td>      <td>1960</td>    </tr>    <tr>      <th>142</th>      <td>1960.750000</td>      <td>461</td>      <td>1960</td>      <td>10.0</td>      <td>1960</td>    </tr>    <tr>      <th>143</th>      <td>1960.833333</td>      <td>390</td>      <td>1960</td>      <td>11.0</td>      <td>1960</td>    </tr>    <tr>      <th>144</th>      <td>1960.916667</td>      <td>432</td>      <td>1960</td>      <td>12.0</td>      <td>1960</td>    </tr>  </tbody></table></div><h2 id="2-Add-a-“month”-column"><a href="#2-Add-a-“month”-column" class="headerlink" title="#2 Add a “month” column"></a>#2 Add a “month” column</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">passengers[<span class="string">&#x27;month&#x27;</span>] = (passengers[<span class="string">&#x27;time&#x27;</span>] -passengers[<span class="string">&#x27;year&#x27;</span>])*<span class="number">12</span>+<span class="number">1</span></span><br><span class="line">passengers[<span class="string">&#x27;Month&#x27;</span>]= (passengers[<span class="string">&#x27;Year&#x27;</span>]-<span class="built_in">min</span>(passengers[<span class="string">&#x27;Year&#x27;</span>]))*<span class="number">12</span>+passengers[<span class="string">&#x27;month&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-Generate-the-plot-below-of-passengers-vs-time-using-each-monthly-count"><a href="#3-Generate-the-plot-below-of-passengers-vs-time-using-each-monthly-count" class="headerlink" title="#3 Generate the plot below of passengers vs. time using each monthly count"></a>#3 Generate the plot below of passengers vs. time using each monthly count</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(temp[<span class="string">&#x27;Month&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_12_1.png" alt="png"></p><h2 id="4-Generate-the-plot-below-of-passengers-vs-time-using-an-annual-count"><a href="#4-Generate-the-plot-below-of-passengers-vs-time-using-an-annual-count" class="headerlink" title="#4 Generate the plot below of passengers vs. time using an annual count"></a>#4 Generate the plot below of passengers vs. time using an annual count</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Year&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(temp[<span class="string">&#x27;Year&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Year&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_14_1.png" alt="png"></p><h2 id="5-Generate-the-barplot-below-of-passengers-by-year"><a href="#5-Generate-the-barplot-below-of-passengers-by-year" class="headerlink" title="#5 Generate the barplot below of passengers by year"></a>#5 Generate the barplot below of passengers by year</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Year&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.bar(temp[<span class="string">&#x27;Year&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Year&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">6000</span>])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_16_1.png" alt="png"></p><h2 id="6-Generate-the-histogram-below-of-monthly-passengers"><a href="#6-Generate-the-histogram-below-of-monthly-passengers" class="headerlink" title="#6 Generate the histogram below of monthly passengers"></a>#6 Generate the histogram below of monthly passengers</h2><p><strong>Additional requirements:</strong></p><ul><li>Only include 1955 and beyond</li><li>Use a binwidth of 50, a min of 200, and a max of 700</li><li>Set the yticks to start at 0, end at 25 by interval of 5</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers[passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1955</span>].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(temp[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">10</span>,<span class="built_in">range</span>=(<span class="number">200</span>,<span class="number">700</span>))</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_18_1.png" alt="png"></p><h2 id="7-Generate-the-histogram-below-of-monthly-passengers"><a href="#7-Generate-the-histogram-below-of-monthly-passengers" class="headerlink" title="#7 Generate the histogram below of monthly passengers"></a>#7 Generate the histogram below of monthly passengers</h2><p><strong>Additional requirements:</strong></p><ul><li>Generate two groups to compare. Group 1 should be the years 1949-1950. Group 2 should be the years 1959-60.</li><li>Binwidth of 50 from 100 to 700</li><li>yticks from 0 to 24, spaced by 2</li><li>Be sure to include a legend</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp1=passengers[ (passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1949</span>) &amp; (passengers[<span class="string">&#x27;Year&#x27;</span>]&lt;=<span class="number">1950</span>)].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">temp2=passengers[ (passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1959</span>) &amp; (passengers[<span class="string">&#x27;Year&#x27;</span>]&lt;=<span class="number">1960</span>)].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(temp1[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">12</span>,alpha=<span class="number">0.5</span>,<span class="built_in">range</span>=(<span class="number">100</span>,<span class="number">700</span>),label=<span class="string">&#x27;1949-50&#x27;</span>)</span><br><span class="line">plt.hist(temp2[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">12</span>,alpha=<span class="number">0.5</span>,<span class="built_in">range</span>=(<span class="number">100</span>,<span class="number">700</span>),label=<span class="string">&#x27;1959-60&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">24</span>, <span class="number">2.0</span>))</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x2d3705b9d90&gt;</code></pre><p><img src="/content/images/2021-08-25-output_20_1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matplotlib tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add ssh key to access github and simple git commands</title>
      <link href="2021/08/23/2021-08-23-1/"/>
      <url>2021/08/23/2021-08-23-1/</url>
      
        <content type="html"><![CDATA[<p>Here are ways to add ssh key in Github, such that one can connect to github repositories without typing username and passwords.</p><h5 id="For-MAC-Users"><a href="#For-MAC-Users" class="headerlink" title="For MAC Users:"></a>For MAC Users:</h5><p>To create an SSH Key, go to your terminal (or iTerm 2) and enter <code>ssh-keygen</code>. You will be prompted for a file in which to save the key (.ssh/id_rsa). There’s no need to enter anything, just press enter. When prompted for a passphrase, you can enter one or just press enter to ignore creating a passphrase.</p><p>Your SSH key has been saved to ssh/id_rsa.pub, run the command <code>pbcopy &lt; ~/.ssh/id_rsa.pub</code> to copy this to your clipboard.</p><p>Now go to your Github account setting for Key: <a href="https://github.com/settings/keys">https://github.com/settings/keys</a> and click ‘New SSH key’. Add any title you want, like ‘First SSH Key’, paste in your SSH key, and click the ‘Add SSH key’ button.</p><h5 id="For-PC-Users"><a href="#For-PC-Users" class="headerlink" title="For PC Users:"></a>For PC Users:</h5><p>To create an SSH Key, go to your terminal and enter <code>ssh-keygen</code>. You will be prompted for a file in which to save the key (.ssh/id_rsa). There’s no need to enter anything, just press enter. When prompted for a passphrase, you can enter one or just press enter to ignore creating a passphrase.</p><p>Your SSH key has been saved to ssh/id_rsa.pub, run the command <code>cat ~/.ssh/id_rsa.pub</code> to see your SSH key in the proper format. Copy the whole text string starting with ‘ssh’ to the end.</p><p>Now go to your Github account setting for keys: <a href="https://github.com/settings/keys">https://github.com/settings/keys</a> and click ‘New SSH key’. Add any title you want, like ‘First SSH Key’. Paste your SSH key to the Key window, and click the ‘Add SSH key’ button.</p><p>There are a lot of complex functionalities of Git, but for 99% of work there are 4 things that you need to be able to do:</p><ol><li><p>Create a branch <code>git checkout -b my-branch-name</code></p></li><li><p>Add files that you want to queue up for saving.</p><ul><li><p>To add all files: <code>git add .</code></p></li><li><p>To add a specific file: <code>git add path-to-file</code></p></li></ul></li><li><p>Save (commit) changes to the files that you added above. <code>git commit -m &quot;message about this commit. Ex: change button color to green.&quot;</code></p></li><li><p>Sync your branch to Github so that you can later make a pull request. <code>git push -u origin my-branch-name</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git and github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overlay histogram distributions using seaborn or matplotlib</title>
      <link href="2021/07/26/2021-07-26-1/"/>
      <url>2021/07/26/2021-07-26-1/</url>
      
        <content type="html"><![CDATA[<p>In this example, we show two methods to overlay histogram distributions using seaborn and matplotlib separately.</p><h2 id="import-library"><a href="#import-library" class="headerlink" title="import library"></a>import library</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="generate-some-data-for-histgram"><a href="#generate-some-data-for-histgram" class="headerlink" title="generate some data for histgram"></a>generate some data for histgram</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = np.random.randint(<span class="number">100</span>,size=<span class="number">1000</span>)</span><br><span class="line">list2 = np.random.randint(<span class="number">200</span>,size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># also getthem into a dataframe into one column, but add a second column to label source</span></span><br><span class="line"><span class="built_in">list</span> = np.concatenate([list1,list2])</span><br><span class="line">lables = [<span class="string">&#x27;l1&#x27;</span>]*<span class="built_in">len</span>(list1)+[<span class="string">&#x27;l2&#x27;</span>]*<span class="built_in">len</span>(list2)</span><br><span class="line">d = &#123;<span class="string">&#x27;x&#x27;</span>:<span class="built_in">list</span>,<span class="string">&#x27;label&#x27;</span>:lables&#125;</span><br><span class="line">df = pd.DataFrame(d)</span><br><span class="line">display(df)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>x</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>49</td>      <td>l1</td>    </tr>    <tr>      <th>1</th>      <td>93</td>      <td>l1</td>    </tr>    <tr>      <th>2</th>      <td>38</td>      <td>l1</td>    </tr>    <tr>      <th>3</th>      <td>96</td>      <td>l1</td>    </tr>    <tr>      <th>4</th>      <td>32</td>      <td>l1</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>1995</th>      <td>18</td>      <td>l2</td>    </tr>    <tr>      <th>1996</th>      <td>102</td>      <td>l2</td>    </tr>    <tr>      <th>1997</th>      <td>1</td>      <td>l2</td>    </tr>    <tr>      <th>1998</th>      <td>175</td>      <td>l2</td>    </tr>    <tr>      <th>1999</th>      <td>77</td>      <td>l2</td>    </tr>  </tbody></table><p>2000 rows × 2 columns</p></div><h2 id="method-1-overlay-dataframe-columns-with-different-labels"><a href="#method-1-overlay-dataframe-columns-with-different-labels" class="headerlink" title="method 1, overlay dataframe columns with different labels"></a>method 1, overlay dataframe columns with different labels</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.displot(df, x=<span class="string">&quot;x&quot;</span>, hue=<span class="string">&quot;label&quot;</span>, stat=<span class="string">&quot;density&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x24b5c685988&gt;</code></pre><p><img src="/content/images/2021-07-26-1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="method-2-overlay-two-seperate-lists-into-histograms-using-matplotlib"><a href="#method-2-overlay-two-seperate-lists-into-histograms-using-matplotlib" class="headerlink" title="method 2,  overlay two seperate lists into histograms using matplotlib"></a>method 2,  overlay two seperate lists into histograms using matplotlib</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(list1,bins=<span class="number">20</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;data1&quot;</span>)</span><br><span class="line">plt.hist(list2, bins=<span class="number">20</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;data2&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&quot;Data&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Count&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Multiple Histograms with Matplotlib&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x24b5face788&gt;</code></pre><p><img src="/content/images/2021-07-26-2.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>robust method to parse time string to datetime using dateutil parser</title>
      <link href="2021/07/14/2021-07-14-1/"/>
      <url>2021/07/14/2021-07-14-1/</url>
      
        <content type="html"><![CDATA[<p>Convert time strings into datetime format coudd be very tedious, because we need to make sure the every new data<br>follow the exact format required by the conversion functions, otherwise it will raise error.</p><p>One good way is to user the dateutil.parser package, which is much more flexiable. It can also support some fuzziness.<br>With this package, even some data has slightly different formats than we expect, it will not cause error.</p><h2 id="The-traditional-way-to-convert-time-string-to-datatime-format"><a href="#The-traditional-way-to-convert-time-string-to-datatime-format" class="headerlink" title="The traditional way to convert time string to datatime format"></a>The traditional way to convert time string to datatime format</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># without milliseconds format</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"><span class="comment"># with milliseconds format</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12.213&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S.%f&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in the above example if there is a new item that doesn&#x27;t follow the milliseconds format, it will raise error</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S.%f&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>20222022---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input-20-a29c39b3ca30&gt; in &lt;module&gt;     15 # in the above example if there is a new item that doesn&#39;t follow the milliseconds format, it will raise error     16 item = &#39;2022/01/01 10:49:12&#39;---&gt; 17 result = datetime.datetime.strptime(item, &quot;%Y/%m/%d %H:%M:%S.%f&quot;)     18      19 print(result.year)D:\Anaconda3\lib\_strptime.py in _strptime_datetime(cls, data_string, format)    575     &quot;&quot;&quot;Return a class cls instance based on the input string and the    576     format string.&quot;&quot;&quot;--&gt; 577     tt, fraction, gmtoff_fraction = _strptime(data_string, format)    578     tzname, gmtoff = tt[-2:]    579     args = tt[:6] + (fraction,)D:\Anaconda3\lib\_strptime.py in _strptime(data_string, format)    357     if not found:    358         raise ValueError(&quot;time data %r does not match format %r&quot; %--&gt; 359                          (data_string, format))    360     if len(data_string) != found.end():    361         raise ValueError(&quot;unconverted data remains: %s&quot; %ValueError: time data &#39;2022/01/01 10:49:12&#39; does not match format &#39;%Y/%m/%d %H:%M:%S.%f&#39;</code></pre><h2 id="use-parser-in-dateutil-package-a-more-stable-way"><a href="#use-parser-in-dateutil-package-a-more-stable-way" class="headerlink" title="use parser in dateutil package, a more stable way"></a>use parser in dateutil package, a more stable way</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dateutil.parser <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># for the same example above, it will not raiser error</span></span><br><span class="line"></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = parse(item)</span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12.213&#x27;</span></span><br><span class="line">result = parse(item)</span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>20222022</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> feature extraction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dataframe histogram visualization with seaborn using countplot</title>
      <link href="2021/07/06/2021-07-06-1/"/>
      <url>2021/07/06/2021-07-06-1/</url>
      
        <content type="html"><![CDATA[<p>We often need to plot histograms to visualize distributions of certain features or variables.<br>How to quickly obtain a useful plot and get the work done？ If what we care is the frequency of each values, seaborn provides<br>a convenient way, count_plot() function, to get the plot without count the data by ourself and then do the bar chars.</p><p>Check the following example:</p><h1 id="get-the-data-and-do-a-count-plot"><a href="#get-the-data-and-do-a-count-plot" class="headerlink" title="get the data and do a count plot"></a>get the data and do a count plot</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">titanic = sns.load_dataset(<span class="string">&quot;titanic&quot;</span>)</span><br><span class="line">titanic[<span class="string">&#x27;class&#x27;</span>] = titanic[<span class="string">&#x27;class&#x27;</span>].astype(<span class="string">&#x27;str&#x27;</span>)</span><br><span class="line">display(titanic)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>survived</th>      <th>pclass</th>      <th>sex</th>      <th>age</th>      <th>sibsp</th>      <th>parch</th>      <th>fare</th>      <th>embarked</th>      <th>class</th>      <th>who</th>      <th>adult_male</th>      <th>deck</th>      <th>embark_town</th>      <th>alive</th>      <th>alone</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>7.2500</td>      <td>S</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>False</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>71.2833</td>      <td>C</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>C</td>      <td>Cherbourg</td>      <td>yes</td>      <td>False</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>3</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>7.9250</td>      <td>S</td>      <td>Third</td>      <td>woman</td>      <td>False</td>      <td>NaN</td>      <td>Southampton</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>53.1000</td>      <td>S</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>C</td>      <td>Southampton</td>      <td>yes</td>      <td>False</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>8.0500</td>      <td>S</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>True</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>886</th>      <td>0</td>      <td>2</td>      <td>male</td>      <td>27.0</td>      <td>0</td>      <td>0</td>      <td>13.0000</td>      <td>S</td>      <td>Second</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>True</td>    </tr>    <tr>      <th>887</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>19.0</td>      <td>0</td>      <td>0</td>      <td>30.0000</td>      <td>S</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>B</td>      <td>Southampton</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>888</th>      <td>0</td>      <td>3</td>      <td>female</td>      <td>NaN</td>      <td>1</td>      <td>2</td>      <td>23.4500</td>      <td>S</td>      <td>Third</td>      <td>woman</td>      <td>False</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>False</td>    </tr>    <tr>      <th>889</th>      <td>1</td>      <td>1</td>      <td>male</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>30.0000</td>      <td>C</td>      <td>First</td>      <td>man</td>      <td>True</td>      <td>C</td>      <td>Cherbourg</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>890</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>32.0</td>      <td>0</td>      <td>0</td>      <td>7.7500</td>      <td>Q</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Queenstown</td>      <td>no</td>      <td>True</td>    </tr>  </tbody></table><p>891 rows × 15 columns</p></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set_theme(style=<span class="string">&quot;darkgrid&quot;</span>)</span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;embark_town&quot;</span>, data=titanic)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-1.png" alt="png"></p><h1 id="what-if-we-have-too-many-values-for-the-feature-and-we-can’t-plot-all-of-their-distributions-in-the-histogram"><a href="#what-if-we-have-too-many-values-for-the-feature-and-we-can’t-plot-all-of-their-distributions-in-the-histogram" class="headerlink" title="what if we have too many values for the feature, and we can’t plot all of their distributions in the histogram?"></a>what if we have too many values for the feature, and we can’t plot all of their distributions in the histogram?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the distinct values first, then choose the top n values we want to present; here we choose 2 as an example</span></span><br><span class="line"></span><br><span class="line">sub_index = titanic[<span class="string">&#x27;class&#x27;</span>].value_counts().index[:<span class="number">2</span>]</span><br><span class="line">sub_data = titanic[titanic[<span class="string">&#x27;class&#x27;</span>].isin(sub_index)]</span><br><span class="line">sub_data = sub_data.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, data=sub_data)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we can also explicitly require the order to be ascending</span></span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, data=sub_data,order=sub_index[::-<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-3.png" alt="png"></p><h1 id="now-how-to-show-the-value-counts-for-two-categorical-variables"><a href="#now-how-to-show-the-value-counts-for-two-categorical-variables" class="headerlink" title="now how to show the value counts for two categorical variables?"></a>now how to show the value counts for two categorical variables?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, hue=<span class="string">&quot;who&quot;</span>, data=titanic)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-4.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>multi line plot with seaborn</title>
      <link href="2021/07/01/2021-07-01-1/"/>
      <url>2021/07/01/2021-07-01-1/</url>
      
        <content type="html"><![CDATA[<p>In this example, we show to do multi-plot graph using seaborn.<br>In addtion, some of the ways to change fonts sizes are also shown.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = [</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;10&#x27;</span>,<span class="string">&#x27;2021-07-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;12&#x27;</span>,<span class="string">&#x27;2021-08-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;15&#x27;</span>,<span class="string">&#x27;2021-09-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;20&#x27;</span>,<span class="string">&#x27;2021-10-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;20&#x27;</span>,<span class="string">&#x27;2021-07-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;22&#x27;</span>,<span class="string">&#x27;2021-08-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;25&#x27;</span>,<span class="string">&#x27;2021-09-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;30&#x27;</span>,<span class="string">&#x27;2021-10-01&#x27;</span>],</span><br><span class="line">        </span><br><span class="line">       ]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data=data, columns=[<span class="string">&#x27;company&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;date&#x27;</span>])</span><br><span class="line">display(df)</span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>company</th>      <th>price</th>      <th>date</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A</td>      <td>10</td>      <td>2021-07-01</td>    </tr>    <tr>      <th>1</th>      <td>A</td>      <td>12</td>      <td>2021-08-01</td>    </tr>    <tr>      <th>2</th>      <td>A</td>      <td>15</td>      <td>2021-09-01</td>    </tr>    <tr>      <th>3</th>      <td>A</td>      <td>20</td>      <td>2021-10-01</td>    </tr>    <tr>      <th>4</th>      <td>B</td>      <td>20</td>      <td>2021-07-01</td>    </tr>    <tr>      <th>5</th>      <td>B</td>      <td>22</td>      <td>2021-08-01</td>    </tr>    <tr>      <th>6</th>      <td>B</td>      <td>25</td>      <td>2021-09-01</td>    </tr>    <tr>      <th>7</th>      <td>B</td>      <td>30</td>      <td>2021-10-01</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># global change the font scales of the sns plot for easy set up</span></span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">2</span>) </span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;white&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">temp = df.sort_values(by=<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plot_= sns.lineplot(x=<span class="string">&quot;date&quot;</span>, y=<span class="string">&quot;price&quot;</span>, hue=<span class="string">&quot;company&quot;</span>, data=temp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># in case too many data points, we can skip some ticks on the x axis</span></span><br><span class="line"><span class="keyword">for</span> ind, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_.get_xticklabels()):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ind % <span class="number">2</span> == <span class="number">0</span>:  <span class="comment"># every second label is kept</span></span><br><span class="line">        label.set_visible(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        label.set_visible(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># adjust fonrt size of x axis , y axis and title</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#plot_.set_title(&#x27;example plot&#x27;)</span></span><br><span class="line">plot_.axes.set_title(<span class="string">&quot;Title&quot;</span>,fontsize=<span class="number">50</span>)</span><br><span class="line">plot_.set_xlabel(<span class="string">&quot;X Label&quot;</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">plot_.set_ylabel(<span class="string">&quot;Y Label&quot;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># adjust lengend font size</span></span><br><span class="line">plot_.legend(fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)                                                               </span><br><span class="line">plt.tight_layout()     </span><br><span class="line">plt.grid(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-01-1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rare event encoding for categorical feature in machine learning in pandas dataframe</title>
      <link href="2021/06/23/2021-06-23-1/"/>
      <url>2021/06/23/2021-06-23-1/</url>
      
        <content type="html"><![CDATA[<p>If categorical features has too many values, it will generate too many features after encoding, such as one-hot encoding.<br>We could set the threshold, if certan value has percentage less than the threshold, we change the value to be ‘rare event’ or<br>something like that. By doing this, we make sure there are not too many levels for a categorical feature.</p><p>The following code can be applied on a dataframe:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def cat_rare_event(df,threshold=0.005):</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line"></span><br><span class="line">        #print(df[col].dtype)</span><br><span class="line"></span><br><span class="line">        if df[col].dtype ==&#x27;object&#x27;:</span><br><span class="line"></span><br><span class="line">            print(col)</span><br><span class="line"></span><br><span class="line">            df.loc[df[col].value_counts()[df[col]].values &lt; int(len(df)*threshold), col] = &quot;rare_value&quot;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>or we could put this step as a customized pipeline:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">class cat_rare_event_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def __init__(self,threshold=0.005):</span><br><span class="line"></span><br><span class="line">        self.threshold = threshold</span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = cat_rare_event(X,self.threshold)</span><br><span class="line"></span><br><span class="line">               </span><br><span class="line"></span><br><span class="line">        return X    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>handles feature order in training and online production stage to avoid inconsistent error</title>
      <link href="2021/06/18/2021-06-18-1/"/>
      <url>2021/06/18/2021-06-18-1/</url>
      
        <content type="html"><![CDATA[<p>In applying machine learning models in production stage, like lightGBM model or any models.<br>While we all know the order of features shoud be same for both training stage, test stage, and the production stage.</p><p>In practice we might ignore that. In produciton stage, new data might come as a json format, where orders will disappear,<br>it has nothing to do with the original feature order in the model training stage.</p><p>The comming json will be converted to dataframe format, and passed to the model for prediction. We might usually<br>igore the fact that, the new dataframe column order is different from the original training dataframe column order now.<br>And it’s important to make sure they are consistent, and not up to the randome fate.</p><p>There are many ways to achieve this, the following shows how to do it in a pipeline fashion.</p><h2 id="define-piplenow-to-treat-the-effects-systematically"><a href="#define-piplenow-to-treat-the-effects-systematically" class="headerlink" title="define piplenow, to treat the effects systematically"></a>define piplenow, to treat the effects systematically</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LastStepTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.traincolumns = []</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;initialized&#x27;</span>)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Return self, nothing else to do here</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.traincolumns = X.columns</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line">        <span class="comment"># make sure any new data follows the same order of features used in the training stage</span></span><br><span class="line">        <span class="keyword">return</span> X[self.traincolumns]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># make an data process pipeline, using the above transformer as the last steps here.</span></span><br><span class="line"><span class="comment"># in practice, any preprocessing steps can be put here as well</span></span><br><span class="line"></span><br><span class="line">dataPipeline = Pipeline([</span><br><span class="line"> (<span class="string">&#x27;last_step&#x27;</span>,LastStepTransformer())   </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>initialized</code></pre><h2 id="show-an-example"><a href="#show-an-example" class="headerlink" title="show an example"></a>show an example</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">data_train =[[<span class="number">1.1</span>,<span class="number">2.2</span>],[<span class="number">2.1</span>,<span class="number">3.2</span>]]</span><br><span class="line">data_test =[[<span class="number">3.1</span>,<span class="number">5.2</span>],[<span class="number">1.1</span>,<span class="number">2.2</span>]]</span><br><span class="line"></span><br><span class="line">df_train = pd.DataFrame(data=data_train,columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>])</span><br><span class="line">df_test = pd.DataFrame(data=data_test,columns=[<span class="string">&#x27;col2&#x27;</span>,<span class="string">&#x27;col1&#x27;</span>])</span><br><span class="line"></span><br><span class="line">display(df_train)</span><br><span class="line">display(df_test)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.1</td>      <td>2.2</td>    </tr>    <tr>      <th>1</th>      <td>2.1</td>      <td>3.2</td>    </tr>  </tbody></table></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col2</th>      <th>col1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>3.1</td>      <td>5.2</td>    </tr>    <tr>      <th>1</th>      <td>1.1</td>      <td>2.2</td>    </tr>  </tbody></table></div><h2 id="now-in-the-training-stage-we-call-fit-transform-of-the-data-pipeline-so-the-pipeline-will-remembers-the-orignal-order"><a href="#now-in-the-training-stage-we-call-fit-transform-of-the-data-pipeline-so-the-pipeline-will-remembers-the-orignal-order" class="headerlink" title="now in the training stage, we call fit_transform() of the data pipeline, so the pipeline will remembers the orignal order"></a>now in the training stage, we call fit_transform() of the data pipeline, so the pipeline will remembers the orignal order</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dataPipeline.fit_transform(df_train)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.1</td>      <td>2.2</td>    </tr>    <tr>      <th>1</th>      <td>2.1</td>      <td>3.2</td>    </tr>  </tbody></table></div><h2 id="now-in-the-test-stage-we-only-call-transform-of-the-datapipleine-so-any-new-data-will-be-reordered-as-the-training-data"><a href="#now-in-the-test-stage-we-only-call-transform-of-the-datapipleine-so-any-new-data-will-be-reordered-as-the-training-data" class="headerlink" title="now in the test stage, we only call transform() of the datapipleine, so any new data will be reordered as the training data"></a>now in the test stage, we only call transform() of the datapipleine, so any new data will be reordered as the training data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test data, notice the column order&#x27;</span>)</span><br><span class="line">display(df_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after transform, notice the column order now changes&#x27;</span>)</span><br><span class="line">dataPipeline.transform(df_test)</span><br></pre></td></tr></table></figure><pre><code>test data, notice the column order</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col2</th>      <th>col1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>3.1</td>      <td>5.2</td>    </tr>    <tr>      <th>1</th>      <td>1.1</td>      <td>2.2</td>    </tr>  </tbody></table></div><pre><code>after transform, ontice the column order now changes</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.2</td>      <td>3.1</td>    </tr>    <tr>      <th>1</th>      <td>2.2</td>      <td>1.1</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>some handy functions to group continous variables and missing value imputation in dataframe</title>
      <link href="2021/06/15/2021-06-15-1/"/>
      <url>2021/06/15/2021-06-15-1/</url>
      
        <content type="html"><![CDATA[<p>Following example shows how to group age variable into groups,<br>and some simple missing value imputaiton proecdures.</p><p>There is also an example to transform timestamp variable to week day and hour infomation.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># utility functions</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">def age_input(age):</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    if pd.isnull(age):</span><br><span class="line"></span><br><span class="line">        return &#x27;missing&#x27;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    age = int(age)</span><br><span class="line"></span><br><span class="line">    if age&lt;=20:</span><br><span class="line"></span><br><span class="line">        return &#x27;16-20&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=24:</span><br><span class="line"></span><br><span class="line">        return &#x27;21-24&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=34:</span><br><span class="line"></span><br><span class="line">        return &#x27;25-34&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=44:</span><br><span class="line"></span><br><span class="line">        return &#x27;35-44&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=54:</span><br><span class="line"></span><br><span class="line">        return &#x27;45-54&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=64:</span><br><span class="line"></span><br><span class="line">        return &#x27;55-64&#x27;</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        return &#x27;65+&#x27;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"># missing value handelling or imputation in dataframe </span><br><span class="line"></span><br><span class="line">def missing_handle(df):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"></span><br><span class="line">        if df[col].dtype==object:</span><br><span class="line"></span><br><span class="line">            df[col] = df[col].fillna(&#x27;missing&#x27;)</span><br><span class="line"></span><br><span class="line">        elif df[col].dtype == bool:</span><br><span class="line"></span><br><span class="line">            df[col+&#x27;_null&#x27;] = df[col].apply(lambda x: 1 if pd.isnull(x) else 0)</span><br><span class="line"></span><br><span class="line">            df[col] = data[col].fillna(data[col].mode()[0])</span><br><span class="line"></span><br><span class="line">          </span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            df[col] = df[col].fillna(-999)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">class dayandhour_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Customized transformer method</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X[&#x27;dayofweek&#x27;]=pd.to_datetime(X[&#x27;sentat&#x27;]).dt.dayofweek</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X[&#x27;hour&#x27;]=pd.to_datetime(X[&#x27;sentat&#x27;]).dt.hour</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;sentat&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">        # apply age group function here</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">        X[&#x27;age_group&#x27;] = X[&#x27;age&#x27;].apply(age_input)</span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;age&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        # apply missing handelling here</span><br><span class="line"></span><br><span class="line">        X = missing_handle(X)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        return X</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"># define the transformer</span><br><span class="line">dayandhour_transformer = dayandhour_Transformer()</span><br><span class="line"></span><br><span class="line"># usage example</span><br><span class="line">df_new = dayandhour_transformer.transform(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>explode and expand rows to multiple rows or columns to multiple columns using pandas dataframe</title>
      <link href="2021/06/14/2021-06-14-1/"/>
      <url>2021/06/14/2021-06-14-1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="generate-some-example-some-data"><a href="#generate-some-example-some-data" class="headerlink" title="generate some example some data"></a>generate some example some data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [[<span class="string">&#x27;python&#x27;</span>,<span class="string">&#x27;C&#x27;</span>],<span class="string">&#x27;John&#x27;</span>],[[<span class="string">&#x27;python&#x27;</span>,<span class="string">&#x27;Go&#x27;</span>],<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;language&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>       language  name0   [python, C]  John1  [python, Go]  Mark</code></pre><h2 id="1-First-we-expload-the-laguage-column-put-each-of-the-array-element-into-a-single-row"><a href="#1-First-we-expload-the-laguage-column-put-each-of-the-array-element-into-a-single-row" class="headerlink" title="1. First, we expload the laguage column, put each of the array element into a single row"></a>1. First, we expload the laguage column, put each of the array element into a single row</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.explode(<span class="string">&#x27;language&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John0        C  John1   python  Mark1       Go  Mark</code></pre><h3 id="if-we-want-to-reset-the-index…"><a href="#if-we-want-to-reset-the-index…" class="headerlink" title="if we want to reset the index…."></a>if we want to reset the index….</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.explode(<span class="string">&#x27;language&#x27;</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John1        C  John2   python  Mark3       Go  Mark</code></pre><h3 id="1-2-now-how-about-the-original-column-is-not-list-but-strings-we-need-to-split-use-assign-then-chain-it-with-explode"><a href="#1-2-now-how-about-the-original-column-is-not-list-but-strings-we-need-to-split-use-assign-then-chain-it-with-explode" class="headerlink" title="1.2 now how about the original column is not list, but strings we need to split? use assign then chain it with explode"></a>1.2 now how about the original column is not list, but strings we need to split? use assign then chain it with explode</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [<span class="string">&#x27;python,C&#x27;</span>,<span class="string">&#x27;John&#x27;</span>],[<span class="string">&#x27;python,Go&#x27;</span>,<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;language&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>    language  name0   python,C  John1  python,Go  Mark</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.assign(language=df.language.<span class="built_in">str</span>.split(<span class="string">&quot;,&quot;</span>)).explode(<span class="string">&#x27;language&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John0        C  John1   python  Mark1       Go  Mark</code></pre><h2 id="2-in-the-above-example-we-expand-rows-into-multiple-rows-by-one-column’s-list-like-element-now-sometimes-we-need-to-expand-columns-into-multiple-columns"><a href="#2-in-the-above-example-we-expand-rows-into-multiple-rows-by-one-column’s-list-like-element-now-sometimes-we-need-to-expand-columns-into-multiple-columns" class="headerlink" title="2. in the above example, we expand rows into multiple rows by one column’s list like element; now sometimes we need to expand columns into multiple columns"></a>2. in the above example, we expand rows into multiple rows by one column’s list like element; now sometimes we need to expand columns into multiple columns</h2><h3 id="let’s-generate-some-data-again"><a href="#let’s-generate-some-data-again" class="headerlink" title="let’s generate some data again"></a>let’s generate some data again</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [[<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;27&#x27;</span>],<span class="string">&#x27;John&#x27;</span>],[[<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;30&#x27;</span>],<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;age_info&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>    age_info  name0  [age, 27]  John1  [age, 30]  Mark</code></pre><h3 id="we-could-use-to-list"><a href="#we-could-use-to-list" class="headerlink" title="we could use to_list()"></a>we could use to_list()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;attribute&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]] = df.age_info.to_list()</span><br><span class="line"><span class="comment"># or df[[&#x27;First&#x27;,&#x27;Last&#x27;]] = df[&#x27;age_info&#x27;].to_list()</span></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>    age_info  name attribute value0  [age, 27]  John       age    271  [age, 30]  Mark       age    30</code></pre><h3 id="now-same-quesiton-how-about-the-column-is-a-string-that-can-be-split"><a href="#now-same-quesiton-how-about-the-column-is-a-string-that-can-be-split" class="headerlink" title="now same quesiton, how about the column is a string that can be split?"></a>now same quesiton, how about the column is a string that can be split?</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [<span class="string">&#x27;john,f&#x27;</span>,<span class="string">&#x27;1&#x27;</span>],[<span class="string">&#x27;mark,y&#x27;</span>,<span class="string">&#x27;2&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;full_name&#x27;</span>,<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df[[<span class="string">&#x27;First&#x27;</span>,<span class="string">&#x27;Last&#x27;</span>]] = df.full_name.<span class="built_in">str</span>.split(<span class="string">&quot;,&quot;</span>,expand=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>  full_name id0    john,f  11    mark,y  2  full_name id First Last0    john,f  1  john    f1    mark,y  2  mark    y</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aggregate features from different rows into one row in pandas dataframe</title>
      <link href="2021/06/12/2021-06-12-1/"/>
      <url>2021/06/12/2021-06-12-1/</url>
      
        <content type="html"><![CDATA[<p>In many use cases, different features of the same event are stored in a table by multiple rows.<br>multiple columns will indicate each characteristics of one feature, such as name, value, timestamp, etc.</p><p>In machine learning, we need to aggregate them into one row for training, and the following shows how do do it in dataframe easily.</p><h2 id="generate-some-example-dataframe"><a href="#generate-some-example-dataframe" class="headerlink" title="generate some example dataframe"></a>generate some example dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = [ [<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name1&#x27;</span>,<span class="string">&#x27;value1&#x27;</span>],[<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name2&#x27;</span>,<span class="string">&#x27;value2&#x27;</span>],[<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name3&#x27;</span>,<span class="string">&#x27;value3&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;name1&#x27;</span>,<span class="string">&#x27;value4&#x27;</span>],[<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;name2&#x27;</span>,<span class="string">&#x27;value5&#x27;</span>] </span><br><span class="line">       ]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data=data)</span><br><span class="line">df.columns =[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">display(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>name</th>      <th>value</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>name1</td>      <td>value1</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>name2</td>      <td>value2</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>name3</td>      <td>value3</td>    </tr>    <tr>      <th>3</th>      <td>2</td>      <td>name1</td>      <td>value4</td>    </tr>    <tr>      <th>4</th>      <td>2</td>      <td>name2</td>      <td>value5</td>    </tr>  </tbody></table></div><h2 id="group-the-dataframe-by-id-then-aggregate-all-the-feature-values-into-one-column"><a href="#group-the-dataframe-by-id-then-aggregate-all-the-feature-values-into-one-column" class="headerlink" title="group the dataframe by id, then aggregate all the feature values into one column"></a>group the dataframe by id, then aggregate all the feature values into one column</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.groupby(<span class="string">&#x27;id&#x27;</span>).apply(<span class="keyword">lambda</span> x: <span class="built_in">dict</span>(x[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]].values.tolist())).reset_index()</span><br><span class="line">df3 = pd.DataFrame(data=df2[<span class="number">0</span>].values.tolist())</span><br><span class="line">display(df3)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name1</th>      <th>name2</th>      <th>name3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>value1</td>      <td>value2</td>      <td>value3</td>    </tr>    <tr>      <th>1</th>      <td>value4</td>      <td>value5</td>      <td>NaN</td>    </tr>  </tbody></table></div><h1 id="put-the-above-the-transformation-into-a-scikit-learn-customed-transformer"><a href="#put-the-above-the-transformation-into-a-scikit-learn-customed-transformer" class="headerlink" title="put the above the transformation into a scikit-learn customed transformer"></a>put the above the transformation into a scikit-learn customed transformer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_Transformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">           </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return self</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Customized transformer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line">        X2 = X.groupby(<span class="string">&#x27;id&#x27;</span>).apply(<span class="keyword">lambda</span> x: <span class="built_in">dict</span>(x[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]].values.tolist())).reset_index()</span><br><span class="line">        X3 = pd.DataFrame(data=X2[<span class="number">0</span>].values.tolist())     </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X3</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X1</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"></span><br><span class="line"><span class="comment"># get a transformer object</span></span><br><span class="line">my_transformer = my_Transformer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the transform on the original data</span></span><br><span class="line"></span><br><span class="line">df_new = my_transformer.transform(df)</span><br><span class="line">display(df_new)</span><br></pre></td></tr></table></figure><pre><code>start</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name1</th>      <th>name2</th>      <th>name3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>value1</td>      <td>value2</td>      <td>value3</td>    </tr>    <tr>      <th>1</th>      <td>value4</td>      <td>value5</td>      <td>NaN</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pipeline </tag>
            
            <tag> preprocessing </tag>
            
            <tag> feature engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>time series feature engineering using tsfresh, training vs test</title>
      <link href="2021/06/10/2021-06-10-1/"/>
      <url>2021/06/10/2021-06-10-1/</url>
      
        <content type="html"><![CDATA[<p>During the test stage, i.e., once the model is on production, for any new data,<br>tsfresh feature generation does not depend the training data. So one can apply the same feature engineering process as the training data<br>without worrying about stroing information from training stage.</p><p>On ther hand, one can also use the following example to leverage scikit learn pipleline style to handel the feature generation<br>for both training and test stages.</p><h1 id="Feature-Selection-in-a-sklearn-pipeline"><a href="#Feature-Selection-in-a-sklearn-pipeline" class="headerlink" title="Feature Selection in a sklearn pipeline"></a>Feature Selection in a sklearn pipeline</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tsfresh.examples <span class="keyword">import</span> load_robot_execution_failures</span><br><span class="line"><span class="keyword">from</span> tsfresh.transformers <span class="keyword">import</span> RelevantFeatureAugmenter</span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br></pre></td></tr></table></figure><h2 id="Load-and-Prepare-the-Data"><a href="#Load-and-Prepare-the-Data" class="headerlink" title="Load and Prepare the Data"></a>Load and Prepare the Data</h2><p>Check out the first example notebook to learn more about the data and format.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh.examples.robot_execution_failures <span class="keyword">import</span> download_robot_execution_failures</span><br><span class="line">download_robot_execution_failures() </span><br><span class="line">df_ts, y = load_robot_execution_failures()</span><br></pre></td></tr></table></figure><p>We want to use the extracted features to predict for each of the robot executions, if it was a failure or not.<br>Therefore our basic “entity” is a single robot execution given by a distinct <code>id</code>.</p><p>A dataframe with these identifiers as index needs to be prepared for the pipeline.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = pd.DataFrame(index=y.index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train and test set</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br></pre></td></tr></table></figure><h2 id="Build-the-pipeline"><a href="#Build-the-pipeline" class="headerlink" title="Build the pipeline"></a>Build the pipeline</h2><p>We build a sklearn pipeline that consists of a feature extraction step (<code>RelevantFeatureAugmenter</code>) with a subsequent <code>RandomForestClassifier</code>.</p><p>The <code>RelevantFeatureAugmenter</code> takes roughly the same arguments as <code>extract_features</code> and <code>select_features</code> do.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;augmenter&#x27;</span>, RelevantFeatureAugmenter(column_id=<span class="string">&#x27;id&#x27;</span>, column_sort=<span class="string">&#x27;time&#x27;</span>)),</span><br><span class="line">        (<span class="string">&#x27;classifier&#x27;</span>, RandomForestClassifier())</span><br><span class="line">      ])</span><br></pre></td></tr></table></figure><div class="alert alert-warning">    <p>Here comes the tricky part!</p><p>The input to the pipeline will be our dataframe <code>X</code>, which one row per identifier.<br>It is currently empty.<br>But which time series data should the <code>RelevantFeatureAugmenter</code> to actually extract the features from?</p><p>We need to pass the time series data (stored in <code>df_ts</code>) to the transformer.</p></div><p>In this case, df_ts contains the time series of both train and test set, if you have different dataframes for<br>train and test set, you have to call set_params two times<br>(see further below on how to deal with two independent data sets)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts);</span><br></pre></td></tr></table></figure><p>We are now ready to fit the pipeline</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>The augmenter has used the input time series data to extract time series features for each of the identifiers in the <code>X_train</code> and selected only the relevant ones using the passed <code>y_train</code> as target.<br>These features have been added to <code>X_train</code> as new columns.<br>The classifier can now use these features during trainings.</p><h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>During interference, the augmentor does only extract the relevant features it has found out in the training phase and the classifier predicts the target using these features.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = ppl.predict(X_test)</span><br></pre></td></tr></table></figure><p>So, finally we inspect the performance:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><p>You can also find out, which columns the augmenter has selected</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.named_steps[<span class="string">&quot;augmenter&quot;</span>].feature_selector.relevant_features</span><br></pre></td></tr></table></figure><div class="alert alert-info">    <p>In this example we passed in an empty (except the index) <code>X_train</code> or <code>X_test</code> into the pipeline.<br>However, you can also fill the input with other features you have (e.g. features extracted from the metadata)<br>or even use other pipeline components before.</p></div><h2 id="Separating-the-time-series-data-containers"><a href="#Separating-the-time-series-data-containers" class="headerlink" title="Separating the time series data containers"></a>Separating the time series data containers</h2><p>In the example above we passed in a single <code>df_ts</code> into the <code>RelevantFeatureAugmenter</code>, which was used both for training and predicting.<br>During training, only the data with the <code>id</code>s from <code>X_train</code> where extracted and during prediction the rest.</p><p>However, it is perfectly fine to call <code>set_params</code> twice: once before training and once before prediction.<br>This can be handy if you for example dump the trained pipeline to disk and re-use it only later for prediction.<br>You only need to make sure that the <code>id</code>s of the enteties you use during training/prediction are actually present in the passed time series data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_ts_train = df_ts[df_ts[<span class="string">&quot;id&quot;</span>].isin(y_train.index)]</span><br><span class="line">df_ts_test = df_ts[df_ts[<span class="string">&quot;id&quot;</span>].isin(y_test.index)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts_train);</span><br><span class="line">ppl.fit(X_train, y_train);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pipeline.pkl&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(ppl, f)</span><br></pre></td></tr></table></figure><p>Later: load the fitted model and do predictions on new, unseen data</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pipeline.pkl&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    ppk = pickle.load(f)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts_test);</span><br><span class="line">y_pred = ppl.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pipeline </tag>
            
            <tag> preprocessing </tag>
            
            <tag> tsfresh </tag>
            
            <tag> time series </tag>
            
            <tag> feature engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word tokenization and sentence tokenization in python using NLTK package</title>
      <link href="2021/06/09/2021-06-09-1/"/>
      <url>2021/06/09/2021-06-09-1/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Tokenization"><a href="#What-is-Tokenization" class="headerlink" title="What is Tokenization?"></a>What is Tokenization?</h1><p>Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens.<br>These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization.<br>Tokenization also helps to substitute sensitive data elements with non-sensitive data elements.</p><p>Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.<br>It becomes vital to understand the pattern in the text to achieve the above-stated purpose.</p><h1 id="Tokenization-of-words"><a href="#Tokenization-of-words" class="headerlink" title="Tokenization of words"></a>Tokenization of words</h1><p>We use the method word_tokenize() to split a sentence into words.<br>The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications.<br>It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming.</p><p>Code example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">text = &quot;God is Great! I won a lottery.&quot;</span><br><span class="line">print(word_tokenize(text))</span><br><span class="line"></span><br><span class="line">Output: [&#x27;God&#x27;, &#x27;is&#x27;, &#x27;Great&#x27;, &#x27;!&#x27;, &#x27;I&#x27;, &#x27;won&#x27;, &#x27;a&#x27;, &#x27;lottery&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure><p>From the above example, one can see the punctuationa are also included. Sometimes we want to exclude that.<br>To achieve this purpose, there are two ways:</p><h2 id="USE-nltk-RegexpTokenizer-TO-REMOVE-ALL-PUNCTUATION-MARKS"><a href="#USE-nltk-RegexpTokenizer-TO-REMOVE-ALL-PUNCTUATION-MARKS" class="headerlink" title="USE nltk.RegexpTokenizer() TO REMOVE ALL PUNCTUATION MARKS"></a>USE nltk.RegexpTokenizer() TO REMOVE ALL PUNCTUATION MARKS</h2><p>Call nltk.RegexpTokenizer(pattern) with pattern as r”\w+” to create a tokenzier that uses pattern to split a string.<br>Call RegexpTokenizer.tokenize(text) with RegexpTokenizer as the previous result and text as a string representing a sentence to return text as a list of words with punctuation’s removed.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sentence  = &quot;Think and wonder, wonder and think.&quot;</span><br><span class="line"></span><br><span class="line">tokenizer = nltk.RegexpTokenizer(r&quot;\w+&quot;)</span><br><span class="line">new_words = tokenizer.tokenize(sentence)</span><br><span class="line"></span><br><span class="line">print(new_words)</span><br><span class="line">OUTPUT</span><br><span class="line">[&#x27;Think&#x27;, &#x27;and&#x27;, &#x27;wonder&#x27;, &#x27;wonder&#x27;, &#x27;and&#x27;, &#x27;think&#x27;]</span><br></pre></td></tr></table></figure><h2 id="USE-nltk-word-tokenize-AND-LIST-COMPREHENSION-TO-REMOVE-ALL-PUNCTUATION-MARKS"><a href="#USE-nltk-word-tokenize-AND-LIST-COMPREHENSION-TO-REMOVE-ALL-PUNCTUATION-MARKS" class="headerlink" title="USE nltk.word_tokenize() AND LIST COMPREHENSION TO REMOVE ALL PUNCTUATION MARKS"></a>USE nltk.word_tokenize() AND LIST COMPREHENSION TO REMOVE ALL PUNCTUATION MARKS</h2><p>Call nltk.word_tokenize(text) with text as a string representing a sentence to return text as a list of words. Use the syntax [word for word in words if condition] with words as the previous result and condition as word.isalnum() to create a list containing each word in words that only contain alphanumeric characters.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sentence  = &quot;Think and wonder, wonder and think.&quot;</span><br><span class="line"></span><br><span class="line">words = nltk.word_tokenize(sentence)</span><br><span class="line">new_words= [word for word in words if word.isalnum()]</span><br><span class="line"></span><br><span class="line">print(new_words)</span><br><span class="line">OUTPUT</span><br><span class="line">[&#x27;Think&#x27;, &#x27;and&#x27;, &#x27;wonder&#x27;, &#x27;wonder&#x27;, &#x27;and&#x27;, &#x27;think&#x27;]</span><br></pre></td></tr></table></figure><h1 id="Tokenization-of-Sentences"><a href="#Tokenization-of-Sentences" class="headerlink" title="Tokenization of Sentences"></a>Tokenization of Sentences</h1><p>Sometimes you need to get sentences out of the texts at first.</p><p>Code example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from nltk.tokenize import sent_tokenize</span><br><span class="line">text = &quot;God is Great! I won a lottery.&quot;</span><br><span class="line">print(sent_tokenize(text))</span><br><span class="line"></span><br><span class="line">Output: [&#x27;God is Great!&#x27;, &#x27;I won a lottery &#x27;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tokenization </tag>
            
            <tag> NLTK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>missing value or null value processing in pandas dataframe</title>
      <link href="2021/06/08/2021-06-08-1/"/>
      <url>2021/06/08/2021-06-08-1/</url>
      
        <content type="html"><![CDATA[<h1 id="obtain-null-or-missing-values-of-a-dataframe"><a href="#obtain-null-or-missing-values-of-a-dataframe" class="headerlink" title="obtain null or missing values of a dataframe"></a>obtain null or missing values of a dataframe</h1><p>Suppose the dataframe has the following formats, with 10 rows and 5 clomns:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">          0         1         2         3         4         5</span><br><span class="line">0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281</span><br><span class="line">1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952</span><br><span class="line">2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425</span><br><span class="line">3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797</span><br><span class="line">4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722</span><br><span class="line">5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814</span><br><span class="line">6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368</span><br><span class="line">7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN</span><br><span class="line">8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN</span><br><span class="line">9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>the isnull() function which would return a dataframe like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       0      1      2      3      4      5</span><br><span class="line">0  False  False  False  False  False  False</span><br><span class="line">1  False   True  False  False  False  False</span><br><span class="line">2  False   True  False  False  False  False</span><br><span class="line">3  False  False  False  False  False  False</span><br><span class="line">4  False  False  False  False  False  False</span><br><span class="line">5  False  False  False   True  False  False</span><br><span class="line">6  False  False  False  False  False  False</span><br><span class="line">7  False  False  False  False  False   True</span><br><span class="line">8  False  False  False  False  False   True</span><br><span class="line">9  False  False  False  False  False  False</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>following command will select rows that has any null values</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df.isnull().any(axis=1)]</span><br></pre></td></tr></table></figure><p>following command will select columns that has any null values</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df.columns[df.isna().any()]]</span><br></pre></td></tr></table></figure><p>follwoing command will select rows that have null values for a specific column, e.g., column=3</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df[3].isnull()]</span><br></pre></td></tr></table></figure><h1 id="Drop-null-values"><a href="#Drop-null-values" class="headerlink" title="Drop null values"></a>Drop null values</h1><blockquote><blockquote><blockquote><p>df = pd.DataFrame({“name”: [‘Alfred’, ‘Batman’, ‘Catwoman’],<br>…                    “toy”: [np.nan, ‘Batmobile’, ‘Bullwhip’],<br>…                    “born”: [pd.NaT, pd.Timestamp(“1940-04-25”),<br>…                             pd.NaT]})</p></blockquote></blockquote></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df</span><br><span class="line">       name        toy       born</span><br><span class="line">0    Alfred        NaN        NaT</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Drop the rows where at least one element is missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna()</span><br><span class="line">     name        toy       born</span><br><span class="line">1  Batman  Batmobile 1940-04-25</span><br></pre></td></tr></table></figure><p>Drop the columns where at least one element is missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(axis=&#x27;columns&#x27;)</span><br><span class="line">       name</span><br><span class="line">0    Alfred</span><br><span class="line">1    Batman</span><br><span class="line">2  Catwoman</span><br></pre></td></tr></table></figure><p>Drop the rows where all elements are missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(how=&#x27;all&#x27;)</span><br><span class="line">       name        toy       born</span><br><span class="line">0    Alfred        NaN        NaT</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Keep only the rows with at least 2 non-NA values.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(thresh=2)</span><br><span class="line">       name        toy       born</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Define in which columns to look for missing values.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(subset=[&#x27;name&#x27;, &#x27;toy&#x27;])</span><br><span class="line">       name        toy       born</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Keep the DataFrame with valid entries in the same variable.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(inplace=True)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">     name        toy       born</span><br><span class="line">1  Batman  Batmobile 1940-04-25</span><br></pre></td></tr></table></figure><h1 id="Fill-missing-values"><a href="#Fill-missing-values" class="headerlink" title="Fill missing values"></a>Fill missing values</h1><p>Filling missing values using fillna(), replace() and interpolate()</p><p>In order to fill null values in a datasets, we use fillna(), replace() and interpolate() function these function replace NaN values with some value of their own. All these function help in filling a null values in datasets of a DataFrame. Interpolate() function is basically used to fill NA values in the dataframe but it uses various interpolation technique to fill the missing values rather than hard-coding the value.</p><p>Code #1: Filling null values with a single value</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling missing value using fillna()  </span><br><span class="line"></span><br><span class="line">df.fillna(0)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Code #2: Filling null values with the previous ones</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling a missing value with</span><br><span class="line"></span><br><span class="line"># previous ones  </span><br><span class="line"></span><br><span class="line">df.fillna(method =&#x27;pad&#x27;)</span><br></pre></td></tr></table></figure><p>Code #3: Filling null value with the next ones</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling  null value using fillna() function  </span><br><span class="line"></span><br><span class="line">df.fillna(method =&#x27;bfill&#x27;) </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> dataframe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>boost exact phrase search results ranking in elasticsearch</title>
      <link href="2021/06/05/2021-06-05-1/"/>
      <url>2021/06/05/2021-06-05-1/</url>
      
        <content type="html"><![CDATA[<p>Elasticsearch use the DSL format to create query.</p><p>One easy search is to use multi_match by passing the query key word, and give the fields to search for.</p><p>Here is an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">from elasticsearch import Elasticsearch</span><br><span class="line">es = Elasticsearch()</span><br><span class="line">indexname = &#x27;myindex&#x27;</span><br><span class="line"></span><br><span class="line">keyword = &#x27;test&#x27;</span><br><span class="line">   dsl=&#123;</span><br><span class="line">       &quot;query&quot;: &#123;</span><br><span class="line">           &quot;multi_match&quot; : &#123;</span><br><span class="line">                   &quot;query&quot;:  keyword,</span><br><span class="line">                   &quot;fields&quot;: [ &quot;content&quot;]</span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">           &#125;</span><br><span class="line"> </span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   dsize=10</span><br><span class="line">   result_r = es.search(index=indexname, body=dsl,size=dsize)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The problem with the above query is that:if your query keyword is a phrase, you might find many results that have the exact match are ranked lower.</p><p>To solve this problem, you might want to try this new dsl format:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dsl=&#123;</span><br><span class="line">   </span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">       &quot;bool&quot;: &#123;</span><br><span class="line">         &quot;must&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">                  &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ]</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         ],</span><br><span class="line">         &quot;should&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">                  &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ],</span><br><span class="line">               &quot;type&quot;: &quot;phrase&quot;,</span><br><span class="line">               &quot;boost&quot;: 10</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;,</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">               &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ],</span><br><span class="line">               &quot;operator&quot;: &quot;and&quot;,</span><br><span class="line">               &quot;boost&quot;: 4</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         ]</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">           </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>one thing to explain in the above the query, is the ‘operator’; According to the official elasticsearch webpage:</p><p>operator and minimum_should_match<br>The best_fields and most_fields types are field-centric — they generate a match query per field. This means that the operator and minimum_should_match parameters are applied to each field individually, which is probably not what you want.</p><p>Take this query for example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;multi_match&quot; : &#123;</span><br><span class="line">      &quot;query&quot;:      &quot;Will Smith&quot;,</span><br><span class="line">      &quot;type&quot;:       &quot;best_fields&quot;,</span><br><span class="line">      &quot;fields&quot;:     [ &quot;first_name&quot;, &quot;last_name&quot; ],</span><br><span class="line">      &quot;operator&quot;:   &quot;and&quot; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This query is executed as:</p><p>  (+first_name:will +first_name:smith)<br>| (+last_name:will  +last_name:smith)</p><p>In other words, all terms must be present in a single field for a document to match.</p>]]></content>
      
      
      <categories>
          
          <category> search </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>how to convert timestamp column of pandas dataframe into hour and day features using transformer</title>
      <link href="2021/06/04/2021-06-04-1/"/>
      <url>2021/06/04/2021-06-04-1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">class dayandhour_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line">       return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # Customized transformer method</span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line">        X[&#x27;dayofweek&#x27;]=pd.to_datetime(X[&#x27;timestamp&#x27;]).dt.dayofweek</span><br><span class="line"></span><br><span class="line">        X[&#x27;hour&#x27;]=pd.to_datetime(X[&#x27;timestamp&#x27;]).dt.hour</span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;timestamp&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">        return X</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">dayandhour_transformer = dayandhour_Transformer() </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">df = dayandhour_transformer.transform(df)</span><br></pre></td></tr></table></figure><p>Suppose we have a dataframe df with a column “timestamp”.</p><p>before apply the code, we have:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(df[&#x27;timestamp&#x27;])</span><br><span class="line"></span><br><span class="line">0        2021-03-28 03:28:10.205000</span><br><span class="line"></span><br><span class="line">1        2021-03-28 21:31:43.290000</span><br><span class="line"></span><br><span class="line">2        2021-03-28 21:16:18.771000</span><br><span class="line"></span><br><span class="line">3        2021-03-28 18:39:13.344000</span><br><span class="line"></span><br><span class="line">4        2021-03-28 00:54:57.544000</span><br></pre></td></tr></table></figure><p>after we apply the code, we have:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print( df[[&#x27;hour&#x27;,&#x27;dayofweek&#x27;]])</span><br><span class="line"></span><br><span class="line">       hour  dayofweek</span><br><span class="line"></span><br><span class="line">0         3          6</span><br><span class="line"></span><br><span class="line">1        21          6</span><br><span class="line"></span><br><span class="line">2        21          6</span><br><span class="line"></span><br><span class="line">3        18          6</span><br><span class="line"></span><br><span class="line">4         0          6</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> preprocessing </tag>
            
            <tag> pandas </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
