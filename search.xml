<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ten important Python libraries that Data Scientist must know</title>
      <link href="2022/03/13/2022-03-12-%201/"/>
      <url>2022/03/13/2022-03-12-%201/</url>
      
        <content type="html"><![CDATA[<p>Good libraries are like useful toolbox, and learning these libraries can make you more productive, whether you’re a novice or a data science expert.</p><p>Below is a basic introduction to some of the most popular Python libraries for data science and machine learning.</p><h2 id="1-Scikit-learn"><a href="#1-Scikit-learn" class="headerlink" title="1. Scikit-learn"></a>1. Scikit-learn</h2><p>This is the most basic and popular Python library for machine learning. In fact, Scikit-learn is the main library for machine learning. It has algorithms and modules for preprocessing, cross-validation, and other similar purposes.</p><p>Some of these algorithms involve regression, decision trees, ensemble modeling, and unsupervised learning algorithms such as clustering.</p><p>Project address: <a href="https://github.com/scikit-learn/scikit-learn">https://github.com/scikit-learn/scikit-learn</a></p><h2 id="2-NumPy"><a href="#2-NumPy" class="headerlink" title="2. NumPy"></a>2. NumPy</h2><p>NumPy is another wonderful Python library for machine learning and heavy computing. NumPy facilitates simple and efficient numerical computation. It has many other libraries built on top of it, such as Pandas.</p><p>You should at least make sure to learn about NumPy arrays, which are fundamental and have many applications in machine learning, data science, and artificial intelligence-based programs.</p><p>Project address: <a href="https://github.com/numpy/numpy">https://github.com/numpy/numpy</a></p><h2 id="3-Pandas"><a href="#3-Pandas" class="headerlink" title="3. Pandas"></a>3. Pandas</h2><p>This is a Python library built on top of NumPy. It is handy in terms of data structures and exploratory analysis. Another important feature it provides is a DataFrame, a two-dimensional data structure with potentially different types of columns.</p><p>Pandas will be one of the most important libraries you will ever need, which is why it is so important to learn Pandas well.</p><p>Project address: <a href="https://github.com/pandas-dev/pandas">https://github.com/pandas-dev/pandas</a></p><h2 id="4-Matplotlib"><a href="#4-Matplotlib" class="headerlink" title="4. Matplotlib"></a>4. Matplotlib</h2><p>If you need to plot, then Matlotlib is an option. It provides a flexible plotting and visualization library, and Matplotlib is powerful. However, it is cumbersome, so, you can choose Seaborn instead.</p><p>Project address: <a href="https://github.com/matplotlib/matplotlib">https://github.com/matplotlib/matplotlib</a></p><h2 id="5-Seaborn"><a href="#5-Seaborn" class="headerlink" title="5. Seaborn"></a>5. Seaborn</h2><p>Like Matplotlib, it’s a great plotting library, but with Seaborn it’s easier than ever to draw common data visualizations.</p><p>It builds on top of Matplotlib and provides a more pleasant high-level wrapper. You should learn effective data visualization.</p><p>Project address: <a href="https://github.com/seaborn">https://github.com/seaborn</a></p><h2 id="6-SciPy"><a href="#6-SciPy" class="headerlink" title="6. SciPy"></a>6. SciPy</h2><p>This is a Python library for scientific and technical computing. It will give you all the tools you need for scientific and technical computing.</p><p>It has modules for optimization, linear algebra, integration, interpolation, special functions, fast Fourier transforms, signal and image processing, independent dependency estimation solvers, and other tasks.</p><p>Project address: <a href="https://github.com/scipy/scipy">https://github.com/scipy/scipy</a></p><h2 id="7-OpenCV"><a href="#7-OpenCV" class="headerlink" title="7. OpenCV"></a>7. OpenCV</h2><p>This is another great library for Python developers in computer vision. In case you didn’t know, computer vision is one of the most exciting fields in machine learning and artificial intelligence.</p><p>It has applications in many industries such as self-driving cars, robotics, augmented reality, etc., and OpenCV is the best computer vision library.</p><p>Although you can use OpenCV in many programming languages ​​like C++, its Python version is beginner friendly and easy to use, which makes it a great library to be included in this list.</p><p>If you want to learn Python and OpenCV for basic image processing, and do image classification and object detection, and need a course, then I highly recommend taking a hands-on course that will teach you an OpenCV through several labs and exercises.</p><p>Project address: <a href="https://github.com/opencv/opencv">https://github.com/opencv/opencv</a></p><h2 id="8-TensorFlow"><a href="#8-TensorFlow" class="headerlink" title="8. TensorFlow"></a>8. TensorFlow</h2><p>This is one of the most popular machine learning libraries, and chances are you’ve already heard of it. You probably know TensorFlow from Google, invented by their Google Brain team, and used in the RankBrain algorithm that powers millions of search questions on Google’s search engine.</p><p>In general, it is a symbolic math library that is also used in machine learning applications such as neural networks. TensorFlow has many applications, and you can find many stories online, such as how a Japanese farmer used TensorFlow to sort cucumbers.</p><p>Project address: <a href="https://github.com/tensorflow/tensorflow">https://github.com/tensorflow/tensorflow</a></p><h2 id="9-PyTorch"><a href="#9-PyTorch" class="headerlink" title="9. PyTorch"></a>9. PyTorch</h2><p>This is another exciting and powerful Python library for data science and machine learning, something every data scientist should learn.</p><p>In case you didn’t know, PyTorch is one of the best deep learning libraries developed by Facebook for deep learning applications such as face recognition self-driving cars and more.</p><p>You can also use PyTorch to build machine learning models like NLP and computer vision, to name a few. You can also use PyTorch to create deep neural networks.</p><p>Project address: <a href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</a></p><h2 id="10-Keras"><a href="#10-Keras" class="headerlink" title="10. Keras"></a>10. Keras</h2><p>One of the main problems with creating machine learning and deep learning-based solutions is that implementing them can be tedious, requiring many lines of complex code. Keras is a library that makes it easier for you to create these deep learning solutions.</p><p>With just a few lines of code, you can create a model that may require hundreds of lines of traditional code.</p><p>Project address: <a href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</a> </p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convert arbitrary date to the date of  Monday or Sundy within the same week in Python</title>
      <link href="2022/02/22/2022-02-22-%201/"/>
      <url>2022/02/22/2022-02-22-%201/</url>
      
        <content type="html"><![CDATA[<p>It is common practice to convert date to the week number for normalization or feature preprocessing. While this is pretty useful, sometimes we want to compare the week number in a more strict fashion, such that we can order week 50 of last year and week 2 of this year, for example.</p><p>Following code shows both<br>(1） how to get week number of datetime object<br>(2) get the exacte date of the Monday or Sunday within the same week</p><h2 id="get-week-number-for-any-date"><a href="#get-week-number-for-any-date" class="headerlink" title="get week number for any date"></a>get week number for any date</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta, datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># example of a date str</span></span><br><span class="line">dt_str = <span class="string">&#x27;2022/02/22 05:23:20&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert string to datetime object</span></span><br><span class="line">dt_obj = datetime.strptime(dt_str, <span class="string">&#x27;%Y/%m/%d %H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Monday </span></span><br><span class="line">year, week_num, day_of_week = dt_obj.isocalendar()</span><br><span class="line"><span class="built_in">print</span>(week_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>8</code></pre><h2 id="get-date-of-Monday-or-Sunday-for-any-date-within-the-same-week"><a href="#get-date-of-Monday-or-Sunday-for-any-date-within-the-same-week" class="headerlink" title="get date of Monday or Sunday for any date within the same week"></a>get date of Monday or Sunday for any date within the same week</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> timedelta, datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the date of Monday for that week</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start_date_of_week</span>(<span class="params">date</span>):</span></span><br><span class="line">    start = date - timedelta(days=date.weekday())</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(start)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># get the date of Sunday for that week</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">end_date_of_week</span>(<span class="params">date</span>):</span></span><br><span class="line">    start = date - timedelta(days=date.weekday())</span><br><span class="line">    end = start + timedelta(days=<span class="number">6</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">str</span>(end)[:<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># example of a date str</span></span><br><span class="line">dt_str = <span class="string">&#x27;2022/02/22 05:23:20&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># convert string to datetime object</span></span><br><span class="line">dt_obj = datetime.strptime(dt_str, <span class="string">&#x27;%Y/%m/%d %H:%M:%S&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Monday </span></span><br><span class="line"><span class="built_in">print</span>(start_date_of_week(dt_obj) )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get date of that Sunday </span></span><br><span class="line"><span class="built_in">print</span>(end_date_of_week(dt_obj) )</span><br></pre></td></tr></table></figure><pre><code>2022-02-212022-02-27</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A transformer example to maintain same feature order and add missing features back for feature engineering</title>
      <link href="2022/01/27/2022-01-27-1/"/>
      <url>2022/01/27/2022-01-27-1/</url>
      
        <content type="html"><![CDATA[<p>For data science projects, one important steps in feature engineering is to make sure the order of feature columns during training<br>and prediction/test time is the same. Otherwise, we will not get the results as we expect.</p><p>This is usually not a problem in train/test split or cross validation stages, where training and test data are generally split form the<br>same dataframe. However, once model is put online, and the transformer need to handel each single event, which usually comes in the<br>format of json data, then transformed to dataframe. During this process, the orignal order may not hold.</p><p>To ensure the same feature order is used, we could build a transformer for the pipeline; During the fit stage, the orignal order will be<br>remembered, and during the transform stage, the same order will be enforced; Meanwhile, if there is any missing column, we will add a null value<br>column. </p><h2 id="set-up-some-example-dataframe"><a href="#set-up-some-example-dataframe" class="headerlink" title="set up some example dataframe"></a>set up some example dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># training example, where we have 3 features</span></span><br><span class="line">df_train = pd.DataFrame(data=[[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;f&#x27;</span>],[<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>]])</span><br><span class="line">df_train.columns =  [<span class="string">&#x27;cat1&#x27;</span>,<span class="string">&#x27;cat2&#x27;</span>,<span class="string">&#x27;cat3&#x27;</span>]</span><br><span class="line">display(df_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># test example where we missing the 3rd feature</span></span><br><span class="line">df_test = pd.DataFrame(data=[[<span class="string">&#x27;h&#x27;</span>,<span class="string">&#x27;j&#x27;</span>]])</span><br><span class="line">df_test.columns =  [<span class="string">&#x27;cat2&#x27;</span>,<span class="string">&#x27;cat1&#x27;</span>]</span><br><span class="line">display(df_test)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>a</td>      <td>b</td>      <td>f</td>    </tr>    <tr>      <th>1</th>      <td>c</td>      <td>d</td>      <td>e</td>    </tr>  </tbody></table></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat2</th>      <th>cat1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>h</td>      <td>j</td>    </tr>  </tbody></table></div><h2 id="a-transformer-which-can-be-added-to-a-full-pipeline"><a href="#a-transformer-which-can-be-added-to-a-full-pipeline" class="headerlink" title="a transformer which can be added to a full pipeline"></a>a transformer which can be added to a full pipeline</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">orderMaitain_Transformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"></span><br><span class="line">     <span class="comment"># Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.dtype_dict = &#123;&#125; </span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;initialized&#x27;</span>)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return self</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line">        </span><br><span class="line">        self.dtype_dict = X.dtypes.apply(<span class="keyword">lambda</span> x: x.name).to_dict()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line">        <span class="comment">#print(self.dtype_dict)</span></span><br><span class="line">        train_columns = []</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># add missing column if any</span></span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> self.dtype_dict:</span><br><span class="line">            train_columns.append(col)</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">not</span> <span class="keyword">in</span> X.columns:</span><br><span class="line">                <span class="comment"># null boolean are treated as False; can also use other strategy as well</span></span><br><span class="line">                <span class="keyword">if</span> self.dtype_dict[col].startswith(<span class="string">&#x27;bool&#x27;</span>):</span><br><span class="line">                    X[col]=<span class="literal">False</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[col] = pd.Series(dtype=self.dtype_dict[col])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># apply same order to both training and test</span></span><br><span class="line">        <span class="built_in">print</span>(train_columns)</span><br><span class="line">        X = X[train_columns]  </span><br><span class="line">        <span class="keyword">return</span> X  </span><br><span class="line">    </span><br><span class="line">orderMaitain_transformer = orderMaitain_Transformer()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>initialized</code></pre><h2 id="apply-transfomer-during-training-and-test-stages"><a href="#apply-transfomer-during-training-and-test-stages" class="headerlink" title="apply transfomer during training and test stages"></a>apply transfomer during training and test stages</h2><h3 id="during-training-stage"><a href="#during-training-stage" class="headerlink" title="during training stage"></a>during training stage</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">orderMaitain_transformer.fit_transform(df_train)</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat1&#39;, &#39;cat2&#39;, &#39;cat3&#39;]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>a</td>      <td>b</td>      <td>f</td>    </tr>    <tr>      <th>1</th>      <td>c</td>      <td>d</td>      <td>e</td>    </tr>  </tbody></table></div><h3 id="during-test-and-prediction-stage"><a href="#during-test-and-prediction-stage" class="headerlink" title="during test and prediction stage"></a>during test and prediction stage</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># check that the resuls have an emppty column added, the order is the same as training</span></span><br><span class="line">orderMaitain_transformer.transform(df_test)</span><br></pre></td></tr></table></figure><pre><code>[&#39;cat1&#39;, &#39;cat2&#39;, &#39;cat3&#39;]</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>cat1</th>      <th>cat2</th>      <th>cat3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>j</td>      <td>h</td>      <td>NaN</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pipeline </tag>
            
            <tag> transformer </tag>
            
            <tag> scikit-learn </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Convert short texts to numeric vectors with Character ngram tf-idf vectorizer using scikit learn in Python</title>
      <link href="2022/01/15/2022-01-15-1/"/>
      <url>2022/01/15/2022-01-15-1/</url>
      
        <content type="html"><![CDATA[<p>When apply machine learning algorithms, to handel words or short texts, we usually need to get their numeric embedding vecotors first.<br>Some powerfulmethods including using pre-trained deep learning model such as BERT to more semantic embedding. If computation resource is a limit, or we want to have simpler embedding methods, we could try TF-IDF metrics. </p><p>Here we introduce a very simple way to combine character level n-gram methods and TF-IDF to convert short texts such as a few words to numeric vectors.  Within numeric vectors, we could further apply classification methods such as Gradient Boosted Machine for downstream tasks.</p><p>First, let’s review what’s n-gram:</p><p>Quote some definition from the Wikipedia N-Gram page:</p><p>…an n-gram is a contiguous sequence of n items from a given sequence of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application… An n-gram of size 1 is referred to as a “unigram”; size 2 is a “bigram”; size 3 is a “trigram”. Larger sizes are sometimes referred to by the value of n, e.g., “four-gram”, “five-gram”, and so on.</p><p>The two most common types of N-Grams, by far, are (1) character-level, where the items consist of one or more characters and (2) word-level, where the items consist of one or more words. The size of the item (or token as it’s often called) is defined by n;</p><p>Second, what’s TF-IDF metrics?</p><p>TF-IDF (Term Frequency - Inverse Document Frequency) encoding is an improved way of BOW (bag of words) which is the same as TF. It considers the frequently seen term in various documents to be less of importance.</p><p>TF (Term Frequency): Counts how many term exists in a document<br>IDF (Inverse Document Frequency): Inverse of the number of documents which contains the term</p><p>So TF-IDF is the basically product of TF and IDF metrics.</p><p>It turns out to be very simple to implement character level n-gram TF-IDF encoding of short texts by using scikit-learn package.<br>This means we can easily incorporate the process step in our data process and feature engineering pipeline</p><h2 id="Step-1-Fit-character-n-gram-tf-idf-vectorizer-with-training-data"><a href="#Step-1-Fit-character-n-gram-tf-idf-vectorizer-with-training-data" class="headerlink" title="Step 1 , Fit character n-gram tf idf vectorizer with training data"></a>Step 1 , Fit character n-gram tf idf vectorizer with training data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># using gram of length at 2 for example</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">corpus = [<span class="string">&#x27;great people&#x27;</span>, <span class="string">&#x27;feel happy&#x27;</span>, <span class="string">&#x27;nice work&#x27;</span>,<span class="string">&#x27;warm weather&#x27;</span>]</span><br><span class="line">vectorizer = TfidfVectorizer(analyzer=<span class="string">&#x27;char&#x27;</span>, ngram_range=(<span class="number">2</span>,<span class="number">2</span>)).fit(corpus)</span><br><span class="line"><span class="built_in">print</span>(vectorizer.get_feature_names())</span><br></pre></td></tr></table></figure><pre><code>[&#39; h&#39;, &#39; p&#39;, &#39; w&#39;, &#39;ap&#39;, &#39;ar&#39;, &#39;at&#39;, &#39;ce&#39;, &#39;e &#39;, &#39;ea&#39;, &#39;ee&#39;, &#39;el&#39;, &#39;eo&#39;, &#39;er&#39;, &#39;fe&#39;, &#39;gr&#39;, &#39;ha&#39;, &#39;he&#39;, &#39;ic&#39;, &#39;l &#39;, &#39;le&#39;, &#39;m &#39;, &#39;ni&#39;, &#39;op&#39;, &#39;or&#39;, &#39;pe&#39;, &#39;pl&#39;, &#39;pp&#39;, &#39;py&#39;, &#39;re&#39;, &#39;rk&#39;, &#39;rm&#39;, &#39;t &#39;, &#39;th&#39;, &#39;wa&#39;, &#39;we&#39;, &#39;wo&#39;]</code></pre><h2 id="Step-2-Transform-new-text-to-tf-idf-weighted-vector"><a href="#Step-2-Transform-new-text-to-tf-idf-weighted-vector" class="headerlink" title="Step 2, Transform new text to tf-idf weighted vector"></a>Step 2, Transform new text to tf-idf weighted vector</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">new_text = [<span class="string">&#x27;pineapple milk&#x27;</span>]</span><br><span class="line">tfidf_vector = vectorizer.transform(new_text).toarray()</span><br><span class="line"><span class="built_in">print</span>(tfidf_vector)</span><br></pre></td></tr></table></figure><pre><code>[[0.         0.         0.         0.42176478 0.         0.  0.         0.42176478 0.3325242  0.         0.         0.  0.         0.         0.         0.         0.         0.  0.         0.42176478 0.         0.         0.         0.  0.         0.42176478 0.42176478 0.         0.         0.  0.         0.         0.         0.         0.         0.        ]]</code></pre>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> text mining </tag>
            
            <tag> natural language processing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Use Python to solve Linear Programming optimization problem</title>
      <link href="2021/12/29/2021-12-29-1/"/>
      <url>2021/12/29/2021-12-29-1/</url>
      
        <content type="html"><![CDATA[<p>Let’s first define the standard format of linear programming problem,<br>in which we will minimize the following equation<br>$$<br>    z=CX<br>$$<br>where， C is the vector of coefficients，X is the vector of variables to be optimized.<br>The constraints of this minimization can be written as:<br>$$<br>AX&lt;=B<br>$$<br>where，A and B are both coefficient matrix。</p><p>Let’s see an specific example：<br>$$<br>min , z=10x_1 +15x_2+25x_3<br>\s.t<br>\-1x_1-1x_2-1x_3&lt;=-1000<br>\-1x_1+2x_2-0x_3&lt;=0<br>\0x_1+0x_2-1x_3&lt;=-300<br>\-1x_1&lt;=0<br>\-1x_2&lt;=0<br>\-1x_3&lt;=0<br>$$</p><p>In Python, we could call the linprog function in Scipy package to solove linear programming problem,<br>and here is a simple demo of coding in Python: </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import required libraries</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linprog</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the inequality constraints matrix</span></span><br><span class="line"><span class="comment"># Note: the inequality constraints must be in the form of &lt;=</span></span><br><span class="line">A = np.array([[-<span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>], [-<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, -<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the inequality constraints vector</span></span><br><span class="line">b = np.array([-<span class="number">1000</span>, <span class="number">0</span>, -<span class="number">300</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the coefficients of the linear objective function vector</span></span><br><span class="line">c = np.array([<span class="number">10</span>, <span class="number">15</span>, <span class="number">25</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Solve linear programming problem</span></span><br><span class="line">res = linprog(c, A_ub=A, b_ub=b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print results</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Optimal value:&#x27;</span>, <span class="built_in">round</span>(res.fun, ndigits=<span class="number">2</span>),</span><br><span class="line">      <span class="string">&#x27;\nx values:&#x27;</span>, res.x,</span><br><span class="line">      <span class="string">&#x27;\nNumber of iterations performed:&#x27;</span>, res.nit,</span><br><span class="line">      <span class="string">&#x27;\nStatus:&#x27;</span>, res.message)</span><br></pre></td></tr></table></figure><pre><code>Optimal value: 14500.0 x values: [7.0000000e+02 7.1017063e-09 3.0000000e+02] Number of iterations performed: 7 Status: Optimization terminated successfully.</code></pre>]]></content>
      
      
      <categories>
          
          <category> optimization </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linear programming </tag>
            
            <tag> optimization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Multi-class classification using simple deep learning structure in Pytorch with iris dataset</title>
      <link href="2021/12/26/2021-12-26-1/"/>
      <url>2021/12/26/2021-12-26-1/</url>
      
        <content type="html"><![CDATA[<p>As one can see from the following example, it is very easy to apply deep learning structure in Pytorch<br>to perform multi-class classificatoin task. The valuation performance of this iris dataset is almost perfect.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt </span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">1121</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;torch._C.Generator at 0x19734b24a70&gt;</code></pre><h2 id="load-iris-dataset-for-training-and-valuation"><a href="#load-iris-dataset-for-training-and-valuation" class="headerlink" title="load iris dataset for training and valuation"></a>load iris dataset for training and valuation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">iris = load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">Y = iris.target</span><br><span class="line">X=pd.DataFrame(X)</span><br><span class="line">Y=pd.DataFrame(Y)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display(X)</span><br><span class="line">display(Y)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>      <th>1</th>      <th>2</th>      <th>3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.1</td>      <td>3.5</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>1</th>      <td>4.9</td>      <td>3.0</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>2</th>      <td>4.7</td>      <td>3.2</td>      <td>1.3</td>      <td>0.2</td>    </tr>    <tr>      <th>3</th>      <td>4.6</td>      <td>3.1</td>      <td>1.5</td>      <td>0.2</td>    </tr>    <tr>      <th>4</th>      <td>5.0</td>      <td>3.6</td>      <td>1.4</td>      <td>0.2</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>145</th>      <td>6.7</td>      <td>3.0</td>      <td>5.2</td>      <td>2.3</td>    </tr>    <tr>      <th>146</th>      <td>6.3</td>      <td>2.5</td>      <td>5.0</td>      <td>1.9</td>    </tr>    <tr>      <th>147</th>      <td>6.5</td>      <td>3.0</td>      <td>5.2</td>      <td>2.0</td>    </tr>    <tr>      <th>148</th>      <td>6.2</td>      <td>3.4</td>      <td>5.4</td>      <td>2.3</td>    </tr>    <tr>      <th>149</th>      <td>5.9</td>      <td>3.0</td>      <td>5.1</td>      <td>1.8</td>    </tr>  </tbody></table><p>150 rows × 4 columns</p></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>0</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>    </tr>    <tr>      <th>1</th>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>0</td>    </tr>    <tr>      <th>...</th>      <td>...</td>    </tr>    <tr>      <th>145</th>      <td>2</td>    </tr>    <tr>      <th>146</th>      <td>2</td>    </tr>    <tr>      <th>147</th>      <td>2</td>    </tr>    <tr>      <th>148</th>      <td>2</td>    </tr>    <tr>      <th>149</th>      <td>2</td>    </tr>  </tbody></table><p>150 rows × 1 columns</p></div><h2 id="train-and-valuation-dataset-split"><a href="#train-and-valuation-dataset-split" class="headerlink" title="train and valuation dataset split"></a>train and valuation dataset split</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X=X.values</span><br><span class="line">Y=Y.values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">x, x_val, y, y_val = train_test_split(X, Y, test_size=<span class="number">0.33</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x.shape, y.shape, x_val.shape, y_val.shape</span><br></pre></td></tr></table></figure><pre><code>((100, 4), (100, 1), (50, 4), (50, 1))</code></pre><h3 id="notice-that-y-needs-to-be-one-dimension-indicating-the-class-type-not-encoded-vectors"><a href="#notice-that-y-needs-to-be-one-dimension-indicating-the-class-type-not-encoded-vectors" class="headerlink" title="notice that: y needs to be one dimension indicating the class type, not encoded vectors"></a>notice that: y needs to be one dimension indicating the class type, not encoded vectors</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x_train = x.reshape(-<span class="number">1</span>, x.shape[<span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_train = y.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">x_val = x_val.reshape(-<span class="number">1</span>, x_val.shape[<span class="number">1</span>]).astype(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">y_val = y_val.reshape(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="put-data-into-dataloader-so-we-could-train-by-batches-easily"><a href="#put-data-into-dataloader-so-we-could-train-by-batches-easily" class="headerlink" title="put data into dataloader, so we could train by batches easily"></a>put data into dataloader, so we could train by batches easily</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Data</span>(<span class="params">Dataset</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.x=torch.from_numpy(x_train)</span><br><span class="line">        self.y=torch.from_numpy(y_train).long()</span><br><span class="line">        self.<span class="built_in">len</span>=self.x.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span>(<span class="params">self,index</span>):</span>      </span><br><span class="line">        <span class="keyword">return</span> self.x[index], self.y[index]</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br><span class="line">data_set=Data()</span><br><span class="line"></span><br><span class="line"><span class="comment"># define batch sizes here </span></span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line">trainloader=DataLoader(dataset=data_set,batch_size=batch_size)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="build-softmax-classifier"><a href="#build-softmax-classifier" class="headerlink" title="build softmax classifier"></a>build softmax classifier</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># D_in, dimension of input layer</span></span><br><span class="line"><span class="comment"># H , dimension of hidden layer</span></span><br><span class="line"><span class="comment"># D_out, dimension of output layer</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,D_in,H,D_out</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">        self.linear1=nn.Linear(D_in,H)</span><br><span class="line">        self.linear2=nn.Linear(H,D_out)</span><br><span class="line"></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        x=torch.sigmoid(self.linear1(x))  </span><br><span class="line">        x=self.linear2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">    </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">input_dim=<span class="number">4</span>     <span class="comment"># how many features are in the dataset or how many input nodes in the input layer</span></span><br><span class="line">hidden_dim = <span class="number">20</span> <span class="comment"># hidden layer size</span></span><br><span class="line">output_dim=<span class="number">3</span>    <span class="comment"># number of classes</span></span><br><span class="line"><span class="built_in">print</span>(input_dim,hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Instantiate model</span></span><br><span class="line">model=Net(input_dim,hidden_dim,output_dim)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;W:&#x27;</span>,<span class="built_in">list</span>(model.parameters())[<span class="number">0</span>].size())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b&#x27;</span>,<span class="built_in">list</span>(model.parameters())[<span class="number">1</span>].size())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># loss function</span></span><br><span class="line">criterion=nn.CrossEntropyLoss()</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>4 20 3W: torch.Size([20, 4])b torch.Size([20])</code></pre><h2 id="define-the-optimizer"><a href="#define-the-optimizer" class="headerlink" title="define the optimizer"></a>define the optimizer</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">learning_rate=<span class="number">0.05</span></span><br><span class="line">optimizer=torch.optim.SGD(model.parameters(), lr=learning_rate)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">n_epochs=<span class="number">1000</span></span><br><span class="line">loss_list=[]</span><br><span class="line"></span><br><span class="line"><span class="comment">#n_epochs</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">if</span> epoch %<span class="number">100</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(epoch)</span><br><span class="line">    <span class="keyword">for</span> x, y <span class="keyword">in</span> trainloader:</span><br><span class="line">      </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#clear gradient </span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment">#make a prediction </span></span><br><span class="line">        z=model(x)</span><br><span class="line">        <span class="comment">#print(z)</span></span><br><span class="line">        <span class="comment"># calculate loss, da Cross Entropy benutzt wird muss ich in den loss Klassen vorhersagen, </span></span><br><span class="line">        <span class="comment"># also Wahrscheinlichkeit pro Klasse. Das mach torch.max(y,1)[1])</span></span><br><span class="line">        loss=criterion(z,y)</span><br><span class="line">        <span class="comment"># calculate gradients of parameters </span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># update parameters </span></span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        loss_list.append(loss.data)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(&#x27;epoch &#123;&#125;, loss &#123;&#125;&#x27;.format(epoch, loss.item()))</span></span><br></pre></td></tr></table></figure><pre><code>0100200300400500600700800900</code></pre><h2 id="check-performance-using-the-valuation-data"><a href="#check-performance-using-the-valuation-data" class="headerlink" title="check performance using the valuation data"></a>check performance using the valuation data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># predict the class and give probablity for each class label</span></span><br><span class="line">x_val = torch.from_numpy(x_val)</span><br><span class="line">z=model(x_val)</span><br><span class="line">display(z)</span><br></pre></td></tr></table></figure><pre><code>tensor([[-3.1493e+00,  3.6905e+00, -5.0090e-01],        [ 8.6639e+00,  2.5913e+00, -1.1538e+01],        [-7.7513e+00,  4.1365e-01,  7.2997e+00],        [-3.1786e+00,  3.3443e+00, -1.6125e-01],        [-3.0974e+00,  3.8265e+00, -6.9203e-01],        [ 8.4413e+00,  2.6384e+00, -1.1355e+01],        [-1.4718e-02,  4.3311e+00, -4.3462e+00],        [-5.4910e+00,  1.5831e+00,  3.8781e+00],        [-4.7908e+00,  2.2980e+00,  2.5141e+00],        [-1.2132e+00,  4.3453e+00, -3.1134e+00],        [-5.0060e+00,  1.9947e+00,  2.9958e+00],        [ 8.1200e+00,  2.7792e+00, -1.1151e+01],        [ 8.8828e+00,  2.5214e+00, -1.1683e+01],        [ 8.0960e+00,  2.7942e+00, -1.1141e+01],        [ 8.8283e+00,  2.5107e+00, -1.1654e+01],        [-2.5941e+00,  3.7417e+00, -1.1520e+00],        [-7.0112e+00,  6.8188e-01,  6.2887e+00],        [-1.7457e+00,  4.1789e+00, -2.4035e+00],        [-3.3078e+00,  3.3204e+00,  6.4886e-03],        [-7.0187e+00,  6.5605e-01,  6.3214e+00],        [ 7.9682e+00,  2.8094e+00, -1.1046e+01],        [-4.8998e+00,  2.0493e+00,  2.8430e+00],        [ 8.2123e+00,  2.6980e+00, -1.1198e+01],        [-6.9197e+00,  7.3409e-01,  6.1537e+00],        [-5.4359e+00,  2.1912e+00,  3.2544e+00],        [-6.1038e+00,  1.1452e+00,  4.9261e+00],        [-6.9835e+00,  8.1716e-01,  6.1667e+00],        [-6.8937e+00,  7.5489e-01,  6.1001e+00],        [ 8.0317e+00,  2.7680e+00, -1.1061e+01],        [ 7.8392e+00,  2.8607e+00, -1.0952e+01],        [ 9.1619e+00,  2.3973e+00, -1.1890e+01],        [ 9.2721e+00,  2.3691e+00, -1.1973e+01],        [-1.2764e+00,  4.5603e+00, -3.2536e+00],        [ 8.2595e+00,  2.7126e+00, -1.1257e+01],        [ 8.4267e+00,  2.6317e+00, -1.1359e+01],        [-6.1753e+00,  1.1836e+00,  4.9843e+00],        [-1.8449e+00,  4.1870e+00, -2.3373e+00],        [ 8.5922e+00,  2.6133e+00, -1.1488e+01],        [ 8.7905e+00,  2.5362e+00, -1.1630e+01],        [ 9.1236e+00,  2.4451e+00, -1.1888e+01],        [-6.4186e+00,  9.5598e-01,  5.4381e+00],        [-1.9872e+00,  3.8494e+00, -1.8866e+00],        [-2.3989e+00,  4.0929e+00, -1.6734e+00],        [ 9.1132e+00,  2.4113e+00, -1.1844e+01],        [ 8.8116e+00,  2.5441e+00, -1.1646e+01],        [-1.2458e+00,  4.3741e+00, -3.0913e+00],        [-4.9320e+00,  2.3103e+00,  2.6452e+00],        [-5.8854e+00,  1.5035e+00,  4.3829e+00],        [-1.6208e+00,  4.4212e+00, -2.7724e+00],        [-6.7249e+00,  8.5383e-01,  5.8288e+00]], grad_fn=&lt;AddmmBackward&gt;)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># choose the predicted class to be the one with maximum probablity</span></span><br><span class="line">yhat=torch.<span class="built_in">max</span>(z.data,<span class="number">1</span>)</span><br><span class="line">display(yhat)</span><br></pre></td></tr></table></figure><pre><code>torch.return_types.max(values=tensor([3.6905, 8.6639, 7.2997, 3.3443, 3.8265, 8.4413, 4.3311, 3.8781, 2.5141,        4.3453, 2.9958, 8.1200, 8.8828, 8.0960, 8.8283, 3.7417, 6.2887, 4.1789,        3.3204, 6.3214, 7.9682, 2.8430, 8.2123, 6.1537, 3.2544, 4.9261, 6.1667,        6.1001, 8.0317, 7.8392, 9.1619, 9.2721, 4.5603, 8.2595, 8.4267, 4.9843,        4.1870, 8.5922, 8.7905, 9.1236, 5.4381, 3.8494, 4.0929, 9.1132, 8.8116,        4.3741, 2.6452, 4.3829, 4.4212, 5.8288]),indices=tensor([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2,        2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 2, 2,        1, 2]))</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the indicies</span></span><br><span class="line">y_pred=yhat.indices.detach().numpy()</span><br><span class="line">display(y_pred)</span><br></pre></td></tr></table></figure><pre><code>array([1, 0, 2, 1, 1, 0, 1, 2, 2, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,       0, 1, 2, 2, 1, 2], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">display(y_val)</span><br></pre></td></tr></table></figure><pre><code>array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 1, 1, 0,       0, 1, 2, 2, 1, 2])</code></pre><h2 id="check-the-multi-class-confusion-matric"><a href="#check-the-multi-class-confusion-matric" class="headerlink" title="check the multi-class confusion matric"></a>check the multi-class confusion matric</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> multilabel_confusion_matrix</span><br><span class="line">display(multilabel_confusion_matrix(y_val, y_pred))</span><br></pre></td></tr></table></figure><pre><code>array([[[31,  0],        [ 0, 19]],       [[35,  0],        [ 1, 14]],       [[33,  1],        [ 0, 16]]], dtype=int64)</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> machine learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> multi-class </tag>
            
            <tag> classification </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to auto run python jobs using crontab</title>
      <link href="2021/10/24/2021-10-24-1/"/>
      <url>2021/10/24/2021-10-24-1/</url>
      
        <content type="html"><![CDATA[<p>In ubuntu Linux，many times we need to run certain tasks regularly, such as every 10 minutes, at 2 o’clock in the morning every day, etc.<br>The tasks can be varied, but as data scientists, many of our tasks are based on python code. </p><h2 id="A-simple-python-job"><a href="#A-simple-python-job" class="headerlink" title="A simple python job"></a>A simple python job</h2><p>Suppose we have a very simple python text that needs to be run at 2 AM every day. The python text is named test.py, and the code is as follows: </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">a=3</span><br><span class="line">b=5</span><br><span class="line">print(a+b)</span><br></pre></td></tr></table></figure><p>we could write a bash script，run.sh to run the above python job：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">python test.py &gt;&gt; log.txt</span><br></pre></td></tr></table></figure><p>we need to make the run.sh to be excecutable:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">chmod +x run.sh</span><br></pre></td></tr></table></figure><p>We then can run the script，implement the python code，got the answer of a+b and pipeline it to the file log.txt:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">./run.sh</span><br></pre></td></tr></table></figure><p>Our job is now to figure out how to run.sh script file automatically。</p><h2 id="Install-crontab"><a href="#Install-crontab" class="headerlink" title="Install crontab"></a>Install crontab</h2><p>If your ubuntu doesn’t have contab</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">install：apt-get install cron</span><br><span class="line">start：service cron start</span><br><span class="line">restart：service cron restart</span><br><span class="line">stop：service cron stop</span><br><span class="line">check status：service cron status</span><br><span class="line">check scheduled jobs：crontab -l</span><br></pre></td></tr></table></figure><h2 id="add-crontab-job"><a href="#add-crontab-job" class="headerlink" title="add crontab job"></a>add crontab job</h2><p>the command is：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crontab -e</span><br></pre></td></tr></table></figure><p>follow the hint and choose the edit environment you like, such as nano or vi.<br>If we want to run the job 2 am every day, the logic is like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0 2 * * * /home/user_name/run.sh</span><br></pre></td></tr></table></figure><p>If we want to run the job every 5 minutes, then rule is this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">*/5 * * * * /home/user_name/run.sh</span><br></pre></td></tr></table></figure><p>After save the above work, we have successfully scheduled the jobs.<br>Using the following command,</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">crontab -l</span><br></pre></td></tr></table></figure><p>we could confirm all the scheduled jobs including the one we just put in.</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> crontab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to serve the Flask app with nginx as the reverse proxy</title>
      <link href="2021/10/01/2021-10-01-1/"/>
      <url>2021/10/01/2021-10-01-1/</url>
      
        <content type="html"><![CDATA[<p>We are going to build a simple and smooth process using the following steps:</p><ol><li><p> if you don’t have uwsgi installed, try this on ubuntu:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo pip3 install uwsgi</span><br></pre></td></tr></table></figure></li><li><p>locate your flask project folder, if you don’t know how to create a a flask app, check this link:<br><a href="https://datasciencebyexample.com/2021/09/29/2021-09-29-1/">flask app creation</a><br><a href="https://datasciencebyexample.com/2021/09/29/2021-09-29-1/">https://datasciencebyexample.com/2021/09/29/2021-09-29-1/</a></p></li><li><p>create a uswgi init file inside your flask project, for example, you can call it myproject.ini</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[uwsgi]</span><br><span class="line">module = main:app</span><br><span class="line"></span><br><span class="line">master = true</span><br><span class="line">processes = 2</span><br><span class="line"></span><br><span class="line">socket = myproject.sock</span><br><span class="line">chmod-socket = 660</span><br><span class="line">vacuum = true</span><br><span class="line"></span><br><span class="line">die-on-term = true</span><br></pre></td></tr></table></figure></li><li><p>Let’s then create the systemd service unit file. Creating a systemd unit file will allow Ubuntu’s init system to automatically start uWSGI and serve the Flask application whenever the server boots.</p></li></ol><p>Create a unit file ending in .service within the /etc/systemd/system directory to begin, for example</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo emacs /etc/systemd/system/myproject.service</span><br></pre></td></tr></table></figure><p>inside the file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[Unit]</span><br><span class="line">Description=uWSGI instance to serve myproject</span><br><span class="line">After=network.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">User=&lt;your user name&gt;</span><br><span class="line">Group=www-data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WorkingDirectory=&lt;your flask app directory&gt;</span><br><span class="line"></span><br><span class="line">ExecStart=/usr/local/bin/uwsgi --ini myproject.ini</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Then type these in the command line:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl start myproject</span><br><span class="line">sudo systemctl enable myproject</span><br></pre></td></tr></table></figure><p>you can check the service status</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl status myproject</span><br></pre></td></tr></table></figure><p>if any changes to the project, just do this to reflect new web app:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo service myproject restart</span><br></pre></td></tr></table></figure><ol start="5"><li>install the nginx if you don’t have one<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line">sudo apt install nginx</span><br></pre></td></tr></table></figure>At the end of the installation process, Ubuntu (18.04) will start Nginx. The web server should already be up and running.</li></ol><p>We can check with the systemd init system to make sure the service is running by typing:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">systemctl status nginx</span><br></pre></td></tr></table></figure><ol start="6"><li><p>Begin by creating a new server block configuration file in Nginx’s sites-available directory. Let’s call this myproject to keep in line with the rest of the guide:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo emacs /etc/nginx/sites-available/myproject</span><br></pre></td></tr></table></figure><p>inside the file:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 80;</span><br><span class="line">    server_name englishlearningbyexample.com www.englishlearningbyexample.com;</span><br><span class="line"></span><br><span class="line">    location / &#123;</span><br><span class="line">        include uwsgi_params;</span><br><span class="line">        uwsgi_pass unix:&lt;your flask app dir&gt;/myproject.sock;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>above I was using<br><a href="http://www.englishlearningbyexample.com/">www.englishlearningbyexample.com</a><br>as the example for server_name, but you should replace with yours.</p></li><li><p>final steps</p></li></ol><p>To enable the Nginx server block configuration you’ve just created, link the file to the sites-enabled directory:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo ln -s /etc/nginx/sites-available/myproject /etc/nginx/sites-enabled</span><br></pre></td></tr></table></figure><p>then do </p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo systemctl restart nginx</span><br></pre></td></tr></table></figure><p>now you should see the website running</p><p>If you encounter any errors, trying checking the following:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo less /var/log/nginx/error.log: checks the Nginx error logs.</span><br><span class="line">sudo less /var/log/nginx/access.log: checks the Nginx access logs.</span><br><span class="line">sudo journalctl -u nginx: checks the Nginx process logs.</span><br><span class="line">sudo journalctl -u myproject: checks your Flask app’s uWSGI logs.</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask app </tag>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Set up Flask app to implement GET and POST API</title>
      <link href="2021/09/29/2021-09-29-1/"/>
      <url>2021/09/29/2021-09-29-1/</url>
      
        <content type="html"><![CDATA[<p>Flask app is one of the easiest ways to setup APIs on the server side for data engineering and data science purpose.<br>Here we show an simple set up process, an example python file, and how to run it in an development environment.</p><h2 id="packages-to-install"><a href="#packages-to-install" class="headerlink" title="packages to install"></a>packages to install</h2><p>assume we are using python3 environment</p><ol><li>install flask</li></ol><p>sudo apt install python3-flask</p><ol start="2"><li>to allow Cross-Origin Resource Sharing</li></ol><p>pip3 install flask_cors</p><h2 id="A-simple-example-python-file-main-py-to-setup-flask-GET-and-POST-APIS"><a href="#A-simple-example-python-file-main-py-to-setup-flask-GET-and-POST-APIS" class="headerlink" title="A simple example python file (main.py) to setup flask GET and POST APIS"></a>A simple example python file (main.py) to setup flask GET and POST APIS</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask,request</span><br><span class="line"><span class="keyword">from</span> flask_cors <span class="keyword">import</span> CORS</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line">CORS(app)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/&quot;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">helloWorld</span>():</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Hello, cross-origin-world!&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># GET method</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/get&quot;</span>, methods=[<span class="string">&#x27;GET&#x27;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getexample</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># GET data</span></span><br><span class="line">    query = request.args.get(<span class="string">&quot;query&quot;</span>, <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(query)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> query</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#post method</span></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&quot;/post&quot;</span>, methods=[<span class="string">&#x27;POST&#x27;</span>]</span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postexample</span>():</span></span><br><span class="line">    <span class="comment">##  Get POST input form data</span></span><br><span class="line">    <span class="comment">#data = dict((key, request.form.get(key)) for key in request.form.keys())</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#  Get POST input json data</span></span><br><span class="line">    data = <span class="built_in">dict</span>((key, request.json.get(key)) <span class="keyword">for</span> key <span class="keyword">in</span> request.json.keys())</span><br><span class="line">    <span class="built_in">print</span>(data)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;post request with &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(data))</span><br><span class="line"></span><br><span class="line">    output=&#123;<span class="string">&quot;status&quot;</span>:<span class="string">&quot;ok&quot;</span>&#125;</span><br><span class="line">    <span class="keyword">return</span> json.dumps(output)</span><br></pre></td></tr></table></figure><h2 id="to-run-the-flask-API-in-the-command-line-to-run-the-following-command"><a href="#to-run-the-flask-API-in-the-command-line-to-run-the-following-command" class="headerlink" title="to run the flask API: in the command line to run the following command"></a>to run the flask API: in the command line to run the following command</h2><p>sudo FLASK_APP=main.py flask run –host=0.0.0.0 –port 3000</p>]]></content>
      
      
      <categories>
          
          <category> data engineering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flask app </tag>
            
            <tag> API </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Data Prep and Visualization Example using matplotlib in Python</title>
      <link href="2021/08/25/2021-08-25-1/"/>
      <url>2021/08/25/2021-08-25-1/</url>
      
        <content type="html"><![CDATA[<p>Some simple ways to make histogram or line plots using matplotlib.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Install the pydataset package. This package gives us data sets to work with very easily</span></span><br><span class="line">! pip install pydataset</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The convention for importing matplotlib with an alias is &quot;plt&quot;. We&#x27;ll also need pandas and numpy</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><h2 id="The-Air-Passengers-Dataset"><a href="#The-Air-Passengers-Dataset" class="headerlink" title="The Air Passengers Dataset"></a>The Air Passengers Dataset</h2><p>This dataset shows the number of passengers flying United States airlines by month from 1949-1960. </p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pydataset <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">passengers = data(<span class="string">&#x27;AirPassengers&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">passengers.head(<span class="number">12</span>)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>AirPassengers</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>1949.000000</td>      <td>112</td>    </tr>    <tr>      <th>2</th>      <td>1949.083333</td>      <td>118</td>    </tr>    <tr>      <th>3</th>      <td>1949.166667</td>      <td>132</td>    </tr>    <tr>      <th>4</th>      <td>1949.250000</td>      <td>129</td>    </tr>    <tr>      <th>5</th>      <td>1949.333333</td>      <td>121</td>    </tr>    <tr>      <th>6</th>      <td>1949.416667</td>      <td>135</td>    </tr>    <tr>      <th>7</th>      <td>1949.500000</td>      <td>148</td>    </tr>    <tr>      <th>8</th>      <td>1949.583333</td>      <td>148</td>    </tr>    <tr>      <th>9</th>      <td>1949.666667</td>      <td>136</td>    </tr>    <tr>      <th>10</th>      <td>1949.750000</td>      <td>119</td>    </tr>    <tr>      <th>11</th>      <td>1949.833333</td>      <td>104</td>    </tr>    <tr>      <th>12</th>      <td>1949.916667</td>      <td>118</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="1-Add-a-‘year’-column-to-passengers-that-reflects-the-current-year"><a href="#1-Add-a-‘year’-column-to-passengers-that-reflects-the-current-year" class="headerlink" title="#1 Add a ‘year’ column to passengers that reflects the current year"></a>#1 Add a ‘year’ column to passengers that reflects the current year</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">passengers[<span class="string">&#x27;Year&#x27;</span>] = passengers[<span class="string">&#x27;time&#x27;</span>].apply(<span class="keyword">lambda</span> x: <span class="built_in">int</span>(x))</span><br><span class="line">passengers</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>time</th>      <th>AirPassengers</th>      <th>year</th>      <th>month</th>      <th>Year</th>    </tr>  </thead>  <tbody>    <tr>      <th>1</th>      <td>1949.000000</td>      <td>112</td>      <td>1949</td>      <td>1.0</td>      <td>1949</td>    </tr>    <tr>      <th>2</th>      <td>1949.083333</td>      <td>118</td>      <td>1949</td>      <td>2.0</td>      <td>1949</td>    </tr>    <tr>      <th>3</th>      <td>1949.166667</td>      <td>132</td>      <td>1949</td>      <td>3.0</td>      <td>1949</td>    </tr>    <tr>      <th>4</th>      <td>1949.250000</td>      <td>129</td>      <td>1949</td>      <td>4.0</td>      <td>1949</td>    </tr>    <tr>      <th>5</th>      <td>1949.333333</td>      <td>121</td>      <td>1949</td>      <td>5.0</td>      <td>1949</td>    </tr>    <tr>      <th>6</th>      <td>1949.416667</td>      <td>135</td>      <td>1949</td>      <td>6.0</td>      <td>1949</td>    </tr>    <tr>      <th>7</th>      <td>1949.500000</td>      <td>148</td>      <td>1949</td>      <td>7.0</td>      <td>1949</td>    </tr>    <tr>      <th>8</th>      <td>1949.583333</td>      <td>148</td>      <td>1949</td>      <td>8.0</td>      <td>1949</td>    </tr>    <tr>      <th>9</th>      <td>1949.666667</td>      <td>136</td>      <td>1949</td>      <td>9.0</td>      <td>1949</td>    </tr>    <tr>      <th>10</th>      <td>1949.750000</td>      <td>119</td>      <td>1949</td>      <td>10.0</td>      <td>1949</td>    </tr>    <tr>      <th>11</th>      <td>1949.833333</td>      <td>104</td>      <td>1949</td>      <td>11.0</td>      <td>1949</td>    </tr>    <tr>      <th>12</th>      <td>1949.916667</td>      <td>118</td>      <td>1949</td>      <td>12.0</td>      <td>1949</td>    </tr>    <tr>      <th>13</th>      <td>1950.000000</td>      <td>115</td>      <td>1950</td>      <td>1.0</td>      <td>1950</td>    </tr>    <tr>      <th>14</th>      <td>1950.083333</td>      <td>126</td>      <td>1950</td>      <td>2.0</td>      <td>1950</td>    </tr>    <tr>      <th>15</th>      <td>1950.166667</td>      <td>141</td>      <td>1950</td>      <td>3.0</td>      <td>1950</td>    </tr>    <tr>      <th>16</th>      <td>1950.250000</td>      <td>135</td>      <td>1950</td>      <td>4.0</td>      <td>1950</td>    </tr>    <tr>      <th>17</th>      <td>1950.333333</td>      <td>125</td>      <td>1950</td>      <td>5.0</td>      <td>1950</td>    </tr>    <tr>      <th>18</th>      <td>1950.416667</td>      <td>149</td>      <td>1950</td>      <td>6.0</td>      <td>1950</td>    </tr>    <tr>      <th>19</th>      <td>1950.500000</td>      <td>170</td>      <td>1950</td>      <td>7.0</td>      <td>1950</td>    </tr>    <tr>      <th>20</th>      <td>1950.583333</td>      <td>170</td>      <td>1950</td>      <td>8.0</td>      <td>1950</td>    </tr>    <tr>      <th>21</th>      <td>1950.666667</td>      <td>158</td>      <td>1950</td>      <td>9.0</td>      <td>1950</td>    </tr>    <tr>      <th>22</th>      <td>1950.750000</td>      <td>133</td>      <td>1950</td>      <td>10.0</td>      <td>1950</td>    </tr>    <tr>      <th>23</th>      <td>1950.833333</td>      <td>114</td>      <td>1950</td>      <td>11.0</td>      <td>1950</td>    </tr>    <tr>      <th>24</th>      <td>1950.916667</td>      <td>140</td>      <td>1950</td>      <td>12.0</td>      <td>1950</td>    </tr>    <tr>      <th>25</th>      <td>1951.000000</td>      <td>145</td>      <td>1951</td>      <td>1.0</td>      <td>1951</td>    </tr>    <tr>      <th>26</th>      <td>1951.083333</td>      <td>150</td>      <td>1951</td>      <td>2.0</td>      <td>1951</td>    </tr>    <tr>      <th>27</th>      <td>1951.166667</td>      <td>178</td>      <td>1951</td>      <td>3.0</td>      <td>1951</td>    </tr>    <tr>      <th>28</th>      <td>1951.250000</td>      <td>163</td>      <td>1951</td>      <td>4.0</td>      <td>1951</td>    </tr>    <tr>      <th>29</th>      <td>1951.333333</td>      <td>172</td>      <td>1951</td>      <td>5.0</td>      <td>1951</td>    </tr>    <tr>      <th>30</th>      <td>1951.416667</td>      <td>178</td>      <td>1951</td>      <td>6.0</td>      <td>1951</td>    </tr>    <tr>      <th>31</th>      <td>1951.500000</td>      <td>199</td>      <td>1951</td>      <td>7.0</td>      <td>1951</td>    </tr>    <tr>      <th>32</th>      <td>1951.583333</td>      <td>199</td>      <td>1951</td>      <td>8.0</td>      <td>1951</td>    </tr>    <tr>      <th>33</th>      <td>1951.666667</td>      <td>184</td>      <td>1951</td>      <td>9.0</td>      <td>1951</td>    </tr>    <tr>      <th>34</th>      <td>1951.750000</td>      <td>162</td>      <td>1951</td>      <td>10.0</td>      <td>1951</td>    </tr>    <tr>      <th>35</th>      <td>1951.833333</td>      <td>146</td>      <td>1951</td>      <td>11.0</td>      <td>1951</td>    </tr>    <tr>      <th>36</th>      <td>1951.916667</td>      <td>166</td>      <td>1951</td>      <td>12.0</td>      <td>1951</td>    </tr>    <tr>      <th>37</th>      <td>1952.000000</td>      <td>171</td>      <td>1952</td>      <td>1.0</td>      <td>1952</td>    </tr>    <tr>      <th>38</th>      <td>1952.083333</td>      <td>180</td>      <td>1952</td>      <td>2.0</td>      <td>1952</td>    </tr>    <tr>      <th>39</th>      <td>1952.166667</td>      <td>193</td>      <td>1952</td>      <td>3.0</td>      <td>1952</td>    </tr>    <tr>      <th>40</th>      <td>1952.250000</td>      <td>181</td>      <td>1952</td>      <td>4.0</td>      <td>1952</td>    </tr>    <tr>      <th>41</th>      <td>1952.333333</td>      <td>183</td>      <td>1952</td>      <td>5.0</td>      <td>1952</td>    </tr>    <tr>      <th>42</th>      <td>1952.416667</td>      <td>218</td>      <td>1952</td>      <td>6.0</td>      <td>1952</td>    </tr>    <tr>      <th>43</th>      <td>1952.500000</td>      <td>230</td>      <td>1952</td>      <td>7.0</td>      <td>1952</td>    </tr>    <tr>      <th>44</th>      <td>1952.583333</td>      <td>242</td>      <td>1952</td>      <td>8.0</td>      <td>1952</td>    </tr>    <tr>      <th>45</th>      <td>1952.666667</td>      <td>209</td>      <td>1952</td>      <td>9.0</td>      <td>1952</td>    </tr>    <tr>      <th>46</th>      <td>1952.750000</td>      <td>191</td>      <td>1952</td>      <td>10.0</td>      <td>1952</td>    </tr>    <tr>      <th>47</th>      <td>1952.833333</td>      <td>172</td>      <td>1952</td>      <td>11.0</td>      <td>1952</td>    </tr>    <tr>      <th>48</th>      <td>1952.916667</td>      <td>194</td>      <td>1952</td>      <td>12.0</td>      <td>1952</td>    </tr>    <tr>      <th>49</th>      <td>1953.000000</td>      <td>196</td>      <td>1953</td>      <td>1.0</td>      <td>1953</td>    </tr>    <tr>      <th>50</th>      <td>1953.083333</td>      <td>196</td>      <td>1953</td>      <td>2.0</td>      <td>1953</td>    </tr>    <tr>      <th>51</th>      <td>1953.166667</td>      <td>236</td>      <td>1953</td>      <td>3.0</td>      <td>1953</td>    </tr>    <tr>      <th>52</th>      <td>1953.250000</td>      <td>235</td>      <td>1953</td>      <td>4.0</td>      <td>1953</td>    </tr>    <tr>      <th>53</th>      <td>1953.333333</td>      <td>229</td>      <td>1953</td>      <td>5.0</td>      <td>1953</td>    </tr>    <tr>      <th>54</th>      <td>1953.416667</td>      <td>243</td>      <td>1953</td>      <td>6.0</td>      <td>1953</td>    </tr>    <tr>      <th>55</th>      <td>1953.500000</td>      <td>264</td>      <td>1953</td>      <td>7.0</td>      <td>1953</td>    </tr>    <tr>      <th>56</th>      <td>1953.583333</td>      <td>272</td>      <td>1953</td>      <td>8.0</td>      <td>1953</td>    </tr>    <tr>      <th>57</th>      <td>1953.666667</td>      <td>237</td>      <td>1953</td>      <td>9.0</td>      <td>1953</td>    </tr>    <tr>      <th>58</th>      <td>1953.750000</td>      <td>211</td>      <td>1953</td>      <td>10.0</td>      <td>1953</td>    </tr>    <tr>      <th>59</th>      <td>1953.833333</td>      <td>180</td>      <td>1953</td>      <td>11.0</td>      <td>1953</td>    </tr>    <tr>      <th>60</th>      <td>1953.916667</td>      <td>201</td>      <td>1953</td>      <td>12.0</td>      <td>1953</td>    </tr>    <tr>      <th>61</th>      <td>1954.000000</td>      <td>204</td>      <td>1954</td>      <td>1.0</td>      <td>1954</td>    </tr>    <tr>      <th>62</th>      <td>1954.083333</td>      <td>188</td>      <td>1954</td>      <td>2.0</td>      <td>1954</td>    </tr>    <tr>      <th>63</th>      <td>1954.166667</td>      <td>235</td>      <td>1954</td>      <td>3.0</td>      <td>1954</td>    </tr>    <tr>      <th>64</th>      <td>1954.250000</td>      <td>227</td>      <td>1954</td>      <td>4.0</td>      <td>1954</td>    </tr>    <tr>      <th>65</th>      <td>1954.333333</td>      <td>234</td>      <td>1954</td>      <td>5.0</td>      <td>1954</td>    </tr>    <tr>      <th>66</th>      <td>1954.416667</td>      <td>264</td>      <td>1954</td>      <td>6.0</td>      <td>1954</td>    </tr>    <tr>      <th>67</th>      <td>1954.500000</td>      <td>302</td>      <td>1954</td>      <td>7.0</td>      <td>1954</td>    </tr>    <tr>      <th>68</th>      <td>1954.583333</td>      <td>293</td>      <td>1954</td>      <td>8.0</td>      <td>1954</td>    </tr>    <tr>      <th>69</th>      <td>1954.666667</td>      <td>259</td>      <td>1954</td>      <td>9.0</td>      <td>1954</td>    </tr>    <tr>      <th>70</th>      <td>1954.750000</td>      <td>229</td>      <td>1954</td>      <td>10.0</td>      <td>1954</td>    </tr>    <tr>      <th>71</th>      <td>1954.833333</td>      <td>203</td>      <td>1954</td>      <td>11.0</td>      <td>1954</td>    </tr>    <tr>      <th>72</th>      <td>1954.916667</td>      <td>229</td>      <td>1954</td>      <td>12.0</td>      <td>1954</td>    </tr>    <tr>      <th>73</th>      <td>1955.000000</td>      <td>242</td>      <td>1955</td>      <td>1.0</td>      <td>1955</td>    </tr>    <tr>      <th>74</th>      <td>1955.083333</td>      <td>233</td>      <td>1955</td>      <td>2.0</td>      <td>1955</td>    </tr>    <tr>      <th>75</th>      <td>1955.166667</td>      <td>267</td>      <td>1955</td>      <td>3.0</td>      <td>1955</td>    </tr>    <tr>      <th>76</th>      <td>1955.250000</td>      <td>269</td>      <td>1955</td>      <td>4.0</td>      <td>1955</td>    </tr>    <tr>      <th>77</th>      <td>1955.333333</td>      <td>270</td>      <td>1955</td>      <td>5.0</td>      <td>1955</td>    </tr>    <tr>      <th>78</th>      <td>1955.416667</td>      <td>315</td>      <td>1955</td>      <td>6.0</td>      <td>1955</td>    </tr>    <tr>      <th>79</th>      <td>1955.500000</td>      <td>364</td>      <td>1955</td>      <td>7.0</td>      <td>1955</td>    </tr>    <tr>      <th>80</th>      <td>1955.583333</td>      <td>347</td>      <td>1955</td>      <td>8.0</td>      <td>1955</td>    </tr>    <tr>      <th>81</th>      <td>1955.666667</td>      <td>312</td>      <td>1955</td>      <td>9.0</td>      <td>1955</td>    </tr>    <tr>      <th>82</th>      <td>1955.750000</td>      <td>274</td>      <td>1955</td>      <td>10.0</td>      <td>1955</td>    </tr>    <tr>      <th>83</th>      <td>1955.833333</td>      <td>237</td>      <td>1955</td>      <td>11.0</td>      <td>1955</td>    </tr>    <tr>      <th>84</th>      <td>1955.916667</td>      <td>278</td>      <td>1955</td>      <td>12.0</td>      <td>1955</td>    </tr>    <tr>      <th>85</th>      <td>1956.000000</td>      <td>284</td>      <td>1956</td>      <td>1.0</td>      <td>1956</td>    </tr>    <tr>      <th>86</th>      <td>1956.083333</td>      <td>277</td>      <td>1956</td>      <td>2.0</td>      <td>1956</td>    </tr>    <tr>      <th>87</th>      <td>1956.166667</td>      <td>317</td>      <td>1956</td>      <td>3.0</td>      <td>1956</td>    </tr>    <tr>      <th>88</th>      <td>1956.250000</td>      <td>313</td>      <td>1956</td>      <td>4.0</td>      <td>1956</td>    </tr>    <tr>      <th>89</th>      <td>1956.333333</td>      <td>318</td>      <td>1956</td>      <td>5.0</td>      <td>1956</td>    </tr>    <tr>      <th>90</th>      <td>1956.416667</td>      <td>374</td>      <td>1956</td>      <td>6.0</td>      <td>1956</td>    </tr>    <tr>      <th>91</th>      <td>1956.500000</td>      <td>413</td>      <td>1956</td>      <td>7.0</td>      <td>1956</td>    </tr>    <tr>      <th>92</th>      <td>1956.583333</td>      <td>405</td>      <td>1956</td>      <td>8.0</td>      <td>1956</td>    </tr>    <tr>      <th>93</th>      <td>1956.666667</td>      <td>355</td>      <td>1956</td>      <td>9.0</td>      <td>1956</td>    </tr>    <tr>      <th>94</th>      <td>1956.750000</td>      <td>306</td>      <td>1956</td>      <td>10.0</td>      <td>1956</td>    </tr>    <tr>      <th>95</th>      <td>1956.833333</td>      <td>271</td>      <td>1956</td>      <td>11.0</td>      <td>1956</td>    </tr>    <tr>      <th>96</th>      <td>1956.916667</td>      <td>306</td>      <td>1956</td>      <td>12.0</td>      <td>1956</td>    </tr>    <tr>      <th>97</th>      <td>1957.000000</td>      <td>315</td>      <td>1957</td>      <td>1.0</td>      <td>1957</td>    </tr>    <tr>      <th>98</th>      <td>1957.083333</td>      <td>301</td>      <td>1957</td>      <td>2.0</td>      <td>1957</td>    </tr>    <tr>      <th>99</th>      <td>1957.166667</td>      <td>356</td>      <td>1957</td>      <td>3.0</td>      <td>1957</td>    </tr>    <tr>      <th>100</th>      <td>1957.250000</td>      <td>348</td>      <td>1957</td>      <td>4.0</td>      <td>1957</td>    </tr>    <tr>      <th>101</th>      <td>1957.333333</td>      <td>355</td>      <td>1957</td>      <td>5.0</td>      <td>1957</td>    </tr>    <tr>      <th>102</th>      <td>1957.416667</td>      <td>422</td>      <td>1957</td>      <td>6.0</td>      <td>1957</td>    </tr>    <tr>      <th>103</th>      <td>1957.500000</td>      <td>465</td>      <td>1957</td>      <td>7.0</td>      <td>1957</td>    </tr>    <tr>      <th>104</th>      <td>1957.583333</td>      <td>467</td>      <td>1957</td>      <td>8.0</td>      <td>1957</td>    </tr>    <tr>      <th>105</th>      <td>1957.666667</td>      <td>404</td>      <td>1957</td>      <td>9.0</td>      <td>1957</td>    </tr>    <tr>      <th>106</th>      <td>1957.750000</td>      <td>347</td>      <td>1957</td>      <td>10.0</td>      <td>1957</td>    </tr>    <tr>      <th>107</th>      <td>1957.833333</td>      <td>305</td>      <td>1957</td>      <td>11.0</td>      <td>1957</td>    </tr>    <tr>      <th>108</th>      <td>1957.916667</td>      <td>336</td>      <td>1957</td>      <td>12.0</td>      <td>1957</td>    </tr>    <tr>      <th>109</th>      <td>1958.000000</td>      <td>340</td>      <td>1958</td>      <td>1.0</td>      <td>1958</td>    </tr>    <tr>      <th>110</th>      <td>1958.083333</td>      <td>318</td>      <td>1958</td>      <td>2.0</td>      <td>1958</td>    </tr>    <tr>      <th>111</th>      <td>1958.166667</td>      <td>362</td>      <td>1958</td>      <td>3.0</td>      <td>1958</td>    </tr>    <tr>      <th>112</th>      <td>1958.250000</td>      <td>348</td>      <td>1958</td>      <td>4.0</td>      <td>1958</td>    </tr>    <tr>      <th>113</th>      <td>1958.333333</td>      <td>363</td>      <td>1958</td>      <td>5.0</td>      <td>1958</td>    </tr>    <tr>      <th>114</th>      <td>1958.416667</td>      <td>435</td>      <td>1958</td>      <td>6.0</td>      <td>1958</td>    </tr>    <tr>      <th>115</th>      <td>1958.500000</td>      <td>491</td>      <td>1958</td>      <td>7.0</td>      <td>1958</td>    </tr>    <tr>      <th>116</th>      <td>1958.583333</td>      <td>505</td>      <td>1958</td>      <td>8.0</td>      <td>1958</td>    </tr>    <tr>      <th>117</th>      <td>1958.666667</td>      <td>404</td>      <td>1958</td>      <td>9.0</td>      <td>1958</td>    </tr>    <tr>      <th>118</th>      <td>1958.750000</td>      <td>359</td>      <td>1958</td>      <td>10.0</td>      <td>1958</td>    </tr>    <tr>      <th>119</th>      <td>1958.833333</td>      <td>310</td>      <td>1958</td>      <td>11.0</td>      <td>1958</td>    </tr>    <tr>      <th>120</th>      <td>1958.916667</td>      <td>337</td>      <td>1958</td>      <td>12.0</td>      <td>1958</td>    </tr>    <tr>      <th>121</th>      <td>1959.000000</td>      <td>360</td>      <td>1959</td>      <td>1.0</td>      <td>1959</td>    </tr>    <tr>      <th>122</th>      <td>1959.083333</td>      <td>342</td>      <td>1959</td>      <td>2.0</td>      <td>1959</td>    </tr>    <tr>      <th>123</th>      <td>1959.166667</td>      <td>406</td>      <td>1959</td>      <td>3.0</td>      <td>1959</td>    </tr>    <tr>      <th>124</th>      <td>1959.250000</td>      <td>396</td>      <td>1959</td>      <td>4.0</td>      <td>1959</td>    </tr>    <tr>      <th>125</th>      <td>1959.333333</td>      <td>420</td>      <td>1959</td>      <td>5.0</td>      <td>1959</td>    </tr>    <tr>      <th>126</th>      <td>1959.416667</td>      <td>472</td>      <td>1959</td>      <td>6.0</td>      <td>1959</td>    </tr>    <tr>      <th>127</th>      <td>1959.500000</td>      <td>548</td>      <td>1959</td>      <td>7.0</td>      <td>1959</td>    </tr>    <tr>      <th>128</th>      <td>1959.583333</td>      <td>559</td>      <td>1959</td>      <td>8.0</td>      <td>1959</td>    </tr>    <tr>      <th>129</th>      <td>1959.666667</td>      <td>463</td>      <td>1959</td>      <td>9.0</td>      <td>1959</td>    </tr>    <tr>      <th>130</th>      <td>1959.750000</td>      <td>407</td>      <td>1959</td>      <td>10.0</td>      <td>1959</td>    </tr>    <tr>      <th>131</th>      <td>1959.833333</td>      <td>362</td>      <td>1959</td>      <td>11.0</td>      <td>1959</td>    </tr>    <tr>      <th>132</th>      <td>1959.916667</td>      <td>405</td>      <td>1959</td>      <td>12.0</td>      <td>1959</td>    </tr>    <tr>      <th>133</th>      <td>1960.000000</td>      <td>417</td>      <td>1960</td>      <td>1.0</td>      <td>1960</td>    </tr>    <tr>      <th>134</th>      <td>1960.083333</td>      <td>391</td>      <td>1960</td>      <td>2.0</td>      <td>1960</td>    </tr>    <tr>      <th>135</th>      <td>1960.166667</td>      <td>419</td>      <td>1960</td>      <td>3.0</td>      <td>1960</td>    </tr>    <tr>      <th>136</th>      <td>1960.250000</td>      <td>461</td>      <td>1960</td>      <td>4.0</td>      <td>1960</td>    </tr>    <tr>      <th>137</th>      <td>1960.333333</td>      <td>472</td>      <td>1960</td>      <td>5.0</td>      <td>1960</td>    </tr>    <tr>      <th>138</th>      <td>1960.416667</td>      <td>535</td>      <td>1960</td>      <td>6.0</td>      <td>1960</td>    </tr>    <tr>      <th>139</th>      <td>1960.500000</td>      <td>622</td>      <td>1960</td>      <td>7.0</td>      <td>1960</td>    </tr>    <tr>      <th>140</th>      <td>1960.583333</td>      <td>606</td>      <td>1960</td>      <td>8.0</td>      <td>1960</td>    </tr>    <tr>      <th>141</th>      <td>1960.666667</td>      <td>508</td>      <td>1960</td>      <td>9.0</td>      <td>1960</td>    </tr>    <tr>      <th>142</th>      <td>1960.750000</td>      <td>461</td>      <td>1960</td>      <td>10.0</td>      <td>1960</td>    </tr>    <tr>      <th>143</th>      <td>1960.833333</td>      <td>390</td>      <td>1960</td>      <td>11.0</td>      <td>1960</td>    </tr>    <tr>      <th>144</th>      <td>1960.916667</td>      <td>432</td>      <td>1960</td>      <td>12.0</td>      <td>1960</td>    </tr>  </tbody></table></div><h2 id="2-Add-a-“month”-column"><a href="#2-Add-a-“month”-column" class="headerlink" title="#2 Add a “month” column"></a>#2 Add a “month” column</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">passengers[<span class="string">&#x27;month&#x27;</span>] = (passengers[<span class="string">&#x27;time&#x27;</span>] -passengers[<span class="string">&#x27;year&#x27;</span>])*<span class="number">12</span>+<span class="number">1</span></span><br><span class="line">passengers[<span class="string">&#x27;Month&#x27;</span>]= (passengers[<span class="string">&#x27;Year&#x27;</span>]-<span class="built_in">min</span>(passengers[<span class="string">&#x27;Year&#x27;</span>]))*<span class="number">12</span>+passengers[<span class="string">&#x27;month&#x27;</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-Generate-the-plot-below-of-passengers-vs-time-using-each-monthly-count"><a href="#3-Generate-the-plot-below-of-passengers-vs-time-using-each-monthly-count" class="headerlink" title="#3 Generate the plot below of passengers vs. time using each monthly count"></a>#3 Generate the plot below of passengers vs. time using each monthly count</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(temp[<span class="string">&#x27;Month&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_12_1.png" alt="png"></p><h2 id="4-Generate-the-plot-below-of-passengers-vs-time-using-an-annual-count"><a href="#4-Generate-the-plot-below-of-passengers-vs-time-using-an-annual-count" class="headerlink" title="#4 Generate the plot below of passengers vs. time using an annual count"></a>#4 Generate the plot below of passengers vs. time using an annual count</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Year&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(temp[<span class="string">&#x27;Year&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Year&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_14_1.png" alt="png"></p><h2 id="5-Generate-the-barplot-below-of-passengers-by-year"><a href="#5-Generate-the-barplot-below-of-passengers-by-year" class="headerlink" title="#5 Generate the barplot below of passengers by year"></a>#5 Generate the barplot below of passengers by year</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers.groupby([<span class="string">&#x27;Year&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.bar(temp[<span class="string">&#x27;Year&#x27;</span>],temp[<span class="string">&#x27;AirPassengers&#x27;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Year&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.ylim([<span class="number">0</span>, <span class="number">6000</span>])</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_16_1.png" alt="png"></p><h2 id="6-Generate-the-histogram-below-of-monthly-passengers"><a href="#6-Generate-the-histogram-below-of-monthly-passengers" class="headerlink" title="#6 Generate the histogram below of monthly passengers"></a>#6 Generate the histogram below of monthly passengers</h2><p><strong>Additional requirements:</strong></p><ul><li>Only include 1955 and beyond</li><li>Use a binwidth of 50, a min of 200, and a max of 700</li><li>Set the yticks to start at 0, end at 25 by interval of 5</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp=passengers[passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1955</span>].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(temp[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">10</span>,<span class="built_in">range</span>=(<span class="number">200</span>,<span class="number">700</span>))</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5, 1.0, &#39;plot with Matplotlib&#39;)</code></pre><p><img src="/content/images/2021-08-25-output_18_1.png" alt="png"></p><h2 id="7-Generate-the-histogram-below-of-monthly-passengers"><a href="#7-Generate-the-histogram-below-of-monthly-passengers" class="headerlink" title="#7 Generate the histogram below of monthly passengers"></a>#7 Generate the histogram below of monthly passengers</h2><p><strong>Additional requirements:</strong></p><ul><li>Generate two groups to compare. Group 1 should be the years 1949-1950. Group 2 should be the years 1959-60.</li><li>Binwidth of 50 from 100 to 700</li><li>yticks from 0 to 24, spaced by 2</li><li>Be sure to include a legend</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">temp1=passengers[ (passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1949</span>) &amp; (passengers[<span class="string">&#x27;Year&#x27;</span>]&lt;=<span class="number">1950</span>)].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line">temp2=passengers[ (passengers[<span class="string">&#x27;Year&#x27;</span>]&gt;=<span class="number">1959</span>) &amp; (passengers[<span class="string">&#x27;Year&#x27;</span>]&lt;=<span class="number">1960</span>)].groupby([<span class="string">&#x27;Month&#x27;</span>])[<span class="string">&#x27;AirPassengers&#x27;</span>].<span class="built_in">sum</span>().reset_index()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(temp1[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">12</span>,alpha=<span class="number">0.5</span>,<span class="built_in">range</span>=(<span class="number">100</span>,<span class="number">700</span>),label=<span class="string">&#x27;1949-50&#x27;</span>)</span><br><span class="line">plt.hist(temp2[<span class="string">&#x27;AirPassengers&#x27;</span>],bins=<span class="number">12</span>,alpha=<span class="number">0.5</span>,<span class="built_in">range</span>=(<span class="number">100</span>,<span class="number">700</span>),label=<span class="string">&#x27;1959-60&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Month&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Hundreds of thousands&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.yticks(np.arange(<span class="number">0</span>, <span class="number">24</span>, <span class="number">2.0</span>))</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;plot with Matplotlib&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x2d3705b9d90&gt;</code></pre><p><img src="/content/images/2021-08-25-output_20_1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> matplotlib tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Add ssh key to access github and simple git commands</title>
      <link href="2021/08/23/2021-08-23-1/"/>
      <url>2021/08/23/2021-08-23-1/</url>
      
        <content type="html"><![CDATA[<p>Here are ways to add ssh key in Github, such that one can connect to github repositories without typing username and passwords.</p><h5 id="For-MAC-Users"><a href="#For-MAC-Users" class="headerlink" title="For MAC Users:"></a>For MAC Users:</h5><p>To create an SSH Key, go to your terminal (or iTerm 2) and enter <code>ssh-keygen</code>. You will be prompted for a file in which to save the key (.ssh/id_rsa). There’s no need to enter anything, just press enter. When prompted for a passphrase, you can enter one or just press enter to ignore creating a passphrase.</p><p>Your SSH key has been saved to ssh/id_rsa.pub, run the command <code>pbcopy &lt; ~/.ssh/id_rsa.pub</code> to copy this to your clipboard.</p><p>Now go to your Github account setting for Key: <a href="https://github.com/settings/keys">https://github.com/settings/keys</a> and click ‘New SSH key’. Add any title you want, like ‘First SSH Key’, paste in your SSH key, and click the ‘Add SSH key’ button.</p><h5 id="For-PC-Users"><a href="#For-PC-Users" class="headerlink" title="For PC Users:"></a>For PC Users:</h5><p>To create an SSH Key, go to your terminal and enter <code>ssh-keygen</code>. You will be prompted for a file in which to save the key (.ssh/id_rsa). There’s no need to enter anything, just press enter. When prompted for a passphrase, you can enter one or just press enter to ignore creating a passphrase.</p><p>Your SSH key has been saved to ssh/id_rsa.pub, run the command <code>cat ~/.ssh/id_rsa.pub</code> to see your SSH key in the proper format. Copy the whole text string starting with ‘ssh’ to the end.</p><p>Now go to your Github account setting for keys: <a href="https://github.com/settings/keys">https://github.com/settings/keys</a> and click ‘New SSH key’. Add any title you want, like ‘First SSH Key’. Paste your SSH key to the Key window, and click the ‘Add SSH key’ button.</p><p>There are a lot of complex functionalities of Git, but for 99% of work there are 4 things that you need to be able to do:</p><ol><li><p>Create a branch <code>git checkout -b my-branch-name</code></p></li><li><p>Add files that you want to queue up for saving.</p><ul><li><p>To add all files: <code>git add .</code></p></li><li><p>To add a specific file: <code>git add path-to-file</code></p></li></ul></li><li><p>Save (commit) changes to the files that you added above. <code>git commit -m &quot;message about this commit. Ex: change button color to green.&quot;</code></p></li><li><p>Sync your branch to Github so that you can later make a pull request. <code>git push -u origin my-branch-name</code></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> git and github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>overlay histogram distributions using seaborn or matplotlib</title>
      <link href="2021/07/26/2021-07-26-1/"/>
      <url>2021/07/26/2021-07-26-1/</url>
      
        <content type="html"><![CDATA[<p>In this example, we show two methods to overlay histogram distributions using seaborn and matplotlib separately.</p><h2 id="import-library"><a href="#import-library" class="headerlink" title="import library"></a>import library</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><h2 id="generate-some-data-for-histgram"><a href="#generate-some-data-for-histgram" class="headerlink" title="generate some data for histgram"></a>generate some data for histgram</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list1 = np.random.randint(<span class="number">100</span>,size=<span class="number">1000</span>)</span><br><span class="line">list2 = np.random.randint(<span class="number">200</span>,size=<span class="number">1000</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># also getthem into a dataframe into one column, but add a second column to label source</span></span><br><span class="line"><span class="built_in">list</span> = np.concatenate([list1,list2])</span><br><span class="line">lables = [<span class="string">&#x27;l1&#x27;</span>]*<span class="built_in">len</span>(list1)+[<span class="string">&#x27;l2&#x27;</span>]*<span class="built_in">len</span>(list2)</span><br><span class="line">d = &#123;<span class="string">&#x27;x&#x27;</span>:<span class="built_in">list</span>,<span class="string">&#x27;label&#x27;</span>:lables&#125;</span><br><span class="line">df = pd.DataFrame(d)</span><br><span class="line">display(df)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>x</th>      <th>label</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>49</td>      <td>l1</td>    </tr>    <tr>      <th>1</th>      <td>93</td>      <td>l1</td>    </tr>    <tr>      <th>2</th>      <td>38</td>      <td>l1</td>    </tr>    <tr>      <th>3</th>      <td>96</td>      <td>l1</td>    </tr>    <tr>      <th>4</th>      <td>32</td>      <td>l1</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>1995</th>      <td>18</td>      <td>l2</td>    </tr>    <tr>      <th>1996</th>      <td>102</td>      <td>l2</td>    </tr>    <tr>      <th>1997</th>      <td>1</td>      <td>l2</td>    </tr>    <tr>      <th>1998</th>      <td>175</td>      <td>l2</td>    </tr>    <tr>      <th>1999</th>      <td>77</td>      <td>l2</td>    </tr>  </tbody></table><p>2000 rows × 2 columns</p></div><h2 id="method-1-overlay-dataframe-columns-with-different-labels"><a href="#method-1-overlay-dataframe-columns-with-different-labels" class="headerlink" title="method 1, overlay dataframe columns with different labels"></a>method 1, overlay dataframe columns with different labels</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.displot(df, x=<span class="string">&quot;x&quot;</span>, hue=<span class="string">&quot;label&quot;</span>, stat=<span class="string">&quot;density&quot;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x24b5c685988&gt;</code></pre><p><img src="/content/images/2021-07-26-1.png" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="method-2-overlay-two-seperate-lists-into-histograms-using-matplotlib"><a href="#method-2-overlay-two-seperate-lists-into-histograms-using-matplotlib" class="headerlink" title="method 2,  overlay two seperate lists into histograms using matplotlib"></a>method 2,  overlay two seperate lists into histograms using matplotlib</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.hist(list1,bins=<span class="number">20</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;data1&quot;</span>)</span><br><span class="line">plt.hist(list2, bins=<span class="number">20</span>, alpha=<span class="number">0.5</span>, label=<span class="string">&quot;data2&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&quot;Data&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Count&quot;</span>, size=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">&quot;Multiple Histograms with Matplotlib&quot;</span>)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;upper right&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.legend.Legend at 0x24b5face788&gt;</code></pre><p><img src="/content/images/2021-07-26-2.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>robust method to parse time string to datetime using dateutil parser</title>
      <link href="2021/07/14/2021-07-14-1/"/>
      <url>2021/07/14/2021-07-14-1/</url>
      
        <content type="html"><![CDATA[<p>Convert time strings into datetime format coudd be very tedious, because we need to make sure the every new data<br>follow the exact format required by the conversion functions, otherwise it will raise error.</p><p>One good way is to user the dateutil.parser package, which is much more flexiable. It can also support some fuzziness.<br>With this package, even some data has slightly different formats than we expect, it will not cause error.</p><h2 id="The-traditional-way-to-convert-time-string-to-datatime-format"><a href="#The-traditional-way-to-convert-time-string-to-datatime-format" class="headerlink" title="The traditional way to convert time string to datatime format"></a>The traditional way to convert time string to datatime format</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"></span><br><span class="line"><span class="comment"># without milliseconds format</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"><span class="comment"># with milliseconds format</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12.213&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S.%f&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"><span class="comment"># in the above example if there is a new item that doesn&#x27;t follow the milliseconds format, it will raise error</span></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = datetime.datetime.strptime(item, <span class="string">&quot;%Y/%m/%d %H:%M:%S.%f&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>20222022---------------------------------------------------------------------------ValueError                                Traceback (most recent call last)&lt;ipython-input-20-a29c39b3ca30&gt; in &lt;module&gt;     15 # in the above example if there is a new item that doesn&#39;t follow the milliseconds format, it will raise error     16 item = &#39;2022/01/01 10:49:12&#39;---&gt; 17 result = datetime.datetime.strptime(item, &quot;%Y/%m/%d %H:%M:%S.%f&quot;)     18      19 print(result.year)D:\Anaconda3\lib\_strptime.py in _strptime_datetime(cls, data_string, format)    575     &quot;&quot;&quot;Return a class cls instance based on the input string and the    576     format string.&quot;&quot;&quot;--&gt; 577     tt, fraction, gmtoff_fraction = _strptime(data_string, format)    578     tzname, gmtoff = tt[-2:]    579     args = tt[:6] + (fraction,)D:\Anaconda3\lib\_strptime.py in _strptime(data_string, format)    357     if not found:    358         raise ValueError(&quot;time data %r does not match format %r&quot; %--&gt; 359                          (data_string, format))    360     if len(data_string) != found.end():    361         raise ValueError(&quot;unconverted data remains: %s&quot; %ValueError: time data &#39;2022/01/01 10:49:12&#39; does not match format &#39;%Y/%m/%d %H:%M:%S.%f&#39;</code></pre><h2 id="use-parser-in-dateutil-package-a-more-stable-way"><a href="#use-parser-in-dateutil-package-a-more-stable-way" class="headerlink" title="use parser in dateutil package, a more stable way"></a>use parser in dateutil package, a more stable way</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> dateutil.parser <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment"># for the same example above, it will not raiser error</span></span><br><span class="line"></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12&#x27;</span></span><br><span class="line">result = parse(item)</span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">item = <span class="string">&#x27;2022/01/01 10:49:12.213&#x27;</span></span><br><span class="line">result = parse(item)</span><br><span class="line"><span class="built_in">print</span>(result.year)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>20222022</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> feature extraction </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dataframe histogram visualization with seaborn using countplot</title>
      <link href="2021/07/06/2021-07-06-1/"/>
      <url>2021/07/06/2021-07-06-1/</url>
      
        <content type="html"><![CDATA[<p>We often need to plot histograms to visualize distributions of certain features or variables.<br>How to quickly obtain a useful plot and get the work done？ If what we care is the frequency of each values, seaborn provides<br>a convenient way, count_plot() function, to get the plot without count the data by ourself and then do the bar chars.</p><p>Check the following example:</p><h1 id="get-the-data-and-do-a-count-plot"><a href="#get-the-data-and-do-a-count-plot" class="headerlink" title="get the data and do a count plot"></a>get the data and do a count plot</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">titanic = sns.load_dataset(<span class="string">&quot;titanic&quot;</span>)</span><br><span class="line">titanic[<span class="string">&#x27;class&#x27;</span>] = titanic[<span class="string">&#x27;class&#x27;</span>].astype(<span class="string">&#x27;str&#x27;</span>)</span><br><span class="line">display(titanic)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>survived</th>      <th>pclass</th>      <th>sex</th>      <th>age</th>      <th>sibsp</th>      <th>parch</th>      <th>fare</th>      <th>embarked</th>      <th>class</th>      <th>who</th>      <th>adult_male</th>      <th>deck</th>      <th>embark_town</th>      <th>alive</th>      <th>alone</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>22.0</td>      <td>1</td>      <td>0</td>      <td>7.2500</td>      <td>S</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>False</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>38.0</td>      <td>1</td>      <td>0</td>      <td>71.2833</td>      <td>C</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>C</td>      <td>Cherbourg</td>      <td>yes</td>      <td>False</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>3</td>      <td>female</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>7.9250</td>      <td>S</td>      <td>Third</td>      <td>woman</td>      <td>False</td>      <td>NaN</td>      <td>Southampton</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>3</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>35.0</td>      <td>1</td>      <td>0</td>      <td>53.1000</td>      <td>S</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>C</td>      <td>Southampton</td>      <td>yes</td>      <td>False</td>    </tr>    <tr>      <th>4</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>35.0</td>      <td>0</td>      <td>0</td>      <td>8.0500</td>      <td>S</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>True</td>    </tr>    <tr>      <th>...</th>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>      <td>...</td>    </tr>    <tr>      <th>886</th>      <td>0</td>      <td>2</td>      <td>male</td>      <td>27.0</td>      <td>0</td>      <td>0</td>      <td>13.0000</td>      <td>S</td>      <td>Second</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>True</td>    </tr>    <tr>      <th>887</th>      <td>1</td>      <td>1</td>      <td>female</td>      <td>19.0</td>      <td>0</td>      <td>0</td>      <td>30.0000</td>      <td>S</td>      <td>First</td>      <td>woman</td>      <td>False</td>      <td>B</td>      <td>Southampton</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>888</th>      <td>0</td>      <td>3</td>      <td>female</td>      <td>NaN</td>      <td>1</td>      <td>2</td>      <td>23.4500</td>      <td>S</td>      <td>Third</td>      <td>woman</td>      <td>False</td>      <td>NaN</td>      <td>Southampton</td>      <td>no</td>      <td>False</td>    </tr>    <tr>      <th>889</th>      <td>1</td>      <td>1</td>      <td>male</td>      <td>26.0</td>      <td>0</td>      <td>0</td>      <td>30.0000</td>      <td>C</td>      <td>First</td>      <td>man</td>      <td>True</td>      <td>C</td>      <td>Cherbourg</td>      <td>yes</td>      <td>True</td>    </tr>    <tr>      <th>890</th>      <td>0</td>      <td>3</td>      <td>male</td>      <td>32.0</td>      <td>0</td>      <td>0</td>      <td>7.7500</td>      <td>Q</td>      <td>Third</td>      <td>man</td>      <td>True</td>      <td>NaN</td>      <td>Queenstown</td>      <td>no</td>      <td>True</td>    </tr>  </tbody></table><p>891 rows × 15 columns</p></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sns.set_theme(style=<span class="string">&quot;darkgrid&quot;</span>)</span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;embark_town&quot;</span>, data=titanic)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-1.png" alt="png"></p><h1 id="what-if-we-have-too-many-values-for-the-feature-and-we-can’t-plot-all-of-their-distributions-in-the-histogram"><a href="#what-if-we-have-too-many-values-for-the-feature-and-we-can’t-plot-all-of-their-distributions-in-the-histogram" class="headerlink" title="what if we have too many values for the feature, and we can’t plot all of their distributions in the histogram?"></a>what if we have too many values for the feature, and we can’t plot all of their distributions in the histogram?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># get the distinct values first, then choose the top n values we want to present; here we choose 2 as an example</span></span><br><span class="line"></span><br><span class="line">sub_index = titanic[<span class="string">&#x27;class&#x27;</span>].value_counts().index[:<span class="number">2</span>]</span><br><span class="line">sub_data = titanic[titanic[<span class="string">&#x27;class&#x27;</span>].isin(sub_index)]</span><br><span class="line">sub_data = sub_data.reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, data=sub_data)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-2.png" alt="png"></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># we can also explicitly require the order to be ascending</span></span><br><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, data=sub_data,order=sub_index[::-<span class="number">1</span>])</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-3.png" alt="png"></p><h1 id="now-how-to-show-the-value-counts-for-two-categorical-variables"><a href="#now-how-to-show-the-value-counts-for-two-categorical-variables" class="headerlink" title="now how to show the value counts for two categorical variables?"></a>now how to show the value counts for two categorical variables?</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ax = sns.countplot(x=<span class="string">&quot;class&quot;</span>, hue=<span class="string">&quot;who&quot;</span>, data=titanic)</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-06-4.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>multi line plot with seaborn</title>
      <link href="2021/07/01/2021-07-01-1/"/>
      <url>2021/07/01/2021-07-01-1/</url>
      
        <content type="html"><![CDATA[<p>In this example, we show to do multi-plot graph using seaborn.<br>In addtion, some of the ways to change fonts sizes are also shown.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data = [</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;10&#x27;</span>,<span class="string">&#x27;2021-07-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;12&#x27;</span>,<span class="string">&#x27;2021-08-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;15&#x27;</span>,<span class="string">&#x27;2021-09-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;20&#x27;</span>,<span class="string">&#x27;2021-10-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;20&#x27;</span>,<span class="string">&#x27;2021-07-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;22&#x27;</span>,<span class="string">&#x27;2021-08-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;25&#x27;</span>,<span class="string">&#x27;2021-09-01&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;30&#x27;</span>,<span class="string">&#x27;2021-10-01&#x27;</span>],</span><br><span class="line">        </span><br><span class="line">       ]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data=data, columns=[<span class="string">&#x27;company&#x27;</span>,<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;date&#x27;</span>])</span><br><span class="line">display(df)</span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>company</th>      <th>price</th>      <th>date</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>A</td>      <td>10</td>      <td>2021-07-01</td>    </tr>    <tr>      <th>1</th>      <td>A</td>      <td>12</td>      <td>2021-08-01</td>    </tr>    <tr>      <th>2</th>      <td>A</td>      <td>15</td>      <td>2021-09-01</td>    </tr>    <tr>      <th>3</th>      <td>A</td>      <td>20</td>      <td>2021-10-01</td>    </tr>    <tr>      <th>4</th>      <td>B</td>      <td>20</td>      <td>2021-07-01</td>    </tr>    <tr>      <th>5</th>      <td>B</td>      <td>22</td>      <td>2021-08-01</td>    </tr>    <tr>      <th>6</th>      <td>B</td>      <td>25</td>      <td>2021-09-01</td>    </tr>    <tr>      <th>7</th>      <td>B</td>      <td>30</td>      <td>2021-10-01</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># global change the font scales of the sns plot for easy set up</span></span><br><span class="line">sns.<span class="built_in">set</span>(font_scale=<span class="number">2</span>) </span><br><span class="line">sns.<span class="built_in">set</span>(style=<span class="string">&#x27;white&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">temp = df.sort_values(by=<span class="string">&#x27;date&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plot_= sns.lineplot(x=<span class="string">&quot;date&quot;</span>, y=<span class="string">&quot;price&quot;</span>, hue=<span class="string">&quot;company&quot;</span>, data=temp)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># in case too many data points, we can skip some ticks on the x axis</span></span><br><span class="line"><span class="keyword">for</span> ind, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(plot_.get_xticklabels()):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> ind % <span class="number">2</span> == <span class="number">0</span>:  <span class="comment"># every second label is kept</span></span><br><span class="line">        label.set_visible(<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        label.set_visible(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># adjust fonrt size of x axis , y axis and title</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#plot_.set_title(&#x27;example plot&#x27;)</span></span><br><span class="line">plot_.axes.set_title(<span class="string">&quot;Title&quot;</span>,fontsize=<span class="number">50</span>)</span><br><span class="line">plot_.set_xlabel(<span class="string">&quot;X Label&quot;</span>,fontsize=<span class="number">30</span>)</span><br><span class="line">plot_.set_ylabel(<span class="string">&quot;Y Label&quot;</span>,fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># adjust lengend font size</span></span><br><span class="line">plot_.legend(fontsize=<span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">plt.xticks(rotation=<span class="number">45</span>)                                                               </span><br><span class="line">plt.tight_layout()     </span><br><span class="line">plt.grid(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/content/images/2021-07-01-1.png" alt="png"></p>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> visualization </tag>
            
            <tag> seaborn tips </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rare event encoding for categorical feature in machine learning in pandas dataframe</title>
      <link href="2021/06/23/2021-06-23-1/"/>
      <url>2021/06/23/2021-06-23-1/</url>
      
        <content type="html"><![CDATA[<p>If categorical features has too many values, it will generate too many features after encoding, such as one-hot encoding.<br>We could set the threshold, if certan value has percentage less than the threshold, we change the value to be ‘rare event’ or<br>something like that. By doing this, we make sure there are not too many levels for a categorical feature.</p><p>The following code can be applied on a dataframe:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def cat_rare_event(df,threshold=0.005):</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line"></span><br><span class="line">        #print(df[col].dtype)</span><br><span class="line"></span><br><span class="line">        if df[col].dtype ==&#x27;object&#x27;:</span><br><span class="line"></span><br><span class="line">            print(col)</span><br><span class="line"></span><br><span class="line">            df.loc[df[col].value_counts()[df[col]].values &lt; int(len(df)*threshold), col] = &quot;rare_value&quot;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>or we could put this step as a customized pipeline:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">class cat_rare_event_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def __init__(self,threshold=0.005):</span><br><span class="line"></span><br><span class="line">        self.threshold = threshold</span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = cat_rare_event(X,self.threshold)</span><br><span class="line"></span><br><span class="line">               </span><br><span class="line"></span><br><span class="line">        return X    </span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>handles feature order in training and online production stage to avoid inconsistent error</title>
      <link href="2021/06/18/2021-06-18-1/"/>
      <url>2021/06/18/2021-06-18-1/</url>
      
        <content type="html"><![CDATA[<p>In applying machine learning models in production stage, like lightGBM model or any models.<br>While we all know the order of features shoud be same for both training stage, test stage, and the production stage.</p><p>In practice we might ignore that. In produciton stage, new data might come as a json format, where orders will disappear,<br>it has nothing to do with the original feature order in the model training stage.</p><p>The comming json will be converted to dataframe format, and passed to the model for prediction. We might usually<br>igore the fact that, the new dataframe column order is different from the original training dataframe column order now.<br>And it’s important to make sure they are consistent, and not up to the randome fate.</p><p>There are many ways to achieve this, the following shows how to do it in a pipeline fashion.</p><h2 id="define-piplenow-to-treat-the-effects-systematically"><a href="#define-piplenow-to-treat-the-effects-systematically" class="headerlink" title="define piplenow, to treat the effects systematically"></a>define piplenow, to treat the effects systematically</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LastStepTransformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.traincolumns = []</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;initialized&#x27;</span>)</span><br><span class="line"></span><br><span class="line">     <span class="comment"># Return self, nothing else to do here</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        self.traincolumns = X.columns</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line">        <span class="comment"># make sure any new data follows the same order of features used in the training stage</span></span><br><span class="line">        <span class="keyword">return</span> X[self.traincolumns]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># make an data process pipeline, using the above transformer as the last steps here.</span></span><br><span class="line"><span class="comment"># in practice, any preprocessing steps can be put here as well</span></span><br><span class="line"></span><br><span class="line">dataPipeline = Pipeline([</span><br><span class="line"> (<span class="string">&#x27;last_step&#x27;</span>,LastStepTransformer())   </span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>initialized</code></pre><h2 id="show-an-example"><a href="#show-an-example" class="headerlink" title="show an example"></a>show an example</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">data_train =[[<span class="number">1.1</span>,<span class="number">2.2</span>],[<span class="number">2.1</span>,<span class="number">3.2</span>]]</span><br><span class="line">data_test =[[<span class="number">3.1</span>,<span class="number">5.2</span>],[<span class="number">1.1</span>,<span class="number">2.2</span>]]</span><br><span class="line"></span><br><span class="line">df_train = pd.DataFrame(data=data_train,columns=[<span class="string">&#x27;col1&#x27;</span>,<span class="string">&#x27;col2&#x27;</span>])</span><br><span class="line">df_test = pd.DataFrame(data=data_test,columns=[<span class="string">&#x27;col2&#x27;</span>,<span class="string">&#x27;col1&#x27;</span>])</span><br><span class="line"></span><br><span class="line">display(df_train)</span><br><span class="line">display(df_test)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.1</td>      <td>2.2</td>    </tr>    <tr>      <th>1</th>      <td>2.1</td>      <td>3.2</td>    </tr>  </tbody></table></div><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col2</th>      <th>col1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>3.1</td>      <td>5.2</td>    </tr>    <tr>      <th>1</th>      <td>1.1</td>      <td>2.2</td>    </tr>  </tbody></table></div><h2 id="now-in-the-training-stage-we-call-fit-transform-of-the-data-pipeline-so-the-pipeline-will-remembers-the-orignal-order"><a href="#now-in-the-training-stage-we-call-fit-transform-of-the-data-pipeline-so-the-pipeline-will-remembers-the-orignal-order" class="headerlink" title="now in the training stage, we call fit_transform() of the data pipeline, so the pipeline will remembers the orignal order"></a>now in the training stage, we call fit_transform() of the data pipeline, so the pipeline will remembers the orignal order</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">dataPipeline.fit_transform(df_train)</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1.1</td>      <td>2.2</td>    </tr>    <tr>      <th>1</th>      <td>2.1</td>      <td>3.2</td>    </tr>  </tbody></table></div><h2 id="now-in-the-test-stage-we-only-call-transform-of-the-datapipleine-so-any-new-data-will-be-reordered-as-the-training-data"><a href="#now-in-the-test-stage-we-only-call-transform-of-the-datapipleine-so-any-new-data-will-be-reordered-as-the-training-data" class="headerlink" title="now in the test stage, we only call transform() of the datapipleine, so any new data will be reordered as the training data"></a>now in the test stage, we only call transform() of the datapipleine, so any new data will be reordered as the training data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;test data, notice the column order&#x27;</span>)</span><br><span class="line">display(df_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;after transform, notice the column order now changes&#x27;</span>)</span><br><span class="line">dataPipeline.transform(df_test)</span><br></pre></td></tr></table></figure><pre><code>test data, notice the column order</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col2</th>      <th>col1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>3.1</td>      <td>5.2</td>    </tr>    <tr>      <th>1</th>      <td>1.1</td>      <td>2.2</td>    </tr>  </tbody></table></div><pre><code>after transform, ontice the column order now changes</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>col1</th>      <th>col2</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>5.2</td>      <td>3.1</td>    </tr>    <tr>      <th>1</th>      <td>2.2</td>      <td>1.1</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
            <tag> machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>some handy functions to group continous variables and missing value imputation in dataframe</title>
      <link href="2021/06/15/2021-06-15-1/"/>
      <url>2021/06/15/2021-06-15-1/</url>
      
        <content type="html"><![CDATA[<p>Following example shows how to group age variable into groups,<br>and some simple missing value imputaiton proecdures.</p><p>There is also an example to transform timestamp variable to week day and hour infomation.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"># utility functions</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">def age_input(age):</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    if pd.isnull(age):</span><br><span class="line"></span><br><span class="line">        return &#x27;missing&#x27;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    age = int(age)</span><br><span class="line"></span><br><span class="line">    if age&lt;=20:</span><br><span class="line"></span><br><span class="line">        return &#x27;16-20&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=24:</span><br><span class="line"></span><br><span class="line">        return &#x27;21-24&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=34:</span><br><span class="line"></span><br><span class="line">        return &#x27;25-34&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=44:</span><br><span class="line"></span><br><span class="line">        return &#x27;35-44&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=54:</span><br><span class="line"></span><br><span class="line">        return &#x27;45-54&#x27;</span><br><span class="line"></span><br><span class="line">    elif age&lt;=64:</span><br><span class="line"></span><br><span class="line">        return &#x27;55-64&#x27;</span><br><span class="line"></span><br><span class="line">    else:</span><br><span class="line"></span><br><span class="line">        return &#x27;65+&#x27;</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"># missing value handelling or imputation in dataframe </span><br><span class="line"></span><br><span class="line">def missing_handle(df):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    for col in df.columns:</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"></span><br><span class="line">        if df[col].dtype==object:</span><br><span class="line"></span><br><span class="line">            df[col] = df[col].fillna(&#x27;missing&#x27;)</span><br><span class="line"></span><br><span class="line">        elif df[col].dtype == bool:</span><br><span class="line"></span><br><span class="line">            df[col+&#x27;_null&#x27;] = df[col].apply(lambda x: 1 if pd.isnull(x) else 0)</span><br><span class="line"></span><br><span class="line">            df[col] = data[col].fillna(data[col].mode()[0])</span><br><span class="line"></span><br><span class="line">          </span><br><span class="line"></span><br><span class="line">        else:</span><br><span class="line"></span><br><span class="line">            df[col] = df[col].fillna(-999)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    return df</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">class dayandhour_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Customized transformer method</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X[&#x27;dayofweek&#x27;]=pd.to_datetime(X[&#x27;sentat&#x27;]).dt.dayofweek</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X[&#x27;hour&#x27;]=pd.to_datetime(X[&#x27;sentat&#x27;]).dt.hour</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;sentat&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">        # apply age group function here</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">        X[&#x27;age_group&#x27;] = X[&#x27;age&#x27;].apply(age_input)</span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;age&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">                </span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        # apply missing handelling here</span><br><span class="line"></span><br><span class="line">        X = missing_handle(X)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        return X</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"># define the transformer</span><br><span class="line">dayandhour_transformer = dayandhour_Transformer()</span><br><span class="line"></span><br><span class="line"># usage example</span><br><span class="line">df_new = dayandhour_transformer.transform(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>explode and expand rows to multiple rows or columns to multiple columns using pandas dataframe</title>
      <link href="2021/06/14/2021-06-14-1/"/>
      <url>2021/06/14/2021-06-14-1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="generate-some-example-some-data"><a href="#generate-some-example-some-data" class="headerlink" title="generate some example some data"></a>generate some example some data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [[<span class="string">&#x27;python&#x27;</span>,<span class="string">&#x27;C&#x27;</span>],<span class="string">&#x27;John&#x27;</span>],[[<span class="string">&#x27;python&#x27;</span>,<span class="string">&#x27;Go&#x27;</span>],<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;language&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>       language  name0   [python, C]  John1  [python, Go]  Mark</code></pre><h2 id="1-First-we-expload-the-laguage-column-put-each-of-the-array-element-into-a-single-row"><a href="#1-First-we-expload-the-laguage-column-put-each-of-the-array-element-into-a-single-row" class="headerlink" title="1. First, we expload the laguage column, put each of the array element into a single row"></a>1. First, we expload the laguage column, put each of the array element into a single row</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.explode(<span class="string">&#x27;language&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John0        C  John1   python  Mark1       Go  Mark</code></pre><h3 id="if-we-want-to-reset-the-index…"><a href="#if-we-want-to-reset-the-index…" class="headerlink" title="if we want to reset the index…."></a>if we want to reset the index….</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.explode(<span class="string">&#x27;language&#x27;</span>).reset_index(drop=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John1        C  John2   python  Mark3       Go  Mark</code></pre><h3 id="1-2-now-how-about-the-original-column-is-not-list-but-strings-we-need-to-split-use-assign-then-chain-it-with-explode"><a href="#1-2-now-how-about-the-original-column-is-not-list-but-strings-we-need-to-split-use-assign-then-chain-it-with-explode" class="headerlink" title="1.2 now how about the original column is not list, but strings we need to split? use assign then chain it with explode"></a>1.2 now how about the original column is not list, but strings we need to split? use assign then chain it with explode</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [<span class="string">&#x27;python,C&#x27;</span>,<span class="string">&#x27;John&#x27;</span>],[<span class="string">&#x27;python,Go&#x27;</span>,<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;language&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>    language  name0   python,C  John1  python,Go  Mark</code></pre><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.assign(language=df.language.<span class="built_in">str</span>.split(<span class="string">&quot;,&quot;</span>)).explode(<span class="string">&#x27;language&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(df2)</span><br></pre></td></tr></table></figure><pre><code>  language  name0   python  John0        C  John1   python  Mark1       Go  Mark</code></pre><h2 id="2-in-the-above-example-we-expand-rows-into-multiple-rows-by-one-column’s-list-like-element-now-sometimes-we-need-to-expand-columns-into-multiple-columns"><a href="#2-in-the-above-example-we-expand-rows-into-multiple-rows-by-one-column’s-list-like-element-now-sometimes-we-need-to-expand-columns-into-multiple-columns" class="headerlink" title="2. in the above example, we expand rows into multiple rows by one column’s list like element; now sometimes we need to expand columns into multiple columns"></a>2. in the above example, we expand rows into multiple rows by one column’s list like element; now sometimes we need to expand columns into multiple columns</h2><h3 id="let’s-generate-some-data-again"><a href="#let’s-generate-some-data-again" class="headerlink" title="let’s generate some data again"></a>let’s generate some data again</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [[<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;27&#x27;</span>],<span class="string">&#x27;John&#x27;</span>],[[<span class="string">&#x27;age&#x27;</span>,<span class="string">&#x27;30&#x27;</span>],<span class="string">&#x27;Mark&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;age_info&#x27;</span>,<span class="string">&#x27;name&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>    age_info  name0  [age, 27]  John1  [age, 30]  Mark</code></pre><h3 id="we-could-use-to-list"><a href="#we-could-use-to-list" class="headerlink" title="we could use to_list()"></a>we could use to_list()</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df[[<span class="string">&#x27;attribute&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]] = df.age_info.to_list()</span><br><span class="line"><span class="comment"># or df[[&#x27;First&#x27;,&#x27;Last&#x27;]] = df[&#x27;age_info&#x27;].to_list()</span></span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>    age_info  name attribute value0  [age, 27]  John       age    271  [age, 30]  Mark       age    30</code></pre><h3 id="now-same-quesiton-how-about-the-column-is-a-string-that-can-be-split"><a href="#now-same-quesiton-how-about-the-column-is-a-string-that-can-be-split" class="headerlink" title="now same quesiton, how about the column is a string that can be split?"></a>now same quesiton, how about the column is a string that can be split?</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = [ [<span class="string">&#x27;john,f&#x27;</span>,<span class="string">&#x27;1&#x27;</span>],[<span class="string">&#x27;mark,y&#x27;</span>,<span class="string">&#x27;2&#x27;</span>] ]</span><br><span class="line">df = pd.DataFrame(data=data,columns=[<span class="string">&#x27;full_name&#x27;</span>,<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">df[[<span class="string">&#x27;First&#x27;</span>,<span class="string">&#x27;Last&#x27;</span>]] = df.full_name.<span class="built_in">str</span>.split(<span class="string">&quot;,&quot;</span>,expand=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><pre><code>  full_name id0    john,f  11    mark,y  2  full_name id First Last0    john,f  1  john    f1    mark,y  2  mark    y</code></pre>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> preprocessing </tag>
            
            <tag> pandas tip </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>aggregate features from different rows into one row in pandas dataframe</title>
      <link href="2021/06/12/2021-06-12-1/"/>
      <url>2021/06/12/2021-06-12-1/</url>
      
        <content type="html"><![CDATA[<p>In many use cases, different features of the same event are stored in a table by multiple rows.<br>multiple columns will indicate each characteristics of one feature, such as name, value, timestamp, etc.</p><p>In machine learning, we need to aggregate them into one row for training, and the following shows how do do it in dataframe easily.</p><h2 id="generate-some-example-dataframe"><a href="#generate-some-example-dataframe" class="headerlink" title="generate some example dataframe"></a>generate some example dataframe</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data = [ [<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name1&#x27;</span>,<span class="string">&#x27;value1&#x27;</span>],[<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name2&#x27;</span>,<span class="string">&#x27;value2&#x27;</span>],[<span class="string">&#x27;1&#x27;</span>,<span class="string">&#x27;name3&#x27;</span>,<span class="string">&#x27;value3&#x27;</span>],</span><br><span class="line">         [<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;name1&#x27;</span>,<span class="string">&#x27;value4&#x27;</span>],[<span class="string">&#x27;2&#x27;</span>,<span class="string">&#x27;name2&#x27;</span>,<span class="string">&#x27;value5&#x27;</span>] </span><br><span class="line">       ]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(data=data)</span><br><span class="line">df.columns =[<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]</span><br><span class="line">display(df)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>id</th>      <th>name</th>      <th>value</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>1</td>      <td>name1</td>      <td>value1</td>    </tr>    <tr>      <th>1</th>      <td>1</td>      <td>name2</td>      <td>value2</td>    </tr>    <tr>      <th>2</th>      <td>1</td>      <td>name3</td>      <td>value3</td>    </tr>    <tr>      <th>3</th>      <td>2</td>      <td>name1</td>      <td>value4</td>    </tr>    <tr>      <th>4</th>      <td>2</td>      <td>name2</td>      <td>value5</td>    </tr>  </tbody></table></div><h2 id="group-the-dataframe-by-id-then-aggregate-all-the-feature-values-into-one-column"><a href="#group-the-dataframe-by-id-then-aggregate-all-the-feature-values-into-one-column" class="headerlink" title="group the dataframe by id, then aggregate all the feature values into one column"></a>group the dataframe by id, then aggregate all the feature values into one column</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df2 = df.groupby(<span class="string">&#x27;id&#x27;</span>).apply(<span class="keyword">lambda</span> x: <span class="built_in">dict</span>(x[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]].values.tolist())).reset_index()</span><br><span class="line">df3 = pd.DataFrame(data=df2[<span class="number">0</span>].values.tolist())</span><br><span class="line">display(df3)</span><br><span class="line">    </span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name1</th>      <th>name2</th>      <th>name3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>value1</td>      <td>value2</td>      <td>value3</td>    </tr>    <tr>      <th>1</th>      <td>value4</td>      <td>value5</td>      <td>NaN</td>    </tr>  </tbody></table></div><h1 id="put-the-above-the-transformation-into-a-scikit-learn-customed-transformer"><a href="#put-the-above-the-transformation-into-a-scikit-learn-customed-transformer" class="headerlink" title="put the above the transformation into a scikit-learn customed transformer"></a>put the above the transformation into a scikit-learn customed transformer</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> BaseEstimator, TransformerMixin</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_Transformer</span>(<span class="params">BaseEstimator, TransformerMixin</span>):</span></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Class Constructor</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start&#x27;</span>)</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">           </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Return self</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, X, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    <span class="comment">#Customized transformer</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">transform</span>(<span class="params">self, X_, y=<span class="literal">None</span></span>):</span></span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line">        X2 = X.groupby(<span class="string">&#x27;id&#x27;</span>).apply(<span class="keyword">lambda</span> x: <span class="built_in">dict</span>(x[[<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;value&#x27;</span>]].values.tolist())).reset_index()</span><br><span class="line">        X3 = pd.DataFrame(data=X2[<span class="number">0</span>].values.tolist())     </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X3</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> X1</span><br><span class="line"></span><br><span class="line">     </span><br><span class="line"></span><br><span class="line"><span class="comment"># get a transformer object</span></span><br><span class="line">my_transformer = my_Transformer()</span><br><span class="line"></span><br><span class="line"><span class="comment"># apply the transform on the original data</span></span><br><span class="line"></span><br><span class="line">df_new = my_transformer.transform(df)</span><br><span class="line">display(df_new)</span><br></pre></td></tr></table></figure><pre><code>start</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>name1</th>      <th>name2</th>      <th>name3</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>value1</td>      <td>value2</td>      <td>value3</td>    </tr>    <tr>      <th>1</th>      <td>value4</td>      <td>value5</td>      <td>NaN</td>    </tr>  </tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pipeline </tag>
            
            <tag> feature engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>time series feature engineering using tsfresh, training vs test</title>
      <link href="2021/06/10/2021-06-10-1/"/>
      <url>2021/06/10/2021-06-10-1/</url>
      
        <content type="html"><![CDATA[<p>During the test stage, i.e., once the model is on production, for any new data,<br>tsfresh feature generation does not depend the training data. So one can apply the same feature engineering process as the training data<br>without worrying about stroing information from training stage.</p><p>On ther hand, one can also use the following example to leverage scikit learn pipleline style to handel the feature generation<br>for both training and test stages.</p><h1 id="Feature-Selection-in-a-sklearn-pipeline"><a href="#Feature-Selection-in-a-sklearn-pipeline" class="headerlink" title="Feature Selection in a sklearn pipeline"></a>Feature Selection in a sklearn pipeline</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tsfresh.examples <span class="keyword">import</span> load_robot_execution_failures</span><br><span class="line"><span class="keyword">from</span> tsfresh.transformers <span class="keyword">import</span> RelevantFeatureAugmenter</span><br><span class="line"><span class="keyword">from</span> tsfresh.utilities.dataframe_functions <span class="keyword">import</span> impute</span><br></pre></td></tr></table></figure><h2 id="Load-and-Prepare-the-Data"><a href="#Load-and-Prepare-the-Data" class="headerlink" title="Load and Prepare the Data"></a>Load and Prepare the Data</h2><p>Check out the first example notebook to learn more about the data and format.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tsfresh.examples.robot_execution_failures <span class="keyword">import</span> download_robot_execution_failures</span><br><span class="line">download_robot_execution_failures() </span><br><span class="line">df_ts, y = load_robot_execution_failures()</span><br></pre></td></tr></table></figure><p>We want to use the extracted features to predict for each of the robot executions, if it was a failure or not.<br>Therefore our basic “entity” is a single robot execution given by a distinct <code>id</code>.</p><p>A dataframe with these identifiers as index needs to be prepared for the pipeline.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = pd.DataFrame(index=y.index)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split data into train and test set</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y)</span><br></pre></td></tr></table></figure><h2 id="Build-the-pipeline"><a href="#Build-the-pipeline" class="headerlink" title="Build the pipeline"></a>Build the pipeline</h2><p>We build a sklearn pipeline that consists of a feature extraction step (<code>RelevantFeatureAugmenter</code>) with a subsequent <code>RandomForestClassifier</code>.</p><p>The <code>RelevantFeatureAugmenter</code> takes roughly the same arguments as <code>extract_features</code> and <code>select_features</code> do.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl = Pipeline([</span><br><span class="line">        (<span class="string">&#x27;augmenter&#x27;</span>, RelevantFeatureAugmenter(column_id=<span class="string">&#x27;id&#x27;</span>, column_sort=<span class="string">&#x27;time&#x27;</span>)),</span><br><span class="line">        (<span class="string">&#x27;classifier&#x27;</span>, RandomForestClassifier())</span><br><span class="line">      ])</span><br></pre></td></tr></table></figure><div class="alert alert-warning">    <p>Here comes the tricky part!</p><p>The input to the pipeline will be our dataframe <code>X</code>, which one row per identifier.<br>It is currently empty.<br>But which time series data should the <code>RelevantFeatureAugmenter</code> to actually extract the features from?</p><p>We need to pass the time series data (stored in <code>df_ts</code>) to the transformer.</p></div><p>In this case, df_ts contains the time series of both train and test set, if you have different dataframes for<br>train and test set, you have to call set_params two times<br>(see further below on how to deal with two independent data sets)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts);</span><br></pre></td></tr></table></figure><p>We are now ready to fit the pipeline</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.fit(X_train, y_train)</span><br></pre></td></tr></table></figure><p>The augmenter has used the input time series data to extract time series features for each of the identifiers in the <code>X_train</code> and selected only the relevant ones using the passed <code>y_train</code> as target.<br>These features have been added to <code>X_train</code> as new columns.<br>The classifier can now use these features during trainings.</p><h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><p>During interference, the augmentor does only extract the relevant features it has found out in the training phase and the classifier predicts the target using these features.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_pred = ppl.predict(X_test)</span><br></pre></td></tr></table></figure><p>So, finally we inspect the performance:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure><p>You can also find out, which columns the augmenter has selected</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.named_steps[<span class="string">&quot;augmenter&quot;</span>].feature_selector.relevant_features</span><br></pre></td></tr></table></figure><div class="alert alert-info">    <p>In this example we passed in an empty (except the index) <code>X_train</code> or <code>X_test</code> into the pipeline.<br>However, you can also fill the input with other features you have (e.g. features extracted from the metadata)<br>or even use other pipeline components before.</p></div><h2 id="Separating-the-time-series-data-containers"><a href="#Separating-the-time-series-data-containers" class="headerlink" title="Separating the time series data containers"></a>Separating the time series data containers</h2><p>In the example above we passed in a single <code>df_ts</code> into the <code>RelevantFeatureAugmenter</code>, which was used both for training and predicting.<br>During training, only the data with the <code>id</code>s from <code>X_train</code> where extracted and during prediction the rest.</p><p>However, it is perfectly fine to call <code>set_params</code> twice: once before training and once before prediction.<br>This can be handy if you for example dump the trained pipeline to disk and re-use it only later for prediction.<br>You only need to make sure that the <code>id</code>s of the enteties you use during training/prediction are actually present in the passed time series data.</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_ts_train = df_ts[df_ts[<span class="string">&quot;id&quot;</span>].isin(y_train.index)]</span><br><span class="line">df_ts_test = df_ts[df_ts[<span class="string">&quot;id&quot;</span>].isin(y_test.index)]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts_train);</span><br><span class="line">ppl.fit(X_train, y_train);</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pipeline.pkl&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(ppl, f)</span><br></pre></td></tr></table></figure><p>Later: load the fitted model and do predictions on new, unseen data</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;pipeline.pkl&quot;</span>, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    ppk = pickle.load(f)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ppl.set_params(augmenter__timeseries_container=df_ts_test);</span><br><span class="line">y_pred = ppl.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(y_test, y_pred))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> preprocessing </tag>
            
            <tag> pipeline </tag>
            
            <tag> tsfresh </tag>
            
            <tag> time series </tag>
            
            <tag> feature engineering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>word tokenization and sentence tokenization in python using NLTK package</title>
      <link href="2021/06/09/2021-06-09-1/"/>
      <url>2021/06/09/2021-06-09-1/</url>
      
        <content type="html"><![CDATA[<h1 id="What-is-Tokenization"><a href="#What-is-Tokenization" class="headerlink" title="What is Tokenization?"></a>What is Tokenization?</h1><p>Tokenization is the process by which a large quantity of text is divided into smaller parts called tokens.<br>These tokens are very useful for finding patterns and are considered as a base step for stemming and lemmatization.<br>Tokenization also helps to substitute sensitive data elements with non-sensitive data elements.</p><p>Natural language processing is used for building applications such as Text classification, intelligent chatbot, sentimental analysis, language translation, etc.<br>It becomes vital to understand the pattern in the text to achieve the above-stated purpose.</p><h1 id="Tokenization-of-words"><a href="#Tokenization-of-words" class="headerlink" title="Tokenization of words"></a>Tokenization of words</h1><p>We use the method word_tokenize() to split a sentence into words.<br>The output of word tokenization can be converted to Data Frame for better text understanding in machine learning applications.<br>It can also be provided as input for further text cleaning steps such as punctuation removal, numeric character removal or stemming.</p><p>Code example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">text = &quot;God is Great! I won a lottery.&quot;</span><br><span class="line">print(word_tokenize(text))</span><br><span class="line"></span><br><span class="line">Output: [&#x27;God&#x27;, &#x27;is&#x27;, &#x27;Great&#x27;, &#x27;!&#x27;, &#x27;I&#x27;, &#x27;won&#x27;, &#x27;a&#x27;, &#x27;lottery&#x27;, &#x27;.&#x27;]</span><br></pre></td></tr></table></figure><p>From the above example, one can see the punctuationa are also included. Sometimes we want to exclude that.<br>To achieve this purpose, there are two ways:</p><h2 id="USE-nltk-RegexpTokenizer-TO-REMOVE-ALL-PUNCTUATION-MARKS"><a href="#USE-nltk-RegexpTokenizer-TO-REMOVE-ALL-PUNCTUATION-MARKS" class="headerlink" title="USE nltk.RegexpTokenizer() TO REMOVE ALL PUNCTUATION MARKS"></a>USE nltk.RegexpTokenizer() TO REMOVE ALL PUNCTUATION MARKS</h2><p>Call nltk.RegexpTokenizer(pattern) with pattern as r”\w+” to create a tokenzier that uses pattern to split a string.<br>Call RegexpTokenizer.tokenize(text) with RegexpTokenizer as the previous result and text as a string representing a sentence to return text as a list of words with punctuation’s removed.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sentence  = &quot;Think and wonder, wonder and think.&quot;</span><br><span class="line"></span><br><span class="line">tokenizer = nltk.RegexpTokenizer(r&quot;\w+&quot;)</span><br><span class="line">new_words = tokenizer.tokenize(sentence)</span><br><span class="line"></span><br><span class="line">print(new_words)</span><br><span class="line">OUTPUT</span><br><span class="line">[&#x27;Think&#x27;, &#x27;and&#x27;, &#x27;wonder&#x27;, &#x27;wonder&#x27;, &#x27;and&#x27;, &#x27;think&#x27;]</span><br></pre></td></tr></table></figure><h2 id="USE-nltk-word-tokenize-AND-LIST-COMPREHENSION-TO-REMOVE-ALL-PUNCTUATION-MARKS"><a href="#USE-nltk-word-tokenize-AND-LIST-COMPREHENSION-TO-REMOVE-ALL-PUNCTUATION-MARKS" class="headerlink" title="USE nltk.word_tokenize() AND LIST COMPREHENSION TO REMOVE ALL PUNCTUATION MARKS"></a>USE nltk.word_tokenize() AND LIST COMPREHENSION TO REMOVE ALL PUNCTUATION MARKS</h2><p>Call nltk.word_tokenize(text) with text as a string representing a sentence to return text as a list of words. Use the syntax [word for word in words if condition] with words as the previous result and condition as word.isalnum() to create a list containing each word in words that only contain alphanumeric characters.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sentence  = &quot;Think and wonder, wonder and think.&quot;</span><br><span class="line"></span><br><span class="line">words = nltk.word_tokenize(sentence)</span><br><span class="line">new_words= [word for word in words if word.isalnum()]</span><br><span class="line"></span><br><span class="line">print(new_words)</span><br><span class="line">OUTPUT</span><br><span class="line">[&#x27;Think&#x27;, &#x27;and&#x27;, &#x27;wonder&#x27;, &#x27;wonder&#x27;, &#x27;and&#x27;, &#x27;think&#x27;]</span><br></pre></td></tr></table></figure><h1 id="Tokenization-of-Sentences"><a href="#Tokenization-of-Sentences" class="headerlink" title="Tokenization of Sentences"></a>Tokenization of Sentences</h1><p>Sometimes you need to get sentences out of the texts at first.</p><p>Code example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from nltk.tokenize import sent_tokenize</span><br><span class="line">text = &quot;God is Great! I won a lottery.&quot;</span><br><span class="line">print(sent_tokenize(text))</span><br><span class="line"></span><br><span class="line">Output: [&#x27;God is Great!&#x27;, &#x27;I won a lottery &#x27;]</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tokenization </tag>
            
            <tag> NLTK </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>missing value or null value processing in pandas dataframe</title>
      <link href="2021/06/08/2021-06-08-1/"/>
      <url>2021/06/08/2021-06-08-1/</url>
      
        <content type="html"><![CDATA[<h1 id="obtain-null-or-missing-values-of-a-dataframe"><a href="#obtain-null-or-missing-values-of-a-dataframe" class="headerlink" title="obtain null or missing values of a dataframe"></a>obtain null or missing values of a dataframe</h1><p>Suppose the dataframe has the following formats, with 10 rows and 5 clomns:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">          0         1         2         3         4         5</span><br><span class="line">0  0.520113  0.884000  1.260966 -0.236597  0.312972 -0.196281</span><br><span class="line">1 -0.837552       NaN  0.143017  0.862355  0.346550  0.842952</span><br><span class="line">2 -0.452595       NaN -0.420790  0.456215  1.203459  0.527425</span><br><span class="line">3  0.317503 -0.917042  1.780938 -1.584102  0.432745  0.389797</span><br><span class="line">4 -0.722852  1.704820 -0.113821 -1.466458  0.083002  0.011722</span><br><span class="line">5 -0.622851 -0.251935 -1.498837       NaN  1.098323  0.273814</span><br><span class="line">6  0.329585  0.075312 -0.690209 -3.807924  0.489317 -0.841368</span><br><span class="line">7 -1.123433 -1.187496  1.868894 -2.046456 -0.949718       NaN</span><br><span class="line">8  1.133880 -0.110447  0.050385 -1.158387  0.188222       NaN</span><br><span class="line">9 -0.513741  1.196259  0.704537  0.982395 -0.585040 -1.693810</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>the isnull() function which would return a dataframe like this:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">       0      1      2      3      4      5</span><br><span class="line">0  False  False  False  False  False  False</span><br><span class="line">1  False   True  False  False  False  False</span><br><span class="line">2  False   True  False  False  False  False</span><br><span class="line">3  False  False  False  False  False  False</span><br><span class="line">4  False  False  False  False  False  False</span><br><span class="line">5  False  False  False   True  False  False</span><br><span class="line">6  False  False  False  False  False  False</span><br><span class="line">7  False  False  False  False  False   True</span><br><span class="line">8  False  False  False  False  False   True</span><br><span class="line">9  False  False  False  False  False  False</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>following command will select rows that has any null values</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df.isnull().any(axis=1)]</span><br></pre></td></tr></table></figure><p>following command will select columns that has any null values</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df.columns[df.isna().any()]]</span><br></pre></td></tr></table></figure><p>follwoing command will select rows that have null values for a specific column, e.g., column=3</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">df[df[3].isnull()]</span><br></pre></td></tr></table></figure><h1 id="Drop-null-values"><a href="#Drop-null-values" class="headerlink" title="Drop null values"></a>Drop null values</h1><blockquote><blockquote><blockquote><p>df = pd.DataFrame({“name”: [‘Alfred’, ‘Batman’, ‘Catwoman’],<br>…                    “toy”: [np.nan, ‘Batmobile’, ‘Bullwhip’],<br>…                    “born”: [pd.NaT, pd.Timestamp(“1940-04-25”),<br>…                             pd.NaT]})</p></blockquote></blockquote></blockquote><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df</span><br><span class="line">       name        toy       born</span><br><span class="line">0    Alfred        NaN        NaT</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Drop the rows where at least one element is missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna()</span><br><span class="line">     name        toy       born</span><br><span class="line">1  Batman  Batmobile 1940-04-25</span><br></pre></td></tr></table></figure><p>Drop the columns where at least one element is missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(axis=&#x27;columns&#x27;)</span><br><span class="line">       name</span><br><span class="line">0    Alfred</span><br><span class="line">1    Batman</span><br><span class="line">2  Catwoman</span><br></pre></td></tr></table></figure><p>Drop the rows where all elements are missing.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(how=&#x27;all&#x27;)</span><br><span class="line">       name        toy       born</span><br><span class="line">0    Alfred        NaN        NaT</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Keep only the rows with at least 2 non-NA values.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(thresh=2)</span><br><span class="line">       name        toy       born</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Define in which columns to look for missing values.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(subset=[&#x27;name&#x27;, &#x27;toy&#x27;])</span><br><span class="line">       name        toy       born</span><br><span class="line">1    Batman  Batmobile 1940-04-25</span><br><span class="line">2  Catwoman   Bullwhip        NaT</span><br></pre></td></tr></table></figure><p>Keep the DataFrame with valid entries in the same variable.</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df.dropna(inplace=True)</span><br><span class="line">&gt;&gt;&gt; df</span><br><span class="line">     name        toy       born</span><br><span class="line">1  Batman  Batmobile 1940-04-25</span><br></pre></td></tr></table></figure><h1 id="Fill-missing-values"><a href="#Fill-missing-values" class="headerlink" title="Fill missing values"></a>Fill missing values</h1><p>Filling missing values using fillna(), replace() and interpolate()</p><p>In order to fill null values in a datasets, we use fillna(), replace() and interpolate() function these function replace NaN values with some value of their own. All these function help in filling a null values in datasets of a DataFrame. Interpolate() function is basically used to fill NA values in the dataframe but it uses various interpolation technique to fill the missing values rather than hard-coding the value.</p><p>Code #1: Filling null values with a single value</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling missing value using fillna()  </span><br><span class="line"></span><br><span class="line">df.fillna(0)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>Code #2: Filling null values with the previous ones</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling a missing value with</span><br><span class="line"></span><br><span class="line"># previous ones  </span><br><span class="line"></span><br><span class="line">df.fillna(method =&#x27;pad&#x27;)</span><br></pre></td></tr></table></figure><p>Code #3: Filling null value with the next ones</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"># importing pandas as pd</span><br><span class="line"></span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># importing numpy as np</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># dictionary of lists</span><br><span class="line"></span><br><span class="line">dict = &#123;&#x27;First Score&#x27;:[100, 90, np.nan, 95],</span><br><span class="line"></span><br><span class="line">        &#x27;Second Score&#x27;: [30, 45, 56, np.nan],</span><br><span class="line"></span><br><span class="line">        &#x27;Third Score&#x27;:[np.nan, 40, 80, 98]&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># creating a dataframe from dictionary</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"># filling  null value using fillna() function  </span><br><span class="line"></span><br><span class="line">df.fillna(method =&#x27;bfill&#x27;) </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pandas </tag>
            
            <tag> dataframe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>boost exact phrase search results ranking in elasticsearch</title>
      <link href="2021/06/05/2021-06-05-1/"/>
      <url>2021/06/05/2021-06-05-1/</url>
      
        <content type="html"><![CDATA[<p>Elasticsearch use the DSL format to create query.</p><p>One easy search is to use multi_match by passing the query key word, and give the fields to search for.</p><p>Here is an example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">from elasticsearch import Elasticsearch</span><br><span class="line">es = Elasticsearch()</span><br><span class="line">indexname = &#x27;myindex&#x27;</span><br><span class="line"></span><br><span class="line">keyword = &#x27;test&#x27;</span><br><span class="line">   dsl=&#123;</span><br><span class="line">       &quot;query&quot;: &#123;</span><br><span class="line">           &quot;multi_match&quot; : &#123;</span><br><span class="line">                   &quot;query&quot;:  keyword,</span><br><span class="line">                   &quot;fields&quot;: [ &quot;content&quot;]</span><br><span class="line">               &#125;</span><br><span class="line"></span><br><span class="line">           &#125;</span><br><span class="line"> </span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">   </span><br><span class="line">   dsize=10</span><br><span class="line">   result_r = es.search(index=indexname, body=dsl,size=dsize)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>The problem with the above query is that:if your query keyword is a phrase, you might find many results that have the exact match are ranked lower.</p><p>To solve this problem, you might want to try this new dsl format:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">dsl=&#123;</span><br><span class="line">   </span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">       &quot;bool&quot;: &#123;</span><br><span class="line">         &quot;must&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">                  &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ]</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         ],</span><br><span class="line">         &quot;should&quot;: [</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">                  &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ],</span><br><span class="line">               &quot;type&quot;: &quot;phrase&quot;,</span><br><span class="line">               &quot;boost&quot;: 10</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;,</span><br><span class="line">           &#123;</span><br><span class="line">             &quot;multi_match&quot;: &#123;</span><br><span class="line">               &quot;query&quot;: keyword,</span><br><span class="line">               &quot;fields&quot;: [</span><br><span class="line">               &quot;content1&quot;,&quot;content2&quot;</span><br><span class="line">               ],</span><br><span class="line">               &quot;operator&quot;: &quot;and&quot;,</span><br><span class="line">               &quot;boost&quot;: 4</span><br><span class="line">             &#125;</span><br><span class="line">           &#125;</span><br><span class="line">         ]</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">           </span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>one thing to explain in the above the query, is the ‘operator’; According to the official elasticsearch webpage:</p><p>operator and minimum_should_match<br>The best_fields and most_fields types are field-centric — they generate a match query per field. This means that the operator and minimum_should_match parameters are applied to each field individually, which is probably not what you want.</p><p>Take this query for example:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">GET /_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;multi_match&quot; : &#123;</span><br><span class="line">      &quot;query&quot;:      &quot;Will Smith&quot;,</span><br><span class="line">      &quot;type&quot;:       &quot;best_fields&quot;,</span><br><span class="line">      &quot;fields&quot;:     [ &quot;first_name&quot;, &quot;last_name&quot; ],</span><br><span class="line">      &quot;operator&quot;:   &quot;and&quot; </span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>This query is executed as:</p><p>  (+first_name:will +first_name:smith)<br>| (+last_name:will  +last_name:smith)</p><p>In other words, all terms must be present in a single field for a document to match.</p>]]></content>
      
      
      <categories>
          
          <category> search </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>how to convert timestamp column of pandas dataframe into hour and day features using transformer</title>
      <link href="2021/06/04/2021-06-04-1/"/>
      <url>2021/06/04/2021-06-04-1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">from sklearn.base import BaseEstimator, TransformerMixin</span><br><span class="line">import pandas as pd</span><br><span class="line"></span><br><span class="line">class dayandhour_Transformer(BaseEstimator, TransformerMixin):</span><br><span class="line"></span><br><span class="line">    # Class Constructor</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line"></span><br><span class="line">        print(&#x27;initialized&#x27;)</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">    # Return self, nothing else to do here</span><br><span class="line"></span><br><span class="line">    def fit(self, X, y=None):</span><br><span class="line"></span><br><span class="line">       return self</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    # Customized transformer method</span><br><span class="line"></span><br><span class="line">    def transform(self, X_, y=None):</span><br><span class="line"></span><br><span class="line">        X = X_.copy()</span><br><span class="line"></span><br><span class="line">        X[&#x27;dayofweek&#x27;]=pd.to_datetime(X[&#x27;timestamp&#x27;]).dt.dayofweek</span><br><span class="line"></span><br><span class="line">        X[&#x27;hour&#x27;]=pd.to_datetime(X[&#x27;timestamp&#x27;]).dt.hour</span><br><span class="line"></span><br><span class="line">        X = X.drop(&#x27;timestamp&#x27;,axis=1)</span><br><span class="line"></span><br><span class="line">        return X</span><br><span class="line"></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">dayandhour_transformer = dayandhour_Transformer() </span><br><span class="line"></span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">df = dayandhour_transformer.transform(df)</span><br></pre></td></tr></table></figure><p>Suppose we have a dataframe df with a column “timestamp”.</p><p>before apply the code, we have:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print(df[&#x27;timestamp&#x27;])</span><br><span class="line"></span><br><span class="line">0        2021-03-28 03:28:10.205000</span><br><span class="line"></span><br><span class="line">1        2021-03-28 21:31:43.290000</span><br><span class="line"></span><br><span class="line">2        2021-03-28 21:16:18.771000</span><br><span class="line"></span><br><span class="line">3        2021-03-28 18:39:13.344000</span><br><span class="line"></span><br><span class="line">4        2021-03-28 00:54:57.544000</span><br></pre></td></tr></table></figure><p>after we apply the code, we have:</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">print( df[[&#x27;hour&#x27;,&#x27;dayofweek&#x27;]])</span><br><span class="line"></span><br><span class="line">       hour  dayofweek</span><br><span class="line"></span><br><span class="line">0         3          6</span><br><span class="line"></span><br><span class="line">1        21          6</span><br><span class="line"></span><br><span class="line">2        21          6</span><br><span class="line"></span><br><span class="line">3        18          6</span><br><span class="line"></span><br><span class="line">4         0          6</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> data science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> pandas </tag>
            
            <tag> preprocessing </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
